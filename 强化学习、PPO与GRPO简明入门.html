

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/linxkon_blog.png">
  <link rel="icon" href="/img/linxkon_blog.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="linxkon">
  <meta name="keywords" content="技术分享，项目实战，生活记录">
  
    <meta name="description" content="从InstructGPT (2022.1)到Deepseek R1(2024.4)，从RLHF到GRPO，强化学习在LLM领域愈发重要，本文简单说明了强化学习中的一些常见概念，包括策略函数、优势函数、KL散度惩罚、PPO与GRPO，供大家简单入门 1.策略函数（Policy） 在强化学习中，\(\pi(a_t \mid s_t)\) 表示在状态 \(s_t\) 下采取动作 \(a_t\">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习、PPO与GRPO简明入门">
<meta property="og:url" content="https://linxkon.github.io/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%81PPO%E4%B8%8EGRPO%E7%AE%80%E6%98%8E%E5%85%A5%E9%97%A8.html">
<meta property="og:site_name" content="AI·你所爱">
<meta property="og:description" content="从InstructGPT (2022.1)到Deepseek R1(2024.4)，从RLHF到GRPO，强化学习在LLM领域愈发重要，本文简单说明了强化学习中的一些常见概念，包括策略函数、优势函数、KL散度惩罚、PPO与GRPO，供大家简单入门 1.策略函数（Policy） 在强化学习中，\(\pi(a_t \mid s_t)\) 表示在状态 \(s_t\) 下采取动作 \(a_t\">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://linxkon.github.io/images/index_pic/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.png">
<meta property="article:published_time" content="2025-04-05T02:29:12.000Z">
<meta property="article:modified_time" content="2025-07-22T02:40:41.281Z">
<meta property="article:author" content="linxkon">
<meta property="article:tag" content="笔记整理">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://linxkon.github.io/images/index_pic/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.png">
  
  
  
  <title>强化学习、PPO与GRPO简明入门 - AI·你所爱</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"linxkon.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"enable":true,"app_id":"XLEbEr6BfzRRh34xJtmOEom0-MdYXbMMI","app_key":"3bwflR7evMRYC6JTohHAE31C","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>AI·你所爱 | Linxkon@gmail.com</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/pursenight.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="强化学习、PPO与GRPO简明入门"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-04-05 10:29" pubdate>
          2025年4月5日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.5k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          38 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">强化学习、PPO与GRPO简明入门</h1>
            
            
              <div class="markdown-body">
                
                <span id="more"></span>
<p>从InstructGPT (2022.1)到Deepseek
R1(2024.4)，从RLHF到GRPO，强化学习在LLM领域愈发重要，本文简单说明了强化学习中的一些常见概念，包括策略函数、优势函数、KL散度惩罚、PPO与GRPO，供大家简单入门</p>
<h2 id="策略函数policy">1.策略函数（Policy）</h2>
<p>在强化学习中，<span class="math inline">\(\pi(a_t \mid s_t)\)</span>
表示在状态 <span class="math inline">\(s_t\)</span> 下采取动作 <span
class="math inline">\(a_t\)</span> 的条件概率。具体来说，它是由策略函数
<span class="math inline">\(\pi\)</span> 决定的。</p>
<h3 id="详细说明">详细说明</h3>
<ul>
<li><p><strong><span class="math inline">\(s_t\)</span></strong>:
表示在时间步 <span class="math inline">\(t\)</span> 时的状态（state）。
状态是环境对智能体的当前描述，例如在游戏中可能是角色的位置、速度等信息。</p></li>
<li><p><strong><span class="math inline">\(a_t\)</span></strong>:
表示在时间步 <span class="math inline">\(t\)</span>
时智能体采取的动作（action）。
动作是智能体在给定状态下可以执行的操作，例如在游戏中可能是“向左移动”或“跳跃”。</p></li>
<li><p><strong><span class="math inline">\(\pi(a_t \mid
s_t)\)</span></strong>: 是策略函数（policy），表示在状态 <span
class="math inline">\(s_t\)</span> 下选择动作 <span
class="math inline">\(a_t\)</span> 的概率。 如果是确定性策略，<span
class="math inline">\(\pi(a_t \mid s_t)\)</span>
会直接输出一个确定的动作；如果是随机策略，它会输出一个动作的概率分布。</p></li>
<li><p><strong><span
class="math inline">\(r_t(\theta)\)</span></strong>: <span
class="math display">\[r_t(\theta) = \frac{\pi_\theta(a_t \mid
s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}\]</span> 在 PPO 中，<span
class="math inline">\(r_t(\theta)\)</span> 是新策略 <span
class="math inline">\(\pi_\theta\)</span> 和旧策略 <span
class="math inline">\(\pi_{\theta_{\text{old}}}\)</span> 在状态 <span
class="math inline">\(s_t\)</span> 下选择动作 <span
class="math inline">\(a_t\)</span> 的概率比。
这个比值用于衡量策略更新的幅度，并通过裁剪机制限制其变化范围，确保训练的稳定性。</p></li>
</ul>
<h3 id="举例说明">举例说明</h3>
<p>假设我们有一个简单的游戏环境：</p>
<ul>
<li><strong>状态 <span
class="math inline">\(s_t\)</span></strong>：角色的位置。</li>
<li><strong>动作 <span
class="math inline">\(a_t\)</span></strong>：可以执行的动作是“向左”或“向右”。</li>
<li><strong>策略 <span class="math inline">\(\pi(a_t \mid
s_t)\)</span></strong>：在某个位置 <span
class="math inline">\(s_t\)</span> 下，策略可能以 70%
的概率选择“向左”，以 30% 的概率选择“向右”。</li>
</ul>
<p>在 PPO 中，我们会比较新旧策略在相同状态 <span
class="math inline">\(s_t\)</span> 下选择相同动作 <span
class="math inline">\(a_t\)</span> 的概率，从而计算概率比 <span
class="math inline">\(r_t(\theta)\)</span>，并用于优化目标函数。</p>
<h3 id="小结">小结</h3>
<p><span class="math inline">\(\pi(a_t \mid s_t)\)</span> 表示在状态
<span class="math inline">\(s_t\)</span> 下选择动作 <span
class="math inline">\(a_t\)</span> 的条件概率，由策略函数 <span
class="math inline">\(\pi\)</span> 决定。在 PPO
中，这一概率用于计算新旧策略的比值，从而控制策略更新的幅度。</p>
<h2 id="近端策略优化ppo">2.近端策略优化（PPO）</h2>
<p><strong>PPO（Proximal Policy Optimization）</strong>
是一种用于强化学习的策略优化算法，由 <a
target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=710831409&amp;content_type=Answer&amp;match_order=1&amp;q=OpenAI&amp;zhida_source=entity">OpenAI</a>
提出。它通过限制策略更新的幅度，确保训练过程的稳定性。</p>
<h3 id="核心思想">核心思想</h3>
<p>PPO
的核心在于限制策略更新的幅度，避免因更新过大导致性能下降。它通过引入“裁剪”机制，控制新旧策略之间的差异。</p>
<h3 id="公式">公式</h3>
<p>PPO 的替代目标函数 <span
class="math inline">\(\mathcal{J}_{PPO}(\theta)\)</span> 用于优化策略
<span class="math inline">\(\pi_\theta\)</span>，公式如下：</p>
<p><span class="math display">\[ \mathcal{J}_{PPO}(\theta) =
\mathbb{E}_{[q \sim P(Q), o \sim \pi_{\theta_{old}}(O|q)]} \frac{1}{|o|}
\sum_{t=1}^{|o|} \min \left[ \frac{\pi_\theta(o_{t} | q,
o_{&lt;t})}{\pi_{\theta_{old}}(o_{t} | q, o_{&lt;t})} A_{t}, \text{clip}
\left( \frac{\pi_\theta(o_{t} | q, o_{&lt;t})}{\pi_{\theta_{old}}(o_{t}
| q, o_{&lt;t})}, 1 - \varepsilon, 1 + \varepsilon\right) A_{t} \right]
\]</span></p>
<p>其中：</p>
<p><strong>期望符号 <span class="math inline">\(\mathbb{E}\)</span>
表示对查询 <span class="math inline">\(q\)</span> 和输出 <span
class="math inline">\(o\)</span> 的期望</strong>:</p>
<ul>
<li><span class="math inline">\(q \sim P(Q)\)</span>: 查询 <span
class="math inline">\(q\)</span> 从分布 <span
class="math inline">\(P(Q)\)</span> 中采样。</li>
<li><span class="math inline">\(o \sim \pi_{\theta_{old}}(O|q)\)</span>:
输出 <span class="math inline">\(o\)</span> 由旧策略 <span
class="math inline">\(\pi_{\theta_{old}}\)</span> 生成。</li>
</ul>
<p><strong><span class="math inline">\(\frac{1}{|o|}
\sum_{t=1}^{|o|}\)</span> 对输出 <span class="math inline">\(o\)</span>
的每个时间步 <span class="math inline">\(t\)</span> 求平均</strong>:</p>
<ul>
<li><span class="math inline">\(|o|\)</span> 是输出序列的长度。</li>
</ul>
<p>其核心目标函数为：</p>
<p><span class="math display">\[ L^{CLIP}(\theta) = \mathbb{E}_t \left[
\min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 -
\epsilon, 1 + \epsilon) \hat{A}_t \right) \right] \]</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(r_t(\theta) =
\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\)</span>
是新旧策略的概率比。</li>
<li><span class="math inline">\(\hat{A}_t\)</span>
是优势函数，衡量动作的相对好坏。</li>
<li><span class="math inline">\(\epsilon\)</span> 是裁剪参数，通常为 0.1
或 0.2。</li>
</ul>
<h3 id="步骤">步骤</h3>
<ol type="1">
<li><strong>采样</strong>：使用当前策略与环境交互，收集数据，在语言模型中，可以类比为生成补全（generating
completions）。</li>
<li><strong>计算优势值</strong>：基于收集的数据计算优势值函数 <span
class="math inline">\(\hat{A}_t\)</span>。</li>
<li><strong>优化目标函数</strong>：通过<strong>梯度上升</strong>优化目标函数
<span class="math inline">\(L^{CLIP}(\theta)\)</span>。</li>
<li><strong>更新策略</strong>：重复上述步骤，直到策略收敛。</li>
</ol>
<h3 id="优点">优点</h3>
<ul>
<li><strong>稳定性</strong>：通过裁剪机制，避免策略更新过大。</li>
<li><strong>高效性</strong>：相比 TRPO，PPO
实现更简单，计算效率更高。</li>
</ul>
<h3 id="补充">补充</h3>
<p>在强化学习中，策略的目标是最大化期望回报，而不是最小化损失。所以，在PPO中使用的是梯度上升，原因在于它的优化目标是最大化目标函数（如强化学习中的期望回报），而不是最小化损失函数（如分类或回归问题）。</p>
<hr />
<h2 id="advantage优势函数">3.Advantage（优势函数）</h2>
<h3 id="定义">定义</h3>
<p>Advantage函数用于衡量在某个状态（State）下，采取某个动作（Action）相对于平均表现的优劣程度。它的数学定义为：
<span class="math inline">\(A(s, a) = Q(s, a) - V(s)\)</span>,
其中：</p>
<ul>
<li><span class="math inline">\(Q(s, a)\)</span>
是<strong>动作值函数</strong>，表示在状态 <span
class="math inline">\(s\)</span> 下采取动作 <span
class="math inline">\(a\)</span> 后，未来累积回报的期望。</li>
<li><span class="math inline">\(V(s)\)</span>
是<strong>状态值函数</strong>，表示在状态 <span
class="math inline">\(s\)</span>
下，按照当前策略采取动作后，未来累积回报的期望。</li>
<li><span class="math inline">\(A(s, a)\)</span>
是<strong>优势函数</strong>，表示在状态 <span
class="math inline">\(s\)</span> 下采取动作 <span
class="math inline">\(a\)</span> 比平均表现好多少（或差多少）。</li>
</ul>
<h3 id="作用">作用</h3>
<ul>
<li>Advantage函数用于指导策略更新：
<ul>
<li>如果 <span class="math inline">\(A(s, a) &gt; 0\)</span>，说明动作
<span class="math inline">\(a\)</span>
比平均表现更好，策略应该更倾向于选择这个动作；</li>
<li>如果 <span class="math inline">\(A(s, a) &lt; 0\)</span>，说明动作
<span class="math inline">\(a\)</span>
比平均表现更差，策略应该减少选择这个动作的概率。</li>
</ul></li>
<li>在PPO等算法中，Advantage函数通常通过<strong>GAE（Generalized
Advantage Estimation）</strong>来估计。</li>
</ul>
<h3 id="直观理解">直观理解</h3>
<p>Advantage函数就像一个“评分”，告诉模型某个动作在当前状态下是好还是坏，以及好（或坏）的程度。</p>
<hr />
<h2 id="kl-penaltykl散度惩罚">4.KL Penalty（KL散度惩罚）</h2>
<h3 id="定义-1">定义</h3>
<p>KL Penalty是基于<strong>KL散度（Kullback-Leibler
Divergence）</strong>的一种正则化手段。KL散度用于衡量两个概率分布之间的差异。在强化学习中，KL
Penalty通常用于限制当前策略 <span
class="math inline">\(\pi_{\theta}\)</span> 和参考策略 <span
class="math inline">\(\pi_{\text{ref}}\)</span>
之间的差异。其数学定义为： <span class="math inline">\(\text{KL Penalty}
= D_{\text{KL}}(\pi_{\text{ref}} \| \pi_{\theta})\)</span> 其中：</p>
<ul>
<li><span class="math inline">\(\pi_{\theta}\)</span>
是当前策略（由模型参数 <span class="math inline">\(\theta\)</span>
决定）。</li>
<li><span class="math inline">\(\pi_{\text{ref}}\)</span>
是参考策略（通常是更新前的策略或某个基线策略）。</li>
<li><span class="math inline">\(D_{\text{KL}}\)</span>
是KL散度，用于衡量两个策略之间的差异。</li>
</ul>
<h3 id="作用-1">作用</h3>
<ul>
<li>KL
Penalty用于防止策略更新过大，确保当前策略不会偏离参考策略太远。这样可以避免训练过程中的不稳定现象（如策略崩溃）。</li>
<li>在PPO等算法中，KL Penalty通常被添加到目标函数中，作为正则化项。</li>
</ul>
<h3 id="直观理解-1">直观理解</h3>
<p>KL
Penalty就像一个“约束”，告诉模型在更新策略时不要“步子迈得太大”，以免失去稳定性。</p>
<h2 id="advantage和kl-penalty的关系">5.Advantage和KL Penalty的关系</h2>
<ul>
<li><p><strong>Advantage</strong>
用于指导策略更新，告诉模型哪些动作更好。</p></li>
<li><p><strong>KL Penalty</strong>
用于约束策略更新，防止策略变化过大。</p></li>
<li><p>在PPO等算法中，Advantage和KL
Penalty共同作用，既鼓励模型选择更好的动作，又确保更新过程稳定可靠。</p></li>
</ul>
<h3 id="举例说明-1">举例说明</h3>
<p>假设我们训练一个机器人走迷宫：</p>
<ul>
<li><p><strong>Advantage</strong>：机器人发现“向右转”比“向左转”更容易找到出口，于是Advantage函数会给“向右转”一个正的值，鼓励策略更倾向于选择“向右转”。</p></li>
<li><p><strong>KL
Penalty</strong>：为了防止策略突然变得只选择“向右转”而忽略其他可能性，KL
Penalty会限制策略的变化幅度，确保策略更新是平滑的。</p></li>
</ul>
<h3 id="总结">总结</h3>
<ul>
<li><p><strong>Advantage（优势函数）</strong>：衡量某个动作比平均表现好多少，用于指导策略更新。</p></li>
<li><p><strong>KL
Penalty（KL散度惩罚）</strong>：限制策略更新的幅度，确保训练过程的稳定性。</p></li>
</ul>
<hr />
<h2 id="群体相对策略优化grpo">6.群体相对策略优化（GRPO）</h2>
<p>GRPO 是一种在线学习算法（online learning
algorithm），这意味着它通过使用训练过程中由训练模型自身生成的数据来迭代改进。GRPO
的目标直觉是最大化生成补全（completions）的优势函数（advantage），同时确保模型保持在参考策略（reference
policy）附近。</p>
<p>其目标函数为： <span class="math display">\[ J_{\text{GRPO}}(\theta)
= \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\text{old}}(O|q)}
\left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|}
\left( r_{i,t}(\theta) \hat{A}_{i,t} - \beta D_{\text{KL}}(\pi_\theta ||
\pi_{\text{ref}}) \right) \right] \]</span></p>
<p>为了理解 GRPO 的工作原理，可以将其分解为四个主要步骤：</p>
<ol type="1">
<li><p>生成补全（Generating completions）</p></li>
<li><p>计算优势值（Computing the advantage）</p></li>
<li><p>估计KL散度（Estimating the KL divergence）</p></li>
<li><p>计算损失（Computing the loss）</p></li>
</ol>
<h3 id="生成补全generating-completions">1. 生成补全（Generating
completions）</h3>
<p>在每一个训练步骤中，我们从提示（prompts）中采样一个批次（batch），并为每个提示生成一组
<span class="math inline">\(G\)</span> 个补全（completions）（记为 <span
class="math inline">\(o_i\)</span>）。</p>
<h3 id="计算优势值computing-the-advantage">2. 计算优势值（Computing the
advantage）</h3>
<p>对于每一个 <span class="math inline">\(G\)</span>
序列，使用奖励模型（reward
model）计算其奖励（reward）。为了与奖励模型的比较性质保持一致——通常奖励模型是基于同一问题的输出之间的比较数据集进行训练的——优势的计算反映了这些相对比较。其归一化公式如下：</p>
<p><span class="math display">\[ \hat{A}_{i,t} = \frac{r_i -
\text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})} \]</span></p>
<p>这种方法赋予了该方法其名称：<strong>群体相对策略优化（Group Relative
Policy Optimization, GRPO）</strong></p>
<p>GRPO通过优化PPO算法，解决了计算优势值时需要同时依赖奖励模型（reward
model）和价值模型（value model）的问题，成功移除了value
model（价值模型），显著降低了推理时的内存占用和时间开销。<strong>Advantage（优势值）</strong>的核心价值在于为模型输出提供更精准的评估，不仅衡量答案的绝对质量，还通过相对比较（与其他回答的对比）来更全面地定位其优劣。</p>
<h3 id="估计kl散度estimating-the-kl-divergence">3.
估计KL散度（Estimating the KL divergence）</h3>
<p>在实际算法实现中，直接计算KL散度可能会面临一些挑战：</p>
<ul>
<li><strong>计算复杂度高</strong>：KL散度的定义涉及对两个概率分布的对数比值的期望计算。对于复杂的策略分布，直接计算KL散度可能需要大量的计算资源；</li>
<li><strong>数值稳定性</strong>：在实际计算中，直接计算KL散度可能会遇到数值不稳定的问题，尤其是当两个策略的概率分布非常接近时，对数比值可能会趋近于零或无穷大。近似器可以通过引入一些数值稳定性的技巧（如截断或平滑）来避免这些问题；</li>
<li><strong>在线学习</strong>：在强化学习中，策略通常需要在每一步或每几步更新一次。如果每次更新都需要精确计算KL散度，可能会导致训练过程变得非常缓慢。近似器可以快速估计KL散度，从而支持在线学习和实时更新。</li>
</ul>
<p>[Schulman et al. (2020)]
提出的近似器可以根据当前策略和参考策略的差异动态调整估计的精度，从而在保证计算效率的同时，尽可能减少估计误差，其定义如下：</p>
<p><span class="math display">\[ \mathbb{D}_{\text{KL}}\left[\pi_\theta
\|\pi_{\text{ref}}\right] = \frac{\pi_{\text{ref}}(o_{i,t} \mid q,
o_{i,&lt;t})}{\pi_\theta(o_{i,t} \mid q, o_{i,&lt;t})} - \log
\frac{\pi_{\text{ref}}(o_{i,t} \mid q, o_{i,&lt;t})}{\pi_\theta(o_{i,t}
\mid q, o_{i,&lt;t})} - 1 \]</span></p>
<p>这个近似器的核心思想是通过对当前策略和参考策略的概率比值的简单变换来估计KL散度。具体来说：</p>
<ul>
<li><strong>第一项</strong>：<span
class="math inline">\(\frac{\pi_{\text{ref}}(o_{i,t} \mid q,
o_{i,&lt;t})}{\pi_\theta(o_{i,t} \mid q, o_{i,&lt;t})}\)</span>
是参考策略与当前策略的概率比值。</li>
<li><strong>第二项</strong>：<span class="math inline">\(\log
\frac{\pi_{\text{ref}}(o_{i,t} \mid q, o_{i,&lt;t})}{\pi_\theta(o_{i,t}
\mid q, o_{i,&lt;t})}\)</span> 是对数概率比值。</li>
<li><strong>第三项</strong>：<span class="math inline">\(-1\)</span>
是一个常数项，用于调整近似器的偏差。</li>
</ul>
<p>这个近似器的优势在于它只需要计算当前策略和参考策略的概率比值，而不需要直接计算KL散度的积分或期望。因此，它可以在保证一定精度的同时，显著降低计算复杂度。</p>
<p><strong>近似器的直观理解</strong></p>
<p>这个近似器的设计灵感来自于泰勒展开。KL散度可以看作是两个分布之间的某种“距离”，而这个近似器通过一阶或二阶近似来估计这个距离。具体来说：</p>
<ul>
<li>当 <span class="math inline">\(\pi_\theta\)</span> 和 <span
class="math inline">\(\pi_{\text{ref}}\)</span> 非常接近时，<span
class="math inline">\(\frac{\pi_{\text{ref}}}{\pi_\theta} \approx
1\)</span>，此时 <span class="math inline">\(\log
\frac{\pi_{\text{ref}}}{\pi_\theta} \approx
0\)</span>，近似器的值趋近于零，符合KL散度的性质。</li>
<li>当 <span class="math inline">\(\pi_\theta\)</span> 和 <span
class="math inline">\(\pi_{\text{ref}}\)</span>
差异较大时，近似器会给出一个较大的正值，反映出两个分布之间的差异。</li>
</ul>
<h3 id="计算损失computing-the-loss">4. 计算损失（Computing the
loss）</h3>
<p>这一步的目标是最大化优势，同时确保模型保持在参考策略附近。因此，损失定义如下：</p>
<p><span class="math display">\[ \mathcal{L}_{\text{GRPO}}(\theta) =
-\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left[
\frac{\pi_\theta(o_{i,t} \mid q, o_{i,&lt; t})}{\left[\pi_\theta(o_{i,t}
\mid q, o_{i,&lt; t})\right]_{\text{no grad}}} \hat{A}_{i,t} - \beta
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]
\right] \]</span></p>
<p>其中第一项表示缩放后的优势，第二项通过KL散度惩罚与参考策略的偏离。</p>
<p>在原始论文中，该公式被推广为在每次生成后通过利用<strong>裁剪替代目标（clipped
surrogate objective）</strong>进行多次更新：</p>
<p><span class="math display">\[ \mathcal{L}_{\text{GRPO}}(\theta) = -
\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left[ \min
\left( \frac{\pi_\theta(o_{i,t} \mid q, o_{i,&lt;
t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i,&lt; t})}
\hat{A}_{i,t}, \, \text{clip}\left( \frac{\pi_\theta(o_{i,t} \mid q,
o_{i,&lt; t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i,&lt; t})},
1 - \epsilon, 1 + \epsilon \right) \hat{A}_{i,t} \right) - \beta
\mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]
\right] \]</span></p>
<p>其中 <span class="math inline">\(\text{clip}(\cdot, 1 - \epsilon, 1 +
\epsilon)\)</span> 通过将策略比率限制在 <span class="math inline">\(1 -
\epsilon\)</span> 和 <span class="math inline">\(1 + \epsilon\)</span>
之间，确保更新不会过度偏离参考策略。</p>
<p>在很多代码实现，比如Huggingface的TRL中，与原始论文一样每次生成只进行一次更新，因此可以将损失简化为第一种形式。</p>
<h3 id="总结-1">总结</h3>
<p>GRPO通过优化PPO算法，移除了价值模型，降低了计算开销，同时利用群体相对优势函数和KL散度惩罚，确保策略更新既高效又稳定。
GRPO和PPO的核心区别有两点： - 到底是用critic model拟合出base
(期望)，还是用采样simulate出base。 - action到底是token-level 还是
solution-level的。
GRPO实际上是PPO的极端简化版本（类似思路的的还有rloo），之所以work本质上还是因为NLP任务reward的稀疏性，游戏任务则很不适合。</p>
<h2 id="代码示例">7.代码示例</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset,Dataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM<br><span class="hljs-keyword">import</span> trl<br><span class="hljs-keyword">from</span> trl <span class="hljs-keyword">import</span> GRPOConfig, GRPOTrainer<br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig, get_peft_model, TaskType<br><span class="hljs-keyword">import</span> nltk<br>SYSTEM_PROMPT = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">按照如下格式生成：</span><br><span class="hljs-string">&lt;think&gt;</span><br><span class="hljs-string">...</span><br><span class="hljs-string">&lt;/think&gt;</span><br><span class="hljs-string">&lt;answer&gt;</span><br><span class="hljs-string">...</span><br><span class="hljs-string">&lt;/answer&gt;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_data</span>(<span class="hljs-params">data</span>):<br>    data = data.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: &#123;<br>        <span class="hljs-string">&#x27;prompt&#x27;</span>: [<br>            &#123;<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;system&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: SYSTEM_PROMPT&#125;,<br>            &#123;<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: x[<span class="hljs-string">&#x27;question&#x27;</span>]&#125;<br>        ],<br>        <span class="hljs-string">&#x27;think&#x27;</span>: x[<span class="hljs-string">&#x27;answer&#x27;</span>],<br>        <span class="hljs-string">&#x27;answer&#x27;</span>: x[<span class="hljs-string">&#x27;answer_only&#x27;</span>]<br>    &#125;) <br>    <span class="hljs-keyword">return</span> data<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">extract_think</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;&lt;think&gt;&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> text <span class="hljs-keyword">or</span> <span class="hljs-string">&quot;&lt;/think&gt;&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> text:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span><br>    think = text.split(<span class="hljs-string">&quot;&lt;think&gt;&quot;</span>)[-<span class="hljs-number">1</span>]<br>    think = think.split(<span class="hljs-string">&quot;&lt;/think&gt;&quot;</span>)[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">return</span> think.strip()<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">extract_answer</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;&lt;answer&gt;&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> text <span class="hljs-keyword">or</span> <span class="hljs-string">&quot;&lt;/answer&gt;&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> text:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span><br>    answer = text.split(<span class="hljs-string">&quot;&lt;answer&gt;&quot;</span>)[-<span class="hljs-number">1</span>]<br>    answer = answer.split(<span class="hljs-string">&quot;&lt;/answer&gt;&quot;</span>)[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">return</span> answer.strip()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mark_num</span>(<span class="hljs-params">text</span>):<br>    reward = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">if</span> text.count(<span class="hljs-string">&quot;&lt;think&gt;\n&quot;</span>) == <span class="hljs-number">1</span>:<br>        reward += <span class="hljs-number">0.125</span><br>        <br>    <span class="hljs-keyword">if</span> text.count(<span class="hljs-string">&quot;&lt;/think&gt;\n&quot;</span>) == <span class="hljs-number">1</span>:<br>        reward += <span class="hljs-number">0.125</span><br>        <br>    <span class="hljs-keyword">if</span> text.count(<span class="hljs-string">&quot;&lt;answer&gt;\n&quot;</span>) == <span class="hljs-number">1</span>:<br>        reward += <span class="hljs-number">0.125</span><br>        <br>    <span class="hljs-keyword">if</span> text.count(<span class="hljs-string">&quot;&lt;/answer&gt;\n&quot;</span>) == <span class="hljs-number">1</span>:<br>        reward += <span class="hljs-number">0.125</span><br>    <span class="hljs-keyword">return</span> reward<br><span class="hljs-comment"># 生成答案是否正确的奖励</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">correctness_reward</span>(<span class="hljs-params">prompts, completions, answer, **kwargs</span>):<br>    responses = [completion[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;content&#x27;</span>] <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]<br>    extracted_responses = [extract_answer(r) <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> responses]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;问题:\n<span class="hljs-subst">&#123;prompts[<span class="hljs-number">0</span>][-<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;content&#x27;</span>]&#125;</span>&quot;</span>, <span class="hljs-string">f&quot;\n答案:\n<span class="hljs-subst">&#123;answer[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>, <span class="hljs-string">f&quot;\n模型输出:\n<span class="hljs-subst">&#123;responses[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>, <span class="hljs-string">f&quot;\n提取后的答案:\n<span class="hljs-subst">&#123;extracted_responses[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> [<span class="hljs-number">2.0</span> <span class="hljs-keyword">if</span> response == <span class="hljs-built_in">str</span>(ans) <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span> <span class="hljs-keyword">for</span> response, ans <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(extracted_responses, answer)]<br><span class="hljs-comment"># 生成思考是否正确的奖励</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">think_reward</span>(<span class="hljs-params">prompts, completions, think, **kwargs</span>):<br>    responses = [completion[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;content&#x27;</span>] <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]<br>    extracted_thinks = [extract_think(r) <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> responses]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;问题:\n<span class="hljs-subst">&#123;prompts[<span class="hljs-number">0</span>][-<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;content&#x27;</span>]&#125;</span>&quot;</span>, <span class="hljs-string">f&quot;\n思考:\n<span class="hljs-subst">&#123;think[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>, <span class="hljs-string">f&quot;\n模型输出:\n<span class="hljs-subst">&#123;responses[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>, <span class="hljs-string">f&quot;\n提取后的思考:\n<span class="hljs-subst">&#123;extracted_thinks[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>([ nltk.translate.bleu_score.sentence_bleu(response_thk.split(<span class="hljs-string">&#x27; &#x27;</span>), thk.split(<span class="hljs-string">&#x27; &#x27;</span>), weights=(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>)) <span class="hljs-keyword">for</span> response_thk, thk <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(extracted_thinks, think)])<br>    <span class="hljs-keyword">return</span> [ nltk.translate.bleu_score.sentence_bleu(response_thk.split(<span class="hljs-string">&#x27; &#x27;</span>), thk.split(<span class="hljs-string">&#x27; &#x27;</span>), weights=(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>)) <span class="hljs-keyword">for</span> response_thk, thk <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(extracted_thinks, think)]<br><br><span class="hljs-comment"># 生成答案是否是数字的奖励（单纯依赖结果是否正确进行奖励，条件很苛刻，会导致奖励比较稀疏，模型难以收敛，所以加上答案是否是数字的奖励，虽然答案错误，但是至少生成的是数字（对于数学问题），也要给予适当奖励）</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">digit_reward</span>(<span class="hljs-params">completions, **kwargs</span>):<br>    responses = [completion[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;content&#x27;</span>] <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]<br>    extracted_responses = [extract_answer(r) <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> responses]<br>    <span class="hljs-keyword">return</span> [<span class="hljs-number">0.5</span> <span class="hljs-keyword">if</span> response.isdigit() <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span> <span class="hljs-keyword">for</span> response <span class="hljs-keyword">in</span> extracted_responses]<br><span class="hljs-comment"># 格式奖励</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">hard_format_reward</span>(<span class="hljs-params">completions, **kwargs</span>):<br>    pattern = <span class="hljs-string">r&quot;&lt;think&gt;\n.*?\n&lt;/think&gt;\n&lt;answer&gt;\n.*?\n&lt;/answer&gt;&quot;</span><br>    responses = [completion[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;content&quot;</span>] <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]<br>    matches = [re.<span class="hljs-keyword">match</span>(pattern, response) <span class="hljs-keyword">for</span> response <span class="hljs-keyword">in</span> responses]<br>    <span class="hljs-keyword">return</span> [<span class="hljs-number">0.5</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">match</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span> <span class="hljs-keyword">for</span> <span class="hljs-keyword">match</span> <span class="hljs-keyword">in</span> matches]<br><span class="hljs-comment"># 格式奖励</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">soft_format_reward</span>(<span class="hljs-params">completions, **kwargs</span>):<br>    pattern = <span class="hljs-string">r&quot;&lt;think&gt;.*?&lt;/think&gt;\s*&lt;answer&gt;.*?&lt;/answer&gt;&quot;</span><br>    responses = [completion[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;content&quot;</span>] <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]<br>    matches = [re.<span class="hljs-keyword">match</span>(pattern, response) <span class="hljs-keyword">for</span> response <span class="hljs-keyword">in</span> responses]<br>    <span class="hljs-keyword">return</span> [<span class="hljs-number">0.5</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">match</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span> <span class="hljs-keyword">for</span> <span class="hljs-keyword">match</span> <span class="hljs-keyword">in</span> matches]<br><span class="hljs-comment"># 标记奖励（改善格式奖励稀疏问题）</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mark_reward</span>(<span class="hljs-params">completions, **kwargs</span>):<br>    responses = [completion[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;content&quot;</span>] <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]<br>    <span class="hljs-keyword">return</span> [mark_num(response) <span class="hljs-keyword">for</span> response <span class="hljs-keyword">in</span> responses]<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    model_name = <span class="hljs-string">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span><br>    model = AutoModelForCausalLM.from_pretrained(model_name)<br>    <span class="hljs-comment"># 如果使用lora方法训练，取消如下注释</span><br>    <span class="hljs-comment"># lora_config = LoraConfig(</span><br>    <span class="hljs-comment"># r=8,  </span><br>    <span class="hljs-comment"># lora_alpha=256,  </span><br>    <span class="hljs-comment"># target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;, &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],</span><br>    <span class="hljs-comment"># lora_dropout=0.1, </span><br>    <span class="hljs-comment"># task_type=TaskType.CAUSAL_LM)</span><br>    <span class="hljs-comment"># # 使用lora方法训练</span><br>    <span class="hljs-comment"># model = get_peft_model(model, lora_config)</span><br>    model.cuda()<br>    <br>    tokenizer = AutoTokenizer.from_pretrained(model_name)<br>    <br>    ds = load_dataset(<span class="hljs-string">&#x27;swulling/gsm8k_chinese&#x27;</span>)<br>    data = process_data(ds[<span class="hljs-string">&#x27;train&#x27;</span>])<br>    <br>    output_dir=<span class="hljs-string">&quot;output&quot;</span><br>    training_args = GRPOConfig(<br>        output_dir=output_dir,<br>        learning_rate=<span class="hljs-number">5e-6</span>,<br>        adam_beta1 = <span class="hljs-number">0.9</span>,<br>        adam_beta2 = <span class="hljs-number">0.99</span>,<br>        weight_decay = <span class="hljs-number">0.1</span>,<br>        warmup_ratio = <span class="hljs-number">0.1</span>,<br>        lr_scheduler_type=<span class="hljs-string">&#x27;cosine&#x27;</span>,<br>        logging_steps=<span class="hljs-number">1</span>,<br>        bf16=<span class="hljs-literal">True</span>,<br>        per_device_train_batch_size=<span class="hljs-number">1</span>,<br>        gradient_accumulation_steps=<span class="hljs-number">4</span>,<br>        num_generations=<span class="hljs-number">8</span>,<br>        max_prompt_length=<span class="hljs-number">256</span>,<br>        max_completion_length=<span class="hljs-number">200</span>,<br>        num_train_epochs=<span class="hljs-number">1</span>,<br>        save_steps=<span class="hljs-number">100</span>,<br>        max_grad_norm=<span class="hljs-number">0.1</span>,<br>        log_on_each_node=<span class="hljs-literal">False</span>,<br>        use_vllm=<span class="hljs-literal">False</span>,<br>        report_to=<span class="hljs-string">&quot;tensorboard&quot;</span><br>    )<br>    <br>    trainer = GRPOTrainer(<br>    model=model,<br>    processing_class=tokenizer,<br>    reward_funcs=[<br>        mark_reward,<br>        soft_format_reward,<br>        hard_format_reward,<br>        digit_reward,<br>        correctness_reward,<br>        think_reward<br>        ],<br>    args=training_args,<br>    train_dataset=data,<br>    )<br>    trainer.train()<br>    trainer.save_model(output_dir)<br></code></pre></td></tr></table></figure>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="category-chain-item">强化学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/" class="print-no-link">#笔记整理</a>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
        <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="print-no-link">#强化学习</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>强化学习、PPO与GRPO简明入门</div>
      <div>https://linxkon.github.io/强化学习、PPO与GRPO简明入门.html</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>linxkon</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年4月5日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/RL%E6%95%B0%E6%8D%AE%E5%90%88%E6%88%90%E6%A1%86%E6%9E%B6--SyntheticDataRL.html" title="RL数据合成框架--Synthetic Data RL">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">RL数据合成框架--Synthetic Data RL</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/%E5%9F%BA%E4%BA%8Epython%E7%9A%84MCP%E4%BD%BF%E7%94%A8%E7%AE%80%E5%8D%95%E6%95%99%E7%A8%8B.html" title="基于python的MCP使用简单教程">
                        <span class="hidden-mobile">基于python的MCP使用简单教程</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://lib.baomitu.com/gitalk/1.8.0/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"Ov23licg1p15oAGiQtDC","clientSecret":"d6ca3873752e3a6eb2d21a98b92a3021fd462cbf","repo":"Waline","owner":"linxkon","admin":["linxkon"],"language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: 'be3757c954d2c16ea2c16d53b2764b9d'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量
        <span id="leancloud-site-pv"></span>
        次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        访客量
        <span id="leancloud-site-uv"></span>
        次
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js">
</script>
</body>
</html>
