

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/linxkon_blog.png">
  <link rel="icon" href="/img/linxkon_blog.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="linxkon">
  <meta name="keywords" content="技术分享，项目实战，生活记录">
  
    <meta name="description" content="cdd602f68a7c5a1414d69d136a39ac8d  多模态AI系统很可能会在我们的日常生活中无处不在。将这些系统具身化为物理和虚拟环境中的代理是一种有前途的方式，以使其更加互动化。目前，这些系统利用现有的基础模型作为构建具身代理的基本构件。将代理嵌入这样的环境中，有助于模型处理和解释视觉和上下文数据的能力，这是创建更复杂且具备上下文感知的AI系统的关键。例如，一个能够感">
<meta property="og:type" content="article">
<meta property="og:title" content="AI Agent：多模态交互前沿调查-李飞飞团队">
<meta property="og:url" content="https://linxkon.github.io/AI_Agent%E5%85%A8%E6%99%AF%E8%B0%83%E7%A0%94.html">
<meta property="og:site_name" content="AI·你所爱">
<meta property="og:description" content="cdd602f68a7c5a1414d69d136a39ac8d  多模态AI系统很可能会在我们的日常生活中无处不在。将这些系统具身化为物理和虚拟环境中的代理是一种有前途的方式，以使其更加互动化。目前，这些系统利用现有的基础模型作为构建具身代理的基本构件。将代理嵌入这样的环境中，有助于模型处理和解释视觉和上下文数据的能力，这是创建更复杂且具备上下文感知的AI系统的关键。例如，一个能够感">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://linxkon.github.io/images/index_pic/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD.png">
<meta property="article:published_time" content="2024-12-08T02:21:13.000Z">
<meta property="article:modified_time" content="2024-12-08T06:21:06.126Z">
<meta property="article:author" content="linxkon">
<meta property="article:tag" content="AI agent">
<meta property="article:tag" content="论文">
<meta property="article:tag" content="前沿">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://linxkon.github.io/images/index_pic/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD.png">
  
  
  
  <title>AI Agent：多模态交互前沿调查-李飞飞团队 - AI·你所爱</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"linxkon.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"enable":true,"app_id":"XLEbEr6BfzRRh34xJtmOEom0-MdYXbMMI","app_key":"3bwflR7evMRYC6JTohHAE31C","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>AI·你所爱 | Linxkon@gmail.com</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/pursenight.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="AI Agent：多模态交互前沿调查-李飞飞团队"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-12-08 10:21" pubdate>
          2024年12月8日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          40k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          335 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">AI Agent：多模态交互前沿调查-李飞飞团队</h1>
            
            
              <div class="markdown-body">
                
                <figure>
<img src="/images/ai_agent调研/cdd602f68a7c5a1414d69d136a39ac8d.png" srcset="/img/loading.gif" lazyload
alt="cdd602f68a7c5a1414d69d136a39ac8d" />
<figcaption
aria-hidden="true">cdd602f68a7c5a1414d69d136a39ac8d</figcaption>
</figure>
<p>多模态AI系统很可能会在我们的日常生活中无处不在。将这些系统具身化为物理和虚拟环境中的代理是一种有前途的方式，以使其更加互动化。目前，这些系统利用现有的基础模型作为构建具身代理的基本构件。将代理嵌入这样的环境中，有助于模型处理和解释视觉和上下文数据的能力，这是创建更复杂且具备上下文感知的AI系统的关键。例如，一个能够感知用户动作、人类行为、环境对象、音频表达以及场景整体情绪的系统，可以用于指导代理在特定环境中的响应行为。</p>
<figure>
<img src="/images/ai_agent调研/ff14555eb2ee049dfccfe2838baa496f.png" srcset="/img/loading.gif" lazyload
alt="ff14555eb2ee049dfccfe2838baa496f" />
<figcaption
aria-hidden="true">ff14555eb2ee049dfccfe2838baa496f</figcaption>
</figure>
<p>为了加速基于代理的多模态智能研究，我们将“Agent
AI”定义为一类能够感知视觉刺激、语言输入和其他与环境相关的数据，并能够生成有意义的具身动作的交互系统。特别是，我们探讨了通过整合外部知识、多感官输入和人类反馈，提升代理基于下一步具身动作预测的系统。我们认为，通过在有依据的环境中开发具身AI系统，可以减轻大型基础模型产生的“幻觉”以及生成不符合环境的输出的倾向。新兴的Agent
AI领域涵盖了多模态交互中更广泛的具身和代理层面。除了物理世界中的代理行动和交互之外，我们还设想一个未来，人们可以轻松创建任何虚拟现实或模拟场景，并与其中具身的代理互动。</p>
<h1 id="引言"><strong>1 引言</strong></h1>
<h2 id="动机"><strong>1.1 动机</strong></h2>
<p>历史上，AI 系统在 1956
年达特茅斯会议上被定义为能够从环境中收集信息并以有用的方式与之互动的人工生命体。受到这一定义的启发，明斯基（Minsky）在
MIT 团队于 1970 年开发了一种机器人系统，称为“复制演示”（Copy
Demo），该系统能够观察“积木世界”场景并成功重建观察到的多面体积木结构。该系统包含了观察、规划和操作模块，揭示了这些子问题的高度挑战性，表明还需要进一步的研究。</p>
<p>AI
领域逐渐分化为多个专业化的子领域，这些子领域在解决各种问题方面独立取得了巨大进展，但过度简化模糊了
AI 研究的总体目标。</p>
<p>为了超越现状，有必要回归由亚里士多德整体论驱动的 AI
基础。幸运的是，近期大语言模型（LLM）和视觉语言模型（VLM）的革命，使得创建符合整体理想的新型
AI
代理成为可能。抓住这一机会，<strong>本文探讨了整合语言能力、视觉认知、上下文记忆、直觉推理和适应性的模型，并探讨使用
LLM 和 VLM
完成这种整体合成的可能性。</strong>在探索中，我们还重新审视了基于亚里士多德的“最终原因”的系统设计，即“系统存在的目的”，这一点在以往的
AI 开发中可能被忽视。</p>
<p>随着强大预训练的 LLM 和 VLM
的出现，自然语言处理和计算机视觉领域迎来了复兴。LLM
现在展现出解读现实世界语言数据细微差别的强大能力，往往达到甚至超越人类专业水平（OpenAI，2023）。最近，研究人员表明，LLM
可以在各种环境中扩展为代理，当与领域特定的知识和模块结合时，可以执行复杂的操作和任务（Xi
等人，2023）。这些情境通过复杂推理、对代理角色及其环境的理解，以及多步骤的规划，测试了代理在其环境约束下做出细致决策的能力（Wu
等人，2023；Meta 基础 AI 研究（FAIR）外交团队，2022）。</p>
<p>基于这些初步努力，AI
社区正处于重要的范式转变的前沿，即从创建用于被动、结构化任务的 AI
模型，转变为能够在多样和复杂环境中承担动态代理角色的模型。在这一背景下，本文探讨了将
LLM 和 VLM
用作代理的巨大潜力，特别强调了具备语言能力、视觉认知、上下文记忆、直觉推理和适应性结合的模型。将
LLM 和 VLM 用作代理，特别是在游戏、机器人和医疗等领域，不仅提供了最先进
AI 系统的严格评估平台，还预示了代理中心 AI
将在社会和行业中带来的变革性影响。当这些代理模型被充分利用时，可以重新定义人类体验并提升操作标准。这些模型带来的广泛自动化潜力预示着行业和社会经济动态的巨大转变。这些进步将与多方面的挑战交织在一起，不仅是技术的，还有伦理的挑战，我们将在第
11 节中详细阐述。我们还深入探讨了 Agent AI 各子领域的重叠区域，并在图 1
中展示了它们的相互关联。</p>
<h2 id="背景"><strong>1.2 背景</strong></h2>
<p>接下来，我们将介绍支持 Agent AI
概念、理论背景和现代实现的相关研究论文。</p>
<p><strong>大规模基础模型：</strong>大语言模型（LLM）和视觉语言模型（VLM）正在推动开发通用智能机器的努力（Bubeck
等，2023；Mirchandani
等，2023）。尽管这些模型是通过大量文本语料库进行训练的，但它们卓越的问题解决能力不仅限于传统的语言处理领域。LLM
有潜力应对此前被认为是人类专家或特定领域算法专属的复杂任务，从数学推理（Imani
等，2023；Wei 等，2022；Zhu 等，2022）到专业法律问题的解答（Blair-Stanek
等，2023；Choi 等，2023；Nay，2022）。最近的研究表明，LLM
可以用于为机器人和游戏 AI 生成复杂的计划（Liang 等，2022；Wang
等，2023a,b；Yao 等，2023a；Huang 等，2023a），这是 LLM
作为通用智能代理的重要里程碑。</p>
<p><strong>具身
AI：</strong>许多研究利用大语言模型（LLM）进行任务规划（Huang
等，2022a；Wang 等，2023b；Yao 等，2023a；Li 等，2023a），尤其是 LLM
的大规模领域知识和零样本的具身能力，以执行复杂的任务规划和推理。最新的机器人研究也采用
LLM 进行任务规划（Ahn 等，2022a；Huang 等，2022b；Liang
等，2022），通过将自然语言指令分解为子任务序列（可以是自然语言形式或
Python
代码），然后使用低层控制器来执行这些子任务。此外，它们还结合环境反馈以改进任务表现（Huang
等，2022b；Liang 等，2022；Wang 等，2023a；Ikeuchi 等，2023）。</p>
<p><strong>交互式学习：</strong>专为交互式学习设计的 AI
代理通过结合机器学习技术和用户互动来运行。起初，AI
代理在一个大型数据集上进行训练，数据集的内容根据代理的预期功能有所不同。例如，设计用于语言任务的
AI 会接受大量文本数据的训练。训练过程中使用了深度学习等机器学习算法，使
AI 能识别模式、做出预测并基于训练数据生成响应。AI
代理还可以从用户的实时互动中学习，这种交互式学习方式主要有以下几种：</p>
<blockquote>
<ol type="1">
<li>基于反馈的学习：AI 根据用户的直接反馈调整其响应（Li 等，2023b；Yu
等，2023a；Parakh 等，2023；Zha 等，2023；Wake
等，2023a,b,c）。例如，当用户纠正 AI 的回答时，AI
会利用这些信息改进未来的响应（Zha 等，2023；Liu 等，2023a）。</li>
<li>观察学习：AI
通过观察用户互动进行隐性学习。例如，如果用户频繁提出类似的问题或以某种方式与
AI 互动，AI 可能会调整其响应以更好地适应这些模式。这种方式使 AI
代理能够理解和处理人类语言、多模态设置、跨现实情境的解释，并生成用户的响应。随着用户互动和反馈的增多，AI
代理的性能通常会不断提升。此过程通常由人类操作员或开发者监督，以确保 AI
学习得当，不会产生偏见或错误模式。</li>
</ol>
</blockquote>
<h2 id="概述"><strong>1.3 概述</strong></h2>
<p>多模态代理 AI（Multimodal Agent
AI，MAA）是一类基于多模态感知输入理解而生成有效动作的系统。随着大语言模型（LLM）和视觉语言模型（VLM）的发展，许多
MAA
系统在从基础研究到应用的各个领域中不断涌现。尽管这些研究领域通过结合各自领域的传统技术（如视觉问答和视觉导航）迅速发展，它们在数据收集、基准测试和伦理视角方面具有共同的关注点。</p>
<p>本文着眼于 MAA
的一些代表性研究领域，包括多模态、游戏（VR/AR/MR）、机器人和医疗健康，旨在提供这些领域中普遍关注问题的全面知识。预计的学习成果包括：</p>
<p><strong>•MAA
概述：</strong>深入探讨其原理和在当代应用中的作用，帮助研究人员全面了解其重要性和用途。</p>
<p><strong>•方法学：</strong>展示 LLM 和 VLM 如何增强 MAA
的具体例子，通过游戏、机器人和医疗健康等案例研究来说明。</p>
<p>•性能评估：提供评估 MAA 有效性和泛化能力的相关数据集的指导。</p>
<p><strong>•伦理考虑：</strong>讨论部署代理 AI
所带来的社会影响和伦理问题，强调负责任的开发实践。</p>
<p>•新兴趋势和未来方向：分类讨论各个领域的最新发展并展望未来方向。</p>
<p>基于计算的动作和通用代理（GAs）：通用代理在许多任务中都非常有用。为了让通用代理对用户真正有价值，它需要能够自然地互动，并适应各种上下文和模态。我们致力于培育一个充满活力的研究生态系统，在代理
AI 社区中创造共享的身份感和目标。多模态代理
AI（MAA）在包括人类输入在内的各种上下文和模态中具有广泛的应用潜力。因此，我们相信该领域可以吸引多样化的研究人员群体，促进动态的代理
AI
社区和共同目标的形成。在学术界和产业界的知名专家的带领下，我们希望这篇论文能够成为一次互动且充实的体验，通过代理指导、案例研究、任务环节和实验讨论，为所有研究人员提供全面且富有吸引力的学习体验。</p>
<p>本文旨在提供关于代理 AI
领域当前研究的一般性和全面性的知识。为此，本文的余下内容组织如下：第2部分概述了代理
AI
如何通过与相关新兴技术，特别是大型基础模型的集成而受益。第3部分描述了我们为代理
AI 训练提出的新范式和框架。第4部分提供了广泛应用于代理 AI
训练的各种方法概览。第5部分对各类代理进行了分类和讨论。第6部分介绍了代理
AI
在游戏、机器人和医疗健康领域的应用。第7部分探讨了研究界在开发一种适用于多种模态和领域，并能够实现模拟到现实过渡的通用代理
AI 方面的努力。第8部分讨论了代理 AI
的潜力，不仅依赖于预训练的基础模型，还通过与环境和用户的互动不断学习和自我改进。第9部分介绍了我们为多模态代理
AI 训练设计的新数据集。第11部分讨论了代理 AI
的伦理问题、局限性和社会影响这一热点话题。</p>
<h1 id="代理-ai-集成"><strong>2 代理 AI 集成</strong></h1>
<p>基于大型语言模型（LLM）和视觉语言模型（VLM）的基础模型，在具身 AI
领域的表现仍有限，特别是在理解、生成、编辑和在未见过的环境或场景中互动方面（Huang
等，2023a；Zeng 等，2023）。因此，这些限制导致了 AI
代理输出的效果不佳。当前的以代理为中心的 AI
建模方法专注于直接可访问和清晰定义的数据（例如世界状态的文本或字符串表示），并通常使用大规模预训练中学习的领域和环境无关的模式来预测每种环境的动作输出（Xi
等，2023；Wang 等，2023c；Gong 等，2023a；Wu 等，2023）。在 Huang
等人（2023a）的研究中，我们通过结合大型基础模型，探索了知识引导的协作和交互式场景生成任务，展示了知识为基础的
LLM 代理能够提升2D和3D场景理解、生成和编辑的表现，并支持人机交互（Huang
等，2023a）。通过集成代理 AI
框架，大型基础模型能够更深入地理解用户输入，形成一个复杂且自适应的人机交互系统。</p>
<p><strong>LLM 和 VLM</strong> 的新兴能力在生成 AI、具身
AI、知识增强多模态学习、混合现实生成、文本到视觉编辑、人机交互、以及游戏或机器人任务中的2D/3D模拟中具有不可见的潜力。代理
AI
在基础模型上的最新进展为具身代理的通用智能解锁带来了催化剂。大型动作模型，或代理-视觉-语言模型为具身系统中的计划、问题解决和复杂环境中的学习等通用用途打开了新的可能性。代理
AI 在元宇宙中的进一步测试预示着 AGI 的早期版本的路径。</p>
<h2 id="无限-ai-代理"><strong>2.1 无限 AI 代理</strong></h2>
<p>AI
代理能够基于其训练和输入数据进行解释、预测和响应。尽管这些能力在不断进步，但重要的是要认识到其限制以及训练数据对其性能的影响。AI
代理系统通常具备以下能力：</p>
<blockquote>
<ol type="1">
<li>预测建模：AI
代理可以基于历史数据和趋势预测可能的结果或建议下一步行动。例如，它们可以预测文本的续写、问题的答案、机器人下一步的动作，或场景的解决方案。</li>
<li>决策制定：在某些应用中，AI
代理可以基于其推理进行决策。通常，代理会根据最有可能实现特定目标的行动来进行决策。例如，在推荐系统中，代理可以基于对用户偏好的推断来决定推荐的产品或内容。</li>
<li>处理歧义：AI
代理通常能够通过推断最可能的解释来处理模糊的输入，但其能力受限于其训练数据和算法的范围。4)
持续改进：尽管有些 AI
代理能够从新数据和互动中学习，但许多大型语言模型在训练后不会持续更新其知识库或内部表示。它们的推理通常仅基于最新的训练数据。</li>
</ol>
</blockquote>
<p>我们在图 2
中展示了增强型交互代理，支持多模态和跨现实的无关集成，并具备一种新兴机制。一个
AI
代理需要为每个新任务收集大量训练数据，这在许多领域可能代价高昂或不可行。在本研究中，我们开发了一种“无限代理”，它可以从通用基础模型（如
GPT-X、DALL-E）中学习并转移记忆信息，从而在物理或虚拟世界中理解场景、生成内容和进行交互式编辑。</p>
<p><img src="/images/ai_agent调研/image-20241208113609317.png" srcset="/img/loading.gif" lazyload
alt="image-20241208113609317" />图2：用于跨现实中2D/3D具身生成和编辑交互的多模型AI
Agent。</p>
<p>这种无限代理在机器人领域的一个应用是 RoboGen（Wang
等人，2023d）。在这项研究中，作者提出了一个自动执行任务生成、环境生成和技能学习循环的流程。RoboGen
旨在将大型模型中嵌入的知识转移到机器人领域。</p>
<h2 id="基于大型基础模型的代理-ai"><strong>2.2 基于大型基础模型的代理
AI</strong></h2>
<p>最近的研究表明，大型基础模型在生成数据方面起到了关键作用，作为在环境约束下确定代理行为的基准。例如，基础模型在机器人操作（Black
等人，2023；Ko 等人，2023）和导航（Shah 等人，2023a；Zhou
等人，2023a）方面的应用。以 Black
等人的研究为例，他们使用图像编辑模型作为高级规划器，生成未来子目标的图像，从而引导低级策略（Black
等人，2023）。在机器人导航方面，Shah
等人提出了一个系统，使用大型语言模型（LLM）从文本中识别地标，并使用视觉语言模型（VLM）将这些地标与视觉输入关联，从而增强了基于自然语言指令的导航（Shah
等人，2023a）。</p>
<p>此外，生成基于语言和环境因素的条件化人类动作的兴趣日益增长。已经提出了若干
AI 系统，能够生成针对特定语言指令定制的动作（Kim 等人，2023；Zhang
等人，2022；Tevet 等人，2022），并适应各种 3D 场景（Wang
等人，2022a）。这一研究强调了生成模型在增强 AI
代理跨多种场景的适应性和响应能力方面的日益增长的能力。</p>
<h3 id="幻觉现象"><strong>2.2.1 幻觉现象</strong></h3>
<p>生成文本的代理往往容易出现“幻觉”现象，即生成的文本内容不合逻辑或偏离原始提供的信息（Raunak
等人，2021；Maynez 等人，2020）。幻觉可分为两类：内在幻觉和外在幻觉（Ji
等人，2023）。内在幻觉指与源材料矛盾的内容，而外在幻觉指生成的文本包含原材料中并未提供的额外信息。</p>
<p>降低语言生成中幻觉发生率的一些有效途径包括使用检索增强生成（Lewis
等人，2020；Shuster
等人，2021）或通过外部知识检索来扎根于自然语言输出（Dziri
等人，2021；Peng
等人，2023）。通常，这些方法试图通过检索额外的源材料并提供检测生成内容与源材料是否矛盾的机制来增强语言生成。</p>
<p>在多模态代理系统的背景下，视觉语言模型（VLMs）也会出现幻觉现象（Zhou
等人，2023b）。基于视觉的语言生成幻觉的常见原因之一是训练数据中对象和视觉提示的<strong>共现过度依赖</strong>（Rohrbach
等人，2018）。仅依赖预训练的大型语言模型（LLMs）或视觉语言模型（VLMs），并且在环境中特定的微调有限的
AI
代理特别容易产生幻觉，因为它们依赖于预训练模型的内部知识库来生成操作，可能无法准确理解其部署环境的动态状态。</p>
<h3 id="偏见与包容性"><strong>2.2.2 偏见与包容性</strong></h3>
<p>基于 LLMs 或 LMMs（大型多模态模型）的 AI
代理由于其设计和训练过程中的多种因素而存在偏见。在设计这些 AI
代理时，我们必须注意包容性，了解所有终端用户和利益相关者的需求。就 AI
代理而言，包容性是指确保代理的响应和交互具有包容性、尊重性，并对来自不同背景的广泛用户敏感的措施和原则。以下是代理偏见和包容性的关键方面：</p>
<p><strong>•训练数据：</strong>基础模型是基于从互联网上收集的大量文本数据进行训练的，包括书籍、文章、网站和其他文本来源。这些数据往往反映了人类社会中的偏见，模型可能会无意中学习并复制这些偏见，包括与种族、性别、民族、宗教及其他个人属性相关的刻板印象、偏见和倾向性观点。尤其是，通过训练来自互联网数据，且往往仅为英文文本的模型，隐含地学习了“西方、受教育的、工业化的、富裕的和民主的”（WEIRD）社会的文化规范（Henrich
等人，2010），因为这些社会在互联网上的影响力较大。然而，重要的是要认识到，由人类创建的数据集不可能完全没有偏见，因为它们通常反映了社会偏见以及最初生成和/或编译数据的个体的倾向。</p>
<p><strong>•历史和文化偏见：</strong>AI
模型是基于从多样内容来源的大型数据集进行训练的。因此，训练数据通常包括各种文化的历史文本或材料。特别是，来自历史来源的训练数据可能包含反映某个社会文化规范、态度和偏见的冒犯性或贬低性语言。这可能导致模型延续过时的刻板印象，或无法完全理解当代文化变化和细微差别。</p>
<p><strong>•语言和语境限制：</strong>语言模型可能难以理解和准确表达语言中的细微差别，如讽刺、幽默或文化参考。这可能导致在某些语境下的误解或偏见性响应。此外，纯文本数据无法捕捉口语语言的许多方面，可能导致人类对语言的理解和模型对语言的理解之间的潜在差距。</p>
<p><strong>•政策和指导方针：</strong>AI
代理在严格的政策和指导方针下运行，以确保公平和包容性。例如，在生成图像时，有相关规则来多样化人群的描绘，避免与种族、性别和其他属性相关的刻板印象。</p>
<p><strong>•过度概括：</strong>这些模型倾向于基于训练数据中的模式生成响应。这可能导致过度概括，模型可能会产生看似刻板印象或对某些群体做出广泛假设的响应。</p>
<p><strong>•持续监控和更新：</strong>AI
系统不断被监控和更新，以解决任何新出现的偏见或包容性问题。用户反馈和 AI
伦理的持续研究在此过程中发挥着关键作用。</p>
<p><strong>•主流观点的放大：</strong>由于训练数据通常包含来自主流文化或群体的更多内容，模型可能更倾向于这些视角，从而可能低估或误解少数群体的观点。</p>
<p><strong>•伦理与包容性设计：</strong>AI
工具的设计应将伦理考量和包容性作为核心原则。这包括尊重文化差异、促进多样性并确保
AI 不延续有害的刻板印象。</p>
<p><strong>•用户指南：</strong>用户在与 AI
交互时也会得到如何促进包容性和尊重的指导。这包括避免提出可能导致偏见或不适当输出的请求。此外，这有助于防止模型从用户交互中学习到有害材料。</p>
<p>尽管采取了这些措施，AI 代理仍表现出偏见。代理 AI
研究和开发的持续努力集中在进一步减少这些偏见，并增强代理 AI
系统的包容性和公平性。减少偏见的努力：</p>
<p>•多样且包容的训练数据：努力在训练数据中包含更为多样化和包容性的来源。</p>
<p>•偏见检测与纠正：正在进行的研究关注于检测和纠正模型响应中的偏见。</p>
<p><strong>•伦理指南和政策：</strong>模型通常受伦理指南和政策的管理，旨在减轻偏见并确保互动的尊重和包容。</p>
<p><strong>•多样化呈现：</strong>确保 AI
代理生成的内容或提供的响应能代表广泛的人类经历、文化、种族和身份。这在图像生成或叙事构建等场景中特别重要。</p>
<p><strong>•偏见缓解：</strong>积极减少 AI
响应中的偏见，包括与种族、性别、年龄、残疾、性取向和其他个人特征相关的偏见。目标是提供不延续刻板印象或偏见的公平、平衡的回应。</p>
<p><strong>•文化敏感性：</strong>AI
被设计成具有文化敏感性，能够承认和尊重文化规范、实践和价值的多样性。这包括理解并适当回应文化参考和细微差别。</p>
<p><strong>•可访问性：</strong>确保 AI
代理对具有不同能力的用户可访问，包括残障人士。这可能包括为有视觉、听觉、运动或认知障碍的人提供更便捷的互动功能。</p>
<p><strong>•基于语言的包容性：</strong>提供对多种语言和方言的支持，以满足全球用户的需求，并敏感地对待语言中的细微差别和变化（Liu
等人，2023b）。</p>
<p><strong>•伦理和尊重的互动：</strong>代理被编程为与所有用户进行伦理且尊重的互动，避免产生可能被视为冒犯、有害或不尊重的回应。</p>
<p><strong>•用户反馈与适应：</strong>吸收用户反馈以不断改进 AI
代理的包容性和有效性，包括通过交互更好地理解和服务多样化的用户群体。</p>
<p><strong>•遵守包容性指南：</strong>遵循由行业团体、伦理委员会或监管机构制定的
AI 代理包容性指南和标准。</p>
<p>尽管有这些努力，但仍需要意识到 AI
响应中可能存在的偏见，并用批判性思维进行解读。AI
代理技术和伦理实践的不断改进，旨在逐步减少这些偏见。</p>
<p>包容性在代理 AI
中的总体目标之一是创建一个尊重并适合所有用户的代理，无论其背景或身份如何。</p>
<h3 id="数据隐私与使用"><strong>2.2.3 数据隐私与使用</strong></h3>
<p>AI
代理的一个关键伦理考量在于理解这些系统如何处理、存储和潜在地检索用户数据。以下是关键方面的讨论：</p>
<p><strong>•数据收集、使用和目的。</strong>在使用用户数据以提升模型性能时，模型开发者会访问
AI
代理在与用户互动期间收集的数据。有些系统允许用户通过账户查看他们的数据，或者通过请求服务提供商进行查看。用户需要了解代理在互动期间收集了哪些数据，包括文本输入、使用模式、个人偏好，有时甚至包括更为敏感的个人信息。用户还应理解这些数据的用途。如果因某些原因，AI
持有关于某人或某群体的不正确信息，应该有机制帮助用户在识别后予以纠正。这对于准确性和尊重所有用户和群体都非常重要。常见的检索和分析用户数据的用途包括改善用户互动、个性化响应和系统优化。开发者需确保数据不被用于用户未同意的用途，如未经请求的营销。</p>
<p><strong>•存储与安全。</strong>开发者应了解用户互动数据存储的地点以及所采用的安全措施，以防止未经授权的访问或数据泄露。这包括加密、安全服务器和数据保护协议。明确代理数据是否与第三方共享以及在何种情况下共享也非常重要。这需要透明化，并且通常需要用户同意。</p>
<p><strong>•数据删除与保留。</strong>用户还需理解其数据的存储时长以及如何请求删除其数据。许多数据保护法赋予用户“被遗忘权”，即他们可以请求删除其数据。AI
代理必须遵守 GDPR（欧盟）或
CCPA（加利福尼亚州）等数据保护法，这些法律规范了数据处理实践及用户的个人数据权利。</p>
<p><strong>•数据便携性与隐私政策。</strong>开发者必须制定 AI
代理的隐私政策，以向用户详细说明其数据的处理方式。该政策应详细阐述数据的收集、使用、存储以及用户权利。开发者应确保获取用户对数据收集的同意，尤其是涉及敏感信息时。用户通常可以选择退出或限制他们提供的数据。在某些司法管辖区，用户甚至有权请求以可转移至其他服务提供商的格式提供其数据副本。</p>
<p><strong>•匿名化。</strong>在广泛分析或 AI
训练中使用的数据应尽量匿名化以保护个人身份。开发者必须理解其 AI
代理在互动中如何检索和使用历史用户数据，以进行个性化或提高响应的相关性。</p>
<p>总之，理解 AI
代理的数据隐私包括了解用户数据的收集、使用、存储和保护方式，并确保用户了解其在访问、纠正和删除数据方面的权利。理解数据检索的机制对用户和
AI 代理来说都非常重要，这对于全面理解数据隐私至关重要。</p>
<h3 id="可解释性与解释性"><strong>2.2.4 可解释性与解释性</strong></h3>
<p><strong>模仿学习→解耦。</strong>代理通常通过强化学习（RL）或模仿学习（IL）的连续反馈循环训练，从随机初始化策略开始。然而，在不熟悉的环境中获取初始奖励尤其困难，特别是在奖励稀少或仅在长步骤互动结束时可获得时。因此，采用通过
IL
训练的无限记忆代理是一种优越的解决方案，可以从专家数据中学习策略，提高对未见环境空间的探索和利用能力，具有图
3
中所示的基础设施。通过专家特性帮助代理更好地探索并利用未知的环境空间，代理
AI 可以直接从专家数据中学习策略和新范式流程。</p>
<p>传统的 IL
方法让代理模仿专家示范的行为来学习策略。然而，直接学习专家策略并不总是最佳方式，因为代理可能无法很好地泛化到未见情况。为了解决这一问题，我们提出了使用上下文提示或隐含奖励函数来捕捉专家行为关键方面的学习方法，如图
3
所示。这使无限记忆代理拥有从专家示范中学习的物理世界行为数据以执行任务，有助于克服模仿学习中现存的缺点，如需要大量专家数据和在复杂任务中可能出现的错误。</p>
<p>代理 AI
的关键思想包括两部分：1）无限代理收集物理世界专家示范作为状态-行动对；2）模拟代理生成器的虚拟环境。模仿代理产生的行动模仿专家的行为，同时代理通过减少专家行动与学习策略生成的行动之间差异的损失函数来学习状态到行动的映射。</p>
<p><strong>解耦→泛化。</strong>与依赖特定任务的奖励函数不同，代理从专家示范中学习，这些示范提供了涵盖各种任务方面的多样化状态-动作对。然后，代理通过模仿专家行为来学习一个从状态到动作的策略。模仿学习中的解耦是指将学习过程与任务特定的奖励函数分离，使得该策略能够在不同任务之间泛化，而不依赖于特定的奖励函数。通过解耦，代理可以从专家示范中学习，形成一种适用于多种情境的策略。解耦还支持迁移学习，即在一个领域中学习的策略可以通过少量的微调适应到其他领域。通过学习一个与特定奖励函数无关的通用策略，代理可以利用其在一个任务中获得的知识来出色地完成其他相关任务。由于代理不依赖特定的奖励函数，它可以适应奖励函数或环境的变化，而无需大量的再训练。这使得所学策略在不同环境中更为稳健且具有广泛的适应性。在该背景下，解耦指的是学习过程中的两个任务的分离：学习奖励函数和学习最优策略。</p>
<p><strong>泛化→涌现行为。</strong>泛化解释了如何从较简单的组件或规则中产生涌现特性或行为。关键在于识别系统行为的基本元素或规则，例如个体神经元或基本算法。通过观察这些简单组件或规则之间的相互作用，这些组件的相互作用往往会导致复杂行为的涌现，而仅通过观察单个组件难以预测这些复杂行为。跨越不同复杂性层次的泛化，使系统能够学习适用于这些层次的通用原则，从而产生涌现特性。这种泛化能力使系统能够适应新情境，展示出从简单规则中涌现的更复杂的行为。此外，跨越不同复杂性层次的泛化能力促进了从一个领域到另一个领域的知识迁移，这有助于在新环境中适应并产生复杂行为。</p>
<figure>
<img src="/images/ai_agent调研/640.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图3：涌现交互机制示例
此示例展示了使用代理从候选项中识别与图像相关的文本的过程。任务包括使用一个多模态AI代理，从网络和人类标注的知识交互样本中获取信息，以整合外部世界的信息。</p>
<h3 id="推理增强"><strong>2.2.5 推理增强</strong></h3>
<p>AI代理的推理能力体现在其基于训练数据和输入信息进行解释、预测和回应的能力。尽管这些能力不断进步，但也应认识到其在基础数据上的局限性和影响力。特别是在大语言模型的背景下，推理指的是AI根据其训练数据和输入得出结论、进行预测并生成回应的能力。推理增强指的是通过额外的工具、技术或数据来提升AI的自然推理能力，以提高其性能、准确性和实用性。以下是推理增强的关键来源：</p>
<p><strong>•数据丰富化：</strong>整合额外的、通常是外部数据源，提供更多上下文或背景，有助于AI代理做出更有见地的推断，尤其在其训练数据有限的领域。例如，AI代理可以从对话或文本的上下文中推断含义，通过分析信息以理解用户查询的意图和相关细节。</p>
<p><strong>•算法增强：</strong>改进AI的基础算法以进行更优推理。这可能涉及更先进的机器学习模型，结合不同类型的AI（如自然语言处理与图像识别的整合），或更新算法以更好地处理复杂任务。语言模型的推理涉及理解和生成人类语言，包括语气、意图和语言结构的细微差别。</p>
<p><strong>•人机协作（Human-in-the-Loop，HITL）：</strong>在人类判断尤为重要的领域，如伦理考量、创意任务或模棱两可的场景中，引入人类输入以增强AI推理。人类可以提供指导、纠正错误或提供AI无法自行推断的见解。</p>
<p><strong>•实时反馈整合：</strong>使用来自用户或环境的实时反馈来增强推理，例如AI可以根据实时用户响应或动态系统中的变化条件调整推荐内容，或在模拟环境中，若AI代理执行的操作违背某些规则，可以动态给予反馈以帮助其自我修正。</p>
<p><strong>•跨领域知识迁移：</strong>利用一个领域的知识或模型来改善另一个领域的推理，尤其在专业学科中输出结果时更为有用。例如，将语言翻译的技术应用于代码生成，或将医学诊断中的见解应用于机械设备的预测性维护。</p>
<p><strong>•特定应用的定制化：</strong>根据特定应用或行业定制AI的推理能力，可能涉及在专用数据集上训练AI或微调模型以更好地适应特定任务，如法律分析、医学诊断或金融预测。由于特定领域内的信息与其他领域的差异，微调代理以适应特定领域信息通常会有所裨益。</p>
<p><strong>•伦理与偏见考量：</strong>确保增强过程不会引入新的偏见或伦理问题，这需要仔细考量额外数据的来源或新的推理增强算法对公平性和透明度的影响。尤其在处理敏感话题时，AI代理应避免有害的刻板印象，尊重隐私并确保公平性。</p>
<p><strong>•持续学习与适应：</strong>定期更新和优化AI的能力，以跟上新发展、数据环境变化以及用户需求的演变。</p>
<p>总结来说，AI代理的推理增强涉及通过额外数据、改进算法、人类输入等多种方法来增强其自然推理能力。视具体应用而定，这种增强对于处理复杂任务和确保输出准确性往往必不可少。</p>
<h3 id="监管"><strong>2.2.6 监管</strong></h3>
<p>近年来，Agent
AI取得了显著进展，其与具身系统的集成为通过更沉浸、动态和互动的方式与代理交互开辟了新可能。为了加快进程并简化AI
Agent开发中的繁琐工作，我们提出开发下一代AI驱动的代理互动管道。通过构建一个人机协作系统，让人类和机器可以进行有意义的沟通和互动。该系统可以利用大语言模型（LLM）或视觉语言模型（VLM）的对话能力以及丰富的动作，来与人类用户对话并识别其需求，并在请求时执行适当的动作以帮助用户。</p>
<p>在使用大语言模型（LLM）或视觉语言模型（VLM）进行人机协作系统时，需注意这些模型作为“黑箱”运行，生成不可预测的输出。这种不确定性在物理环境中（例如操作实际机器人）尤其关键。为应对这一挑战，可以通过提示工程（prompt
engineering）来约束LLM/VLM的关注点。例如，在机器人任务规划中，提供环境信息的提示比单纯依赖文本能产生更稳定的输出（Gramopadhye
和
Szafir，2022）。这一发现得到了明斯基AI框架理论（Minsky，1975）的支持，该理论指出LLM/VLMs所要解决的问题空间由所提供的提示定义。</p>
<p>另一种方法是设计提示，让LLM/VLM生成解释性文本，使用户了解模型的关注点或识别内容。此外，通过人类指导引入预执行验证和修改的更高层次控制，可以更好地操作在此类指导下运行的系统（见图4）。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411348-1.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图4：一个由 Wake
等人（2023c）开发的机器人教学系统。（左图）系统工作流程。该过程包含三个步骤：任务规划，即
ChatGPT
根据指令和环境信息规划机器人任务；演示环节，用户视觉演示动作序列。用户对所有步骤进行审核，如果某一步骤失败或显示出缺陷，可根据需要返回到前面的步骤进行修改。（右图）一个支持上传演示数据的网页应用，用于用户与
ChatGPT 的交互。</p>
<h2 id="基于生成能力的ai-agent"><strong>2.3 基于生成能力的AI
Agent</strong></h2>
<p>尽管交互式AI
Agent系统的应用日益增多，但现有大部分方法在未见过的环境或情境中依然面临泛化能力的挑战。当前的建模实践要求开发者为每个领域准备大量数据集以微调/预训练模型；然而，此过程成本高昂，且在新的领域可能不可行。为应对这一问题，我们构建了能够利用通用基础模型（如ChatGPT、DALL-E、GPT-4等）的知识记忆的交互式代理，以适应新情境，尤其是在建立人与代理间的协作空间方面。</p>
<p>我们发现了一种新兴机制——称之为知识推理交互的混合现实（Mixed Reality
with Knowledge Inference
Interaction），该机制可促进人机协作以在复杂的现实环境中解决挑战性任务，并使代理能够在虚拟现实中探索未见环境以实现适应。通过该机制，代理能够学习：1.跨模态的微反应：从显性网络资源中收集每个交互任务的相关知识（例如理解未见场景），并从预训练模型的输出中隐性推理出知识。2.现实无关的宏观行为：提升语言和多模态领域的交互维度和模式，基于特定目标变量及混合现实和大语言模型中的协作信息多样化，做出角色化变化。我们研究了知识引导下的协作场景生成任务，通过结合多种OpenAI模型，展示了交互式代理系统如何在我们的设置中进一步增强大型基础模型的表现。此系统提升了复杂适应性AI系统的泛化深度、意识性和可解释性。</p>
<h1 id="ai-agent的范式"><strong>3 AI Agent的范式</strong></h1>
<p>本节将讨论AI
Agent训练的新范式和框架。我们希望通过提出的框架达成以下目标：</p>
<p>•利用现有的预训练模型和预训练策略，快速引导代理对关键模态（如文本或视觉输入）的有效理解。</p>
<p>•支持足够的长期任务规划能力。</p>
<p>•引入一个记忆框架，使得学习到的知识可以被编码并在后续调用。</p>
<p>•允许环境反馈用于有效训练代理，以便其学习采取合适的行动。</p>
<p>图5展示了此类系统的重要子模块的高层次新代理示意图。</p>
<figure>
<img src="/images/ai_agent调研/fdef9a641f0423fe1fb58c237f76e9a4.jpg" srcset="/img/loading.gif" lazyload
alt="fdef9a641f0423fe1fb58c237f76e9a4" />
<figcaption
aria-hidden="true">fdef9a641f0423fe1fb58c237f76e9a4</figcaption>
</figure>
<p>图5：我们提出的多模态通用代理新范式。图中显示了五个主要模块：1）环境与感知模块，包括任务规划和技能观察；2）代理学习；3）记忆；4）代理动作；5）认知。</p>
<h2 id="大语言模型llms与视觉语言模型vlms"><strong>3.1
大语言模型（LLMs）与视觉语言模型（VLMs）</strong></h2>
<p>可以使用LLM或VLM模型来引导代理的各个组件，如图5所示。尤其是，LLMs在任务规划方面表现出色（Gong等，2023a），具备丰富的世界知识（Yu等，2023b），并展现了出色的逻辑推理能力（Creswell等，2022）。此外，VLMs（如CLIP，Radford等，2021）提供了与语言对齐的通用视觉编码器，并具备零样本视觉识别能力。例如，开源的先进多模态模型如LLaVA（Liu等，2023c）和InstructBLIP（Dai等，2023）依赖冻结的CLIP模型作为视觉编码器。</p>
<h2 id="代理transformer的定义"><strong>3.2
代理Transformer的定义</strong></h2>
<p>除了将冻结的LLMs和VLMs用于AI代理外，还可以使用单一的代理Transformer模型，其输入包括视觉令牌和语言令牌，类似于Gato模型（Reed等人，2022）。除了视觉和语言外，我们添加了第三种通用的输入类型，称为代理令牌。概念上，代理令牌用于在模型的输入和输出空间中保留一个特定的子空间，以执行代理行为。在机器人或游戏操作中，这些代理令牌可以表示控制器的输入动作空间。在训练代理使用特定工具（如图像生成或图像编辑模型）或进行其他API调用时，也可以使用代理令牌。正如图7所示，我们可以将代理令牌与视觉和语言令牌结合起来，生成用于多模态AI
Agent训练的统一接口。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411348-2.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图6：展示当前多模态 AI
代理的创建范式，通过结合大型语言模型（LLM）和大型视觉模型（LVM）实现。通常，这些模型接收视觉或语言输入，并使用预训练和冻结的视觉和语言模型，学习连接和桥接模态的小型子网络。示例包括
Flamingo（Alayrac 等，2022）、BLIP-2（Li 等，2023c）、InstructBLIP（Dai
等，2023）和 LLaVA（Liu 等，2023c）。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411349-3.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图7：统一的代理多模态 Transformer
模型。我们提出了一种统一的端到端训练范式，用于代理系统，而不是仅连接冻结的子模块并将现有的基础模型作为构建块。我们仍然可以像图6中那样用
LLM 和 LVM
初始化子模块，但同时引入代理标记，即专门用于训练模型在特定领域（如机器人）执行代理行为的标记。关于代理标记的更多细节，请参见第3.2节。</p>
<p>与将大型专有的LLMs作为代理相比，使用代理Transformer有几个优势：</p>
<blockquote>
<p>1.定制化：该模型可以轻松定制以适应非常特定的代理任务，而这些任务可能难以用自然语言表示（例如控制器输入或其他特定动作）。因此，代理可以通过与环境的互动和特定领域的数据学习来提升性能。</p>
<p>2.可解释性：通过访问代理令牌的概率，可以更容易理解模型为何会或不会采取特定行动。</p>
<p>3.数据隐私：在一些数据隐私要求严格的领域（如医疗和法律）中，代理Transformer能够更好地满足这些需求。</p>
<p>4.成本效益：相对较小的代理Transformer在成本上可能显著低于大型的专有语言模型。这一架构能在多个方面优化AI代理的性能和适应性。</p>
</blockquote>
<h2 id="代理transformer的创建"><strong>3.3
代理Transformer的创建</strong></h2>
<p>如上图5所示，我们可以使用新的代理范式，通过LLM和VLM启动代理，同时利用大型基础模型生成的数据来训练代理Transformer模型，以学习如何执行特定目标。在此过程中，代理模型被训练为针对特定任务和领域进行专业化和定制化。这种方法使您能够利用现有的基础模型的学习特征和知识。以下是该过程的两个简化步骤：</p>
<p><strong>•在领域内定义目标：</strong>为了训练代理Transformer，需要在每个特定环境的背景下明确代理的目标和动作空间。这包括确定代理需要执行的特定任务或动作，并为每个任务分配独特的代理令牌。此外，任何可以用于自动识别任务成功完成的规则或程序都可以显著增加用于训练的数据量。否则，将需要基础模型生成的数据或人工标注的数据来训练模型。在数据收集完成并可以评估代理的性能后，可以开始持续改进的过程。</p>
<p><strong>•持续改进：</strong>对模型性能的持续监控和反馈收集是该过程中的重要步骤。应使用反馈进行进一步的微调和更新。同时，确保模型不会延续偏见或产生不道德的结果也至关重要。这需要仔细检查训练数据，定期检查输出中的偏见，并在必要时训练模型识别和避免偏见。一旦模型达到了满意的性能，就可以部署在预期的应用中。持续监控仍然至关重要，以确保模型按预期表现并进行必要的调整。有关该过程、训练数据来源以及AI
Agent持续学习的详细信息，请参见第8节。</p>
<h1 id="ai-agent学习"><strong>4 AI Agent学习</strong></h1>
<h2 id="策略和机制"><strong>4.1 策略和机制</strong></h2>
<p>在不同领域扩展交互式AI的策略包括调用大型基础模型的范式，通过一个主动收集用户反馈、行动信息、生成和交互有用知识的训练代理来实现。有时，LLM/VLM模型无需重新训练，我们可以通过在测试时为代理提供改进的上下文提示来提高其性能。另一方面，这通常涉及知识/推理/常识/推断的交互建模，通过三个系统的组合：一个从多模态查询中检索知识，第二个从相关代理进行交互生成，第三个使用强化学习或模仿学习以改进方式进行自监督训练或预训练。</p>
<h3 id="强化学习-rl"><strong>4.1.1 强化学习 (RL)</strong></h3>
<p>强化学习（RL）在训练具有智能行为的交互式代理方面有着丰富的历史。RL是一种基于行动结果获得的奖励（或惩罚）来学习状态与行动之间最佳关系的方法。RL是一个高度可扩展的框架，已被应用于包括机器人在内的众多应用中，但也面临一些挑战，LLM/VLMs展示了在某些方面减轻或克服这些困难的潜力：</p>
<p><strong>•奖励设计：</strong>策略学习的效率在很大程度上取决于奖励函数的设计。设计奖励函数不仅需要RL算法的知识，还需要对任务本质的深入理解，因此通常需要基于专家经验来构建函数。一些研究探索了使用LLM/VLMs来设计奖励函数的可能性。</p>
<p><strong>•数据收集和效率：</strong>由于其探索性质，基于RL的策略学习需要大量的数据。当策略涉及管理长序列或整合复杂的动作时，对数据的需求尤其显著。近年来，许多研究致力于增强数据生成以支持策略学习。</p>
<p><strong>•长时间跨度步骤：</strong>在数据效率问题的基础上，随着动作序列长度的增加，RL变得更加具有挑战性。通常的处理方法是将长而复杂的任务分解为一系列子目标，并应用预训练的策略来解决每个子目标。这种方法属于任务和运动规划（TAMP）的框架。TAMP包含两个主要部分：任务规划，即识别高层次行动的序列，和运动规划，即找到物理一致、无碰撞的轨迹以实现任务计划的目标。</p>
<p>LLMs非常适合TAMP，许多最新的研究采用了一种方法，使用LLMs执行高层次的任务规划，而低层次控制由基于RL的策略解决。</p>
<h3 id="模仿学习-il"><strong>4.1.2 模仿学习 (IL)</strong></h3>
<p>与RL通过环境交互的探索行为和最大化奖励来训练策略的目标不同，模仿学习（IL）旨在利用专家数据来模仿经验丰富的代理或专家的行为。例如，在机器人学领域，基于IL的主要框架之一是行为克隆（Behavioral
Cloning,
BC）。BC是一种训练机器人直接复制专家执行的动作的方法。在此方法中，记录专家执行特定任务的动作，然后训练机器人在类似情况下复制这些动作。近年来，基于BC的方法通常结合LLM/VLM技术，实现了更高级的端到端模型。例如，Brohan等人提出了RT-1和RT-2，基于Transformer的模型，输入一系列图像和语言，输出底座和机械臂的动作序列。研究表明，通过大量数据训练，这些模型在泛化性能上表现出色。</p>
<h3 id="传统rgb"><strong>4.1.3 传统RGB</strong></h3>
<p>多年来，人们一直对利用图像输入来学习智能代理行为感兴趣。使用RGB输入的固有挑战是维度的诅咒。为了解决这个问题，研究人员要么使用更多的数据，要么在模型设计中引入归纳偏置以提高样本效率。特别地，研究人员将3D结构整合到模型架构中以进行操控；对于机器人导航，研究人员利用地图作为表示。地图可以通过神经网络聚合所有之前的RGB输入来学习，或通过诸如神经辐射场的3D重建方法实现。为了获得更多数据，研究人员使用图形模拟器生成合成数据，并尝试缩小模拟到现实的差距。最近，一些集体努力致力于创建大规模数据集，以解决数据稀缺的问题；另一方面，为了提高样本复杂度，数据增强技术也得到了广泛研究。</p>
<h3 id="上下文学习"><strong>4.1.4 上下文学习</strong></h3>
<p>上下文学习在NLP领域被证明是一种有效的任务解决方法，尤其是在大型语言模型（如GPT-3）出现之后。通过在LLM提示上下文中提供任务示例，少量的提示被视为在NLP的多种任务中将模型输出置于上下文中的有效方式。在多模态基础模型的上下文中，Flamingo和BLIP-2等模型在仅提供少量示例的情况下被证明在各种视觉理解任务上效果显著。通过在执行特定动作时加入环境特定反馈，上下文学习可以进一步提升代理在环境中的表现。</p>
<h3 id="代理系统的优化"><strong>4.1.5 代理系统的优化</strong></h3>
<p>代理系统的优化可分为空间和时间两个方面。空间优化涉及代理在物理空间中执行任务的方式，包括机器人间的协调、资源分配和保持空间有序性。</p>
<p>为了有效优化AI
Agent系统，特别是那些有大量代理并行执行的系统，以往的研究集中于使用大批量强化学习。由于特定任务的多代理交互数据集稀缺，自对抗强化学习使得一组代理可以随时间推移而不断改进。然而，这可能会导致代理过于依赖自对抗训练范式，难以适应与人类或其他独立代理的协作。为了解决此问题，我们可以通过发现多样化的约定集并训练一个意识到广泛约定的代理来实现。基础模型进一步帮助建立与人类或其他独立代理的约定，从而实现与新代理的平滑协作。</p>
<p>另一方面，时间优化专注于代理如何随着时间的推移执行任务。这包括任务调度、顺序安排和时间线效率。例如，优化机器人的手臂轨迹以在连续任务之间实现高效移动是优化运动的一个例子。在任务调度的层面上，LLM-DP和ReAct等方法被提出以通过互动地整合环境因素来解决高效任务规划的问题。</p>
<h2 id="agent-systems零样本和少样本级别"><strong>4.2 Agent
Systems（零样本和少样本级别）</strong></h2>
<h3 id="代理模块"><strong>4.2.1 代理模块</strong></h3>
<p>我们在代理范式中开发了用于交互式多模态代理的AI“模块”，使用了LLM或VLM。初始的代理模块支持训练或上下文学习，并采用简化设计，以展示代理在调度和协调上的有效性。我们还探索了初步的基于提示的记忆技术，以帮助更好地进行规划，并为未来的领域内行动提供信息。具体来说，我们的“MindAgent”基础架构包含五个主要模块：1）环境感知与任务规划，2）代理学习，3）记忆，4）通用代理行动预测，5）认知，如图5所示。</p>
<h3 id="代理基础设施"><strong>4.2.2 代理基础设施</strong></h3>
<p>在娱乐、研究和工业领域，基于代理的AI是一个庞大且快速发展的社区。大型基础模型的发展显著提升了AI
Agent系统的性能。然而，创建此类代理的挑战在于高质量数据集的创建和整体成本的增加。在Microsoft，通过使用先进的硬件、多样的数据源和强大的软件库，建立高质量的代理基础设施极大地推动了多模态代理的协同作业。随着Microsoft不断推进代理技术的边界，AI代理平台将在未来多模态智能的世界中保持主导地位。然而，AI
Agent的交互过程仍然复杂，需要多种技能的组合。大规模生成AI模型领域的最新进展有望大幅降低当前的高成本和时间，使大公司和独立创作者能够超越当前能力创造高质量体验。目前多模态代理的主要人机交互系统主要基于规则，尽管它们对用户操作具备一定的智能行为并拥有一定的网络知识，但由于开发成本，系统中的特定行为较为有限。此外，现有模型在用户无法完成特定任务时，尚不足以帮助用户实现目标。因此，AI
Agent系统基础设施需要能够分析用户行为，并在需要时提供适当支持。</p>
<h2 id="基础模型的代理化预训练和微调级别"><strong>4.3
基础模型的代理化（预训练和微调级别）</strong></h2>
<p>使用预训练的基础模型能够广泛适用于多种使用场景，为各种应用的定制解决方案的开发提供了显著优势，无需为每个特定任务创建大量标注数据集。例如，在导航领域，LM-Nav系统结合了GPT-3和CLIP，采用了一种创新方法。它利用语言模型生成的文本标记物，并将其锚定在由机器人获取的图像中以实现导航。这种方法展示了文本与视觉数据的无缝融合，极大提升了机器人导航的能力，同时具备广泛的适用性。</p>
<p>在机器人操作方面，多个研究提出了使用现成的LLM（如ChatGPT）和开放词汇物体检测器的结合。LLM与先进的物体检测器（如Detic）相结合，有助于理解人类指令，同时将文本信息与场景信息对接。此外，最新的进展展示了利用GPT-4V(ision)等高级多模态模型进行提示工程的潜力，这种技术为多模态任务规划开启了新的方向，突显了预训练模型在多种情境下的多样性和适应性。</p>
<h1 id="ai-agent分类"><strong>5 AI Agent分类</strong></h1>
<h2 id="通用代理领域"><strong>5.1 通用代理领域</strong></h2>
<p>计算机化的行动代理和通用代理（GAs）对许多任务都很有用。大型基础模型和交互式AI领域的最新进展为GAs增添了新的功能。然而，为了让GA真正为用户带来价值，它必须易于交互，且能够广泛泛化到多种情境和模态。我们在第6节重点讨论了代理基础AI的高质量章节，特别是在这些主题相关的领域：</p>
<p>多模态AI
Agent（MMA）是一个新兴论坛2，旨在为我们的研究和工业社区提供交流平台，并与更广泛的研究和技术社区互动。大型基础模型和交互式AI的进展使通用代理（GAs）具有预测用户行为和在受限环境中进行任务规划等新功能。代表性工作包括MindAgent、细粒度多模态视频理解、机器人技术等，或提供包含知识反馈的聊天伴侣（如医疗系统的客户支持网站）。本文及论坛涵盖了以下主要主题，但不限于这些主题：</p>
<blockquote>
<p>•主要主题：多模态AI Agent、通用AI Agent</p>
<p><strong>•次要主题：</strong>具象代理、行动代理、基于语言的代理、视觉与语言代理、知识和推理代理、游戏、机器人、医疗等领域的代理</p>
<p><strong>•扩展主题：</strong>视觉导航、模拟环境、重排、代理基础模型、VR/AR/MR、具象视觉与语言。</p>
</blockquote>
<p>接下来，我们列出了一些具体的代表性代理类别。</p>
<h2 id="具象代理"><strong>5.2 具象代理</strong></h2>
<p>我们的生物思维存在于身体之中，而我们的身体在不断变化的世界中移动。具象人工智能的目标是创造能够与环境互动来解决复杂任务的代理（如机器人）。尽管这是一个巨大的挑战，但深度学习的重大进展以及像ImageNet这样大型数据集的日益普及，使得在过去被认为难以处理的各种AI任务中取得了超人级的表现。计算机视觉、语音识别和自然语言处理已在诸如语言翻译和图像分类等被动输入输出任务中实现了变革性进展，而强化学习也在游戏等交互性任务中达到了世界级的表现。这些进展推动了具象AI的发展，使越来越多的用户能够迅速朝着智能代理的方向前进。</p>
<h3 id="行动代理"><strong>5.2.1 行动代理</strong></h3>
<p>行动代理指需要在模拟的物理环境或现实世界中执行物理行动的代理。特别是，它们需要积极参与与环境的互动。我们根据应用领域将行动代理大致分为两个类别：游戏AI和机器人。</p>
<p>在游戏AI中，代理会与游戏环境和其他独立实体互动。在这些场景中，自然语言可以促进代理与人类之间的顺畅沟通。根据游戏的不同，可能有具体任务要完成，从而提供明确的奖励信号。例如，在竞争性游戏《Diplomacy》中，通过使用人类对话数据训练语言模型，以及结合强化学习的行动策略，使代理能够达到人类水平的表现（Meta
Fundamental AI Research (FAIR) Diplomacy Team等，2022年）。</p>
<p>此外，还有一些场景中代理只是作为一个虚拟城镇的普通居民，不以优化特定目标为目的（Park等，2023年）。在这些场景中，基础模型非常有用，因为它们可以通过模仿人类行为来模拟出更加自然的互动。当结合外部记忆时，它们可以生成能够对话、安排日常生活、建立关系并拥有虚拟生活的可信代理。</p>
<h3 id="交互代理"><strong>5.2.2 交互代理</strong></h3>
<p>交互代理指能够与世界互动的代理，其互动形式不一定需要物理行动，可能只是向用户传达信息或修改环境。例如，一个具象交互代理可以通过对话回答用户关于某个主题的问题，或帮助用户处理已有信息，类似于聊天机器人。通过扩展代理的能力以包含信息共享，Agent
AI的核心设计和算法可以有效地应用于各种应用场景，例如诊断代理（Lee等，2023年）和知识检索代理（Peng等，2023年）。</p>
<h2 id="仿真和环境代理"><strong>5.3 仿真和环境代理</strong></h2>
<p>让AI代理通过与环境的交互进行试错学习是一种有效的训练方法。强化学习（RL）是一种典型方法，通常需要代理经历大量失败来进行训练。尽管存在使用物理代理的方案（Kalashnikov等，2018年），但物理代理的训练耗时且昂贵，特别是在实际环境中失败可能导致危险的情况下（如自动驾驶、深海设备），在物理环境中训练往往不可行。因此，使用仿真器来学习策略成为了普遍做法。</p>
<p>许多仿真平台已经被提出用于具象AI的研究，从导航（Tsoi等，2022年；Deitke等，2020年；Kolve等，2017年）到物体操作（Wang等，2023年；Mees等，2022年；Yang等，2023年；Ehsani等，2021年）。例如，Habitat（Savva等，2019年；Szot等，2021年）提供了一个3D室内环境，人类和机器人代理可以在其中执行各种任务，如导航、跟随指令和回答问题。另一个代表性的仿真平台是VirtualHome（Puig等，2018年），支持在3D室内环境中进行对象操作的虚拟人偶。游戏领域的Carroll等人推出了“Overcooked-AI”——一个用于研究人类与AI之间协作任务的基准环境（Carroll等，2019年）。类似地，其他一些研究旨在超越代理与环境的互动，探索人类实际干预的方式（Puig等，2023年；Li等，2021年；Srivastava等，2022年）。</p>
<p>这些仿真器有助于代理和机器人互动的实际策略学习，以及利用人类示范操作的模仿学习策略。在某些场景下，学习策略的过程可能需要在仿真器中集成特定功能。例如，在学习基于图像的策略时，真实感渲染通常是适应实际环境的关键（Mittal等，2023年；Zhong等，2023年）。使用真实感渲染引擎可以生成反映不同条件（如光照环境）的图像。此外，仿真器需要使用物理引擎来模拟与物体的物理交互（Liu和Negrut，2021年）。研究表明，在仿真中集成物理引擎有助于获取可应用于真实场景的技能（Saito等，2023年）。</p>
<h2 id="生成型代理"><strong>5.4 生成型代理</strong></h2>
<p>大规模生成式AI模型领域的最新进展大大降低了当前互动内容制作的高成本和耗时，这不仅有利于大型游戏工作室，也为小型独立工作室带来了创造高质量体验的可能。此外，将大型AI模型嵌入沙盒环境中，将允许用户创造属于他们自己的体验并以目前难以实现的方式表达他们的创造力。</p>
<p>这些代理的目标不仅仅是为场景添加互动的3D内容，还包括以下几个方面：</p>
<blockquote>
<p>•为对象添加任意行为和交互规则，允许用户仅需最小提示即可创建自己的VR规则。</p>
<p>•通过使用多模态GPT4-v模型以及涉及视觉AI模型的其他模型链，从纸上的草图生成完整的关卡几何。</p>
<p>•使用扩散模型对场景中的内容重新着色。</p>
<p>•从简单的用户提示创建自定义着色器和视觉特效。</p>
</blockquote>
<p>短期内的一个潜在应用是虚拟现实（VR）故事板/原型工具的创建，允许单个用户以比当前可行速度快一个数量级的方式创建体验/游戏的粗略（但功能性）草图。之后，这样的原型可以使用这些工具进一步拓展和优化。</p>
<h3 id="arvr混合现实代理"><strong>5.4.1 AR/VR/混合现实代理</strong></h3>
<p>增强现实（AR）、虚拟现实（VR）和混合现实（XR）通常需要熟练的艺术家和动画师来创建角色、环境和对象，以用于虚拟世界的交互建模。这是一个昂贵的过程，涉及概念艺术、3D建模、纹理处理、绑定和动画制作。XR代理可以通过促进创作者之间的互动，并构建工具来帮助构建最终的虚拟环境，从而简化此过程。</p>
<p>我们的早期实验已经表明，即使没有任何额外的微调，GPT模型也可以在Unity引擎中的少量样例情况下使用引擎特定方法，调用API从互联网上下载3D模型并将其放置在场景中，还可以为其分配行为和动画的状态树。这种行为可能由于在开源游戏代码库中存在类似代码而自然涌现出来。因此，GPT模型可以根据简单的用户提示加载多个对象到场景中，从而构建丰富的视觉场景。</p>
<p>本类别代理的目标是构建一个平台和一套工具，为大型AI模型（包括GPT系列和扩散图像模型）和渲染引擎之间提供高效接口。我们在此探索两个主要方向：</p>
<blockquote>
<p>•将大模型集成到代理基础设施的各种编辑器工具中，从而显著加快开发速度。</p>
<p>•从用户体验中控制渲染引擎，通过生成符合用户指令的代码并在运行时编译，允许用户以任意方式编辑其交互的VR/模拟场景，甚至引入新的代理机制。</p>
</blockquote>
<p>引入专注于XR场景的AI副驾将对XR创作者非常有用，他们可以使用副驾完成一些繁琐的任务，如提供简单的素材或编写代码模板，从而让创作者能够专注于他们的创意愿景，并快速迭代出想法。此外，代理可以帮助用户通过添加新素材、改变环境动态或构建新场景来交互式地修改环境。这种在运行时动态生成的形式也可以由创作者指定，使用户的体验保持新鲜并随时间不断演变。</p>
<h2 id="知识和逻辑推理代理"><strong>5.5 知识和逻辑推理代理</strong></h2>
<p>推理和应用知识的能力是人类认知的一个显著特征，特别是在逻辑推理和理解“心智理论”等复杂任务中尤为明显。在知识上进行推理确保了AI的响应和行为与已知事实和逻辑原则一致。这种一致性是保持AI系统中信任和可靠性的关键机制，特别是在医疗诊断或法律分析等关键应用中。我们在这里介绍了一些将知识与推理相结合的代理，旨在处理特定的智能和推理方面。</p>
<h3 id="知识代理"><strong>5.5.1 知识代理</strong></h3>
<p>知识代理可以通过隐性和显性两个方向推理其获得的知识系统。隐性知识通常是通过在大量文本数据上训练的大规模语言模型（如GPT系列）所包含的知识，这些模型生成的响应给人以理解的印象，因为它们从训练过程中隐性学习到的模式和信息中汲取灵感。相反，显性知识是结构化的，可以直接查询，例如在知识库或数据库中找到的信息，传统上用于通过参考可验证的外部资源来增强AI推理能力。</p>
<p>尽管语言模型取得了进展，但其隐性知识是静态的，随着世界的变化而过时。这一局限性要求集成持续更新的显性知识源，以确保AI系统能够提供准确且最新的响应。隐性和显性知识的融合为AI代理提供了更细致的理解力和上下文应用能力，类似于人类智能。这种整合对于打造以知识为中心的AI代理至关重要，使其不仅具备信息，还能够理解、解释和应用这些信息，从而缩小广泛学习和深度知识之间的差距。这些代理被设计成可以灵活地利用关于世界的动态信息，以提高其稳健性和适应性。</p>
<h3 id="逻辑代理"><strong>5.5.2 逻辑代理</strong></h3>
<p>逻辑代理通常是系统中一个旨在应用逻辑推理处理数据或解决特定推理任务的组件。在大型基础模型（如GPT-4）中，逻辑代理是专门处理逻辑推理任务的子模块。这些任务通常涉及理解和操作抽象概念，从给定前提中推导结论或解决需要结构化、逻辑方法的问题。GPT-4等基础模型能够在训练数据的模式中进行逻辑推理，但并不遵循正式逻辑规则。它们的逻辑能力并非独立的逻辑代理，而是嵌入整体架构的一部分，基于数据模式生成响应。</p>
<p>例如，一种方法是在模型架构中嵌入独立的逻辑子模块，将文本解析为逻辑片段并在嵌入中显式建模逻辑层次结构，从而改进模型的逻辑推理能力。</p>
<h3 id="情感推理代理"><strong>5.5.3 情感推理代理</strong></h3>
<p>情感理解和共情在许多人与机器交互中至关重要。对话代理的一个重要目标是使其具备更强的情感和共情能力，同时减少不适当或冒犯性输出。例如，NICE数据集包含了近两百万张图片及其对应的人类生成的评论和情感注释，能够帮助生成更具情感和社会适当的评论。创建具备情感理解能力的代理是一个有前途的方向，并且重要的是要打造具备跨越多种群体和人群的情感理解能力的代理。</p>
<h3 id="神经符号代理"><strong>5.5.4 神经符号代理</strong></h3>
<p>神经符号代理通过神经和符号相结合的混合系统操作。在自然语言输入的情况下，问题的解决需要明确捕获输入中隐含的离散符号结构信息。神经符号代理使用结构化神经表示法，编码和解码过程结合符号和关系的绑定和解绑定。通过这种方式，代理能够捕获符号层面的知识并进行推理，提高对特定区域和精确推理的支持。</p>
<h2 id="llm和vlm代理"><strong>5.6 LLM和VLM代理</strong></h2>
<p>许多研究利用LLM作为代理来进行任务规划，并利用LLM的大规模领域知识和零样本规划能力来执行任务，例如规划和推理。最近的机器人研究也利用LLM进行任务规划，通过将自然语言指令分解为一系列子任务，并使用低级控制器执行这些子任务。此外，环境反馈的集成进一步改善了任务性能。最近的研究展示了通用视觉对齐的LLM通过在大规模文本、图像和视频数据上训练，可以成为创建多模态代理的基础，并在各种环境中执行任务。</p>
<h1 id="agent-ai-应用任务"><strong>6. Agent AI 应用任务</strong></h1>
<h2 id="游戏代理"><strong>6.1 游戏代理</strong></h2>
<p>游戏为测试 LLM 和 VLM
的代理行为提供了一个独特的沙盒环境，不断拓展它们的协作和决策能力的边界。以下三个领域突显了代理与人类玩家和其他代理互动的能力，以及在环境中采取有意义行动的能力。</p>
<h3 id="npc-行为"><strong>6.1.1 NPC 行为</strong></h3>
<p>在现代游戏系统中，非玩家角色 (NPC)
的行为主要由开发者编写的预定义脚本控制。这些脚本涵盖了一系列基于游戏环境中各种触发条件或玩家行为的反应和互动。然而，这种脚本化的特性往往导致
NPC
行为的可预测性和重复性，无法根据玩家的行为或游戏环境的动态变化进行调整。这种僵化性妨碍了在动态游戏环境中创造沉浸式体验的目的。因此，目前对使用
LLM 来提高 NPC
行为的自主性和适应性，进而使互动更具细腻感和吸引力的兴趣日益浓厚。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411349-4.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图8：具身代理用于用户互动式游戏动作预测和互动编辑，通过 GPT-4V 实现
Minecraft Dungeons 游戏场景的模拟与生成。</p>
<p>LLM
可以通过处理大量文本，学习模式并生成更加多样和类似人类的响应，显著推动
NPC 行为的发展。它们可以用于创建动态对话系统，使与 NPC
的互动更具吸引力和不可预测性。此外，LLM
可以根据玩家的反馈和游戏内数据不断改进 NPC
的行为，使其更加符合玩家的期望和游戏的动态。</p>
<h3 id="人类与-npc-互动"><strong>6.1.2 人类与 NPC 互动</strong></h3>
<p>人类玩家与 NPC
的互动是游戏体验的关键方面。传统的互动模式主要是单向的，NPC
以预设的方式对玩家输入作出反应。这种限制阻碍了更自然和丰富的互动，难以在虚拟世界中模拟人类之间的互动。LLM
和 VLM
技术的出现有望改变这一互动模式。通过运用这些技术，游戏系统可以分析并从人类行为中学习，以提供更类人化的互动。这不仅提升了游戏的真实感和参与度，还为在一个复杂但受控的环境中探索和理解人机互动提供了平台。</p>
<h3 id="基于代理的游戏分析"><strong>6.1.3
基于代理的游戏分析</strong></h3>
<p>游戏已成为日常生活中不可或缺的一部分，据估计吸引了全球约一半人口参与。此外，它对心理健康具有积极影响。然而，当今的游戏系统在与人类玩家的互动方面存在不足，其行为主要由游戏开发者手工设计，这些预编程的行为常常无法适应玩家的需求。因此，游戏领域亟需新的
AI
系统来分析玩家行为并在必要时提供适当支持。智能交互系统有潜力彻底改变玩家与游戏系统的互动方式。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411349-5.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图9：当提供“动作历史”和“游戏目标”作为提示时，GPT-4V
能有效预测高层次的下一个动作。此外，GPT-4V
准确识别出玩家手中拿着木材，并能将这一感知信息纳入未来动作规划中。尽管
GPT-4V
似乎具备预测某些低层次动作（如按下“E”键打开背包）的能力，但模型的输出并不完全适用于原始低层次动作预测（包括鼠标移动），可能需要额外的模块来控制低层次动作。</p>
<p>通过结合玩家互动与反馈、像素输入以及自然语言的规划和理解，代理模型可以帮助不断改进游戏动态，从而推动游戏环境更以玩家为中心的演进。</p>
<h3 id="游戏中的场景合成"><strong>6.1.4 游戏中的场景合成</strong></h3>
<p>场景合成是创建和增强沉浸式游戏环境的重要组成部分，涉及在游戏中自动或半自动生成三维
(3D)
场景和环境。此过程包括地形生成、物体放置、创建逼真的光照效果，有时还包括动态天气系统。</p>
<p>现代游戏通常包含广阔的开放世界环境。手动设计这些景观既耗时又耗费资源。自动地形生成通常使用程序化或
AI 驱动的技术，可以减少手工劳动，生成复杂逼真的景观。LLM 和 VLM
可以利用互联网规模的知识来制定规则，以设计出视觉效果令人惊叹且独特的非重复景观。此外，LLM
和 VLM 还可以确保生成的素材在语义上的一致性和多样性。</p>
<p>在场景中放置建筑物、植被和其他元素，以一种现实且美观的方式布置是增加沉浸感的关键。通过使用
LLM 和
VLM，开发人员可以自动化场景元素的布置，使得生成的场景更加逼真，为玩家提供更具沉浸感的体验。这些技术有助于将对象自然地整合到虚拟环境中，从而创造出具有深度和吸引力的游戏世界。</p>
<h3 id="对象放置和照明效果"><strong>6.1.4
对象放置和照明效果</strong></h3>
<p>VLM 和 LLM
可以通过遵循预定义或学习的规则和美学原则来辅助对象放置，从而加速关卡设计过程。它们还可以被进一步训练以理解设计和美学的原理，从而帮助进行程序化内容生成。它们可以帮助制定程序算法遵循的规则或指导方针，以生成既符合视觉吸引力又具有情境适用性的对象和场景。</p>
<p>逼真的光照和气氛效果对于创建可信且吸引人的游戏环境至关重要。高级算法可以模拟自然光照条件和动态天气效果，增强场景的真实感和氛围。LLM
可以帮助开发系统，从多个创新角度实现更逼真的光照和气氛效果。VLM
可以分析大量真实世界的光照和大气条件数据集，以帮助开发更逼真的算法，从而在游戏中模拟这些效果。通过了解自然光照和天气的模式和细微之处，这些模型可以推动模拟逼真效果的算法的开发。此外，LLM
和 VLM
还可以用于开发实时调整光照和气氛效果的系统，基于玩家的行为、游戏状态或外部输入进行调整，从而提供更具互动性和沉浸感的体验。</p>
<h3 id="实验与结果"><strong>6.1.5 实验与结果</strong></h3>
<p>零样本/少样本学习使用 LLM 或 VLM。如图 8 和图 9 所示，我们使用 GPT-4V
进行了高级描述和动作预测。图 8 展示了一些使用 GPT-4V
进行动作描述生成和编辑的定性示例。增强文本生成提供了一种新方法，通过使用游戏动作先验生成3D场景，从而提高场景的自然性。因此，GPT-4V
可以为游戏视频生成相关的高级描述，这些描述与游戏内容高度匹配并适当。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411349-6.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图10：在未见过的 Minecraft
视频上进行的遮罩视频预测。从左到右依次为：原始帧、遮罩帧、重构帧和带有补丁的重构帧。</p>
<p>小型代理预训练模型。为了展示我们的代理视觉-语言架构，我们首先通过在
Minecraft 数据上预训练来研究其在游戏代理中的应用。如图 7
所示，给定输入动作代理、视频关键帧和相应文本，标准的编码器-解码器可以将代理动作和图像转换为动作文本标记和图像块标记，然后使用代理-视觉-语言解码器将其转换为动作预测句子。整体架构如图
7 所示。我们通过多个 Minecraft 示例对我们的方法进行了评估。Minecraft
视频数据包含 5 分钟的剪辑，用于预训练的数据包含 78,000
个视频，我们使用了其中的 5,000 个视频（预训练数据的
6%）进行第一轮预训练。我们在 16 个 NVIDIA V100 GPU 上训练了一个 2.5
亿参数的模型，训练时间为一天，并在图 10 和图 11 中展示了模型的输出。图
10 表明我们的小型代理架构可以对训练过程中未见过的 Minecraft
场景生成合理的输出。图 11
展示了模型的预测与人类玩家的真实操作对比，显示出该小型代理模型具有潜在的低层次理解能力。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411349-7.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图11：使用小型代理预训练模型在 Minecraft
游戏场景中进行低层次的下一步动作预测。</p>
<p>多代理基础设施。如图 5
所示的代理范式中，我们设计了一种新型基础设施用于一个名为“CuisineWorld”（Gong
et al., 2023a）的新游戏场景。我们的方法详细展示在图 12 中。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411349-8.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图12：MindAgent
的游戏环境中上下文学习基础设施。规划技能与工具使用：游戏环境需要多样化的规划技能和工具使用来完成任务。系统生成相关的游戏信息，并将游戏数据转换为
LLM
可处理的结构化文本格式。LLM：作为基础设施的主要执行者，负责决策，充当多代理系统的调度器。记忆历史：用于存储相关信息的实用工具。动作模块：从文本输入中提取动作，并将其转换为特定领域的语言，验证
DSL（领域特定语言），以确保在执行过程中不出错。</p>
<p>该基础设施通过将 GPT-4
用作中央规划器，实现了跨多个游戏领域的多代理协作。我们对系统的多代理规划能力进行了研究，并将该基础设施部署到真实视频游戏中，以展示其在多代理和人机协作中的有效性。此外，我们还提出了“CuisineWorld”，一个基于文本的多代理协作基准，提供了一个新的自动指标协作评分（CoS）来量化协作效率。有关游戏描述、高级动作预测和
GPT-4V 提示的更多示例和详细信息，请参见附录。我们在图 32 和附录 B
中展示了《Bleeding Edge》的示例，图 33 和附录 C 展示了《Microsoft Flight
Simulator》，图 34 和附录 D 展示了《ASSASSIN’s CREED ODYSSEY》，图 35
和附录 E 展示了《GEARS of WAR 4》，图 36 和附录 F
展示了《Starfield》。我们还在附录 A 的图 31 中提供了用于生成 Minecraft
示例的 GPT-4V 提示过程的详细截图。</p>
<h2 id="机器人"><strong>6.2 机器人</strong></h2>
<p>机器人是典型的需要与环境进行有效交互的代理。在本节中，我们将介绍实现高效机器人操作的关键要素，回顾应用了最新LLM（大语言模型）/VLM（视觉语言模型）技术的研究课题，并分享我们最新研究的发现。</p>
<p><strong>视觉运动控制。</strong>视觉运动控制指的是在机器人系统中将视觉感知与运动操作结合，以有效执行任务。这种整合对于机器人解释环境中的视觉数据并据此调整其运动操作与环境准确互动至关重要。例如，在装配线上，配备视觉运动控制的机器人可以感知物体的位置和方向，并准确地调整其机械手与这些物体进行交互。这种能力对于确保机器人操作在各种应用中（从工业自动化到辅助老年人日常事务）具有精确性和有效性是必不可少的。此外，视觉运动控制使机器人能够适应动态环境，当环境状态可能迅速变化时，需要根据视觉反馈实时调整运动操作。</p>
<p>此外，在安全操作的背景下，视觉信息对于检测执行错误以及确认每个机器人动作的前提和后置条件至关重要。在不受控制的环境中（例如未知的家庭环境），机器人可能会由于家具形状变化、光照变化、物体滑动等不可预测的因素而遇到意外结果。在这种环境中，单纯依靠预先规划的前馈动作计划可能带来显著风险。因此，通过视觉反馈在每个步骤持续验证结果，是确保机器人系统稳健和可靠运行的关键。</p>
<p><strong>基于语言的条件操作。</strong>基于语言的条件操作指的是机器人系统能够理解并根据语言指令执行任务的能力。这在创建直观且用户友好的人机交互界面方面尤为重要。通过自然语言指令，用户可以用类似人类之间交流的方式为机器人指定目标和任务，从而降低了操作机器人系统的难度。例如，用户可以指示服务机器人“从桌子上拿起那个红苹果”，机器人会解析这一指令，识别目标物体并执行拿起任务（Wake等人，2023年）。开发稳健的自然语言处理和理解算法，以准确解读从直接命令到更抽象指令的多种指令，并将这些指令转换为可执行的任务，是该领域的核心挑战。此外，确保机器人能够在不同任务和环境中泛化这些指令，对于提高其在现实世界应用中的多功能性和实用性至关重要。使用语言输入来指导机器人任务规划在任务和运动规划（Task
and Motion Planning, TAMP）框架中受到了关注（Garrett等人，2021年）。</p>
<p><strong>技能优化。</strong>最新研究显示LLM在机器人任务规划中表现出高效性。然而，任务的最佳执行，尤其是涉及物理交互（如抓取）的任务，需要对环境有更深入的理解，超越了单纯解读人类指令的范围。例如，机器人抓取任务需要精确的接触点（Wake等人，2023年）和手臂姿势（Sasabuchi等人，2021年），以便高效地执行后续操作。尽管互联网规模的VLM取得了进展，但从场景中捕捉这些间接的细微线索并有效转化为机器人技能，仍然是一个重要的挑战。为应对这一问题，机器人领域越来越关注增强数据集的收集（如Wang等人，2023年；Padalkar等人，2023年），或开发从人类示范中直接获取技能的方法（Wake等人，2021年）。学习示范（Learning-from-Demonstration）和模仿学习（Imitation
Learning）框架在物理技能优化方面发挥着关键作用。</p>
<h3 id="用于机器人领域的llmvlm代理"><strong>6.2.1
用于机器人领域的LLM/VLM代理</strong></h3>
<p>最新研究展示了LLM/VLM在机器人代理中与人类环境交互的潜力。以下是一些旨在利用最新LLM/VLM技术的研究主题：</p>
<p><strong>多模态系统。</strong>最近的研究集中在开发将最新的LLM和VLM技术作为输入信息编码器的端到端系统，尤其是对这些基础模型进行修改以处理多模态信息（Jiang等人，2022；Brohan等人，2023，2022；Li等人，2023d；Ahn等人，2022b；Shah等人，2023b；Li等人，2023e）。这一改进旨在基于语言指令和视觉线索引导机器人动作，从而实现高效的身体化智能。</p>
<p><strong>任务规划与技能训练。</strong>与端到端系统不同，任务和运动规划（TAMP）系统首先计算高级任务计划，然后通过低级别的机器人控制（称为技能）来实现这些计划。LLM的高级语言处理能力展示了将指令解析并分解成机器人动作步骤的能力，极大地推动了任务规划技术的发展（Ni等人，2023；Li等人，2023b；Parakh等人，2023；Wake等人，2023c）。对于技能训练，已有研究探索了使用LLM/VLM设计奖励函数（Yu等人，2023a；Katara等人，2023；Ma等人，2023）、生成数据以支持策略学习（Kumar等人，2023；Du等人，2023）或作为奖励函数的一部分（Sontakke等人，2023）。结合RL和IL等训练框架，这些研究将有助于高效机器人控制器的开发。</p>
<p><strong>现场优化。</strong>由于环境条件的不可预见性和不确定性，机器人执行长任务步骤时可能遇到困难。因此，机器人领域的一个重要挑战是通过将任务计划与实时环境数据相结合，动态调整和改进机器人技能。例如，Ahn等人（2022b）提出了一种方法，通过视觉信息计算动作的可行性（即可供性），并将其与计划任务进行对比。此外，还有一些方法关注于使LLM输出任务步骤的前提条件和后置条件（如对象状态及其相互关系），以优化其执行过程（Zhou等人，2023c）并检测前提条件错误以便对任务计划进行必要的修订（Raman等人，2023）。这些策略旨在通过整合环境信息并在任务计划或控制器层面调整机器人的动作，实现基于环境的机器人执行。</p>
<p><strong>对话代理。</strong>在创建对话机器人方面，LLM可以促进与人类的自然、上下文敏感的交互（Ye等人，2023a；Wake等人，2023f）。这些模型能够处理和生成类似人类对话的响应，使机器人能够参与有意义的对话。此外，LLM在评估话语的概念（Hensel等人，2023；Teshima等人，2022）和情感属性（Zhao等人，2023；Yang等人，2023b；Wake等人，2023d）方面也发挥了重要作用。这些属性有助于理解人类意图并生成有意义的手势，从而增强了人机通信的自然性和效果。</p>
<p><strong>导航代理。</strong>机器人导航有着悠久的研究历史，重点是基于地图的路径规划和同时定位与地图构建（SLAM）等核心方面，以创建环境地图。这些功能已经成为广泛使用的机器人中间件（如机器人操作系统ROS）中的标准（Guimarães等人，2016）。</p>
<p>虽然传统的导航技术在许多机器人应用中仍然很常见，但它们通常依赖于静态或预创建的地图。最近，在更加挑战性环境中使用先进技术使机器人导航的兴趣有所增加，利用了计算机视觉和自然语言处理等领域的突破。一个典型任务是对象导航（Chaplot等人，2020a；Batra等人，2020；Gervet等人，2023；Ramakrishnan等人，2022；Zhang等人，2021），其中机器人使用对象名称进行导航而不是地图坐标，要求将对象名称与环境中的视觉信息对接。此外，近来对零样本对象导航（zero-shot
object
navigation）技术的关注有所增加，这种技术基于基础模型，使机器人在完全陌生的新环境中进行导航（Gadre等人，2023；Dorbala等人，2023；Cai等人，2023）。此外，视觉语言导航（VLN）也是一个代表性任务，其中任务涉及通过自然语言指令在以前未见过的现实环境中引导代理进行导航（Shah等人，2023a；Zhou等人，2023a；Dorbala等人，2022；Liang等人，2023；Huang等人，2023b）。VLN解析句子而不是对象名称，例如“去你左边的浴室。”因此，它需要更高的功能来解析输入文本（Wang等人，2019）。基础模型的出现有助于这些自适应、即时导航技术的发展，通过增强对人类语言指令的理解和环境信息的视觉解读。更详细的代表性VLN研究说明见6.2.2。</p>
<h3 id="实验与结果-1"><strong>6.2.2 实验与结果</strong></h3>
<p>大量证据表明，近期的视觉语言模型（VLM）和大语言模型（LLM）在符号任务规划（例如“要做什么”）方面具有良好的潜力。然而，要实现与环境的成功交互，每个任务还需要低级控制策略（例如“如何做”）。虽然强化学习和模仿学习是通过数据驱动来学习策略的有效方法，另一种前景广阔的方法是通过现场演示直接从人类获取策略，这种方法称为“观察学习”（Wake等，2021a；Ikeuchi等）。在本节中，我们介绍了一项研究，该研究使用ChatGPT进行任务规划，并通过提供可供性信息参数化任务计划，以便实现更有效和精确的执行（见图13）。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411349-9.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图13：集成了 ChatGPT
驱动的任务规划器的机器人教学系统概览。该过程包括两个步骤：任务规划，用户使用任务规划器创建动作序列，并根据需要通过反馈调整结果；演示，用户视觉演示动作序列，为机器人操作提供所需信息。视觉系统收集用于机器人执行的视觉参数。</p>
<p>该流程由两个模块组成：任务规划和参数化。在任务规划阶段，系统接收语言指令和工作环境的描述。这些指令连同预定义的机器人动作和输出规格一起，形成一个提供给ChatGPT的综合提示，ChatGPT随后生成一系列分解的任务及其文本描述（图13左侧面板）。值得注意的是，我们采用了少样本方法，这意味着ChatGPT并未专门为此任务训练，具备无需硬件依赖的数据采集和模型训练的优势。此外，输出中的文本描述使用户能够检查并在必要时调整结果，这对于安全可靠的操作至关重要。</p>
<p>图14展示了在VirtualHome（Puig等，2018）上进行的代理模拟实验的定性结果。结果显示了任务计划的合理性及其输出调整的灵活性，表明了该方法的广泛适用性。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411349-10.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图
14：通过自动生成的反馈调整输出序列的示例。我们在实验中使用了开源的模拟器
VirtualHome。给定指令“拿起桌上的派并用炉子加热它。”任务规划器会规划出
VirtualHome
中提供的一系列功能。如果在执行过程中检测到错误，任务规划器会根据自动生成的错误消息来纠正其输出。</p>
<p>尽管任务规划确保了任务序列的连贯性，但在现实操作中仍需详细的参数。例如，抓取类型对于保持容器内容不洒出非常重要，而在模拟器中通常忽略了这类参数（见图14中的派抓取示例）。因此，在我们的机器人系统中，用户需要视觉演示每个动作（图13右侧面板）。任务中预定义了执行所需的参数，视觉系统则从视频中提取这些参数（Wake等，2021b）。值得注意的是，我们的机器人系统并非设计用于精确复制人类的动作（即远程操作），而是为了应对现实世界中对象位置变化等各种情况。因此，从人类演示中提取的参数不仅是精确的运动路径，更是用于指导有效环境交互的可供性信息（例如碰撞规避的路径点（Wake等，2023a）、抓取类型（Wake等，2023e）以及上肢姿态（Sasabuchi等，2021；Wake等，2021a））。上肢姿态对于高自由度的机器人至关重要，旨在为共存的人类提供可预测的姿态。带有可供性的任务序列最终转化为通过强化学习获得的可复用机器人技能，并由机器人执行（Takamatsu等，2022）。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411349-11.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 15：多模态任务规划器的概览，该规划器利用了 GPT-4V 和
GPT-4。该系统处理视频演示和文本指令，生成用于机器人执行的任务计划。</p>
<p>通过与VLM集成，基于LLM的任务规划可以扩展为更通用的机器人系统。这里我们展示了一个示例，使用GPT-4V(ision)扩展上述任务规划器，以支持多模态输入（图15），其中人类执行的动作旨在由机器人复现。在本文中，仅显示了部分提示，完整提示可在microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts上获取。该流程接收演示视频和文本，并输出一系列机器人动作。视觉分析器旨在理解视频中人类执行的动作。我们使用GPT-4V并提供了提示，以生成类似人类沟通风格的文本指令。图16展示了文本输入如何允许用户对GPT-4V的识别结果进行反馈，以便修正识别结果的准确性并增强操作的可靠性。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-12.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 16：视频分析器输出示例。五帧图像按固定间隔提取并输入
GPT-4V。整个流程在第 6.2.2 节中描述。</p>
<p>接下来，场景分析器基于指令和视频数据的首帧（或环境图像）将预期的工作环境编译为文本信息。此环境信息包括GPT-4V识别的对象名称列表、对象的可抓取特性及对象之间的空间关系。尽管这些计算过程在GPT-4V中是一个黑箱，但输出的信息基于GPT-4V的知识和图像/文本输入。图17展示了场景分析器的示例输出。如图所示，当人类将罐头容器放置在桌上时，GPT-4V成功选择了与操作相关的对象（例如桌子）；而在冰箱开启任务中则忽略了桌子。这些结果表明，场景分析器根据人类的动作对场景信息进行编码。我们提示GPT-4V解释对象选择过程的结果和原因。在实践中，这种方法产生了合理的输出。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-13.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 17：场景分析器输出示例，使用了 GPT-4V。我们在第 6.2.2
节中描述了整个流程。</p>
<p><strong>面向机器人导航的具身代理。</strong>视觉语言导航（VLN）指的是让具身代理在真实的3D环境中执行自然语言指令的导航任务。在3D环境中的导航（Zhu等人，2017a；Mirowski等人，2016；Mousavian等人，2018；Hemachandra等人，2015）是一个移动智能系统在物理世界中执行任务的关键能力。在过去几年中，提出了大量任务和评估协议（Savva等人，2017；Kolve等人，2017；Song等人，2017；Xia等人，2018；Anderson等人，2018a），并在（Anderson等人，2018b）中进行了总结。VLN（Anderson等人，2018a）专注于真实3D环境中的语言引导导航。</p>
<p>为了解决VLN任务，Anderson等人（2018a）建立了一个基于注意力的序列到序列基准模型。随后，Wang等人（2018）提出了一个混合方法，结合了无模型和基于模型的强化学习（RL），以提高模型的泛化能力。最后，Fried等人（2018）提出了一种“说者-跟随者”模型，该模型采用数据增强、全景动作空间和修改后的束搜索来实现VLN，在Room-to-Room数据集上确立了当前的最佳性能。</p>
<p>在前人的工作基础上，我们在Wang等人（2019）中提出了用于VLN的强化跨模态匹配（RCM）方法。RCM模型基于Fried等人（2018）的模型，但在多个重要方面有所不同：（1）RCM结合了一种新颖的多重奖励RL与模仿学习，而“说者-跟随者”模型（Fried等人，2018）仅使用监督学习（与Anderson等人，2018a相同）；（2）RCM推理导航器执行跨模态的场景理解，而不是在单一模态输入上使用时间注意力机制；（3）RCM匹配评估器在架构设计上与说者类似，但前者用于在RL和SIL训练中提供循环重建内在奖励，而后者用于增强监督学习的训练数据。</p>
<p>在Wang等人（2019）的研究中，我们探讨了如何解决该任务的三个关键挑战：跨模态的场景理解、不良反馈以及泛化问题。正如图18所示，我们提出了一种新颖的强化跨模态匹配方法，通过强化学习在局部和全局范围内加强跨模态的匹配。特别地，匹配评估器用于提供内在奖励，以鼓励指令和轨迹之间的全局匹配，而推理导航器则在局部视觉场景中执行跨模态的场景理解。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-14.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 18：用于 VLN 任务（Wang 等,
2019）的嵌入式智能体演示。展示了指令、局部视觉场景和俯视视角中的全局路径。智能体无法访问俯视视角。路径
A 是按照指令的演示路径，路径 B 和 C 是智能体执行的两条不同路径。</p>
<p>在VLN基准数据集上的评估显示，我们的RCM模型在SPL指标上比之前的方法提升了10%，并达到了新的最先进性能。为了提高学习策略的泛化能力，我们进一步引入了一种自监督模仿学习（SIL）方法，通过模仿其过去的良好决策来探索未知环境。结果表明，SIL可以近似一个更好且更高效的策略，从而大大缩小了在已知环境和未知环境之间的成功率表现差距（从30.7%降至11.7%）。</p>
<p>此外，在Wang等人（2019）的研究中，我们引入了一种自监督模仿学习方法用于探索，以明确应对泛化问题，这是以前的研究中未得到充分研究的问题。同时，Thomason等人（2018）、Ke等人（2019）和Ma等人（2019a,
b）从多个方面研究了VLN任务，而Nguyen等人（2018）引入了VLN任务的一个变体，以在需要时通过请求语言协助来寻找对象。值得注意的是，我们是首个提出在VLN任务中探索未知环境的研究。</p>
<h2 id="医疗保健领域"><strong>6.3 医疗保健领域</strong></h2>
<p>在医疗保健领域，大型语言模型（LLMs）和视觉语言模型（VLMs）可以作为诊断代理、患者护理助手，甚至是辅助治疗工具，但它们也伴随着独特的挑战和责任。AI代理在提高患者护理质量和拯救生命方面具有巨大潜力，但其不当使用或匆忙部署也可能对数千乃至数百万人的生命安全构成威胁。我们将探讨AI代理在医疗保健领域的几种有前景的应用路径，并讨论面临的关键挑战。</p>
<p><strong>诊断代理。</strong>近年来，使用LLMs作为医疗聊天机器人进行患者诊断引起了极大关注，因为对医学专家的需求很高，而LLMs有潜力帮助进行分诊和诊断（Lee等人，2023）。对话代理，尤其是那些能够有效地向不同患者群体传达重要医疗信息的代理，有潜力为历来处于不利地位或边缘化的群体提供公平的医疗服务。此外，全球各地的医生和医疗系统普遍负担过重、资源不足，导致数亿人无法获得足够的医疗服务（世界卫生组织和世界银行，2015）。诊断代理为改善数百万人的医疗健康提供了特别有利的途径，因为它们可以理解多种语言、文化和健康状况。初步研究表明，可以利用大规模网络数据训练具备医疗知识的LLMs（Li等人，2023f）。尽管这一方向令人期待，但诊断代理的应用也伴随着风险。我们在下文中着重介绍了在医疗环境中出现“幻觉”现象的风险及其潜在的解决方案。</p>
<p><strong>知识检索代理。</strong>在医疗环境中，模型的“幻觉”现象特别危险，可能导致严重的患者伤害甚至死亡，具体取决于错误的严重程度。例如，如果患者错误地被告知他们没有某种实际存在的疾病，可能导致灾难性后果，包括延迟或不当治疗，甚至完全缺乏必要的医疗干预。未被诊断或被误诊的疾病可能导致医疗费用增加、治疗周期延长、身体额外负担增加，甚至在极端情况下造成严重的伤害或死亡。因此，采用能够更可靠地检索知识的代理（Peng等人，2023）或基于检索生成文本的代理（Guu等人，2020）是一个有前景的方向。将诊断代理与医疗知识检索代理配对，有望显著减少幻觉现象，同时提高诊断对话代理的回答质量和准确性。</p>
<p><strong>远程医疗和远程监控。</strong>基于代理的AI在远程医疗和远程监控领域也具有巨大潜力，可以改善医疗获取、增进医护人员与患者的沟通效率，并减少频繁的医患互动成本（Amjad等人，2023）。初级保健医生花费大量时间筛选患者信息、报告和邮件，其中许多信息对于他们来说是不必要的或无关紧要的。支持代理可以帮助分诊来自医生、患者和其他医护人员的消息，并帮助突出所有相关方的重要信息。通过让智能AI系统协调患者、医生和其他AI代理，有望彻底变革远程医疗和数字健康行业。</p>
<h3 id="当前医疗能力"><strong>6.3.1 当前医疗能力</strong></h3>
<p><strong>图像理解。</strong>我们展示了现代多模态代理（如GPT-4V）在医疗保健中的当前能力和局限性（如图19所示）。可以看出，尽管GPT-4V具备医院护理所需设备和流程的显著内部知识，但它不总是能够对用户的具体诊断查询做出精确回应。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-15.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 19：在医疗图像理解领域中使用 GPT-4V
的示例提示和响应。从左到右依次为：(1) 护士和医生进行 CT 扫描的图像，(2)
不规则心电图的合成图像，(3) 来自 ISIC（Codella 等,
2018）皮肤病变数据集的图像。可以看出，GPT-4V
具备显著的医学知识，能够对医学图像进行推理。然而，由于安全性训练，GPT-4V
无法对某些医学图像做出诊断。</p>
<p><strong>视频理解。</strong>我们在两个情境下探讨了VLM代理在医学视频理解方面的表现。首先，我们研究了VLM代理在临床环境中识别重要患者护理活动的能力。其次，我们探索了VLM在处理更具技术性的医学视频（如超声视频）中的应用。具体来说，在图20中，我们展示了GPT-4V在医院护理和医学视频分析方面的当前能力和局限性。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-16.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 20：在医疗视频理解领域中使用 GPT-4V
的示例提示和响应。我们将示例视频输入为带有顺序文本的 2x2
网格帧。在前两个示例中，我们提示 GPT-4V
检查视频帧，以检测对志愿患者进行的临床床边活动。在最后一个示例中，我们尝试提示
GPT-4V 评估心脏超声波视频，但由于 GPT-4V
的安全性训练，它未提供详细响应。为清晰起见，我们将描述主要活动的文本加粗，并简化不必要的模型响应。同时，我们对个体的面部进行了灰度处理，以保护其隐私。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-17.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图
21：交互式多模态智能体包含四个主要支柱：交互、语音、视觉和语言。协同智能体由不同的服务组成。1）交互服务帮助创建用于自动化操作、认知和决策的统一平台。2）音频服务将音频和语音处理集成到应用程序和服务中。3）视觉服务识别并分析图像、视频和数字墨水中的内容。4）语言服务从结构化和非结构化文本中提取意义。</p>
<h2 id="多模态代理"><strong>6.4 多模态代理</strong></h2>
<p>视觉和语言理解的整合对于开发先进的多模态AI代理至关重要。这包括图像描述、视觉问答、视频语言生成和视频理解等任务。我们致力于深入探讨这些视觉-语言任务，探索它们在AI代理环境中的挑战和机遇。</p>
<h3 id="图像-语言理解与生成"><strong>6.4.1
图像-语言理解与生成</strong></h3>
<p>图像-语言理解是一项任务，涉及通过语言解释给定图像中的视觉内容，并生成相关的语言描述。这项任务对开发能够更人性化地与世界互动的AI代理至关重要。一些常见的任务包括图像描述（Lin等人，2014；Sharma等人，2018；Young等人，2014；Krishna等人，2016）、指称表达（Yu等人，2016；Karpathy等人，2014）和视觉问答（Antol等人，2015；Ren等人，2015；Singh等人，2019）。</p>
<p>最近，知识密集型视觉问答任务（如OKVQA、KBVQA、FVQA和WebQA）被提出。多模态代理应能够识别图像中的物体，理解它们的空间关系，生成关于场景的准确描述性句子，并利用推理能力处理知识密集型的视觉推理。这不仅需要物体识别能力，还需要对空间关系、视觉语义的深刻理解，以及将这些视觉元素与世界知识相结合并映射到语言结构的能力。</p>
<h3 id="视频和语言的理解与生成"><strong>6.4.2
视频和语言的理解与生成</strong></h3>
<p><strong>视频语言生成。</strong>视频字幕或视频叙事是一项生成视频帧序列的连贯句子的任务。受益于在视频和语言任务中广泛使用的循环大型基础模型，基于代理增强的模型在视频语言生成任务中展现了良好效果。主要挑战在于神经编码-解码模型的强性能在视觉叙事任务上难以推广，因为这项任务需要全面理解每帧的内容以及帧与帧之间的关系。该领域的一个重要目标是创建一个能够有效编码帧序列并生成主题连贯多句段落的代理感知文本合成模型。</p>
<p><strong>视频理解。</strong>视频理解将图像理解的范围扩展到动态视觉内容。这涉及对视频帧序列的解释和推理，通常伴有音频或文本信息。代理应该能够与视觉、文本和音频多种模态进行互动，以展示其对视频内容的高级理解。该领域的任务包括视频字幕、视频问答和活动识别等。视频理解的挑战多种多样，包括视觉和语言内容的时间对齐、处理长帧序列以及对随时间展开的复杂活动的解读。在音频方面，代理能够处理口头语言、背景噪音、音乐和语音语气，以理解视频内容的情绪、背景和细微差别。</p>
<p>以往的研究主要利用现有的在线视频语言训练数据来建立视频基础模型。支持这些训练流程和功能非常困难，因为这些数据集通常有限且不一致。视频基础模型设计了掩码和对比预训练目标，并在后续任务中进行微调。尽管在多模态基准测试中展示出卓越的结果，但这些模型在仅限视频的任务（如动作识别）中面临困难，原因在于其依赖有限的视频-文本数据，这些数据通常由嘈杂的音频转录而来。这一局限性也导致模型缺乏大型语言模型通常具备的稳健性和细粒度推理能力。</p>
<p>类似于图像-语言理解的方法，其他方法也借鉴了大型语言模型的强大推理能力和广泛知识，以改进视频解读的不同方面。通过像ChatGPT和GPT-4这样的语言模型或GPT-4V这样的图像-语言模型将音频、视频和语言模态视为单独的可解释输入数据类型，从而简化视频理解任务。例如，（Huang等人，2023c；Li等人，2023g）通过使用开源视觉分类/检测/描述模型将视频内容文本化，将视频理解任务转换为自然语言处理问答格式。（Lin等人，2023）整合了GPT-4V与视觉、音频和语音的专用工具，以便于复杂的视频理解任务，如为长视频中的角色动作和行为编写脚本。</p>
<p>并行研究探索了从大型模型生成的扩展数据集，然后在生成的数据集上进行视觉指令微调。随后使用了大量音频、语音和视觉感知专家模型将视频语言化。语音通过自动语音识别工具转录，视频描述和相关数据则通过各种标记、定位和字幕模型生成。这些技术展示了在生成数据集上进行视频语言模型指令微调可能提升视频推理能力。</p>
<h3 id="实验与结果-2"><strong>6.4.3 实验与结果</strong></h3>
<p><strong>•知识密集型模型：</strong>如INK（Park等人，2022）和KAT（Gui等人，2022a）所介绍的，这是一个集成了人工注释所需知识的密集神经知识任务，用于支持知识密集型检索任务。</p>
<p><strong>•多模态代理：</strong>多模态语言模型如Chameleon（Lu等人，2023）和MM-React（Yang等人，2023c）引起了越来越多的关注。</p>
<p><strong>•视觉指令微调：</strong>VCL（Gui等人，2022b）、Mini-GPT4（Zhu等人，2023）、MPLUG-OWL（Ye等人，2023b）、LSKD（Park等人，2023c）生成图像级别的指令微调数据集。</p>
<p><strong>知识密集型代理。</strong>如图22和图23所示，基于知识的视觉问答和视觉-语言检索任务是多模态机器学习中的一项挑战性任务，这些任务需要超越图像内容的外部知识。最近关于大型Transformer的研究主要聚焦于通过优化模型参数来最大化信息存储的效率。这类研究探索了一个不同的方面：多模态Transformer是否可以在决策过程中使用显性知识。基于Transformer的预训练方法在多模态知识表示的隐性学习方面取得了显著成功。然而，传统方法主要是单模态的，研究了知识检索和后续的答案预测，但这引发了关于所检索知识的质量和相关性的问题，以及如何利用隐性和显性知识整合推理过程的问题。为了解决这些问题，我们引入了知识增强Transformer（KAT），在2022年的OK-VQA开放领域多模态任务中比其他方法提高了6%的性能。KAT结合了来自GPT-3的隐性知识和来自网站的显性知识，使用编码器-解码器结构，允许在答案生成过程中并行使用这两种知识类型进行推理。此外，整合显性知识提高了模型预测的可解释性。代码和预训练模型可在
https://github.com/guilk/KAT上获得。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-18.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 22：密集神经知识（INK）任务的示例（Park 等,
2022），该任务使用知识从一组文本候选中识别与图像相关的文本。我们的任务涉及利用从网络和人工标注知识中检索到的视觉和文本知识。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-19.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 23：KAT 模型（Gui 等,
2022a）使用基于对比学习的模块从显性知识库中检索知识条目，并使用 GPT-3
检索带有支持证据的隐性知识。知识整合由相应的编码器 Transformer
处理，并通过端到端训练与推理模块和解码器 Transformer 一起生成答案。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-20.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 24：VLC 模型（Gui 等,
2022b）的整体架构。我们的模型由三个模块组成：(1)
特定模态投影。我们使用简单的线性投影来嵌入分块图像，并使用词嵌入层来嵌入分词后的文本；(2)
多模态编码器。我们使用一个 12 层的 ViT（Dosovitskiy 等, 2021），该模型从
MAE（He 等, 2022）（无标签的
ImageNet-1K）初始化，作为我们的主干网络；(3)
特定任务解码器。我们通过遮掩图像/语言建模和图像-文本匹配来学习多模态表示，这些任务仅用于预训练。在微调多模态编码器以进行下游任务时，我们使用
2 层
MLP。重要的是，我们发现遮掩图像建模目标在第二阶段预训练过程中非常重要，不仅用于视觉
Transformer 的初始化。</p>
<p><strong>视觉-语言Transformer代理。</strong>接下来，我们介绍了“从标题中训练视觉-语言Transformer”（VLC）模型（Gui等人，2022b），这是一个仅通过图像-标题对进行预训练的Transformer。尽管VLC只使用了一个简单的线性投影层来进行图像嵌入，但在多种视觉-语言任务中获得了具有竞争力的结果，与依赖对象检测器或监督CNN/ViT网络的方法不同。通过广泛的分析，我们探索了VLC作为视觉-语言Transformer代理的潜力。例如，我们展示了VLC的视觉表示在ImageNet-1K分类中表现出高度有效，并且我们的可视化验证了VLC可以准确地将图像区域匹配到相应的文本标记。随着训练数据的增多，VLC的表现的可扩展性显示了开发大规模、弱监督、开放领域视觉-语言模型的潜力。</p>
<h2 id="视频-语言实验"><strong>6.5 视频-语言实验</strong></h2>
<p>为了评估将预训练的图像LLM（大型语言模型）用于视频理解的实用性，我们在视频字幕生成任务中对InstructBLIP
(Dai等人, 2023)进行时间扩展和微调。具体而言，我们使用与Frozen in Time
(Bain等人,
2021)相同的分割空间-时间注意力机制，扩展了InstructBLIP的视觉编码器（EVA-CLIP-G
(Sun等人, 2023b)），并在训练过程中保持Q-former和LLM（Flan-T5-XL
(Chung等人,
2022)）冻结。我们冻结了视觉编码器的所有空间层，但在字幕生成训练期间保留了时间层未冻结状态。这使得我们的模型能够接收图像和视频作为输入（与InstructBLIP在图像级别的性能一致）。我们在WebVid10M
(Bain等人,
2021)的500万视频-字幕子集上进行了训练。图25中展示了两个示例输出。然而，现有的代理在完全理解视频内容中的精确和细粒度的视觉细节方面仍存在不足。视觉指令微调方法也存在类似的局限性，它们缺乏通用的、接近人类水平的感知能力，这仍需通过多模态模型和代理来解决。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-21.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 25：使用视频微调版本的 InstructBLIP 的示例提示和响应（方法见第 6.5
节）。我们的模型能够生成描述场景的长文本响应，并能够回答与视频中事件时间顺序相关的问题。</p>
<p>指令微调的模型在准确总结视频中的可见动作和识别诸如“人在长凳上坐着”之类的动作方面显示出希望（如图25所示）。然而，它们有时会添加错误的细节，例如“人对着镜头微笑”，这暴露了在捕捉对话主题或视频氛围方面的不足，这些元素对于人类观察者来说是显而易见的。这一缺陷突显了另一个关键限制：缺少音频和语音模态，而这些模态的加入可以通过提供更多上下文信息来丰富视频理解，帮助更准确地解读内容并防止此类误解。弥合这一差距需要整合可用的所有模态，使多模态代理能够达到接近人类感知的理解水平，从而确保视频解读的全方位多模态方法。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-22.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 26：第 6.5
节中描述的音频多模态智能体。虚构内容以红色高亮显示。我们使用 GPT-4V
生成以下内容：1）带有视频帧的聊天摘要；2）带有帧字幕的视频摘要；3）结合帧字幕和音频信息的视频摘要。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411350-23.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图
27：一种交互式多模态智能体，结合视觉、音频和文本模态进行视频理解。我们的流程挖掘困难的负面虚构内容，以生成用于
VideoAnalytica
挑战的复杂查询。交互式音频-视频-语言智能体数据集的更多相关细节在第 9.2
节中描述。</p>
<p>音视频语言代理与 GPT-4V。我们将 GPT-4V
作为多模态代理进行评估，结合视觉、音频和语音，以实现对视频的细致准确理解，遵循（Lin
等人，2023）的方法。图26展示了各种视频代理在视频总结任务上的表现。经过视频指令调整的模型（Li
等人，2023g）内容准确，但在全面性和细节上有所欠缺，漏掉了一些具体动作，例如用扫帚测量树高的步骤。</p>
<p>为了提升视频描述的准确性，我们使用 GPT-4V 为帧生成标题，并从 OpenAI
的 Whisper 模型中获取音频及其转录文本。接着，我们提示 GPT-4V
创建视频总结，先仅使用帧标题，再结合帧标题和音频转录。最初，我们发现单独使用帧标题可能会引发虚构情节，例如在第三段出现“咬住棍子”的情节。这些误差持续影响视频总结，导致描述变成“他以戏谑的方式横向咬住棍子”。没有音频输入时，代理无法纠正这些标题错误，虽然语义上正确，但在视觉上具有误导性。</p>
<p>然而，当我们向代理提供音频转录后，它能够准确地刻画内容，甚至捕捉到详细的物理动作，例如“将扫帚垂直于身体握住，并向下旋转”。这一细节大大增加了信息量，使观众对视频的意图和关键细节有更清晰的理解。这些发现突显了音频、视频和语言交互在开发高质量多模态代理中的重要性。GPT-4V
被证明是这种高级多模态理解和交互的有前景的基础。</p>
<p>具身多模态代理与 GPT-4V。如图27所示，我们主要使用 StackOverflow
获取初始问题，然后通过“Bing
搜索”API检索与问题相关的视频和音频。接下来，我们主要利用 GPT-4V
获取相关文本信息和高层次视频描述。同时，将关键帧的音频转化为关键帧的低层次片段描述，通过
ASR（自动语音识别）实现。最终，我们使用 GPT-4V
生成具有说服力的“幻觉”，作为视频问答任务中的难负面查询。在当前视频帧内，我们支持交互和问答功能，并提供整体高层次视频描述的总结。在推理过程中，还通过网络搜索结合外部知识信息，以增强回答能力。</p>
<p>GPT-4V
主提示信息的描述如下。为了便于理解，整个提示内容进行了缩进，总长度超过一页。</p>
<p>GPT-4V
的任务是为能听到但无法观看视频的视障人士提供描述性、信息丰富且全面的详细视频内容。工作目标是通过综合给定的注释，以
JSON 格式输出高质量、密集的描述。具体来说，GPT-4V
将接收用于搜索视频的原始查询、视频标题、描述、音频转录，以及视频中特定时间点的噪声描述。视频的不同片段按“[开始时间
- 结束时间（以秒为单位）] ‘文本’”的格式进行注释。GPT-4V
将整合或分割时间戳，以提供最佳的视频分段描述。</p>
<p>对 GPT-4V 输出的期望如下：</p>
<ol type="1">
<li>以动作为主的描述：优先描述合理的动作、运动及音频暗示的物理演示，并通过动态的视觉提示丰富叙述。</li>
<li>覆盖整个视频：提供一个持续、一致的音频描述体验，覆盖视频的每一刻，确保内容完整无遗漏。</li>
<li>简洁分段：将描述构建为简洁、聚焦的 1-2
句段落，有效传达视觉动作而不过于详细。</li>
<li>上下文音视频融合：将口述内容与推测的视觉元素无缝融合，形成反映视频中潜在活动的叙述。</li>
<li>富有想象力且合理的推测：通过创造性但可信的视觉细节丰富描述，增强对场景的理解。</li>
<li>精确的时间码对应：确保描述段落与时间码对齐，使推测的视觉细节与音频叙述的时间线同步。</li>
<li>自信的叙述方式：以确信的语气呈现描述，仿佛推测的视觉效果确实正在发生，增强听众的信任感。
## <strong>8.
排除不合理的细节：剔除那些与音频和视觉信息上下文不符的对象或事件描述。</strong></li>
</ol>
<p>最终输出应以 JSON
格式呈现，包含一系列字典，每个字典详细描述视频的一个片段：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><br><br><span class="hljs-attr">&quot;start&quot;</span><span class="hljs-punctuation">:</span> &lt;开始时间（秒）&gt;<span class="hljs-punctuation">,</span><br><br><span class="hljs-attr">&quot;end&quot;</span><span class="hljs-punctuation">:</span> &lt;结束时间（秒）&gt;<span class="hljs-punctuation">,</span><br><br><span class="hljs-attr">&quot;text&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;&lt;详细的一句视听描述&gt;&quot;</span><br><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure>
<p>多项选择题生成：我们还负责生成用于视频到文本检索任务的多项选择问题，这些问题仅通过查看标题和阅读音频转录文本无法轻松解决。我们将提供视频的原始查询、描述、音频转录文本，以及视频中特定时间段的噪声描述。</p>
<p>音频转录的格式为：-[开始时间-结束时间（秒）]
“转录文本”。噪声描述的格式为：-[时间（秒）] “描述”。</p>
<p>请 GPT-4V
生成四个查询，其中主查询与视频内容对齐，其他三个为微妙不同的干扰项。主查询的选择应基于视频而非单纯的音频转录。干扰项应与视频内容紧密相关但不完全匹配，要求对视频有一定的理解才能区分。例如，可以细微地调整语义，使人需要观看视频才能选择原始查询。</p>
<p>生成格式：</p>
<blockquote>
<p>• 视频分析：xxx</p>
<p>• 查询：[query1, query2, query3, query4]</p>
<p>• 解释：xxx</p>
</blockquote>
<p>NLP代理的改进方向：</p>
<blockquote>
<p><strong>1.
工具使用和知识库查询：</strong>通过外部知识库、网络搜索等提升 AI
代理的推理能力。</p>
<p><strong>2.
增强代理的推理与规划：改进代理理解复杂指令和预测未来场景的能力。</strong></p>
<p><strong>3.
整合系统与人类反馈：</strong>通过系统和人类的反馈不断学习和优化，确保代理适应用户需求。</p>
</blockquote>
<h3 id="通用-llm-代理"><strong>6.6.2 通用 LLM 代理</strong></h3>
<p>识别和理解代理内容及自然语言在交互式 AI
和自然语言处理领域长期以来一直是根本性挑战。随着深度学习的进步，越来越多的研究将这两个领域结合起来，以深度理解代理的规划、人类反馈、知识推理及自然语言生成。这些是许多人人机交互代理的关键组件，如“AutoGen”（Wu
等，2023）和“Retrieve What You Need”（Wang 等，2023g）。</p>
<h3 id="指令跟随型-llm-代理"><strong>6.6.3 指令跟随型 LLM
代理</strong></h3>
<figure>
<img src="/images/ai_agent调研/640-1733638411351-24.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 28：用于训练 Alpaca 模型（Taori 等,
2023）的训练方法。概括而言，现有的大语言模型（LLM）被用于从一小组种子任务中生成大量的指令跟随示例。生成的这些指令跟随示例随后用于对一个可获取底层模型权重的
LLM 进行指令微调。</p>
<p>指令跟随型 LLM
代理的创建成为研究热点，旨在训练能够有效遵循人类指令的代理。早期模型通过一种称为人类反馈强化学习（RLHF）的方法，用人类反馈训练一个代理奖励模型，模拟人类偏好（Ouyang
等，2022）。该方法产生了 InstructGPT 和 ChatGPT
等模型。为提高训练效率，研究人员开发了无需人类标签的指令调优方法，通过人工生成或由其他
LLM 自动生成指令/响应对直接训练 LLM 代理，如 Dolly 2.0 或 Alpaca（Taori
等，2023）。图28展示了 Alpaca 训练流程。</p>
<h3 id="实验与结果-3"><strong>6.6.4 实验与结果</strong></h3>
<p>尽管对话和自我反馈系统逐渐普及，这些 AI
仍难以从其隐含知识中生成事实正确的响应，通常在推理时使用外部工具如网络搜索和知识检索机制来增强响应。解决此问题将提升许多现实应用中的用户体验。在社交平台（如
Instagram 和 Facebook）或问答网站（如 Ask 或
Quora）中，人们通常通过评论交流并通过网络搜索获取相关知识。因此，在此情境中生成对话轮次不仅依赖传统
NLP 模型，还需代理生成具有知识搜索和获取行为的对话（Peng
等，2023）。因此，用于 NLP
任务的智能代理通过在对话中添加知识检索步骤来扩展任务描述并提升响应的可解释性。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411351-25.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 29：逻辑 Transformer 智能体模型（Wang 等,
2023e）。我们将逻辑推理模块集成到基于 Transformer
的抽象总结模型中，以赋予逻辑智能体对文本和对话逻辑进行推理的能力，从而生成更高质量的抽象总结，并减少事实性错误。</p>
<p>将这些搜索和检索代理作为反馈纳入对话中，有助于进一步深入社交互动（Wang
等，2023e）。如图29所示，我们提出了一种新的建模范式，用于从输入文本中检测并提取重要逻辑结构，并通过精心设计的多层层次逻辑投影将其融入输入嵌入中，从而在预训练语言模型中引入逻辑结构，作为一种
NLP 代理。（Wang
等，2023e）提出了一种新颖的方法，通过逻辑检测、逻辑映射和层次逻辑投影构建面向逻辑的输入嵌入，然后开发一种新的建模范式，将所有现有的
Transformer 语言模型升级为逻辑
Transformer，以持续提升其性能。所提出的逻辑 Transformer 代理比基线
Transformer
模型表现更优，能够更深入地理解文本的逻辑结构。对于人类用户来说，这些逻辑结构更有助于代理协调对话与信息检索，从而提供有意义且有趣的对话。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411351-26.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 30：提出的 NLP 智能体互学框架的架构（Wang 等,
2023g）。在每个训练周期中，交替执行阶段 1 和阶段 2。在阶段 1
中，阅读器模型的参数保持不变，只有知识选择器的权重被更新。相反，在阶段 2
中，调整阅读器模型的参数，而知识选择器的权重保持不变。</p>
<p>开放领域问答（QA）系统通常采用“检索-阅读”模式，其中检索器用于从大型语料库中提取相关段落，然后阅读器根据提取的段落和原始问题生成答案。（Wang
等，2023g）提出了一种简单的新型互学习框架，通过称为知识选择器代理的中间模块来提高“检索-阅读”模型的性能，并通过强化学习对其进行训练。知识选择器代理在检索-阅读范式中构建一个包含问题相关信息的小段落集合。图30展示了我们创新的互学习框架，将知识选择器代理作为框架组件，并采用策略梯度优化方法，以阅读器的反馈训练知识选择器代理以选择一小部分信息丰富的段落。这种方法避免了穷举搜索或手动设计的启发式方法，无需带注释的查询-文档对用于监督。通过迭代训练阅读器和知识选择器代理，我们在一些开放领域问答基准上实现了更好的预测性能。</p>
<h1 id="ai-代理跨模态跨领域与跨现实"><strong>7 AI
代理跨模态、跨领域与跨现实</strong></h1>
<h2 id="跨模态理解代理"><strong>7.1 跨模态理解代理</strong></h2>
<p>多模态理解是创建通用 AI
代理的重大挑战，主要原因在于缺少包含视觉、语言和代理行为的大规模数据集。一般来说，AI
代理的训练数据通常是特定模态的，导致大多数现代多模态系统依赖冻结的子模块组合来实现。例如，Flamingo（Alayrac
等，2022）、BLIP-2（Li 等，2023c）和 LLaVA（Liu 等，2023c）都使用冻结的
LLM
和视觉编码器。这些子模块分别在单独的数据集上训练，然后再训练适应层以将视觉编码器编码到
LLM 嵌入空间中。然而，为了在 AI
代理的跨模态理解上取得进一步进展，可能需要改变使用冻结 LLM
和视觉编码器的策略。最新的视觉语言模型 RT-2
就是一个例子，通过联合调整视觉编码器和 LLM
实现了在机器人和视觉语言任务中显著的性能提升（Brohan 等，2023）。</p>
<h2 id="跨领域理解代理"><strong>7.2 跨领域理解代理</strong></h2>
<p>创建通用代理的关键挑战之一在于不同领域间的视觉外观差异和动作空间的差异。人类可以在熟悉了特定领域的细节后理解来自现实、视频游戏和机器人等特殊领域的图像和视频，但现有的
LLM 和 VLM
常因训练数据与应用领域的数据差异大而表现不佳。尤其是，要开发能有效学习多种控制系统的通用策略，训练代理模型预测特定动作具有相当的挑战。现代系统在特定领域应用时大多是基于预训练的基础模型，并为每个特定领域微调一个独立的模型，这种方式未能捕捉领域间的共性，同时也限制了用于训练的数据总量，而无法充分利用每个领域的数据。</p>
<h2 id="跨模态与跨现实交互代理"><strong>7.3
跨模态与跨现实交互代理</strong></h2>
<p>开发能够跨现实理解并执行任务的 AI
代理仍是一项挑战，尽管在图像和场景生成方面已有一定进展（Huang
等，2023a）。特别是，代理在同时理解现实世界和虚拟现实环境时面临挑战，因其视觉差异和环境物理特性不同。在跨现实的背景下，从模拟到现实的转移问题尤为重要，尤其是在使用模拟训练的策略应用于现实数据时，这一点将在下一节讨论。</p>
<h2 id="模拟到现实的转移"><strong>7.4 模拟到现实的转移</strong></h2>
<p>使得在模拟中训练的模型能够在现实世界中部署的技术。具身代理，尤其是基于强化学习的代理，通常在模拟环境中进行训练，但这些模拟未能完全再现现实世界的特征（例如，干扰、光照、重力等物理属性）。由于模拟与现实的差异，在模拟中训练的模型在应用于现实世界时往往难以取得良好表现，这被称为“模拟到现实”的问题。解决该问题的几种方法包括：</p>
<p><strong>•
领域随机化：</strong>领域随机化是一种在模拟环境中随机变化参数（例如对象外观、传感器噪声和光学属性）以应对现实世界不确定性的方法（Tobin
等，2017）。例如，在基于 RL
的抓取技能训练中，引入对象形状的随机性可使策略适应形状略有不同的对象（Saito
等，2022）。</p>
<p><strong>•
领域适应：</strong>领域适应（或领域转移）通过在大量模拟图像和少量现实图像的训练中弥合模拟与现实的差距。在实际应用中，由于难以准备跨领域的配对图像，通常使用无配对图像翻译方法如
CycleGAN（Zhu 等，2017b）。在强化学习领域的改进版本如 RL-CycleGAN（Rao
等，2020），在模仿学习中的应用如 RetinaGAN（Ho 等，2021）。</p>
<p><strong>•
模拟改进：</strong>真实感模拟对于模拟到现实的转移至关重要，其中系统识别技术（Zhu
等，2017c；Allevato
等，2020）有助于识别模拟参数，以模拟现实环境。此外，使用照片级真实感的模拟器在基于图像的强化学习中也非常有效（Martinez-Gonzalez
等，2020；Müller 等，2018；Shah 等，2018；Sasabuchi 等，2023）。</p>
<p>模拟到现实的转移仍然是具身代理研究的核心挑战，相关技术不断发展，理论和实验研究对其进一步发展尤为重要。</p>
<h1 id="代理-ai-的持续自我改进"><strong>8 代理 AI
的持续自我改进</strong></h1>
<p>当前基于基础模型的 AI
代理能够从多种不同的数据源学习，从而为训练提供了更灵活的数据来源。其主要结果有两点：(1)
用户和人类互动数据可以用于进一步优化和提升代理，(2)
现有的基础模型和模型成果可用于生成训练数据。接下来将对这些内容进行更详细的讨论。需要注意的是，由于当前
AI
代理主要依赖预训练的基础模型，因此通常无法通过与环境的连续互动进行学习。我们认为这是一个有前景的未来方向，Bousmalis
等人已经通过机器人控制的自我改进代理展示了通过环境互动实现无监督连续学习的初步成果（Bousmalis
等，2023）。</p>
<h2 id="基于人类互动的数据"><strong>8.1 基于人类互动的数据</strong></h2>
<p>利用人类互动数据的核心思想是通过大量的人类与代理的互动来训练和改进代理的后续版本。以下是几种通过人类-代理互动来提升代理的策略：</p>
<p><strong>• 额外训练数据</strong>
最简单的方式是将人类-代理互动实例本身作为未来版本代理的训练数据。这通常需要过滤策略来区分成功的互动示例和失败的互动示例。过滤可以基于规则（例如，达到某个目标状态）、基于模型（例如，分类成功与失败的互动），或通过事后检查和/或修改手动选择互动示例。</p>
<p><strong>• 人类偏好学习</strong>
在用户互动期间，代理系统可以向用户展示不同的模型输出，让用户选择最合适的输出。这种方法常用于大型语言模型（如
ChatGPT 和 GPT-4），用户可以从多个输出中选择最符合偏好的一个。</p>
<p><strong>• 安全训练（红队测试）</strong> 在 Agent AI
中的红队测试指的是安排一支专门的对抗团队（人或计算机），以试图揭露和利用
Agent AI
系统中的漏洞。尽管本质上具有对抗性，但红队测试常用于了解如何改进 AI
的安全措施，减少有害输出的可能性。其核心原则是找到诱发不良代理输出的稳定方法，使模型能通过明确的校正数据进行训练。</p>
<h2 id="基础模型生成的数据"><strong>8.2 基础模型生成的数据</strong></h2>
<p>随着学术界和工业界产生了强大的基础模型成果，通过各种提示和数据配对技术生成有意义的训练数据的方法逐渐增多。</p>
<p><strong>• LLM 指令调优</strong>
从大型语言模型生成指令跟随训练数据的方法使得基于大型专有 LLM
输出微调较小的开源模型成为可能（Wang 等，2022b）。例如，Alpaca（Taori
等，2023）和 Vicuna（Zheng 等，2023）基于开源的 LLaMA 系列模型（Touvron
等，2023），并通过 ChatGPT
和人类参与者的各种输出进行了调优。指令调优可视为一种知识蒸馏形式，其中较大的
LLM
是小型学生模型的教师模型。虽然指令调优能将教师模型的写作风格和某些指令跟随能力传递给学生模型，但教师模型与学生模型之间在事实准确性和能力上仍存在显著差距（Gudibande
等，2023）。</p>
<p><strong>• 视觉-语言对</strong>
近期的多项研究旨在自动生成视觉内容的标题和其他文本，以增加视觉语言模型的预训练数据多样性。例如，LLaVA（Liu
等，2023c）使用了 150,000 个主要由 LLM
生成的文本和视觉输入指令跟随示例。其他研究表明，利用视觉语言模型重新为图像生成标题可以改进训练数据，提升图像生成模型的质量（Segalis
等，2023）。在视频理解领域，利用视觉语言模型和 LLM
重新为视频生成标题已被证明可以改进基于此数据训练的视觉语言模型的性能和质量（Wang
等，2023f；Zhao 等，2022）。</p>
<h1 id="代理数据集与排行榜"><strong>9 代理数据集与排行榜</strong></h1>
<p>为了加速该领域的研究，我们提出了两个基准，分别用于多代理游戏和视觉语言任务。我们将发布两个新数据集——“CuisineWorld”和“VideoAnalytica”，并提供一组基准模型，鼓励参与者探索新模型和系统，并在排行榜的测试集上提交结果。</p>
<h2 id="cuisineworld数据集多代理游戏"><strong>9.1
“CuisineWorld”数据集：多代理游戏</strong></h2>
<p>CuisineWorld 是一个文本互动游戏，类似于 Overcooked！它为 AI
驱动的代理提供了一个合作和协同的互动平台。该数据集将测试多代理系统的协作效率，观察
LLM
和其他系统在动态场景中的协作能力，重点考察代理对目标的理解能力以及代理之间的协调性。该数据集支持集中式调度模式和去中心化模式，参与者可选择游戏模式并提交结果至排行榜。</p>
<h3 id="基准"><strong>9.1.1 基准</strong></h3>
<p>在我们的竞赛中，我们将发布 CuisineWorld
基准，包括一个可扩展的任务定义文件文本接口、多代理互动接口和人机互动接口。游戏互动任务的目标是生成相关、合适的多代理协作策略，以最大化协作效率。我们使用一种新评估指标
CoS 来评估协作效率。</p>
<p>CuisineWorld 数据集由 Microsoft、UCLA
和斯坦福大学联合收集，竞赛旨在探索现有和新型基于 LLM
的互动技术在该基准中的表现，并为多代理游戏基础设施任务建立强大基准。</p>
<h3 id="任务"><strong>9.1.2 任务</strong></h3>
<p>• 提供名为 Microsoft MindAgent 的数据集和相关基准，并发布数据集
“CuisineWorld” 供研究社区使用。</p>
<p>• 提供评估和排名提交的 “MindAgent”
算法的基准，以及由流行的基础设施生成的基准结果。</p>
<h3 id="评估标准和评分"><strong>9.1.3 评估标准和评分</strong></h3>
<p>多代理协作效率的质量由新“cos”自动指标（来自 MindAgent (Gong
等，2023a)）来决定。最终评分是多代理系统在所有任务上评估的协作效率指标的平均值。人类评估者将对单独的响应进行评分，并对用户与代理的互动的参与度、广度和总体质量提供主观评价。</p>
<h3 id="评估"><strong>9.1.4 评估</strong></h3>
<p>•
自动评估：计划在数据集发布日（待定）启动排行榜，注册参与者需在数据集
“CuisineWorld”
的任务上提交结果。提交将于截止日期（待定）结束。每个团队需在测试集上提交生成的结果，以进行“cos”指标的自动评估。</p>
<p>•
排行榜上的人工评估：排行榜参与者需提交通过本地评估脚本生成的提交文件。我们将使用
evalAI
系统检查提交文件，并可选择性地对挑战的顶尖选手代码进行重新运行。因此，团队还需提交包含运行代码说明的
Readme 文件。人工评估将由组织团队完成。</p>
<p>• 获胜者公告：最终将在排行榜上公布获胜者及其评分。</p>
<h2 id="音视频语言预训练数据集"><strong>9.2
音视频语言预训练数据集</strong></h2>
<p>我们引入 VideoAnalytica
作为分析性视频演示理解的新基准。VideoAnalytica
侧重于利用视频演示来更好地理解长篇教学视频中的复杂、高阶推理。该任务旨在评估视频语言模型的认知推理能力，使其不仅限于识别和基本理解，而是能进行更复杂的细致视频理解。VideoAnalytica
强调整合音频、视频和语言等多种模态，并要求模型应用领域知识以对视频中的信息进行上下文化和解释。VideoAnalytica
包含两个主要任务：</p>
<p><strong>1.
视频文本检索：</strong>该任务要求模型从教学视频中准确检索相关文本。挑战在于区分相关信息和无关信息，从而需要对视频内容有深刻理解，并分析演示以检索正确的查询。为提高任务复杂性，我们在数据集中引入了由大型语言模型生成的难负例，并通过人类验证剔除可能使任务无效或不公平的负例（例如，负例本身正确）。</p>
<p><strong>2.
视频辅助的信息问答：</strong>该任务要求模型基于从视频中提取的信息回答问题，重点在于复杂的问题，需进行分析性推理和深入的演示理解。</p>
<p>为推动音视频语言代理在分析性视频理解上的发展，我们为 VideoAnalytica
的两个任务设立了排行榜。</p>
<p><strong>•
排行榜参与：</strong>参与者需提交解决方案以进行评估，评估基于模型在两个任务上的表现，结果将在排行榜上显示。参与者需提交代码，并提供详细的算法和方法说明。</p>
<p><strong>•
伦理考量：</strong>排行榜关注理解和解释视频内容，该技术可能会被用于监控或其他侵犯隐私的应用。因此，考虑技术的伦理影响及其潜在滥用至关重要。我们鼓励参与者在提交中考虑这些方面，推动
AI 的伦理应用。</p>
<h1 id="附录">附录</h1>
<h2 id="a.-agent-ai-的-gpt-4v-提示细节">A. Agent AI 的 GPT-4V
提示细节</h2>
<p>除非特别说明，否则我们使用 GPT-4V 的默认系统提示。我们在图 31
中展示了 GPT-4V 在 Minecraft
游戏中提示的详细描述以及生成响应的过程。</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411351-27.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 31：我们提供代码片段来展示调用 GPT-4V 分析 Minecraft
视频的完整提示过程。首先，对视频的帧进行编码，然后调用 GPT-4V
接口。模型的响应显示在屏幕底部。</p>
<p>B. GPT-4V 应用于 Bleeding Edge</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411351-28.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 32：GPT-4V 能够为 Bleeding Edge
这样的具有第三人称视角和视觉复杂场景的游戏生成有意义的内容。为了向
GPT-4V 输入大量帧（48
帧），我们将这些帧以网格形式排列，并在每帧上叠加帧编号（如上图所示）。</p>
<p>Bleeding Edge
是一款第三人称团队战斗游戏，玩家尝试占领目标点或收集比敌队更多的资源。我们在图
32 中展示了在该游戏中提示 GPT-4V 的示例输入和输出。与 Minecraft
相比，我们发现 GPT-4V 对 Bleeding Edge
的视觉内容和游戏规则的理解较为浅显。这可能是由于 (1) GPT-4V 训练数据中
Minecraft 数据量较大，以及 (2) 相较于 Minecraft，Bleeding Edge
具有更高的视觉复杂性。</p>
<p><strong>C. GPT-4V 应用于 Microsoft Flight Simulator</strong></p>
<figure>
<img src="/images/ai_agent调研/640-1733638411351-29.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 33：GPT-4V 能够为 Microsoft Flight Simulator
游戏生成有意义的内容描述。GPT-4V
智能体可以提供高级动作描述，描述玩家在飞行模拟器中驾驶飞机的过程，通过驾驶舱视角和飞机的外部视角展示，管理各种飞行控制和仪表，以保持适当的空速和高度，并在虚拟空域中导航。为了输入较大的视频，我们选择了若干关键帧（6
帧）发送给 GPT-4V。每个关键帧单独输入，未使用网格（如上图所示）。</p>
<p>如图 33 所示，基于 GPT-4V 的智能体能够为 Microsoft Flight Simulator
提供玩家的高级动作描述。智能体描述了玩家正在驾驶一架飞机，视角显示为驾驶舱视角和外部视角，玩家通过管理各种飞行控制和仪器来保持适当的空速和高度，并在虚拟空域中导航。</p>
<p><strong>D. GPT-4V 应用于 Assassin’s Creed Odyssey</strong></p>
<figure>
<img src="/images/ai_agent调研/640-1733638411351-30.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>如图 34 所示，GPT-4V
智能体提供了角色在历史战斗中的夜间战斗的高级动作描述，角色手持长矛并使用如“猛冲”这样的特殊技能与多个敌人作战，同时管理健康值和能力值条。智能体还描述了玩家参与了大规模的近战场景，控制着一个带有红色装饰的角色，并按照屏幕提示使用“猛冲技能”。环境黑暗而火光四起，表明这是一个战场，玩家的角色面对多个头顶有等级和血条标记的敌人。这一场景可能是教程或战斗序列的一部分，玩家在其中学习并执行战斗技能。</p>
<p>E. GPT-4V 应用于 GEARS of WAR 4</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411351-31.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 35：GPT-4V 能够为《战争机器 4》游戏生成有意义的内容描述。GPT-4V
智能体提供了高级动作描述，描绘玩家在一个视觉丰富、具有未来感的环境中参与战斗场景，环境类似于工业或军事建筑。角色身穿护甲，携带火器，在该区域中穿梭，利用掩体和瞄准系统与敌人交战。捕捉的动作包括角色在掩体后瞄准敌人、向敌人开火、通过屏幕上的视觉提示显示受到伤害，以及进行近距离战斗。环境为夜间照明，人工光源突出了关键区域或动作区域。还出现了一些角色呈现为透明轮廓的时刻，可能表示特殊技能或暂时状态，例如无敌或隐身。为了输入较大的视频，我们选择了若干关键帧（10
帧）发送给 GPT-4V。每个关键帧单独输入，未使用网格（如上图所示）。</p>
<p>如图 35 所示，基于提供的 10 帧视频序列，GPT-4V
智能体可以概述玩家在视频游戏中的动作：</p>
<p>“玩家在夜间引导角色穿越工业环境，利用箱子和墙等物体进行战略掩护。角色携带武器并与敌人展开战斗，可能使用枪械和近战攻击，近距离战斗暗示了这一点。玩家似乎在管理弹药和健康状况，屏幕上的红色闪烁指示符常常显示角色受到伤害。此外，玩家战术性地移动，在不同掩护之间切换以推进或防御敌人。游戏中还出现了保护屏障的激活，表明玩家通过防御技能应对敌方攻击。角色也似乎在重新装填武器，表明战斗中的资源管理元素。血迹和撞击效果显示敌人受到成功攻击。</p>
<p>玩家角色与盟友协同作战，附近有其他角色从事类似的战斗行动。环境中具有互动元素，例如可以利用的爆炸桶，可能为玩家带来战斗优势。”</p>
<p>F. GPT-4V 应用于 Starfield</p>
<p>如图 36 所示，基于提供的 10 帧视频序列，GPT-4V
智能体为《星空》游戏中的玩家动作提供了总体描述。据 GPT-4V
所述：“图像展示了玩家在星空游戏中的一系列事件，反映了其在采矿环境中的互动。以下是该序列的描述：</p>
<figure>
<img src="/images/ai_agent调研/640-1733638411351-32.webp" srcset="/img/loading.gif" lazyload alt="图片" />
<figcaption aria-hidden="true">图片</figcaption>
</figure>
<p>图 36：GPT-4V 能够为 Xbox
主机游戏《星空》生成有意义的内容描述。GPT-4V
智能体为这款太空主题探索游戏提供了高级动作描述。为了输入较大的视频，我们选择若干关键帧（10
帧）发送给 GPT-4V。每个关键帧单独输入，未使用网格（如上图所示）。</p>
<blockquote>
<ol type="1">
<li><p>玩家站在昏暗的地下洞穴中，四周有钟乳石，旁边站着另一位穿着类似宇航服的角色。他们面向一个高亮的路径点，表明该位置是任务目标或感兴趣的位置。</p></li>
<li><p>玩家穿过一个工业化的内部区域，可能是采矿站，靠近另一位角色。HUD（抬头显示）显示了来自“林主管”的对话行，这表明可能是
NPC（非玩家角色）互动，可能是任务简报或更新。</p></li>
<li><p>玩家似乎在跟随 NPC 林主管穿过洞穴，暗示此段游戏为 NPC
引导玩家前往特定地点或任务的内容。</p></li>
<li><p>来自林主管的对话表明可能出现决策点或任务更新，可能引导玩家前往与“大矿脉”相关的新任务。</p></li>
<li><p>玩家再次遇到林主管，对方提供了关于金属矿藏的指示或信息，暗示即将开始采矿任务或探索新区域。</p></li>
<li><p>在此帧中，玩家似乎在观察或操作一台机器，旁边的角色伊莎贝尔暗示与设备操作或技术工作的相关对话或操作。</p></li>
<li><p>玩家俯视粗糙的地形，看到两个发光的物体，可能代表可采集的资源或任务中的兴趣点。</p></li>
<li><p>玩家检查特定资源，HUD
标记显示为“铍矿藏”，表明涉及资源收集或采矿活动。</p></li>
<li><p>玩家穿过洞穴，可能在寻找更多资源或前往下一个任务目标，前方还有另一名穿着宇航服的角色。</p></li>
<li><p>玩家位于洞穴中较暗的区域，有两个发光的标记，指示前方的路径或目标点。</p></li>
</ol>
</blockquote>
<p>从这些图像来看，玩家似乎参与了一个任务，包括探索、与 NPC
的互动、资源收集以及在采矿环境中操作机器等。HUD
元素如路径点、对话文本和任务更新为玩家的动作和游戏目标提供了背景。”</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/categories/" class="category-chain-item">categories</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/AI-agent/" class="print-no-link">#AI agent</a>
      
        <a href="/tags/%E8%AE%BA%E6%96%87/" class="print-no-link">#论文</a>
      
        <a href="/tags/%E5%89%8D%E6%B2%BF/" class="print-no-link">#前沿</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>AI Agent：多模态交互前沿调查-李飞飞团队</div>
      <div>https://linxkon.github.io/AI_Agent全景调研.html</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>linxkon</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年12月8日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/FRP%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B.html" title="">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile"></span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/%E9%80%89%E6%8B%A9RLHF%E8%BF%98%E6%98%AFSFT.html" title="选择RLHF还是SFT">
                        <span class="hidden-mobile">选择RLHF还是SFT</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://lib.baomitu.com/gitalk/1.8.0/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"Ov23licg1p15oAGiQtDC","clientSecret":"d6ca3873752e3a6eb2d21a98b92a3021fd462cbf","repo":"Waline","owner":"linxkon","admin":["linxkon"],"language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: '1eddd67cea96401b133e9201ad23991b'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量
        <span id="leancloud-site-pv"></span>
        次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        访客量
        <span id="leancloud-site-uv"></span>
        次
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js">
</script>
</body>
</html>
