

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/linxkon_blog.png">
  <link rel="icon" href="/img/linxkon_blog.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="linxkon">
  <meta name="keywords" content="技术分享，项目实战，生活记录">
  
    <meta name="description" content="本文是Transfomrer的Pytorch版本实现. 实现的过程中非常考验维度控制的功底. 本文实现参考Transformer 的 PyTorch 实现, 我对其在个别地方进行了修改, 并对所有的数据全部加上了维度注释. 本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网).   右键我在COLAB中打开!  右键我在COLAB中打开! 如果你不能科学">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer的代码实现-基于Pytorch">
<meta property="og:url" content="https://linxkon.github.io/Transformer%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-%E5%9F%BA%E4%BA%8EPytorch.html">
<meta property="og:site_name" content="AI·你所爱">
<meta property="og:description" content="本文是Transfomrer的Pytorch版本实现. 实现的过程中非常考验维度控制的功底. 本文实现参考Transformer 的 PyTorch 实现, 我对其在个别地方进行了修改, 并对所有的数据全部加上了维度注释. 本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网).   右键我在COLAB中打开!  右键我在COLAB中打开! 如果你不能科学">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://linxkon.github.io/images/index_pic/Transformer.png">
<meta property="article:published_time" content="2023-01-11T12:21:13.000Z">
<meta property="article:modified_time" content="2024-06-12T10:20:03.595Z">
<meta property="article:author" content="linxkon">
<meta property="article:tag" content="笔记整理">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="代码实现">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://linxkon.github.io/images/index_pic/Transformer.png">
  
  
  
  <title>Transformer的代码实现-基于Pytorch - AI·你所爱</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"linxkon.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"enable":true,"app_id":"XLEbEr6BfzRRh34xJtmOEom0-MdYXbMMI","app_key":"3bwflR7evMRYC6JTohHAE31C","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>AI·你所爱 | Linxkon</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/pursenight.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Transformer的代码实现-基于Pytorch"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-01-11 20:21" pubdate>
          2023年1月11日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          30 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Transformer的代码实现-基于Pytorch</h1>
            
            
              <div class="markdown-body">
                
                <p>本文是Transfomrer的Pytorch版本实现.
实现的过程中非常考验<strong>维度控制</strong>的功底. 本文实现参考<a
target="_blank" rel="noopener" href="https://wmathor.com/index.php/archives/1455/">Transformer 的
PyTorch 实现</a>, 我对其在个别地方进行了修改,
并对所有的数据<strong>全部</strong>加上了维度注释.</p>
<p>本文的代码已经放到了Colab上, 打开设置GPU就可以复现(需要科学上网).</p>
<figure>
<img src="https://colab.research.google.com/assets/colab-badge.svg" srcset="/img/loading.gif" lazyload
alt="右键我在COLAB中打开!" />
<figcaption aria-hidden="true">右键我在COLAB中打开!</figcaption>
</figure>
<p><strong>右键我在COLAB中打开!</strong></p>
<p>如果你不能科学上网, 应该看不到<code>Open in Colab</code>的图标.</p>
<p>在开头需要说明的是:</p>
<ul>
<li>网上的所有流传的代码, 一般都会把<code>batch_size</code>放在第0维.
因为我们基本上不对batch维做操作,
放在最前面来防止影响后面总需要使用<code>transpose</code>移动.</li>
<li>如果对Transformer不熟悉, 最好熟悉后再来看这篇文章.</li>
<li>注意<code>view</code>和<code>transpose</code>拆维度时不要乱了.</li>
</ul>
<h2 id="preparing">Preparing</h2>
<p>按照惯例, 先导包:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> data <span class="hljs-keyword">as</span> Data<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br></code></pre></td></tr></table></figure>
<p>因为后面需要用到一些关于Transformer的超参数,
所以在开头就先全部定义出来:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">d_model = <span class="hljs-number">512</span> <span class="hljs-comment"># embedding size </span><br>max_len = <span class="hljs-number">1024</span> <span class="hljs-comment"># max length of sequence</span><br>d_ff = <span class="hljs-number">2048</span> <span class="hljs-comment"># feedforward nerual network  dimension</span><br>d_k = d_v = <span class="hljs-number">64</span> <span class="hljs-comment"># dimension of k(same as q) and v</span><br>n_layers = <span class="hljs-number">6</span> <span class="hljs-comment"># number of encoder and decoder layers</span><br>n_heads = <span class="hljs-number">8</span> <span class="hljs-comment"># number of heads in multihead attention</span><br>p_drop = <span class="hljs-number">0.1</span> <span class="hljs-comment"># propability of dropout</span><br></code></pre></td></tr></table></figure>
<p>如果你对Transformer足够熟悉, 看变量名和注释一定能看出来它们的含义,
它们依次是:</p>
<ul>
<li>d_model: Embedding的大小.</li>
<li>max_len: 输入序列的最长大小.</li>
<li>d_ff: 前馈神经网络的隐藏层大小, 一般是d_model的四倍.</li>
<li>d_k, d_v: 自注意力中K和V的维度, Q的维度直接用K的维度代替,
因为这二者必须始终相等.</li>
<li>n_layers: Encoder和Decoder的层数.</li>
<li>n_heads: 自注意力多头的头数.</li>
<li>p_drop: Dropout的概率.</li>
</ul>
<h2 id="mask">Mask</h2>
<p>Mask分为两种, 一种是因为在数据中使用了padding,
不希望pad被加入到注意力中进行计算的Pad Mask for Attention,
还有一种是保证Decoder自回归信息不泄露的Subsequent Mask for Decoder.</p>
<h3 id="pad-mask-for-attention">Pad Mask for Attention</h3>
<p>为了方便, 假设<code>&lt;PAD&gt;</code>在字典中的Index是0,
遇到输入为0直接将其标为True.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_attn_pad_mask</span>(<span class="hljs-params">seq_q, seq_k</span>):<br>  <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">  Padding, because of unequal in source_len and target_len.</span><br><span class="hljs-string"></span><br><span class="hljs-string">  parameters:</span><br><span class="hljs-string">  seq_q: [batch, seq_len]</span><br><span class="hljs-string">  seq_k: [batch, seq_len]</span><br><span class="hljs-string"></span><br><span class="hljs-string">  return:</span><br><span class="hljs-string">  mask: [batch, len_q, len_k]</span><br><span class="hljs-string"></span><br><span class="hljs-string">  &#x27;&#x27;&#x27;</span><br>  batch, len_q = seq_q.size()<br>  batch, len_k = seq_k.size()<br>  <span class="hljs-comment"># we define index of PAD is 0, if tensor equals (zero) PAD tokens</span><br>  pad_attn_mask = seq_k.data.eq(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">1</span>) <span class="hljs-comment"># [batch, 1, len_k]</span><br><br>  <span class="hljs-keyword">return</span> pad_attn_mask.expand(batch, len_q, len_k) <span class="hljs-comment"># [batch, len_q, len_k]</span><br></code></pre></td></tr></table></figure>
<p>在Encoder和Decoder中使用Mask的情况可能各有不同:</p>
<ul>
<li>在Encoder中使用Mask,
是为了将<code>encoder_input</code>中没有内容而打上PAD的部分进行Mask,
方便矩阵运算.</li>
<li>在Decoder中使用Mask,
可能是在Decoder的自注意力对<code>decoder_input</code> 的PAD进行Mask,
也有可能是对Encoder -
Decoder自注意力时对<code>encoder_input</code>和<code>decoder_input</code>的PAD进行Mask.</li>
</ul>
<h3 id="subsequent-mask-for-decoder">Subsequent Mask for Decoder</h3>
<p>该Mask是为了防止Decoder的自回归信息泄露而生的Mask,
直接生成一个上三角矩阵即可:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_attn_subsequent_mask</span>(<span class="hljs-params">seq</span>):<br>  <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">  Build attention mask matrix for decoder when it autoregressing.</span><br><span class="hljs-string"></span><br><span class="hljs-string">  parameters:</span><br><span class="hljs-string">  seq: [batch, target_len]</span><br><span class="hljs-string"></span><br><span class="hljs-string">  return:</span><br><span class="hljs-string">  subsequent_mask: [batch, target_len, target_len] </span><br><span class="hljs-string">  &#x27;&#x27;&#x27;</span><br>  attn_shape = [seq.size(<span class="hljs-number">0</span>), seq.size(<span class="hljs-number">1</span>), seq.size(<span class="hljs-number">1</span>)] <span class="hljs-comment"># [batch, target_len, target_len]</span><br>  subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="hljs-number">1</span>) <span class="hljs-comment"># [batch, target_len, target_len] </span><br>  subsequent_mask = torch.from_numpy(subsequent_mask)<br><br>  <span class="hljs-keyword">return</span> subsequent_mask <span class="hljs-comment"># [batch, target_len, target_len] </span><br></code></pre></td></tr></table></figure>
<p>其中, 用到了生成上三角的函数<code>np.triu</code>, 其用法为:</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">np.triu(np.ones([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]), k=<span class="hljs-number">1</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">array([[0., 1., 1., 1.],</span><br><span class="hljs-string">       [0., 0., 1., 1.],</span><br><span class="hljs-string">       [0., 0., 0., 1.]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>np.triu(np.ones([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]), k=<span class="hljs-number">0</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">array([[1., 1., 1., 1.],</span><br><span class="hljs-string">       [0., 1., 1., 1.],</span><br><span class="hljs-string">       [0., 0., 1., 1.]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>np.triu(np.ones([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]), k=-<span class="hljs-number">1</span>)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">array([[1., 1., 1., 1.],</span><br><span class="hljs-string">       [1., 1., 1., 1.],</span><br><span class="hljs-string">       [0., 1., 1., 1.]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>
<p>其中<code>k</code>能控制上三角的大小, 越大则上三角范围越小.
与之完全<strong>相反</strong>的函数是<code>np.tril</code>,
能够生成下三角矩阵.</p>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>在Transformer中, 使用的是绝对位置编码, 用于传输给模型Self -
Attention所不能传输的位置信息, 编码使用正余弦公式实现:
𝑃𝐸(𝑝𝑜𝑠,2𝑖)=sin⁡(𝑝𝑜𝑠/100002𝑖𝑑𝑚𝑜𝑑𝑒𝑙)𝑃𝐸(𝑝𝑜𝑠,2𝑖+1)=cos⁡(𝑝𝑜𝑠/100002𝑖𝑑𝑚𝑜𝑑𝑒𝑙)
基于上述公式, 我们把它实现出来:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionalEncoding</span>(nn.Module):<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, dropout=<span class="hljs-number">.1</span>, max_len=<span class="hljs-number">1024</span></span>):<br>    <span class="hljs-built_in">super</span>(PositionalEncoding, self).__init__()<br>    self.dropout = nn.Dropout(p=p_drop)<br><br>    positional_encoding = torch.zeros(max_len, d_model) <span class="hljs-comment"># [max_len, d_model]</span><br>    position = torch.arange(<span class="hljs-number">0</span>, max_len).<span class="hljs-built_in">float</span>().unsqueeze(<span class="hljs-number">1</span>) <span class="hljs-comment"># [max_len, 1]</span><br><br>    div_term = torch.exp(torch.arange(<span class="hljs-number">0</span>, d_model, <span class="hljs-number">2</span>).<span class="hljs-built_in">float</span>() * <br>                         (-torch.log(torch.Tensor([<span class="hljs-number">10000</span>])) / d_model)) <span class="hljs-comment"># [max_len / 2]</span><br><br>    positional_encoding[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = torch.sin(position * div_term) <span class="hljs-comment"># even</span><br>    positional_encoding[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = torch.cos(position * div_term) <span class="hljs-comment"># odd</span><br><br>    <span class="hljs-comment"># [max_len, d_model] -&gt; [1, max_len, d_model] -&gt; [max_len, 1, d_model]</span><br>    positional_encoding = positional_encoding.unsqueeze(<span class="hljs-number">0</span>).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># register pe to buffer and require no grads</span><br>    self.register_buffer(<span class="hljs-string">&#x27;pe&#x27;</span>, positional_encoding)<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-comment"># x: [seq_len, batch, d_model]</span><br>    <span class="hljs-comment"># we can add positional encoding to x directly, and ignore other dimension</span><br>    x = x + self.pe[:x.size(<span class="hljs-number">0</span>), ...]<br><br>    <span class="hljs-keyword">return</span> self.dropout(x)<br></code></pre></td></tr></table></figure>
<p>实现1/100002𝑖𝑑𝑚𝑜𝑑𝑒𝑙 时既可以像我写出的那样使用幂指运算,
也可以直接写出.</p>
<p><code>register_buffer</code>能够申请一个缓冲区中的<strong>常量</strong>,
并且它不会被加入到计算图中, 也就不会参与反向传播.</p>
<p>更多关于<code>register</code>在<code>parameter</code>和<code>buffer</code>上的区别请见<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89442276">Pytorch模型中的parameter与buffer</a>.</p>
<h2 id="feed-forward-neural-network">Feed Forward Neural Network</h2>
<p>在Transformer中,
Encoder或者Decoder每个Block都需要用一个前馈神经网络来添加<strong>非线性</strong>:
FFN(𝑥)=ReLU(𝑥𝑊1+𝑏1)𝑊2+𝑏2 注意, 这里它们都是有偏置的,
而且这两个Linear可以用两个1×1 的卷积来实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedForwardNetwork</span>(nn.Module):<br>  <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">  Using nn.Conv1d replace nn.Linear to implements FFN.</span><br><span class="hljs-string">  &#x27;&#x27;&#x27;</span><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(FeedForwardNetwork, self).__init__()<br>    <span class="hljs-comment"># self.ff1 = nn.Linear(d_model, d_ff)</span><br>    <span class="hljs-comment"># self.ff2 = nn.Linear(d_ff, d_model)</span><br>    self.ff1 = nn.Conv1d(d_model, d_ff, <span class="hljs-number">1</span>)<br>    self.ff2 = nn.Conv1d(d_ff, d_model, <span class="hljs-number">1</span>)<br>    self.relu = nn.ReLU()<br><br>    self.dropout = nn.Dropout(p=p_drop)<br>    self.layer_norm = nn.LayerNorm(d_model)<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-comment"># x: [batch, seq_len, d_model]</span><br>    residual = x<br>    x = x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># [batch, d_model, seq_len]</span><br>    x = self.ff1(x)<br>    x = self.relu(x)<br>    x = self.ff2(x)<br>    x = x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># [batch, seq_len, d_model]</span><br><br>    <span class="hljs-keyword">return</span> self.layer_norm(residual + x)<br></code></pre></td></tr></table></figure>
<p>作为一个子层, 不要忘记Transformer中提到的Residual Connection和Layer
Norm.</p>
<p>我选择用两个卷积代替Linear. 在<code>nn.Conv1d</code>中,
要求数据的规格为<code>[batch, x, ...]</code>,
我们是要对<code>d_model</code> 上的数据进行卷积,
所以还是需要<code>transpose</code>一下.</p>
<h2 id="multi---head-attention">Multi - Head Attention</h2>
<p>先说多头注意力, 因为多头注意力能够决定缩放点积注意力的输入大小.
作为一个子层, 其中的Residual Connection和Layer Norm是必须的.</p>
<p>多头注意力是多个不同的头来获取不同的特征,
类似于多个<strong>卷积核</strong>所达到的效果.
在计算完后通过一个Linear调整大小:
MultiHead(𝑄,𝐾,𝑉)=Concat(head1,head2,…,headℎ)𝑊𝑂where
head𝑖=Attention(𝑄𝑊𝑖𝑄,𝐾𝑊𝑖𝐾,𝑉𝑊𝑖𝑉)
多头注意力在Encoder和Decoder中的使用略有区别, 主要区别在于Mask的不同.
我们前面已经实现了两种Mask函数, 在这里会用到.</p>
<p>多头注意力实际上不是通过弄出很多大小相同的矩阵然后相乘来实现的,
只需要合并到一个矩阵进行计算:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_heads=<span class="hljs-number">8</span></span>):<br>    <span class="hljs-built_in">super</span>(MultiHeadAttention, self).__init__()<br>    <span class="hljs-comment"># do not use more instance to implement multihead attention</span><br>    <span class="hljs-comment"># it can be complete in one matrix</span><br>    self.n_heads = n_heads<br><br>    <span class="hljs-comment"># we can&#x27;t use bias because there is no bias term in formular</span><br>    self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=<span class="hljs-literal">False</span>)<br>    self.W_K = nn.Linear(d_model, d_k * n_heads, bias=<span class="hljs-literal">False</span>)<br>    self.W_V = nn.Linear(d_model, d_v * n_heads, bias=<span class="hljs-literal">False</span>)<br>    self.fc = nn.Linear(d_v * n_heads, d_model, bias=<span class="hljs-literal">False</span>)<br>    self.layer_norm = nn.LayerNorm(d_model)<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_Q, input_K, input_V, attn_mask</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    To make sure multihead attention can be used both in encoder and decoder, </span><br><span class="hljs-string">    we use Q, K, V respectively.</span><br><span class="hljs-string">    input_Q: [batch, len_q, d_model]</span><br><span class="hljs-string">    input_K: [batch, len_k, d_model]</span><br><span class="hljs-string">    input_V: [batch, len_v, d_model]</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    residual, batch = input_Q, input_Q.size(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># [batch, len_q, d_model] -- matmul W_Q --&gt; [batch, len_q, d_q * n_heads] -- view --&gt; </span><br>    <span class="hljs-comment"># [batch, len_q, n_heads, d_k,] -- transpose --&gt; [batch, n_heads, len_q, d_k]</span><br><br>    Q = self.W_Q(input_Q).view(batch, -<span class="hljs-number">1</span>, n_heads, d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># [batch, n_heads, len_q, d_k]</span><br>    K = self.W_K(input_K).view(batch, -<span class="hljs-number">1</span>, n_heads, d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># [batch, n_heads, len_k, d_k]</span><br>    V = self.W_V(input_V).view(batch, -<span class="hljs-number">1</span>, n_heads, d_v).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># [batch, n_heads, len_v, d_v]</span><br><br>    attn_mask = attn_mask.unsqueeze(<span class="hljs-number">1</span>).repeat(<span class="hljs-number">1</span>, n_heads, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># [batch, n_heads, seq_len, seq_len]</span><br><br>    <span class="hljs-comment"># prob: [batch, n_heads, len_q, d_v] attn: [batch, n_heads, len_q, len_k]</span><br>    prob, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)<br><br>    prob = prob.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous() <span class="hljs-comment"># [batch, len_q, n_heads, d_v]</span><br>    prob = prob.view(batch, -<span class="hljs-number">1</span>, n_heads * d_v).contiguous() <span class="hljs-comment"># [batch, len_q, n_heads * d_v]</span><br><br>    output = self.fc(prob) <span class="hljs-comment"># [batch, len_q, d_model]</span><br><br>    <span class="hljs-keyword">return</span> self.layer_norm(residual + output), attn<br></code></pre></td></tr></table></figure>
<p>提两个非常重要的点:</p>
<ol type="1">
<li>在拆维度时不要破坏维度原来本身的意义.</li>
<li>虽然新版本已经有<code>reshape</code>函数可以用了, 但是仍然不要忘记,
<code>transpose</code>后如果接<code>permute</code>或者<code>view</code>必须要加<code>contiguous</code>,
这是<strong>数据真实存储连续与否</strong>的问题, 请参见<a
target="_blank" rel="noopener" href="https://adaning.github.io/posts/42255.html">Pytorch之张量基础操作</a>中的<strong>维度变换</strong>部分.</li>
</ol>
<h2 id="scaled-dotproduct-attention">Scaled DotProduct Attention</h2>
<p>Tranformer中非常重要的概念, 缩放点积注意力, 公式如下:
Attention(𝑄,𝐾,𝑉)=softmax(𝑄𝐾𝑇𝑑𝑘)𝑉 实现起来非常简单, 只需要把Q,
K两个矩阵一乘, 然后再缩放, 过一次Softmax, 再和V乘下:</p>
<p>python</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ScaledDotProductAttention</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(ScaledDotProductAttention, self).__init__()<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, Q, K, V, attn_mask</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Q: [batch, n_heads, len_q, d_k]</span><br><span class="hljs-string">    K: [batch, n_heads, len_k, d_k]</span><br><span class="hljs-string">    V: [batch, n_heads, len_v, d_v]</span><br><span class="hljs-string">    attn_mask: [batch, n_heads, seq_len, seq_len]</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    scores = torch.matmul(Q, K.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)) / np.sqrt(d_k) <span class="hljs-comment"># [batch, n_heads, len_q, len_k]</span><br>    scores.masked_fill_(attn_mask, -<span class="hljs-number">1e9</span>)<br><br>    attn = nn.Softmax(dim=-<span class="hljs-number">1</span>)(scores) <span class="hljs-comment"># [batch, n_heads, len_q, len_k]</span><br>    prob = torch.matmul(attn, V) <span class="hljs-comment"># [batch, n_heads, len_q, d_v]</span><br>    <span class="hljs-keyword">return</span> prob, attn<br></code></pre></td></tr></table></figure>
<p><code>masked_fill_</code>能把传进来的Mask为True的地方全都填充上某个值,
这里需要用一个很大的负数来保证𝑒𝑥→0, 使得其在Softmax 中可以被忽略.</p>
<h2 id="encoder-and-decoder">Encoder and Decoder</h2>
<h3 id="encoder">Encoder</h3>
<p>先写出Encoder的每个Layer, 由多头注意力和FFN组成:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderLayer</span>(nn.Module):<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(EncoderLayer, self).__init__()<br>    self.encoder_self_attn = MultiHeadAttention()<br>    self.ffn = FeedForwardNetwork()<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, encoder_input, encoder_pad_mask</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    encoder_input: [batch, source_len, d_model]</span><br><span class="hljs-string">    encoder_pad_mask: [batch, n_heads, source_len, source_len]</span><br><span class="hljs-string"></span><br><span class="hljs-string">    encoder_output: [batch, source_len, d_model]</span><br><span class="hljs-string">    attn: [batch, n_heads, source_len, source_len]</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    encoder_output, attn = self.encoder_self_attn(encoder_input, encoder_input, encoder_input, encoder_pad_mask)<br>    encoder_output = self.ffn(encoder_output) <span class="hljs-comment"># [batch, source_len, d_model]</span><br><br>    <span class="hljs-keyword">return</span> encoder_output, attn<br></code></pre></td></tr></table></figure>
<p>对于给定的<code>encoder_input</code>和<code>encoder_pad_pask</code>,
Encoder应该能够完成整个Block(Layer)的计算流程. 然后实现整个Encoder:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(Encoder, self).__init__()<br>    self.source_embedding = nn.Embedding(source_vocab_size, d_model)<br>    self.positional_embedding = PositionalEncoding(d_model)<br>    self.layers = nn.ModuleList([EncoderLayer() <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_layers)])<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, encoder_input</span>):<br>    <span class="hljs-comment"># encoder_input: [batch, source_len]</span><br>    encoder_output = self.source_embedding(encoder_input) <span class="hljs-comment"># [batch, source_len, d_model]</span><br>    encoder_output = self.positional_embedding(encoder_output.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># [batch, source_len, d_model]</span><br><br>    encoder_self_attn_mask = get_attn_pad_mask(encoder_input, encoder_input) <span class="hljs-comment"># [batch, source_len, source_len]</span><br>    encoder_self_attns = <span class="hljs-built_in">list</span>()<br>    <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:<br>      <span class="hljs-comment"># encoder_output: [batch, source_len, d_model]</span><br>      <span class="hljs-comment"># encoder_self_attn: [batch, n_heads, source_len, source_len]</span><br>      encoder_output, encoder_self_attn = layer(encoder_output, encoder_self_attn_mask)<br>      encoder_self_attns.append(encoder_self_attn)<br><br>    <span class="hljs-keyword">return</span> encoder_output, encoder_self_attns<br></code></pre></td></tr></table></figure>
<p>对于整个Encoder, 直接将Token的Index传入Embedding中, 再添入位置编码,
之后就经过多层Transformer Encoder. 在传入Block前,
先需要计算Padding的Mask, 再将上层的输出作为下层输入依次迭代.</p>
<h3 id="decoder">Decoder</h3>
<p>其实实现了Encoder, Decoder的实现部分都是对应的.
先实现Decoder的Block:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderLayer</span>(nn.Module):<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(DecoderLayer, self).__init__()<br>    self.decoder_self_attn = MultiHeadAttention()<br>    self.encoder_decoder_attn = MultiHeadAttention()<br>    self.ffn = FeedForwardNetwork()<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, decoder_input, encoder_output, decoder_self_mask, decoder_encoder_mask</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    decoder_input: [batch, target_len, d_mdoel]</span><br><span class="hljs-string">    encoder_output: [batch, source_len, d_model]</span><br><span class="hljs-string">    decoder_self_mask: [batch, target_len, target_len]</span><br><span class="hljs-string">    decoder_encoder_mask: [batch, target_len, source_len]</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># masked mutlihead attention</span><br>    <span class="hljs-comment"># Q, K, V all from decoder it self</span><br>    <span class="hljs-comment"># decoder_output: [batch, target_len, d_model]</span><br>    <span class="hljs-comment"># decoder_self_attn: [batch, n_heads, target_len, target_len]</span><br>    decoder_output, decoder_self_attn = self.decoder_self_attn(decoder_input, decoder_input, decoder_input, decoder_self_mask)<br><br>    <span class="hljs-comment"># Q from decoder, K, V from encoder</span><br>    <span class="hljs-comment"># decoder_output: [batch, target_len, d_model]</span><br>    <span class="hljs-comment"># decoder_encoder_attn: [batch, n_heads, target_len, source_len]</span><br>    decoder_output, decoder_encoder_attn = self.encoder_decoder_attn(decoder_output, encoder_output, encoder_output, decoder_encoder_mask)<br>    decoder_output = self.ffn(decoder_output) <span class="hljs-comment"># [batch, target_len, d_model]</span><br><br>    <span class="hljs-keyword">return</span> decoder_output, decoder_self_attn, decoder_encoder_attn<br></code></pre></td></tr></table></figure>
<p>与Encoder相对应, 只不过因为多了一个Encoder - Decoder自注意力,
所以需要额外计算一个Encoder - Decoder的Mask. 然后写出整个Decoder:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Decoder</span>(nn.Module):<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(Decoder, self).__init__()<br>    self.target_embedding = nn.Embedding(target_vocab_size, d_model)<br>    self.positional_embedding = PositionalEncoding(d_model)<br>    self.layers = nn.ModuleList([DecoderLayer() <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_layers)])<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, decoder_input, encoder_input, encoder_output</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    decoder_input: [batch, target_len]</span><br><span class="hljs-string">    encoder_input: [batch, source_len]</span><br><span class="hljs-string">    encoder_output: [batch, source_len, d_model]</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    decoder_output = self.target_embedding(decoder_input) <span class="hljs-comment"># [batch, target_len, d_model]</span><br>    decoder_output = self.positional_embedding(decoder_output.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># [batch, target_len, d_model]</span><br>    decoder_self_attn_mask = get_attn_pad_mask(decoder_input, decoder_input) <span class="hljs-comment"># [batch, target_len, target_len]</span><br>    decoder_subsequent_mask = get_attn_subsequent_mask(decoder_input) <span class="hljs-comment"># [batch, target_len, target_len]</span><br><br>    decoder_encoder_attn_mask = get_attn_pad_mask(decoder_input, encoder_input) <span class="hljs-comment"># [batch, target_len, source_len]</span><br><br>    decoder_self_mask = torch.gt(decoder_self_attn_mask + decoder_subsequent_mask, <span class="hljs-number">0</span>)<br>    decoder_self_attns, decoder_encoder_attns = [], []<br><br>    <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:<br>      <span class="hljs-comment"># decoder_output: [batch, target_len, d_model]</span><br>      <span class="hljs-comment"># decoder_self_attn: [batch, n_heads, target_len, target_len]</span><br>      <span class="hljs-comment"># decoder_encoder_attn: [batch, n_heads, target_len, source_len]</span><br>      decoder_output, decoder_self_attn, decoder_encoder_attn = layer(decoder_output, encoder_output, decoder_self_mask, decoder_encoder_attn_mask)<br>      decoder_self_attns.append(decoder_self_attn)<br>      decoder_encoder_attns.append(decoder_encoder_attn)<br><br>    <span class="hljs-keyword">return</span> decoder_output, decoder_self_attns, decoder_encoder_attns<br></code></pre></td></tr></table></figure>
<p>和Encoder相对应, 但Decoder和Encoder使用了两个不同的Embedding.
对于Mask, 可以把自回归Mask和Padding
Mask用<code>torch.gt</code>整合成一个Mask, 送入其中.</p>
<h2 id="transformer">Transformer</h2>
<p>终于到了这一步, 虽然后面还有一些小小的工作,
但现在终于能看到Transformer的<strong>全貌</strong>了:</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/transformer.jpg" srcset="/img/loading.gif" lazyload
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>里面有一个Encoder, 一个Decoder,
在Decoder端还需要加上投影层来分类:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Transformer</span>(nn.Module):<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(Transformer, self).__init__()<br><br>    self.encoder = Encoder()<br>    self.decoder = Decoder()<br>    self.projection = nn.Linear(d_model, target_vocab_size, bias=<span class="hljs-literal">False</span>)<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, encoder_input, decoder_input</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    encoder_input: [batch, source_len]</span><br><span class="hljs-string">    decoder_input: [batch, target_len]</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># encoder_output: [batch, source_len, d_model]</span><br>    <span class="hljs-comment"># encoder_attns: [n_layers, batch, n_heads, source_len, source_len]</span><br>    encoder_output, encoder_attns = self.encoder(encoder_input)<br>    <span class="hljs-comment"># decoder_output: [batch, target_len, d_model]</span><br>    <span class="hljs-comment"># decoder_self_attns: [n_layers, batch, n_heads, target_len, target_len]</span><br>    <span class="hljs-comment"># decoder_encoder_attns: [n_layers, batch, n_heads, target_len, source_len]</span><br>    decoder_output, decoder_self_attns, decoder_encoder_attns = self.decoder(decoder_input, encoder_input, encoder_output)<br>    decoder_logits = self.projection(decoder_output) <span class="hljs-comment"># [batch, target_len, target_vocab_size]</span><br><br>    <span class="hljs-comment"># decoder_logits: [batch * target_len, target_vocab_size]</span><br>    <span class="hljs-keyword">return</span> decoder_logits.view(-<span class="hljs-number">1</span>, decoder_logits.size(-<span class="hljs-number">1</span>)), encoder_attns, decoder_self_attns, decoder_encoder_attns<br></code></pre></td></tr></table></figure>
<p>最后对logits的处理是<code>view</code>成了<code>[batch * target_len, target_vocab_size]</code>,
前面的大小并不影响我们一会用交叉熵计算损失.</p>
<h2 id="input-data">Input Data</h2>
<p>输入数据没什么好说的,
为了方便直接采用了硬编码的方式构造<code>word2index</code>,
这样我们的输入序列都被转换为了Token的index输入到Embedding层中,
自动转化为嵌入在低维空间的稠密向量:</p>
<p>Decoder的输入构造过程采用了<strong>Teaching Forcing</strong>,
保证了训练过程是可以保持<strong>并行</strong>的.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">sentences = [<br>        <span class="hljs-comment"># enc_input           dec_input         dec_output</span><br>        [<span class="hljs-string">&#x27;ich mochte ein bier P&#x27;</span>, <span class="hljs-string">&#x27;S i want a beer .&#x27;</span>, <span class="hljs-string">&#x27;i want a beer . E&#x27;</span>],<br>        [<span class="hljs-string">&#x27;ich mochte ein cola P&#x27;</span>, <span class="hljs-string">&#x27;S i want a coke .&#x27;</span>, <span class="hljs-string">&#x27;i want a coke . E&#x27;</span>]<br>]<br><br><span class="hljs-comment"># Padding Should be Zero</span><br>source_vocab = &#123;<span class="hljs-string">&#x27;P&#x27;</span> : <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;ich&#x27;</span> : <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;mochte&#x27;</span> : <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;ein&#x27;</span> : <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;bier&#x27;</span> : <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;cola&#x27;</span> : <span class="hljs-number">5</span>&#125;<br>source_vocab_size = <span class="hljs-built_in">len</span>(source_vocab)<br><br>target_vocab = &#123;<span class="hljs-string">&#x27;P&#x27;</span> : <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;i&#x27;</span> : <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;want&#x27;</span> : <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;a&#x27;</span> : <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;beer&#x27;</span> : <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;coke&#x27;</span> : <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;S&#x27;</span> : <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;E&#x27;</span> : <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;.&#x27;</span> : <span class="hljs-number">8</span>&#125;<br>idx2word = &#123;i: w <span class="hljs-keyword">for</span> i, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(target_vocab)&#125;<br>target_vocab_size = <span class="hljs-built_in">len</span>(target_vocab)<br>source_len = <span class="hljs-number">5</span> <span class="hljs-comment"># max length of input sequence</span><br>target_len = <span class="hljs-number">6</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_data</span>(<span class="hljs-params">sentences</span>):<br>  encoder_inputs, decoder_inputs, decoder_outputs = [], [], []<br>  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sentences)):<br>    encoder_input = [source_vocab[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentences[i][<span class="hljs-number">0</span>].split()]<br>    decoder_input = [target_vocab[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentences[i][<span class="hljs-number">1</span>].split()]<br>    decoder_output = [target_vocab[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentences[i][<span class="hljs-number">2</span>].split()]<br>    encoder_inputs.append(encoder_input)<br>    decoder_inputs.append(decoder_input)<br>    decoder_outputs.append(decoder_output)<br><br>  <span class="hljs-keyword">return</span> torch.LongTensor(encoder_inputs), torch.LongTensor(decoder_inputs), torch.LongTensor(decoder_outputs)<br></code></pre></td></tr></table></figure>
<p>数据量非常的少, 所以等会的训练会根本不充分.</p>
<h2 id="dataset">DataSet</h2>
<p>制作一个Seq2Seq的数据集, 只需要按照Index返回Encoder的输出,
Decoder的输入, Decoder的输出(label)就好:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Seq2SeqDataset</span>(Data.Dataset):<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, encoder_input, decoder_input, decoder_output</span>):<br>    <span class="hljs-built_in">super</span>(Seq2SeqDataset, self).__init__()<br>    self.encoder_input = encoder_input<br>    self.decoder_input = decoder_input<br>    self.decoder_output = decoder_output<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-keyword">return</span> self.encoder_input.shape[<span class="hljs-number">0</span>]<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>    <span class="hljs-keyword">return</span> self.encoder_input[idx], self.decoder_input[idx], self.decoder_output[idx]<br></code></pre></td></tr></table></figure>
<h2 id="training">Training</h2>
<p>对训练所需的所有东西进行定义:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">64</span><br>epochs = <span class="hljs-number">64</span><br>lr = <span class="hljs-number">1e-3</span><br><br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br>model = Transformer().to(device)<br>criterion = nn.CrossEntropyLoss(ignore_index=<span class="hljs-number">0</span>)<br>optimizer = optim.Adam(model.parameters(), lr=lr)<br>encoder_inputs, decoder_inputs, decoder_outputs = make_data(sentences)<br>dataset = Seq2SeqDataset(encoder_inputs, decoder_inputs, decoder_outputs)<br>data_loader = Data.DataLoader(dataset, <span class="hljs-number">2</span>, <span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<p>这里有个<code>criterion = nn.CrossEntropyLoss(ignore_index=0)</code>,
其中<code>ignore_index=0</code>指的是PAD在计算交叉熵时不应该被包括进去(前面提到过PAD所对应的Index是0).</p>
<p>我们从定义好的数据集中取出数据到<code>device</code>,
然后用torch三件套:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>  <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">  encoder_input: [batch, source_len]</span><br><span class="hljs-string">  decoder_input: [batch, target_len]</span><br><span class="hljs-string">  decoder_ouput: [batch, target_len]</span><br><span class="hljs-string">  &#x27;&#x27;&#x27;</span><br>  <span class="hljs-keyword">for</span> encoder_input, decoder_input, decoder_output <span class="hljs-keyword">in</span> data_loader:<br>    encoder_input = encoder_input.to(device)<br>    decoder_input = decoder_input.to(device)<br>    decoder_output = decoder_output.to(device)<br><br>    output, encoder_attns, decoder_attns, decoder_encoder_attns = model(encoder_input, decoder_input)<br>    loss = criterion(output, decoder_output.view(-<span class="hljs-number">1</span>))<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch:&#x27;</span>, <span class="hljs-string">&#x27;%04d&#x27;</span> % (epoch + <span class="hljs-number">1</span>), <span class="hljs-string">&#x27;loss =&#x27;</span>, <span class="hljs-string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(loss))<br><br>    optimizer.zero_grad()<br>    loss.backward()<br>    optimizer.step()<br></code></pre></td></tr></table></figure>
<h2 id="attention-visualization">Attention Visualization</h2>
<p>这回有了自己造的Transformer, 经过了<strong>根本不完全的训练:
)</strong>, 我们可以把它的Attention矩阵画出来看看:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">batch 1:</span><br><span class="hljs-string">[[1, 2, 3, 5, 0],</span><br><span class="hljs-string">[1, 2, 3, 4, 0]]</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>temp_batch = <span class="hljs-number">0</span><br>n_layers = <span class="hljs-number">4</span><br>plt.figure(figsize=(n_heads * <span class="hljs-number">3</span>, n_layers * <span class="hljs-number">3</span> + <span class="hljs-number">3</span>))<br><span class="hljs-comment"># encoder_attns: [n_layers, batch, n_heads, source_len, source_len]</span><br>i = <span class="hljs-number">0</span><br>tokens = sentences[temp_batch][<span class="hljs-number">0</span>].split()<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_layers):<br>  <span class="hljs-keyword">for</span> head <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_heads):<br>    i += <span class="hljs-number">1</span><br>    plt.subplot(n_layers, n_heads, i)<br><br>    plt.title(<span class="hljs-string">&#x27;Layer:&#123;&#125;, Head:&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(layer+<span class="hljs-number">1</span>, head+<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">if</span> i % n_heads == <span class="hljs-number">0</span>:<br>      cbar=<span class="hljs-literal">True</span><br>    <span class="hljs-keyword">else</span>:<br>      cbar=<span class="hljs-literal">False</span><br>    sns.heatmap(encoder_attns[layer][temp_batch][head].detach().numpy(), cmap=<span class="hljs-string">&#x27;YlGnBu&#x27;</span>, <br>            xticklabels=tokens, yticklabels=tokens, cbar=cbar, vmin=<span class="hljs-number">0</span>, vmax=<span class="hljs-number">1</span>);<br>    plt.xticks([])<br>    plt.yticks([])<br></code></pre></td></tr></table></figure>
<p>最后两行<code>plt.xticks</code>和<code>plt.yticks</code>纯粹是为了<strong>方便注释掉</strong>,
才又写在了外面.</p>
<p><strong>不要对结果太在意</strong>,
因为<strong>训练是根本不完整的</strong>, 数据也才只有两条.
我只是想画出来看看每个头都大致学到了什么:</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/ADAning/Image/MLDL/pytorchtransformer1.jpg" srcset="/img/loading.gif" lazyload
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>最右侧是Padding, 这一列的权重都被当做是0来计算.
在浅一些的层确实学到了不同Token对不同部分的权重.
再深一些的层基本都没有得到训练, 因为数据实在太少了.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/" class="print-no-link">#笔记整理</a>
      
        <a href="/tags/transformer/" class="print-no-link">#transformer</a>
      
        <a href="/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/" class="print-no-link">#代码实现</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Transformer的代码实现-基于Pytorch</div>
      <div>https://linxkon.github.io/Transformer的代码实现-基于Pytorch.html</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>linxkon</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年1月11日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/%E5%9F%BA%E4%BA%8EGRU%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%84%E5%BB%BA%E5%B8%A6%E7%BC%96-%E8%A7%A3%E7%A0%81%E5%99%A8%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E8%8B%B1%E8%AF%91%E6%B3%95%E4%BB%BB%E5%8A%A1%E5%AE%9E%E7%8E%B0.html" title="基于GRU模型的带编-解码器和注意力机制的英译法任务实现">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">基于GRU模型的带编-解码器和注意力机制的英译法任务实现</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/%E5%9C%A8CentOS%E4%B8%8A%E5%AE%89%E8%A3%85Neo4j%205.21.2%E7%9A%84%E6%AD%A5%E9%AA%A4.html" title="在CentOS上安装Neo4j 5x">
                        <span class="hidden-mobile">在CentOS上安装Neo4j 5x</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://lib.baomitu.com/gitalk/1.8.0/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"Ov23licg1p15oAGiQtDC","clientSecret":"d6ca3873752e3a6eb2d21a98b92a3021fd462cbf","repo":"Waline","owner":"linxkon","admin":["linxkon"],"language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: '653419d286b2415f402f5db352141ccd'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量
        <span id="leancloud-site-pv"></span>
        次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        访客量
        <span id="leancloud-site-uv"></span>
        次
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js">
</script>
</body>
</html>
