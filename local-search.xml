<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>常见大模型榜单整理</title>
    <link href="/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%92%E8%A1%8C%E6%A6%9C.html"/>
    <url>/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%92%E8%A1%8C%E6%A6%9C.html</url>
    
    <content type="html"><![CDATA[<p>大模型的发展日新月异,乱花渐欲迷人眼,那到底哪个模型更强呢,不同模型又有哪些各自擅长的领域呢?这里整理几个比较权威的LLM评测榜单和数据集供诸君参考.</p><span id="more"></span><h3 id="lmsys-chatbot-arena">LMSYS Chatbot Arena</h3><ul><li>LMSYS Chatbot Arena是一个众包的开放平台，用于大语言模型（LLM）的评估。收集了超过 1,000,000次人类成对比较，使用 Bradley-Terry 模型对 LLM 进行排名，并以 Elo评分展示模型的评级。</li></ul><p>https://chat.lmsys.org/?leaderboard</p><h3 id="opencompass">OpenCompass</h3><ul><li>OpenCompass是一个开源项目,提供丰富的算法和功能支持，能够帮助社区更便捷地对NLP模型的性能进行公平全面的评估。</li></ul><p>https://rank.opencompass.org.cn/home</p><h3 id="mmlu">MMLU</h3><ul><li><strong>MMLU</strong> ： 全称Massive Multitask LanguageUnderstanding，是一种针对大模型的<strong>语言理解能力的测评</strong>，是目前最著名的大模型语义理解测评之一，由UCBerkeley大学的研究人员在2020年9月推出。该测试涵盖57项任务，包括初等数学、美国历史、计算机科学、法律等。任务涵盖的知识很广泛，语言是英文，用以评测大模型基本的知识覆盖范围和理解能力。</li></ul><p>https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu</p><h3 id="c-eval">C-Eval</h3><ul><li><strong>C-Eval</strong> ： C-Eval是一个全面的中文基础模型评估套件。由上海交通大学、清华大学和匹兹堡大学研究人员在2023年5月份联合推出，它包含了13948个多项选择题，涵盖了52个不同的学科和四个难度级别。用以<strong>评测大模型中文理解能力</strong>。</li></ul><p>https://cevalbenchmark.com/static/leaderboard.html</p><h3 id="gsm8k">GSM8K</h3><ul><li><strong>GSM8K</strong> ：OpenAI发布的大模型数学推理能力评测基准，涵盖了8500个中学水平的高质量数学题数据集。数据集比之前的数学文字题数据集规模更大，语言更具多样性，题目也更具挑战性。该项测试在2021年10月份发布，至今仍然是非常困难的一种测试基准.</li></ul><p>https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k</p><h3 id="agi-eval">AGI Eval</h3><ul><li><strong>AGI Eval</strong> ：微软发布的大模型基础能力评测基准，在2023年4月推出，主要评测大模型在人类认知和解决问题的一般能力，涵盖全球20种面向普通人类考生的官方、公共和高标准录取和资格考试，包含中英文数据。因此，该测试更加倾向于<strong>人类考试结果</strong>，涵盖了中英文，论文地址：https://arxiv.org/abs/2304.06364</li></ul><h3 id="lmsys榜单">LMSYS榜单</h3><iframe aria-hidden="true" tabindex="-1" src="about:blank" style="box-sizing: border-box; border: 0px; display: block; vertical-align: middle; top: 0px; left: 0px; width: 1146.4px; height: 62.2px; overflow: hidden; opacity: 0; pointer-events: none; z-index: -1;"></iframe><table><thead><tr class="header"><th>🤖 Model</th><th>⭐ Arena Elo</th><th>📈 MT-bench</th><th>📚 MMLU</th><th>Organization</th><th>License</th></tr></thead><tbody><tr class="odd"><td><ahref="https://openai.com/index/hello-gpt-4o/">GPT-4o-2024-05-13</a></td><td>1287</td><td></td><td>88.7</td><td>OpenAI</td><td>Proprietary</td></tr><tr class="even"><td><ahref="https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4">GPT-4-Turbo-2024-04-09</a></td><td>1252</td><td></td><td></td><td>OpenAI</td><td>Proprietary</td></tr><tr class="odd"><td><ahref="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">GPT-4-1106-preview</a></td><td>1250</td><td>9.32</td><td></td><td>OpenAI</td><td>Proprietary</td></tr><tr class="even"><td><ahref="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">Gemini1.5 Pro API-0409-Preview</a></td><td>1248</td><td></td><td>81.9</td><td>Google</td><td>Proprietary</td></tr><tr class="odd"><td><a href="https://www.anthropic.com/news/claude-3-family">Claude 3Opus</a></td><td>1246</td><td></td><td>86.8</td><td>Anthropic</td><td>Proprietary</td></tr><tr class="even"><td><ahref="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">GPT-4-0125-preview</a></td><td>1244</td><td></td><td></td><td>OpenAI</td><td>Proprietary</td></tr><tr class="odd"><td><a href="https://www.01.ai/">Yi-Large-preview</a></td><td>1236</td><td></td><td></td><td>01 AI</td><td>Proprietary</td></tr><tr class="even"><td><a href="https://bard.google.com/">Bard (Gemini Pro)</a></td><td>1208</td><td></td><td></td><td>Google</td><td>Proprietary</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>大模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>排行榜</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从头开始实现llama3</title>
    <link href="/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama3.html"/>
    <url>/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama3.html</url>
    
    <content type="html"><![CDATA[<p>都说大模型是黑箱玄学，这次让我们打开黑箱，一起来探索它内部的世界。</p><span id="more"></span><p>在这个文件中，我从头开始实现了llama3，一次一个张量和矩阵乘法。另外，我将直接从Meta为 llama3提供的模型文件加载张量，您需要在运行此文件之前下载权重。这是下载权重的官方链接：<ahref="https://llama.meta.com/llama-downloads/">https://llama.meta.com/llama-downloads/</a></p><figure><imgsrc="/images/llama3实现/v2-513855262cb2170c7aa8d1db7e5260ed_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h1 id="分词器tokenizer">1.分词器（tokenizer）</h1><p>我不会实现 bpe tokenizer（但 andrej karpathy 有一个非常干净的实现）他的实现链接：[https://github.com/karpathy/minbpe)</p><figure><img src="/images/llama3实现/karpathyminbpe.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs text">from pathlib import Path<br>import tiktoken<br>from tiktoken.load import load_tiktoken_bpe<br>import torch<br>import json<br>import matplotlib.pyplot as plt<br><br>tokenizer_path = &quot;Meta-Llama-3-8B/tokenizer.model&quot;<br>special_tokens = [<br>            &quot;&lt;|begin_of_text|&gt;&quot;,<br>            &quot;&lt;|end_of_text|&gt;&quot;,<br>            &quot;&lt;|reserved_special_token_0|&gt;&quot;,<br>            &quot;&lt;|reserved_special_token_1|&gt;&quot;,<br>            &quot;&lt;|reserved_special_token_2|&gt;&quot;,<br>            &quot;&lt;|reserved_special_token_3|&gt;&quot;,<br>            &quot;&lt;|start_header_id|&gt;&quot;,<br>            &quot;&lt;|end_header_id|&gt;&quot;,<br>            &quot;&lt;|reserved_special_token_4|&gt;&quot;,<br>            &quot;&lt;|eot_id|&gt;&quot;,  # end of turn<br>        ] + [f&quot;&lt;|reserved_special_token_&#123;i&#125;|&gt;&quot; for i in range(5, 256 - 5)]<br>mergeable_ranks = load_tiktoken_bpe(tokenizer_path)<br>tokenizer = tiktoken.Encoding(<br>    name=Path(tokenizer_path).name,<br>    pat_str=r&quot;(?i:&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d)|[^\r\n\p&#123;L&#125;\p&#123;N&#125;]?\p&#123;L&#125;+|\p&#123;N&#125;&#123;1,3&#125;| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;,<br>    mergeable_ranks=mergeable_ranks,<br>    special_tokens=&#123;token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)&#125;,<br>)<br><br>tokenizer.decode(tokenizer.encode(&quot;hello world!&quot;))<br>&#x27;hello world!&#x27;<br></code></pre></td></tr></table></figure><h1 id="读取模型文件">2.读取模型文件</h1><p>通常，阅读本文取决于模型类的编写方式以及其中的变量名称。但由于我们是从头开始实现 llama3，因此我们将一次读取一个张量文件。</p><figure><imgsrc="/images/llama3实现/v2-c237a044abe6bbdc2556cbe7acf044b3_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs text">model = torch.load(&quot;Meta-Llama-3-8B/consolidated.00.pth&quot;)<br>print(json.dumps(list(model.keys())[:20], indent=4))<br>[<br>    &quot;tok_embeddings.weight&quot;,<br>    &quot;layers.0.attention.wq.weight&quot;,<br>    &quot;layers.0.attention.wk.weight&quot;,<br>    &quot;layers.0.attention.wv.weight&quot;,<br>    &quot;layers.0.attention.wo.weight&quot;,<br>    &quot;layers.0.feed_forward.w1.weight&quot;,<br>    &quot;layers.0.feed_forward.w3.weight&quot;,<br>    &quot;layers.0.feed_forward.w2.weight&quot;,<br>    &quot;layers.0.attention_norm.weight&quot;,<br>    &quot;layers.0.ffn_norm.weight&quot;,<br>    &quot;layers.1.attention.wq.weight&quot;,<br>    &quot;layers.1.attention.wk.weight&quot;,<br>    &quot;layers.1.attention.wv.weight&quot;,<br>    &quot;layers.1.attention.wo.weight&quot;,<br>    &quot;layers.1.feed_forward.w1.weight&quot;,<br>    &quot;layers.1.feed_forward.w3.weight&quot;,<br>    &quot;layers.1.feed_forward.w2.weight&quot;,<br>    &quot;layers.1.attention_norm.weight&quot;,<br>    &quot;layers.1.ffn_norm.weight&quot;,<br>    &quot;layers.2.attention.wq.weight&quot;<br>]<br>with open(&quot;Meta-Llama-3-8B/params.json&quot;, &quot;r&quot;) as f:<br>    config = json.load(f)<br>config<br>&#123;&#x27;dim&#x27;: 4096,<br> &#x27;n_layers&#x27;: 32,<br> &#x27;n_heads&#x27;: 32,<br> &#x27;n_kv_heads&#x27;: 8,<br> &#x27;vocab_size&#x27;: 128256,<br> &#x27;multiple_of&#x27;: 1024,<br> &#x27;ffn_dim_multiplier&#x27;: 1.3,<br> &#x27;norm_eps&#x27;: 1e-05,<br> &#x27;rope_theta&#x27;: 500000.0&#125;<br></code></pre></td></tr></table></figure><h2id="我们使用此配置来推断有关模型的详细信息例如">我们使用此配置来推断有关模型的详细信息，例如</h2><ol type="1"><li>该模型有 32 个transformer layers</li><li>每个多头注意力块有 32 个头</li><li>词汇大小等等</li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs text">dim = config[&quot;dim&quot;]<br>n_layers = config[&quot;n_layers&quot;]<br>n_heads = config[&quot;n_heads&quot;]<br>n_kv_heads = config[&quot;n_kv_heads&quot;]<br>vocab_size = config[&quot;vocab_size&quot;]<br>multiple_of = config[&quot;multiple_of&quot;]<br>ffn_dim_multiplier = config[&quot;ffn_dim_multiplier&quot;]<br>norm_eps = config[&quot;norm_eps&quot;]<br>rope_theta = torch.tensor(config[&quot;rope_theta&quot;])<br></code></pre></td></tr></table></figure><h2 id="将文本转换为标记tokens">将文本转换为标记（tokens）</h2><p>这里我们使用 tiktoken （我认为是一个 openai 库）作为 tokenizer</p><figure><imgsrc="/images/llama3实现/v2-1acdee68e45fc5503592b79e34c18258_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs text">prompt = &quot;the answer to the ultimate question of life, the universe, and everything is &quot;<br>tokens = [128000] + tokenizer.encode(prompt)<br>print(tokens)<br>tokens = torch.tensor(tokens)<br>prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens]<br>print(prompt_split_as_tokens)<br>[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]<br>[&#x27;&lt;|begin_of_text|&gt;&#x27;, &#x27;the&#x27;, &#x27; answer&#x27;, &#x27; to&#x27;, &#x27; the&#x27;, &#x27; ultimate&#x27;, &#x27; question&#x27;, &#x27; of&#x27;, &#x27; life&#x27;, &#x27;,&#x27;, &#x27; the&#x27;, &#x27; universe&#x27;, &#x27;,&#x27;, &#x27; and&#x27;, &#x27; everything&#x27;, &#x27; is&#x27;, &#x27; &#x27;]<br></code></pre></td></tr></table></figure><h2 id="将标记转换为嵌入embedding">将标记转换为嵌入（embedding）</h2><p>抱歉，但无论如何，这是代码库中我使用内置神经网络模块的唯一部分，因此我们的[17x1] 标记现在是 [17x4096]，即长度为 4096 的 17 个嵌入（每个标记一个）注意：跟踪形状，它让你更容易理解一切</p><figure><imgsrc="/images/llama3实现/v2-a4436330430517444590607a5af4bfcf_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs text">embedding_layer = torch.nn.Embedding(vocab_size, dim)<br>embedding_layer.weight.data.copy_(model[&quot;tok_embeddings.weight&quot;])<br>token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)<br>token_embeddings_unnormalized.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2 id="然后我们使用-rms-归一化对嵌入进行归一化">然后我们使用 rms归一化对嵌入进行归一化</h2><p>请注意，在这一步之后，形状不会改变，这些值只是需要记住的标准化内容，我们需要一个norm_eps（来自配置），因为我们不想意外地将rms设置为0并除以0，这里是公式：</p><figure><imgsrc="/images/llama3实现/v2-645012127903f431f8f9f5f9dd506e66_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs text"># def rms_norm(tensor, norm_weights):<br>#     rms = (tensor.pow(2).mean(-1, keepdim=True) + norm_eps)**0.5<br>#     return tensor * (norm_weights / rms)<br>def rms_norm(tensor, norm_weights):<br>    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights<br></code></pre></td></tr></table></figure><h1 id="构建transformer的第一层">3.构建transformer的第一层</h1><h2 id="正常化">正常化</h2><p>无论如何，你会看到我从模型字典访问layer.0（这是第一层），所以在标准化之后我们的形状仍然[17x4096]与嵌入相同但标准化</p><figure><imgsrc="/images/llama3实现/v2-979811529314dc783cb499cf2ca93ece_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">token_embeddings = rms_norm(token_embeddings_unnormalized, model[&quot;layers.0.attention_norm.weight&quot;])<br>token_embeddings.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2 id="从头开始实施注意力">从头开始实施注意力</h2><p>让我们加载transformer第一层的注意力头</p><figure><imgsrc="/images/llama3实现/v2-3a86ff1392e4ff420bdd9e65b3ce4d6d_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>&gt; 当我们从模型加载查询、键、值和输出向量时，我们注意到形状为[4096x4096]、[1024x4096]、[1024x4096]、[4096x4096] &gt;乍一看这很奇怪，因为理想情况下我们想要每个 q ,k,v 和 o 分别代表每个头&gt;代码的作者将它们捆绑在一起，因为它很容易，有助于并行化注意力头乘法。&gt; 我要打开所有东西...</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs text">print(<br>    model[&quot;layers.0.attention.wq.weight&quot;].shape,<br>    model[&quot;layers.0.attention.wk.weight&quot;].shape,<br>    model[&quot;layers.0.attention.wv.weight&quot;].shape,<br>    model[&quot;layers.0.attention.wo.weight&quot;].shape<br>)<br>torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])<br></code></pre></td></tr></table></figure><h2 id="展开查询">展开查询</h2><p>在下一节中，我们将从多个注意力头中解开查询，生成的形状为[32x128x4096]，其中 32 是 llama3 中注意力头的数量，128是查询向量的大小，4096 是令牌嵌入的大小</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs text">q_layer0 = model[&quot;layers.0.attention.wq.weight&quot;]<br>head_dim = q_layer0.shape[0] // n_heads<br>q_layer0 = q_layer0.view(n_heads, head_dim, dim)<br>q_layer0.shape<br>torch.Size([32, 128, 4096])<br></code></pre></td></tr></table></figure><h2 id="我要实现第一层的第一个头">我要实现第一层的第一个头</h2><p>这里我访问第一层的查询权重矩阵第一个头，这个查询权重矩阵的大小是[128x4096]</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">q_layer0_head0 = q_layer0[0]<br>q_layer0_head0.shape<br>torch.Size([128, 4096])<br></code></pre></td></tr></table></figure><h2id="我们现在将查询权重与令牌嵌入相乘以接收令牌的查询">我们现在将查询权重与令牌嵌入相乘，以接收令牌的查询</h2><p>在这里你可以看到结果的形状是 [17x128]，这是因为我们有 17个标记，每个标记都有一个 128 长度的查询。</p><figure><imgsrc="/images/llama3实现/v2-b946f4191582804b9e03e0b3c2f0003d_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)<br>q_per_token.shape<br>torch.Size([17, 128])<br></code></pre></td></tr></table></figure><h2 id="定位编码">定位编码</h2><p>我们现在处于这样一个阶段：提示中的每个标记都有一个查询向量，但如果你仔细想想——单独的查询向量不知道提示中的位置。查询：“生命、宇宙和一切的终极问题的答案是”在我们的提示中我们已经使用了“the”三次，我们需要所有 3个“the”标记的查询向量具有不同的查询向量（每个大小[1x128]）基于它们在查询中的位置。我们使用RoPE（旋转位置嵌入）执行这些旋转。</p><h2 id="rope">RoPE</h2><p>观看此视频（这就是我观看的）以理解数学。 <ahref="https://www.youtube.com/watch%3Fv%3Do29P0Kpobz0%26t%3D530s">https://www.youtube.com/watch?v=o29P0Kpobz0&amp;t=530s</a></p><figure><imgsrc="/images/llama3实现/v2-f49af1e8f64b0ce2eb5961232523607b_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)<br>q_per_token_split_into_pairs.shape<br>torch.Size([17, 64, 2])<br></code></pre></td></tr></table></figure><p>在上面的步骤中，我们将查询向量分成对，我们对每对应用旋转角度偏移！我们现在有一个大小为 [17x64x2] 的向量，这是针对提示中的每个标记将 128个长度的查询分为 64 对！这 64 对中的每一对都将旋转 m*(theta)，其中 m是我们旋转查询的标记的位置！</p><figure><imgsrc="/images/llama3实现/v2-79322d3dcc6c412772c7389e8c4ed8b9_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="使用复数的点积来旋转向量">使用复数的点积来旋转向量</h2><figure><imgsrc="/images/llama3实现/v2-58f0207e09f6c337fd2177a8e109a02c_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs text">zero_to_one_split_into_64_parts = torch.tensor(range(64))/64<br>zero_to_one_split_into_64_parts<br>tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,<br>        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,<br>        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,<br>        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,<br>        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,<br>        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,<br>        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,<br>        0.9844])<br>freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)<br>freqs<br>tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,<br>        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,<br>        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,<br>        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,<br>        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,<br>        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,<br>        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,<br>        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,<br>        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,<br>        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,<br>        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])<br>freqs_for_each_token = torch.outer(torch.arange(17), freqs)<br>freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)<br>freqs_cis.shape<br><br># viewing tjhe third row of freqs_cis<br>value = freqs_cis[3]<br>plt.figure()<br>for i, element in enumerate(value[:17]):<br>    plt.plot([0, element.real], [0, element.imag], color=&#x27;blue&#x27;, linewidth=1, label=f&quot;Index: &#123;i&#125;&quot;)<br>    plt.annotate(f&quot;&#123;i&#125;&quot;, xy=(element.real, element.imag), color=&#x27;red&#x27;)<br>plt.xlabel(&#x27;Real&#x27;)<br>plt.ylabel(&#x27;Imaginary&#x27;)<br>plt.title(&#x27;Plot of one row of freqs_cis&#x27;)<br>plt.show()<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-3b27398a4bea72f1f64f3115faf516ae_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2id="现在我们对每个标记token的查询元素都有一个复数角度变化向量">现在我们对每个标记（token）的查询元素都有一个复数（角度变化向量）</h2><p>我们可以将查询（我们分成对的查询）转换为复数，然后进行点积以根据位置诚实旋转查询，这想想就很美好:)</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs text">q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)<br>q_per_token_as_complex_numbers.shape<br>torch.Size([17, 64])<br>q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis<br>q_per_token_as_complex_numbers_rotated.shape<br>torch.Size([17, 64])<br></code></pre></td></tr></table></figure><h2 id="得到旋转向量后">得到旋转向量后</h2><p>我们可以通过再次将复数视为实数来返回成对的查询</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)<br>q_per_token_split_into_pairs_rotated.shape<br>torch.Size([17, 64, 2])<br></code></pre></td></tr></table></figure><p>旋转的对现在被合并，我们现在有一个新的查询向量（旋转查询向量），其形状为[17x128]，其中 17 是标记的数量，128 是查询向量的暗度</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)<br>q_per_token_rotated.shape<br>torch.Size([17, 128])<br></code></pre></td></tr></table></figure><h2 id="键几乎与查询相同">键（几乎与查询相同）</h2><figure><imgsrc="/images/llama3实现/v2-c997910e27f5cebae65b38a6b46d5b85_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>我太懒了，所以我不会对键进行数学计算，你需要记住的唯一事情是： &gt;键也生成维度 128 的键向量 &gt; 键的权重数量只有1/4查询，这是因为键的权重一次在 4 个头之间共享，为了减少需要的计算数量&gt; 键也会旋转以添加位置信息，就像查询一样，原因相同</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs text">k_layer0 = model[&quot;layers.0.attention.wk.weight&quot;]<br>k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim)<br>k_layer0.shape<br>torch.Size([8, 128, 4096])<br>k_layer0_head0 = k_layer0[0]<br>k_layer0_head0.shape<br>torch.Size([128, 4096])<br>k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)<br>k_per_token.shape<br>torch.Size([17, 128])<br>k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)<br>k_per_token_split_into_pairs.shape<br>torch.Size([17, 64, 2])<br>k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)<br>k_per_token_as_complex_numbers.shape<br>torch.Size([17, 64])<br>k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)<br>k_per_token_split_into_pairs_rotated.shape<br>torch.Size([17, 64, 2])<br>k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)<br>k_per_token_rotated.shape<br>torch.Size([17, 128])<br></code></pre></td></tr></table></figure><h2id="在此阶段现在每个标记都有查询和键的旋转值">在此阶段，现在每个标记都有查询和键的旋转值。</h2><figure><imgsrc="/images/llama3实现/v2-b3bacaeb87eba8b665968d1c4e06ad28_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>现在每个查询和键的形状都是 [17x128]。</p><h1id="在下一步中我们将乘以查询和关键矩阵">4.在下一步中，我们将乘以查询和关键矩阵</h1><p>这样做将为我们提供一个将每个标记相互映射的分数，该分数描述了每个标记的查询与每个标记的密钥的相关程度。这是自我注意力:) 注意力分数矩阵 (qk_per_token) 的形状是 [17x17]，其中 17是提示中的标记数量</p><figure><imgsrc="/images/llama3实现/v2-e4b38700f8f38d052ddc34bb770077d3_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5<br>qk_per_token.shape<br>torch.Size([17, 17])<br></code></pre></td></tr></table></figure><h2 id="我们现在必须屏蔽查询关键分数">我们现在必须屏蔽查询关键分数</h2><p>在 llama3 的训练过程中，未来的 token qk分数被屏蔽。为什么？因为在训练期间我们只学习使用过去的标记来预测标记。因此，在推理过程中，我们将未来的标记设置为零。</p><figure><imgsrc="/images/llama3实现/v2-32b947acb627e83f1be3cfaf7bcea213_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs text">def display_qk_heatmap(qk_per_token):<br>    _, ax = plt.subplots()<br>    im = ax.imshow(qk_per_token.to(float).detach(), cmap=&#x27;viridis&#x27;)<br>    ax.set_xticks(range(len(prompt_split_as_tokens)))<br>    ax.set_yticks(range(len(prompt_split_as_tokens)))<br>    ax.set_xticklabels(prompt_split_as_tokens)<br>    ax.set_yticklabels(prompt_split_as_tokens)<br>    ax.figure.colorbar(im, ax=ax)<br>    <br>display_qk_heatmap(qk_per_token)<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-7200e3d7069abc5a550654c2e2c0635a_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs text">mask = torch.full((len(tokens), len(tokens)), float(&quot;-inf&quot;), device=tokens.device)<br>mask = torch.triu(mask, diagonal=1)<br>mask<br>tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])<br>qk_per_token_after_masking = qk_per_token + mask<br>display_qk_heatmap(qk_per_token_after_masking)<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-9d128697d492aac646b8458d83c59a86_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure><imgsrc="/images/llama3实现/v2-f023e1085ebbe8e6b882d54ca7a4e147_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)<br>display_qk_heatmap(qk_per_token_after_masking_after_softmax)<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-af163b0bde4ab6ddc644d5d30a3dea53_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="值values注意力几乎结束">值（values）（注意力几乎结束）</h2><figure><imgsrc="/images/llama3实现/v2-f636fe202c7a600487771fb278566cc9_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>这些分数（0-1）用于确定每个标记使用了多少值矩阵&gt;就像键一样，值权重也每4个注意力头共享（以节省计算）&gt;因此，值的形状下面的权重矩阵是[8x128x4096]</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs text">v_layer0 = model[&quot;layers.0.attention.wv.weight&quot;]<br>v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[0] // n_kv_heads, dim)<br>v_layer0.shape<br>torch.Size([8, 128, 4096])<br></code></pre></td></tr></table></figure><p>下面给出第一层第一头值权重矩阵</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">v_layer0_head0 = v_layer0[0]<br>v_layer0_head0.shape<br>torch.Size([128, 4096])<br></code></pre></td></tr></table></figure><h2 id="值向量value-vectors">值向量（value vectors）</h2><figure><imgsrc="/images/llama3实现/v2-4f9bf9fa8cb5ec2f7f338049e98df276_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>我们现在使用值权重来获取每个标记的注意力值，其大小为 [17x128]，其中17 是提示中标记的数量，128 是每个标记的值向量的暗度</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T)<br>v_per_token.shape<br>torch.Size([17, 128])<br></code></pre></td></tr></table></figure><h1 id="注意力">5.注意力</h1><figure><imgsrc="/images/llama3实现/v2-c022102b0ca593356099825ddc0cb312_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>与每个标记的值相乘后得到的注意力向量的形状为 [17*128]</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>qkv_attention.shape<br>torch.Size([17, 128])<br></code></pre></td></tr></table></figure><h2 id="多头注意力">多头注意力</h2><p>我们现在有了第一层和第一个头的注意力值，现在我将运行一个循环并执行与上面的单元完全相同的数学运算，但对于第一层中的每个头</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs text">qkv_attention_store = []<br><br>for head in range(n_heads):<br>    q_layer0_head = q_layer0[head]<br>    k_layer0_head = k_layer0[head//4] # key weights are shared across 4 heads<br>    v_layer0_head = v_layer0[head//4] # value weights are shared across 4 heads<br>    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T)<br>    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T)<br>    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T)<br><br>    q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)<br>    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)<br>    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis[:len(tokens)])<br>    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)<br><br>    k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)<br>    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)<br>    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis[:len(tokens)])<br>    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)<br><br>    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5<br>    mask = torch.full((len(tokens), len(tokens)), float(&quot;-inf&quot;), device=tokens.device)<br>    mask = torch.triu(mask, diagonal=1)<br>    qk_per_token_after_masking = qk_per_token + mask<br>    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)<br>    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>    qkv_attention_store.append(qkv_attention)<br><br>len(qkv_attention_store)<br>32<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-8031063609f976093e7b47ba068f359d_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>我们现在有了第一层所有 32 个头的 qkv_attention矩阵，接下来我将把所有注意力分数合并到一个大小为 [17x4096] 的大矩阵中，我们即将结束:)</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)<br>stacked_qkv_attention.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h1 id="权重矩阵最后步骤之一">6.权重矩阵，最后步骤之一</h1><figure><imgsrc="/images/llama3实现/v2-076def42e5c789cd9e63170716ed7fea_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>对于第 0 层注意力要做的最后一件事是乘以</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">w_layer0 = model[&quot;layers.0.attention.wo.weight&quot;]<br>w_layer0.shape<br>torch.Size([4096, 4096])<br></code></pre></td></tr></table></figure><h2id="这是一个简单的线性层所以我们只需-matmul">这是一个简单的线性层，所以我们只需matmul</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">embedding_delta = torch.matmul(stacked_qkv_attention, w_layer0.T)<br>embedding_delta.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-e3f7584e47bc523b3f1711d74d64534e_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>我们现在在注意力之后嵌入值发生了变化，这应该添加到原始令牌嵌入中</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">embedding_after_edit = token_embeddings_unnormalized + embedding_delta<br>embedding_after_edit.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h1id="我们进行标准化然后通过嵌入增量运行前馈神经网络">7.我们进行标准化，然后通过嵌入增量运行前馈神经网络</h1><figure><imgsrc="/images/llama3实现/v2-bab8f207f6096fe5f7f0b4908e20a202_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[&quot;layers.0.ffn_norm.weight&quot;])<br>embedding_after_edit_normalized.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2 id="加载-ff-权重并实现前馈网络">加载 ff 权重并实现前馈网络</h2><figure><imgsrc="/images/llama3实现/v2-c632ca45a493952c3ba94901629df5e5_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>在 llama3 中，他们使用了 SwiGLU前馈网络，这种网络架构非常擅长在模型需要时添加非线性。 如今在 llms中使用这种前馈网络架构是相当标准的</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs text">w1 = model[&quot;layers.0.feed_forward.w1.weight&quot;]<br>w2 = model[&quot;layers.0.feed_forward.w2.weight&quot;]<br>w3 = model[&quot;layers.0.feed_forward.w3.weight&quot;]<br>output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)<br>output_after_feedforward.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2id="我们终于在第一层之后为每个令牌有了新编辑的嵌入">我们终于在第一层之后为每个令牌有了新编辑的嵌入</h2><p>在我们完成之前还需要 31 层（一个 for 循环），您可以想象这个编辑后的嵌入包含有关第一层上提出的所有查询的信息，现在每一层都会对所提出的问题编码越来越复杂的查询，直到我们有一个嵌入知道我们需要的下一个令牌的所有信息。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">layer_0_embedding = embedding_after_edit+output_after_feedforward<br>layer_0_embedding.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2 id="天哪一切都同时发生">天哪，一切都同时发生</h2><figure><imgsrc="/images/llama3实现/v2-9abf6da09122332cf71f63526ec24dd9_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>是的，就是这样。我们之前为每一层所做的一切都是一次性完成的。</p><p>祝阅读愉快:)</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs text">final_embedding = token_embeddings_unnormalized<br>for layer in range(n_layers):<br>    qkv_attention_store = []<br>    layer_embedding_norm = rms_norm(final_embedding, model[f&quot;layers.&#123;layer&#125;.attention_norm.weight&quot;])<br>    q_layer = model[f&quot;layers.&#123;layer&#125;.attention.wq.weight&quot;]<br>    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)<br>    k_layer = model[f&quot;layers.&#123;layer&#125;.attention.wk.weight&quot;]<br>    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)<br>    v_layer = model[f&quot;layers.&#123;layer&#125;.attention.wv.weight&quot;]<br>    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)<br>    w_layer = model[f&quot;layers.&#123;layer&#125;.attention.wo.weight&quot;]<br>    for head in range(n_heads):<br>        q_layer_head = q_layer[head]<br>        k_layer_head = k_layer[head//4]<br>        v_layer_head = v_layer[head//4]<br>        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)<br>        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)<br>        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)<br>        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)<br>        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)<br>        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis)<br>        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)<br>        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)<br>        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)<br>        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)<br>        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)<br>        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5<br>        mask = torch.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float(&quot;-inf&quot;))<br>        mask = torch.triu(mask, diagonal=1)<br>        qk_per_token_after_masking = qk_per_token + mask<br>        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)<br>        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>        qkv_attention_store.append(qkv_attention)<br><br>    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)<br>    w_layer = model[f&quot;layers.&#123;layer&#125;.attention.wo.weight&quot;]<br>    embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T)<br>    embedding_after_edit = final_embedding + embedding_delta<br>    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f&quot;layers.&#123;layer&#125;.ffn_norm.weight&quot;])<br>    w1 = model[f&quot;layers.&#123;layer&#125;.feed_forward.w1.weight&quot;]<br>    w2 = model[f&quot;layers.&#123;layer&#125;.feed_forward.w2.weight&quot;]<br>    w3 = model[f&quot;layers.&#123;layer&#125;.feed_forward.w3.weight&quot;]<br>    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)<br>    final_embedding = embedding_after_edit+output_after_feedforward<br></code></pre></td></tr></table></figure><h1id="我们现在有了最终的嵌入模型可以对下一个标记做出的最佳猜测">8.我们现在有了最终的嵌入，模型可以对下一个标记做出的最佳猜测</h1><p>嵌入的形状与常规令牌嵌入 [17x4096] 相同，其中 17 是令牌数量，4096是嵌入暗淡</p><figure><imgsrc="/images/llama3实现/v2-5b3c042a93c2a044a82e2e64f84d3065_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">final_embedding = rms_norm(final_embedding, model[&quot;norm.weight&quot;])<br>final_embedding.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2id="最后让我们将嵌入解码到令牌值中">最后，让我们将嵌入解码到令牌值中</h2><figure><imgsrc="/images/llama3实现/v2-6e7276c4975ec431052164c8c49b946a_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>我们将使用输出解码器将最终的嵌入转换为令牌</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">model[&quot;output.weight&quot;].shape<br>torch.Size([128256, 4096])<br></code></pre></td></tr></table></figure><h2id="我们使用最后一个标记的嵌入来预测下一个值">我们使用最后一个标记的嵌入来预测下一个值</h2><p>希望在我们的例子中，42 :) 注意：42是“生命、宇宙和一切的终极问题的答案”的答案，根据《银河系漫游指南》一书，大多数现代llms 都会回答这里有 42，这应该验证我们的整个代码！祝我好运 ：）</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">logits = torch.matmul(final_embedding[-1], model[&quot;output.weight&quot;].T)<br>logits.shape<br>torch.Size([128256])<br></code></pre></td></tr></table></figure><h2id="模型预测令牌编号-2983-作为下一个令牌这是-42-的令牌编号吗">模型预测令牌编号2983 作为下一个令牌，这是 42 的令牌编号吗？</h2><p>我正在向您宣传，这是代码的最后一个单元格，希望您玩得开心:)</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">next_token = torch.argmax(logits, dim=-1)<br>next_token<br>tensor(2983)<br></code></pre></td></tr></table></figure><h2 id="lets-go">Let's go</h2><figure><imgsrc="/images/llama3实现/v2-e436800962c747e6de871c364891ae90_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">tokenizer.decode([next_token.item()])<br>#输出&#x27;42&#x27;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>nlp</tag>
      
      <tag>原理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型训练Guidelines</title>
    <link href="/%E5%A6%82%E4%BD%95%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B.html"/>
    <url>/%E5%A6%82%E4%BD%95%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B.html</url>
    
    <content type="html"><![CDATA[<p>根据scaling law，模型越大，高质量数据越多，效果越好。</p><p>但还有一个很直观的情况，随着预训练样本的质量不断提升，训练手段的优化。新的模型，往往效果能轻松反超参数量两倍于它的模型。</p><span id="more"></span><h2 id="背景">1 背景</h2><p>根据scaling law，模型越大，高质量数据越多，效果越好。</p><p>但还有一个很直观的情况，随着预训练样本的质量不断提升，训练手段的优化。新的模型，往往效果能轻松反超参数量两倍于它的模型。</p><p>例如，最新出的minicpm，微信内部评测效果也是非常棒的。跟规模相对接近的2b、7b模型比，得分比qwen2b高，和qwen7b比有的高有的低。</p><p>这个是minicpm的详细技术文档。</p><p>[https://shengdinghu.notion.site/MiniCPM-c805a17c5c8046398914e47f0542095a]</p><p>这说明，现有参数量情况下，哪怕是2B尺度，也并没有得到充分训练。</p><h2 id="样本">2 样本</h2><h3 id="样本构成">2.1 样本构成</h3><p>大家已经达成一些基础的共识。</p><p>如中英混合比例大家都大差不差。</p><p>逻辑推理比较强的样本，像代码，数学。这种就是模型越大，混合的比例反而可以越高。</p><p>跟SFT是类似的，越大的模型，越聪明的模型，需要的SFT数据就越少。同理，越大的模型，越聪明，复杂样本混合比例就可以越高。</p><h3 id="样本质量">2.2 样本质量</h3><h3 id="基本清洗">2.1.1 基本清洗</h3><p>导致ppl崩掉的，都要清洗掉，政治敏感数据清洗，去重等，肯定是一个很长的pipeline。</p><p>大家比较一致的结论是，天工开源的那份预训练数据，是一个比较好的满足基础清洗要求的数据。</p><h3 id="进阶清洗">2.1.2 进阶清洗</h3><p>大家都不太方便展开，但可以透露的信息。</p><p>跟SFT一样，产出各种各样的label来刻画数据，有的公司实习生就优化几个label。</p><p>不过随着优化的往后拓展，这些label的投入产出比越来越难以评估。</p><h3 id="phi式的生成synthetic数据">2.1.3 PHI式的生成(synthetic)数据</h3><p>预训练清洗的pipeline搭建，对于开源团队，小公司来讲，成本其实还是蛮高的。</p><p>所以，基于开源数据，做一些聚类的topic。然后基于这些topic，丢到更大的模型，来构建一批高质量的数据，是一个反而比较低成本的方案。</p><h3 id="买数据">2.1.4 买数据</h3><p>嗯，这次大模型，除了李一舟。卖数据的公司，也是真的赚到钱了。</p><h3 id="不同训练阶段的训练样本">2.3 不同训练阶段的训练样本</h3><p>经过讨论，发现有三种方案。</p><h3 id="末期高质量样本minicpm">2.3.1 末期高质量样本（minicpm)</h3><p>快速收敛阶段和平稳阶段，都采用普通样本。</p><p>退火阶段，混入高质量样本来做教科书式的学习。</p><h3 id="初期高质量样本">2.3.2 初期高质量样本</h3><p>快速收敛阶段，以高质量样本为主，让模型快速收敛。</p><p>平稳阶段，逐步调整比例，增加更多的普通样本。</p><p>退火阶段，跟平稳阶段一致</p><h3 id="全程高质量样本phil方式">2.3.3全程高质量样本（PHIL方式）</h3><p>全程都是高质量样本</p><p><strong>这里大家讨论的蛮激烈的，有这么几点。</strong></p><p>第一，初期就加入高质量样本，初期收敛的更快。但高质量样本少，不断的重复学习高质量样本，会不会导致过拟合？但反方认为，人类的本质上就是复读机，特别对于小模型，不断的重复学习，好像也没太大问题。</p><p>第二，初期学习高质量样本，会不会导致初期模型的初始化，局限在特定的区域，后面的普通样本学了之后，也不一定能很好的跳出来，会不会影响泛化？但反方认为，末期加入高质量样本，反而会不会因为最后加入高质量样本，导致泛化能力受损，集中在高质量样本的领域？</p><p>第三，PHIL方式，大家很快达成一致，PHIL就是探索小模型能不能在特定领域达到SOTA。好处，特定榜单/领域效果会特别好。坏处，模型泛化能力会很差（但PHIL从来没说要做世界模型。</p><h3 id="小模型样本的极限在哪">2.4 小模型样本的极限在哪？</h3><p>到底喂了多少tokens，小模型参数才算是充分得到训练？</p><p>当天讨论，并没有一个很好的结论。</p><p>最近YI-9B的公开技术文档，做了一个有趣的尝试。把每层的输入和输出算cos，来评估模型是否训练的非常充分。</p><p>但内部讨论后，发现这种尝试有一个巨大的遗漏点。</p><p>前段时间，我们做longcontext调研，也是把每层也都单独做了一个分析。结论是，如果模型深度足够的话，有些层其实区分度是在降低的，相当于几层做了一层做的事情。</p><p>以及，另外一个可能，小模型每一层cos都小，有可能每一层在干不同的事，或者每一层都会注意到新的东西。大模型某些层cos大，有可能是因为句子太简单，大模型对结果更加肯定，靠后的层的功能没有被激活。</p><p>感觉这种评估方式，仍旧有一定的优化空间，也期待业内能公开更多好用的评估方式。</p><h2 id="训练">3 训练</h2><h3 id="tokenizer">3.1 tokenizer</h3><p>小模型过大的tokenizer的确是一种浪费。很多小模型有大tokenizer，一个潜在的可能性，作者人力不足，直接是把大模型的tokenizer拿来复用了。</p><h3 id="阶段">3.2 阶段</h3><p>现在大家预训练分成三个阶段。</p><p>快速收敛阶段，稳定阶段，<strong>退火阶段(minicpm比较显式的提出这个阶段）</strong></p><h3 id="为什么要分阶段">3.2.1 为什么要分阶段</h3><p>这个阶段来自于大家对loss曲线的观察，发现loss曲线的收敛就是这么一个特点。</p><p>然后，大家发现不同的loss曲线阶段，做一些针对性样本和参数的调整，能带来更好的效果，于是就这么分了。</p><h3 id="不同阶段学的是什么东西">3.2.2 不同阶段学的是什么东西？</h3><p>首先，我们现在的评估手段还是比较粗糙的，假如有了更细的评估手段，可能能观测到更多有趣的东西。</p><p>例如之前俊林做过关于涌现的分享，涌现从指标观测来看，就是突然出现的。但当把指标细化后，可以发现这个涌现好像也没那么突然，这个可以参考<ahref="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2310.03262">https://arxiv.org/abs/2310.03262</a>把原本离散的准确率在1e-5级别的时候的值估计出来了。</p><p>但反方这里又有不同的观点，我们用物理学的一个理论来解释涌现。</p><p>我们可以把涌现替换成相变来聊一聊它和指标突变的辩证关系：当我们谈论相变时，我们指的是物质从一种状态转变为另一种状态的过程，比如水从液态变成固态的过程（冰冻）或者从液态变成气态的过程（蒸发）。而指标突变通常指的是某种性质或者物理量在某个条件下突然发生明显的变化，比如在某个温度或者压力下，某种物质的导电性、磁性或者其他性质突然发生变化。</p><p>相变与指标突变之间存在着密切的关系，因为相变往往伴随着物质性质的突变。当物质经历相变时，它的一些性质会突然改变，这些性质就是我们所说的指标。举个例子，当水温降到0摄氏度以下时，水会由液态变成固态，这就是相变，同时水的密度也会突然增加，导致它的体积变小，这就是指标突变。</p><p>虽然相变往往伴随着物质性质的指标突破，但是不意味着不突变就不是相变，指标的突变不是相变的重点，相变的重点在于从一个状态/性质，变成另外一个状态/性质，这两种状态有着很不一样的差别。</p><p>尽管可以使用一些技巧方法来构造一些看起来特别平滑的指标来反对大模型涌现这个词汇，但是不可否认的事实是，在不同的尺寸变化或者数据量、计算量变化之后，人们可以非常明显地感知到大模型表现的巨大差异，这就是一个相变的结果，就像是炼制一门18连环刃的法器，从第一把的炼制到第18把，从个数的指标上来说是非常平滑的，但是从威力上来说，18把可以构建一个法阵，极大地增加了武器的威力，与之前不可同日而语。</p><h3 id="batch-size">3.3 batch size</h3><p>老调重弹的问题。</p><p>2020年，transformer出来后，当时大家就碰到这么一个问题。模型太大了，用尽可能能塞进内存的batchsize去train模型，来提升速度。</p><p>很快，大家发现batch size有个trade off。</p><p>batchsize过小，波动会比较大，不太容易收敛。但这种波峰，也有助于跳出局部最优，模型更容易有更好的泛化能力。</p><p>batchsize变大，步数整体变少，训练的步数更少，本来就波动就小，步数也少，同样本的情况下，你收敛的会更慢。</p><p>2020年其实有人就研究，如何用大batchsize，更快的训练的同时，也能收敛的更好。一个解决思路是优化了优化器，例如谷歌当年出的LAMB，就把batchsize从512扩充到了6w，收敛效果也不错。</p><h3 id="lr-scheduler">3.4 LR scheduler</h3><p>机器学习的目标，都是为了收敛loss，让学习的target和我们预测的target的loss尽可能低。</p><p>学习的过程，就是基于样本，分批（batchsize）丢进去。根据过去，现在学习的效果，来决定参数更新的方向和大小。</p><p>batch size这里是很清晰的。比较纠结的点是，优化器和LRscheduler这俩好像边界不是很清晰。</p><h3 id="lr-scheduler是什么">3.4.1 LR scheduler是什么</h3><p>假如我们要下山，山脚就是我们的目标，learningrate就是我们每一步要走多远。如果速度太快，可能开到山脚后，发现刹不住车，还会往山上多开一会，于是这样反复在目标处来回震荡。如果太小的话，到山脚的速度又太慢了。</p><p>现在主流的就是cosine，初期warmup learning rate线性增长，然后learningrate就是以余弦函数的周期方式周期性变换。</p><h3 id="优化器做什么">3.4.2 优化器做什么？</h3><p>优化器核心要解决的问题，初期怎么更好的学，那些地方要加速学，那些地方容易陷入局部最优，要如何跳出来。</p><p>现在的主流做法都是基于历史的反馈。</p><p>类似于爬山，某个地方你发现爬的很慢，那么就加下油门。有的地方你发现是下坡路，跑的贼快，那就就要松下油门，免得油门太快，直接从主路跑出去了。</p><p>从momentum，到adagrad，再到adam，这两年还有人在各种折腾。</p><h3 id="优化器和lr-scheduler如何协同工作">3.4.3 优化器和LRscheduler如何协同工作？</h3><p>问题就来了，LR scheduler决定了learningrate的大小。优化器也会根据历史情况来自动调整。</p><p>这俩会不会冲突？</p><p>优化器的优点刚刚说了，但它的缺点就是无论优化器怎么说的高大上，它本质上还规则，是大家基于调参经验，或者一些假设，定的规则。</p><p>规则就很难完美适配所有任务，事实上2020年左右，大家就发现不同的任务上不同的优化器效果是不同的。例如当年的炼丹经验，计算机视觉优先使用SGD(withMomentum)，NLP（特别是用Transformer）优先使用Adam，现在CV都上transformer了，那么就又切到了AdamW。</p><p>除此之外，还有一个learning ratedecay的问题，但这个偏经验，并不一定完全solid！</p><p>用CIFAR或者ImageNet跑一跑常见的模型，就会发现，最后如果不把learningrate降低下去，loss很难收敛到一个很小的数字。</p><p>SGD和Adam的收敛性证明也都是要求learningrate最后会降到足够低的。但自适应优化器的学习率不会在训练中自动降到很低。</p><p>现在大模型预训练，大家其实最关注的就是这个loss的收敛效果。</p><p>这个时候，LRschedule的出现就是一个比较好的补充，能够补足优化器的一些问题。</p><p>所以，你可以理解为，现在我们没有一个完美的油门，所以搞了俩油门，互相辅助。</p><p>优化器是个老司机的油门，好用，但人类的经验是有局限性的，很容易陷入局部最优跑不出来。</p><p>LR schedule像是一个全局的油门，定期更新，帮助老司机跳出局部最优。</p><h3 id="w-s-d的讨论和优化方案">3.4.4 W-S-D的讨论和优化方案</h3><p>minicpm提出了W-S-D LR scheduler，但stable阶段高learningrate，相当于把调节油门的压力全给到优化器了。</p><p>但S-D的确也有很多好处，例如我想train到什么时候就train到什么时候。</p><p>这里提出了一个解决思路，W-S-D是不是可以改成，warm-cosine-stable-decay，cosine占据训练阶段大头，甚至多个余弦波段，余弦波段多了，如上文所说，是不是能更好的跳出局部最优？</p><p>快要结束训练的时候，把cosine的learningrate给升上去，走一段stable+decay。</p><h3 id="退火加sft-和面">3.5 退火加sft &amp;“和面”</h3><p>前段时间，业界流行一个说法，你发现某块效果差，在预训练最后阶段补充一些类似的数据，效果就会蹭蹭的往上涨。</p><p>简称，和面——水多了加面，面多了加水。</p><p>刚开始，大家都很鄙视这种行为，觉得这种行为不就是刷榜么？</p><p>但现在我们来探讨这块的合理性，minicpm可以算是更进一步的“作弊”了，如果按照之前的观点。他都直接把sft数据混入了预训练数据里面，更加明目张胆的“刷榜”。</p><p>我个人觉得这里可以用两个角度去理解:</p><p>角度一，模型学习的训练动态角度，在退火的时候loss下降较stable和正常的cosine都要快，证明这里的学习效率在提升(最聪明的时候?)，而此时在这一刻使用高质量数据来喂给模型,可以尽可能发挥高质量数据的作用;</p><p>角度二， SFT数据较正常的文本数据，我猜测会更偏向于benchmark一些，因为SFT很多都是"QA型"结构的数据,对于模型认识bechmark有一定的改善。</p><p>之前预训练完毕后，直接上SFT数据，语料分布差距很大，其实天然是不太好的。这种作弊的行为，只要控制样本比例得当，反而能保证后面通用对其的一致性。</p><h2 id="再看scaling-law"><strong>4 再看scaling law</strong></h2><p>随着一些common sense的建立，scalinglaw的指导意义的确是在不断下降的。</p><p>举个例子，假如我有300张卡，我要train多大的模型？</p><p>计算方式，往往就变成，我大致知道训练1T-2Ttokens效果往往就不错了，这个模型两个月后我想赶一个发布会。那么就来反推，1T的tokens训练2个月，300张卡能train多大的。</p><p>但我们回到2020年，当大部分人都在基于bert做各种魔改的时候。</p><p>OpenAI发现了这么一个规律。数据，训练，参数一直增长下去，好像loss的确是在不断的下降哎？</p><p>于是，他们拿着这个paper去问微软的CTO，你想不想看看这个loss下降到一定程度会发生什么？</p><p>会发生什么？</p><p>chatgpt就出来了</p><blockquote><p>转载自:<ahref="https://zhuanlan.zhihu.com/p/686664720">如何从零开始训练大模型（minicpm分享&amp;讨论）- 知乎 (zhihu.com)</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>大模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>模型训练</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI行业技能点含金量统计分析</title>
    <link href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E7%9A%84%E4%B8%8D%E5%90%8C%E6%8A%80%E8%83%BD%E6%A0%91%E5%90%AB%E9%87%91%E9%87%8F%E5%88%86%E6%9E%90.html"/>
    <url>/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E7%9A%84%E4%B8%8D%E5%90%8C%E6%8A%80%E8%83%BD%E6%A0%91%E5%90%AB%E9%87%91%E9%87%8F%E5%88%86%E6%9E%90.html</url>
    
    <content type="html"><![CDATA[<p>在人工智能领域，有很多热议的技能选择，如CV还是NLP，TensorFlow还是PyTorch，python还是C++……<br />话题有争议，但是数据非常客观，让我们拭目以待吧~ <span id="more"></span></p><h3 id="数据说明">1.数据说明</h3><p><strong>数据来源：</strong>为BOSS直聘北京地区算法工程师相关岗位,数据时间是2024年五月,有效样本总量为2200个.<br /><strong>数据内容：</strong>招聘标题,薪资,福利待遇,关键词,岗位描述,公司,地址,招聘网址<br /><strong>统计方法：</strong>对样本数据通过相关的岗位关键词进行筛选,并统计筛选结果的薪资均值,岗位数量等信息,汇总为下表。</p><h3 id="ai技能点含金量排名">2.AI技能点含金量排名</h3><div id="container0" style="height: 500px;"></div><script type="text/javascript" src="https://registry.npmmirror.com/echarts/5.5.0/files/dist/echarts.min.js"></script><script type="text/javascript">var dom = document.getElementById('container0');var myChart = echarts.init(dom, null, {  renderer: 'canvas',  useDirtyRect: false});var app = {};var option;option = {  xAxis: {  type: 'category',  data: [    "视觉|cv|视频|图像",    "NLP|自然语言|LLM",    "多模态|大模型",    "机器学习",    "深度学习",    "软件开发",    "控制算法",    "推荐算法",    "数据挖掘|数据分析",    "地图|路径算法"  ],  axisLabel: {    rotate: 45, // 旋转标签以适应显示    interval: 0 // 设置为0表示显示全部标签  }  },  yAxis: {  type: 'value'  },  series: [  {    data: [378.43, 414.87, 439.42, 396.63, 395.89, 359.30, 379.71, 435.72, 358.50, 415.83],    type: 'bar',    showBackground: true,    backgroundStyle: {      color: 'rgba(180, 180, 180, 0.2)'    },  label: {      show: true, // 开启数据标签显示      position: 'top', // 数据标签的位置，这里是顶部      formatter: '{c}' // 格式化函数，这里使用默认的'{c}'表示显示数值    }}  ]  };if (option && typeof option === 'object') {  myChart.setOption(option);}window.addEventListener('resize', myChart.resize);</script><p><strong>多模态大模型</strong>以均值439k的平均年薪拔得头筹，<strong>推荐算法</strong>以436k紧随其后，<strong>NLP自然语言处理与地图路径算法工程师</strong>以415K的薪资并列第三。说起薪资较低的，分别是<strong>AI软件开发岗、数据挖掘分析岗</strong>，将近360K，也是难能可贵了。</p><h3id="ai技能点的需求量与含金量分布">3.AI技能点的需求量与含金量分布</h3><div id="container" style="height: 500px;"></div><script type="text/javascript" src="https://registry.npmmirror.com/echarts/5.5.0/files/dist/echarts.min.js"></script><script type="text/javascript">  var dom = document.getElementById('container');  var myChart = echarts.init(dom, null, {    renderer: 'canvas',    useDirtyRect: false  });  var app = {};  var option;  option = {    dataset: {      source: [        ['平均年薪(k)', '岗位数量', '职业技能'],        [378.43, 708, "视觉|cv|视频|图像"],        [414.87, 350, "NLP|自然语言|LLM"],        [439.42, 545, "多模态|大模型"],        [396.63, 691, "机器学习"],        [395.89, 785, "深度学习"],        [359.3, 154, "软件开发"],        [379.71, 98, "控制算法"],        [435.72, 171, "推荐算法"],        [358.5, 100, "数据挖掘|数据分析"],        [415.83, 36, "地图|路径算法"],        [384.27, 1032, "python"],        [396.68, 1225, "C++"],        [384.61, 206, "TensorFlow"],        [390.4, 193, "PyTorch"],      ]    },    grid: { containLabel: true },    xAxis: { name: '岗位数量' },    yAxis: { type: 'category' },    visualMap: {      orient: 'horizontal',      left: 'center',      min: 350,      max: 450,      text: ['High 平均年薪(k)', 'Low 平均年薪(k)'],      // Map the 平均年薪(k) column to color      dimension: 0,      inRange: {        color: ['#65B581', '#FFCE34', '#FD665F']      }    },    series: [      {        type: 'bar',        encode: {          // Map the "岗位数量" column to X axis.          x: '岗位数量',          // Map the "职业技能" column to Y axis          y: '职业技能'        }      }    ]  };  if (option && typeof option === 'object') {    myChart.setOption(option);  }  window.addEventListener('resize', myChart.resize);</script><p><strong>计算机视觉CV与自然语言NLP</strong>：视觉（CV）与图像、视频打交道，年薪约378k，岗位多；而自然语言处理（NLP）年薪诱人达415k，但岗位少。想多赚钱选NLP，想稳就业选CV。</p><p><strong>多模态与大模型</strong>：新兴的多模态与大模型领域，年薪高达439k，岗位也不少。想站风口就选它！</p><p><strong>机器学习与深度学习</strong>：机器学习年薪约397k，岗位稳定；深度学习略低但需求多。两者薪资相近，看需求选。</p><p><strong>软件开发与控制算法</strong>：软件开发年薪359k但岗位少；控制算法稍好，年薪380k。两者传统但重要。</p><p><strong>推荐算法与数据挖掘</strong>：推荐算法年薪高达436k但岗位少；数据挖掘年薪359k。喜欢数据处理就选它们。</p><p><strong>地图与路径算法</strong>：小众但高薪的地图与路径算法，年薪416k但竞争大。适合专长者挑战。</p><p><strong>编程语言：Python与C++</strong>Python年薪384k岗位多，C++年薪略高且岗位更多。两者都是AI开发利器。</p><p><strong>框架选择：TensorFlow与PyTorch</strong>TensorFlow年薪385k，PyTorch年薪390k。两者差距小，选谁看心情和项目需求。</p><h3id="技能点与教育程度工作经验及薪资的关系">4.技能点与教育程度，工作经验及薪资的关系</h3><h4id="技能点和教育程度对薪资的影响单位k">4.1技能点和教育程度对薪资的影响（单位：K）</h4><table><thead><tr class="header"><th></th><th>专科</th><th>本科</th><th>985/211</th><th>硕士</th><th>博士</th></tr></thead><tbody><tr class="odd"><td>地图/路径</td><td>-</td><td>481</td><td>330</td><td>470</td><td>1260</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>-</td><td>394</td><td>383</td><td>358</td><td>277</td></tr><tr class="odd"><td>推荐算法</td><td>-</td><td>378</td><td>456</td><td>397</td><td>397</td></tr><tr class="even"><td>PyTorch</td><td>360</td><td>370</td><td>427</td><td>383</td><td>365</td></tr><tr class="odd"><td>控制算法</td><td>-</td><td>364</td><td>390</td><td>400</td><td>-</td></tr><tr class="even"><td>c++/C++</td><td>315</td><td>359</td><td>395</td><td>418</td><td>420</td></tr><tr class="odd"><td>多模态/大模型</td><td>-</td><td>352</td><td>367</td><td>465</td><td>459</td></tr><tr class="even"><td>TensorFlow</td><td>360</td><td>343</td><td>367</td><td>383</td><td>365</td></tr><tr class="odd"><td>机器学习</td><td>360</td><td>343</td><td>395</td><td>434</td><td>433</td></tr><tr class="even"><td>深度学习</td><td>315</td><td>339</td><td>408</td><td>423</td><td>429</td></tr><tr class="odd"><td>python</td><td>315</td><td>330</td><td>361</td><td>411</td><td>394</td></tr><tr class="even"><td>NLP/自然语言/LLM</td><td>-</td><td>328</td><td>373</td><td>480</td><td>556</td></tr><tr class="odd"><td>软件/开发</td><td>360</td><td>327</td><td>252</td><td>394</td><td>393</td></tr><tr class="even"><td>视觉/cv/视频/图像</td><td>360</td><td>320</td><td>434</td><td>411</td><td>514</td></tr></tbody></table><h4id="技能点和教育程度对工作岗位数量的影响单位个">4.2技能点和教育程度对工作岗位数量的影响（单位：个）</h4><table><thead><tr class="header"><th></th><th>专科</th><th>本科</th><th>985/211</th><th>硕士</th><th>博士</th></tr></thead><tbody><tr class="odd"><td>c++/C++</td><td>2</td><td>248</td><td>23</td><td>255</td><td>42</td></tr><tr class="even"><td>python</td><td>2</td><td>223</td><td>22</td><td>212</td><td>39</td></tr><tr class="odd"><td>深度学习</td><td>2</td><td>157</td><td>25</td><td>166</td><td>25</td></tr><tr class="even"><td>机器学习</td><td>1</td><td>144</td><td>23</td><td>117</td><td>29</td></tr><tr class="odd"><td>视觉/cv/视频/图像</td><td>1</td><td>126</td><td>16</td><td>159</td><td>29</td></tr><tr class="even"><td>多模态/大模型</td><td>-</td><td>115</td><td>21</td><td>111</td><td>26</td></tr><tr class="odd"><td>NLP/自然语言/LLM</td><td>-</td><td>91</td><td>21</td><td>82</td><td>11</td></tr><tr class="even"><td>推荐算法</td><td>-</td><td>50</td><td>11</td><td>31</td><td>5</td></tr><tr class="odd"><td>TensorFlow</td><td>1</td><td>41</td><td>7</td><td>40</td><td>9</td></tr><tr class="even"><td>PyTorch</td><td>1</td><td>39</td><td>7</td><td>38</td><td>9</td></tr><tr class="odd"><td>软件/开发</td><td>1</td><td>38</td><td>3</td><td>20</td><td>3</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>-</td><td>21</td><td>5</td><td>17</td><td>4</td></tr><tr class="odd"><td>地图/路径</td><td>-</td><td>17</td><td>1</td><td>11</td><td>1</td></tr><tr class="even"><td>控制算法</td><td>-</td><td>13</td><td>3</td><td>19</td><td>-</td></tr></tbody></table><h4id="技能点和工作经验对平均年薪的影响单位k">4.3技能点和工作经验对平均年薪的影响（单位：K）</h4><table><thead><tr class="header"><th></th><th>应届</th><th>一年</th><th>两年</th><th>三年</th><th>四年</th><th>五年及以上</th></tr></thead><tbody><tr class="odd"><td>推荐算法</td><td>351</td><td>261</td><td>433</td><td>433</td><td>-</td><td>462</td></tr><tr class="even"><td>TensorFlow</td><td>272</td><td>343</td><td>324</td><td>415</td><td>360</td><td>427</td></tr><tr class="odd"><td>PyTorch</td><td>285</td><td>350</td><td>318</td><td>409</td><td>360</td><td>517</td></tr><tr class="even"><td>c++/C++</td><td>373</td><td>356</td><td>389</td><td>407</td><td>383</td><td>438</td></tr><tr class="odd"><td>多模态/大模型</td><td>409</td><td>355</td><td>427</td><td>397</td><td>423</td><td>428</td></tr><tr class="even"><td>控制算法</td><td>360</td><td>-</td><td>402</td><td>397</td><td>-</td><td>338</td></tr><tr class="odd"><td>NLP/自然语言/LLM</td><td>544</td><td>359</td><td>392</td><td>392</td><td>442</td><td>416</td></tr><tr class="even"><td>软件/开发</td><td>-</td><td>339</td><td>325</td><td>386</td><td>216</td><td>344</td></tr><tr class="odd"><td>机器学习</td><td>330</td><td>392</td><td>383</td><td>383</td><td>454</td><td>472</td></tr><tr class="even"><td>python</td><td>304</td><td>356</td><td>396</td><td>382</td><td>425</td><td>366</td></tr><tr class="odd"><td>深度学习</td><td>357</td><td>368</td><td>399</td><td>381</td><td>442</td><td>420</td></tr><tr class="even"><td>视觉/cv/视频/图像</td><td>406</td><td>360</td><td>454</td><td>379</td><td>397</td><td>505</td></tr><tr class="odd"><td>地图/路径</td><td>-</td><td>406</td><td>375</td><td>350</td><td>927</td><td>-</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>242</td><td>415</td><td>294</td><td>308</td><td>-</td><td>530</td></tr></tbody></table><h4id="技能点和工作经验对工作岗位数量的影响单位个">4.4技能点和工作经验对工作岗位数量的影响（单位：个）</h4><table><thead><tr class="header"><th></th><th>应届</th><th>一年</th><th>两年</th><th>三年</th><th>四年</th><th>五年及以上</th></tr></thead><tbody><tr class="odd"><td>c++/C++</td><td>38</td><td>70</td><td>142</td><td>211</td><td>17</td><td>92</td></tr><tr class="even"><td>python</td><td>35</td><td>49</td><td>143</td><td>187</td><td>14</td><td>70</td></tr><tr class="odd"><td>深度学习</td><td>19</td><td>44</td><td>99</td><td>146</td><td>9</td><td>58</td></tr><tr class="even"><td>视觉/cv/视频/图像</td><td>18</td><td>32</td><td>89</td><td>125</td><td>8</td><td>59</td></tr><tr class="odd"><td>机器学习</td><td>26</td><td>32</td><td>85</td><td>120</td><td>10</td><td>41</td></tr><tr class="even"><td>多模态/大模型</td><td>13</td><td>25</td><td>71</td><td>116</td><td>9</td><td>39</td></tr><tr class="odd"><td>NLP/自然语言/LLM</td><td>7</td><td>16</td><td>58</td><td>78</td><td>4</td><td>42</td></tr><tr class="even"><td>推荐算法</td><td>6</td><td>4</td><td>40</td><td>33</td><td>-</td><td>14</td></tr><tr class="odd"><td>PyTorch</td><td>5</td><td>16</td><td>19</td><td>32</td><td>1</td><td>21</td></tr><tr class="even"><td>TensorFlow</td><td>7</td><td>18</td><td>21</td><td>32</td><td>1</td><td>19</td></tr><tr class="odd"><td>软件/开发</td><td>-</td><td>6</td><td>22</td><td>25</td><td>1</td><td>11</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>3</td><td>4</td><td>18</td><td>16</td><td>-</td><td>6</td></tr><tr class="odd"><td>控制算法</td><td>1</td><td>-</td><td>13</td><td>13</td><td>-</td><td>8</td></tr><tr class="even"><td>地图/路径</td><td>-</td><td>9</td><td>5</td><td>9</td><td>7</td><td>-</td></tr></tbody></table><h3 id="高性价比工作岗位排名">5高性价比工作岗位排名</h3><h4id="加权工作经验工作岗位数量与薪资计算技能点的得分及排名情况">加权工作经验,工作岗位数量与薪资,计算技能点的得分及排名情况</h4><ul><li>基本原则：学历要求越低，工作经验要求越低，平均年薪越高的技能点越好，其评分会越高</li><li>学历，工作经验数据划分五个等级，然后将三个特征数据归一化处理，彼此相乘，再乘上百分系数，得到最终评分</li></ul><div id="container2" style="height: 400px;"></div><script type="text/javascript" src="https://registry.npmmirror.com/echarts/5.5.0/files/dist/echarts.min.js"></script><script type="text/javascript">  var dom = document.getElementById('container2');  var myChart = echarts.init(dom, null, {    renderer: 'canvas',    useDirtyRect: false  });  var app = {};  var option;  option = {    dataset: [      {        dimensions: ['name','score'],        source: [          ["NLP/自然语言/LLM",30.16],          ["PyTorch",14.78],          ["TensorFlow",15.56],          ["c++/C++",83.54],          ["python",74.84],          ["地图/路径",15.25],          ["多模态/大模型",45.56],          ["控制算法",10.59],          ["推荐算法",29.53],          ["数据挖掘/数据分析",5.26],          ["机器学习",46.19],          ["深度学习",52.13],          ["视觉/cv/视频/图像",42.48],          ["软件/开发",13.56]        ]      },      {        transform: {          type: 'sort',          config: { dimension: 'score', order: 'desc' }        }      }    ],    xAxis: {      type: 'category',      axisLabel: { interval: 0, rotate: 30 }    },    yAxis: {},    series: {      type: 'bar',      encode: { x: 'name', y: 'score' },            datasetIndex: 1    }  };  if (option && typeof option === 'object') {    myChart.setOption(option);  }  window.addEventListener('resize', myChart.resize);</script>]]></content>
    
    
    <categories>
      
      <category>Data Visualization</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ECharts</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【B站】大模型之路-从分治法至端到端,再到存算训一体</title>
    <link href="/B%E7%AB%99_%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E4%B9%8B%E8%B7%AF--%E4%BB%8E%E5%88%86%E6%B2%BB%E6%B3%95%E8%87%B3%E7%AB%AF%E5%88%B0%E7%AB%AF,%E5%86%8D%E5%88%B0%E5%AD%98%E7%AE%97%E8%AE%AD%E4%B8%80%E4%BD%93.html"/>
    <url>/B%E7%AB%99_%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E4%B9%8B%E8%B7%AF--%E4%BB%8E%E5%88%86%E6%B2%BB%E6%B3%95%E8%87%B3%E7%AB%AF%E5%88%B0%E7%AB%AF,%E5%86%8D%E5%88%B0%E5%AD%98%E7%AE%97%E8%AE%AD%E4%B8%80%E4%BD%93.html</url>
    
    <content type="html"><![CDATA[<h1id="大模型发展之路--从分治法至端到端再到存算训一体">大模型发展之路--从分治法至端到端,再到存算训一体</h1><p>安克创新CEO阳萌对人工智能过去、现在和未来的思考。他认为大模型和transformer只是阶段性的算法实现,未来一定是仿生算法的大趋势。他还谈到了分治法作为经典的范式有其明显的局限,而端到端的方案是人类理性解决问题的必经之路。他指出,人工智能领域的范式每五到十年就会出现一个全新的范式,存算一体已经成为业界新宠。</p><span id="more"></span><iframe src="//player.bilibili.com/player.html?isOutside=true&amp;aid=1954475860&amp;bvid=BV1gC41177xR&amp;cid=1538517104&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe><h3id="人工智能领域的发展历程和未来趋势以及斯蒂文杨萌对于人工智能的看法和建议">人工智能领域的发展历程和未来趋势,以及斯蒂文杨萌对于人工智能的看法和建议。</h3><p>00:01大模型解决不了英伟达的难题</p><p>00:45人工智能的发展历程</p><p>03:16人工智能领域的范式转变</p><h3id="分治法在人工智能领域的应用并探讨了端到端算法和分支法的优缺点">分治法在人工智能领域的应用,并探讨了端到端算法和分支法的优缺点。</h3><p>04:38分治法和端到端学习：分治法是一种解决问题的方法，而端到端学习则是一种新兴的方法。</p><p>05:53分治法可以帮助解决自然语言处理和自动驾驶等领域的问题。</p><p>08:29柔性连接和人类智能：人类的智能具有柔性连接的特点，而机器智能需要更多的研究来实现这样的特性。</p><h3id="搜索算法工程师使用分治法进行搜索并探讨了算法硬件和数据在人工智能中的重要性">搜索算法工程师使用分治法进行搜索,并探讨了算法、硬件和数据在人工智能中的重要性。</h3><p>09:04分治法在搜索引擎中的应用</p><p>12:05GPU和transformer算法的关系</p><p>13:10特斯拉和英伟达在自动驾驶领域的竞争</p><h3id="gpu芯片的结构和工作原理以及现代大模型在训练和推理端的不同应用">GPU芯片的结构和工作原理,以及现代大模型在训练和推理端的不同应用。</h3><p>13:28GPU是封装的芯片，其中包括运算核心和内存。在运算过程中，参数存在两边的内存中。</p><p>14:12大模型的训练需要多卡参与，而推理是将参数从内存中搬运到计算中心进行计算。</p><p>16:38Transformer模型不是一段一段地解决问题的，而是通过整体的参数进行端到端的解决问题。</p><h3id="gpu的发展趋势和优势并提出了人类应该借鉴大脑的运行方式来设计未来的芯片">GPU的发展趋势和优势,并提出了人类应该借鉴大脑的运行方式来设计未来的芯片。</h3><p>17:53计算性能与模型发展问题</p><p>21:35GPU的不足与大脑的差异</p><p>22:04适合大脑的计算芯片与内存位置</p><h3id="存算一体的概念以及如何实现存算一体的芯片并探讨了其在未来ai发展中的潜力">存算一体的概念,以及如何实现存算一体的芯片,并探讨了其在未来AI发展中的潜力。</h3><p>22:10存算一体芯片可以实现大模型的算法，节省能耗，是未来AI硬件的发展趋势。</p><p>25:32存算一体芯片可以应用在智能家居、智能音箱等场景中。</p><p>26:31安克创新正在研发存算一体芯片，并已经取得了一些成果。</p><h3id="算法和硬件之间的关系以及未来可能的发展趋势同时探讨了人工智能可能带来的影响">算法和硬件之间的关系，以及未来可能的发展趋势，同时探讨了人工智能可能带来的影响。</h3><p>26:44下一代算法：下一代算法可能会是一种一边学习一边进化的算法。</p>]]></content>
    
    
    <categories>
      
      <category>B站</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
      <tag>视频分享</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一首歌的时间部署本地Llama3大模型</title>
    <link href="/%E4%B8%80%E9%A6%96%E6%AD%8C%E7%9A%84%E6%97%B6%E9%97%B4-%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E4%B8%93%E5%B1%9Ellama3%E5%A4%A7%E6%A8%A1%E5%9E%8B.html"/>
    <url>/%E4%B8%80%E9%A6%96%E6%AD%8C%E7%9A%84%E6%97%B6%E9%97%B4-%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E4%B8%93%E5%B1%9Ellama3%E5%A4%A7%E6%A8%A1%E5%9E%8B.html</url>
    
    <content type="html"><![CDATA[<p>LLaMA3真的是相当相当炸裂啊！远超过去的体验！看数据Llama3-8B超过Mistra-7BMMLU10分；70B超过Claude3Sonet3分。这是一个惊人的成绩，一个开源模型超过闭源模型这样多。我只能说Meta是真正的OpenAI。自从它从Meta这个邪路上转正后，在OpenAI的路上一骑绝尘了！不废话，动手来给自己的电脑部署下吧。 <span id="more"></span></p><h2 id="有什么硬件要求"><strong>有什么硬件要求</strong></h2><p>N卡独占，起步4G显存，建议8G＋。纯CPU也能跑，如果你不嫌慢的话。</p><h2 id="安装lm-studio"><strong>1. 安装LM studio</strong></h2><p>就这个软件(<a href="https://lmstudio.ai/">LM Studio - Discover,download, and run local LLMs</a>)</p><figure><imgsrc="/images/本地部署Llama3大模型/v2-3a61b06246c57b88fcd83f17062c10df_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>安装成功，打开后应该出现如下界面</p><figure><img src="/images/本地部署Llama3大模型/image-20240514080559847.png"alt="image-20240514080559847" /><figcaption aria-hidden="true">image-20240514080559847</figcaption></figure><h2 id="选择llama3-8b模型"><strong>2. 选择llama3-8B模型</strong></h2><p>我们直接搜索llama 3-8B，找到该模型</p><figure><img src="/images/本地部署Llama3大模型/image-20240514081038674.png"alt="image-20240514081038674" /><figcaption aria-hidden="true">image-20240514081038674</figcaption></figure><p>当然我们也可以选择其他模型，模型选择的重要因素是大小，也就是参数量。模型参数量一般写在名字上，比如Dolphin 2.6 Mistral 7b – DPO Laser就是7B大小，也就是70亿参数。根据自己的电脑内存和显存容量选（CPU运行就看内存，GPU运行就看显存，混合运行就两个加起来），我电脑是8G显存，用的7B模型。</p><p>然后就是模型指标，现在huggingface上有成百上千个LLM，可以根据benchmark的成绩选，排名网页在此：<ahref="https://link.zhihu.com/?target=https%3A//huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">OpenLLM Leaderboard - a Hugging Face Space by HuggingFaceH4</a> 。</p><p>还有就是模型特性，比如是否经过审查，适合于什么类型的工作等。</p><h2 id="下载gguf文件"><strong>3. 下载gguf文件</strong></h2><h3 id="在lm-studio内部下载需要配置网络"><strong>1. 在LMStudio内部下载，需要配置网络</strong></h3><p>如果有国际互联网连接就可以直接下载。如果没有见下一步。</p><h3 id="在huggingface下载并转移到lm-studio中"><strong>2.在huggingface下载并转移到LM Studio中</strong></h3><h3 id="下载"><strong>1. 下载</strong></h3><p>手动将网址复制到浏览器下载。</p><figure><img src="/images/本地部署Llama3大模型/image-20240514081632465.png"alt="image-20240514081632465" /><figcaption aria-hidden="true">image-20240514081632465</figcaption></figure><h3 id="移动下载的gguf文件到lm-studio识别的位置"><strong>2.移动下载的gguf文件到LM studio识别的位置</strong></h3><figure><img src="/images/本地部署Llama3大模型/image-20240514081756888.png"alt="image-20240514081756888" /><figcaption aria-hidden="true">image-20240514081756888</figcaption></figure><p>打开My models,找到gguf文件位置，然后在系统文件管理器中管理好你下载的gguf文件路径，格式为models/A/B/xxx.gguf。再重启LMstudio就能看到它。</p><h2 id="运行"><strong>4. 运行</strong></h2><h3 id="cpu运行"><strong>1.CPU运行</strong></h3><p>同GPU运行，但不用改settings 中的 GPU 参数。</p><h3 id="gpu运行"><strong>2.GPU运行</strong></h3><figure><img src="/images/本地部署Llama3大模型/image-20240514082144052.png"alt="image-20240514082144052" /><figcaption aria-hidden="true">image-20240514082144052</figcaption></figure><p>然后点击窗口上方的Select a model toload，加载上一步下载的模型就可以了。任务管理器中可以监视显存占用。</p><p>如果成功加载到显卡，就可以在下方与其对话了。</p><figure><img src="/images/本地部署Llama3大模型/image-20240514082345176.png"alt="image-20240514082345176" /><figcaption aria-hidden="true">image-20240514082345176</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>项目部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北京人工智能行业薪资大揭秘</title>
    <link href="/%E5%8C%97%E4%BA%AC%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E8%96%AA%E8%B5%84%E5%A4%A7%E6%8F%AD%E7%A7%98.html"/>
    <url>/%E5%8C%97%E4%BA%AC%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E8%96%AA%E8%B5%84%E5%A4%A7%E6%8F%AD%E7%A7%98.html</url>
    
    <content type="html"><![CDATA[<p><img src="/images/ai_salary/1714717993428.png" />BOSS直聘数据，含北京市各区[<code>算法工程师|人工智能</code>]岗位数据7534条<span id="more"></span> # <strong>北京2024人工智能行业薪资大揭秘</strong></p><h3 id="数据说明">1.数据说明</h3><p>数据时间:<code>2024年5月</code></p><p>数据来源:**BOSS直聘,爬取北京市各区[<code>算法工程师|人工智能</code>]岗位数据7534条(多次爬取结果)</p><p><strong>数据清洗:</strong>提取岗位内容中包含<code>[人工智能|算法|nlp|cv]</code>的内容,并执行去重操作,得到岗位数据2208个.</p><p><img src="/images/ai_salary/1714721994550.png" /></p><h3 id="薪资分析">2.薪资分析</h3><p><img src="/images/ai_salary/1714716697686.png" /></p><p><strong>薪资上限，星辰大海般的梦想</strong>：首先，让人眼前一亮的是薪资上限——竟然高达<strong>1800k（年薪）</strong>！这意味着在这个行业，如果你拥有出色的才华和丰富的经验，那么年薪百万的梦想并非遥不可及。当然，这样的高薪也对应着极高的工作要求和挑战。</p><p><strong>薪资下限，无良公司真没下限</strong>：而对于那些刚刚步入人工智能行业的新人或者初入这个领域的小伙伴们来说，薪资下限为24k（年薪）,在北京还不够房租的,试问这些无良公司,你们的良心不会痛吗。</p><p><strong>平均年薪，舒适圈的魅力</strong>：说到最吸引人的部分，莫过于平均年薪了。北京人工智能行业的平均年薪高达<strong>400k</strong>左右，这真是一个赏心悦目的数字,另外年薪的众数和中位数都是<strong>360k</strong>,不知道屏幕前的你达到平均水平没有。</p><p><strong>人工智能岗位平均年薪与下限年薪对比:</strong></p><p><img src="/images/ai_salary/1714717993428.png" /></p><h3 id="岗位要求">3.岗位要求</h3><p>人工智能行业这么卷,是不是得<code>985\211起步,研究生占半数</code>呢?我们用数据来说话:</p><p><img src="/images/ai_salary/1714721621417.png" /></p><p>根据统计的2208个岗位数据来看,研究生占比30.4%,反而是<strong>本科生占据了大多数</strong>,占比达<strong>64.9%</strong>,本科生才是人工智能产业的中坚力量.不过AI行业的起步门槛是真高,大专和学历不限的岗位占比仅<strong>2.4%.</strong></p><p>在岗位经验来看,<strong>人工智能行业的包容性还是比较大的</strong>,经验不限的占到了17.91%(越缺人才的行业,这个指标越高),3-5年的岗位占比超过一半(鲜明的新兴行业).现在来看,又是招兵买马又是百模大战,<strong>人工智能的时代才刚开始.</strong></p><h3 id="薪资与学历和经验的关系">4.薪资与学历和经验的关系</h3><h4 id="学历vs薪资">4.1 学历VS薪资</h4><p><strong>大专小鲜肉</strong>：虽然起步稍低，但凭借着一股不服输的劲头，也能拿到251k的薪资，证明了在人工智能领域，实力非常的重要。</p><p><strong>本科高手</strong>：他们像是中流砥柱，稳稳地占据了薪资的中上游，379k的薪资，是对他们扎实基础和广泛知识的认可。</p><p><strong>硕士精英</strong>：他们在学历上更上一层楼，薪资也随之水涨船高，416k的薪资，是他们辛勤付出的回报。</p><p><strong>博士大佬</strong>：一出场就自带光环，稳稳地站在了薪资的金字塔尖，462k的薪资，仿佛在告诉大家：“知识就是力量，学历就是金钱！”</p><p><strong>学历不限</strong>：这个神秘的角色，似乎不受学历的束缚，凭借着自己的独特技能和经验，也能轻松拿到385k的薪资，可谓是“英雄不问出处”。</p><p><img src="/images/ai_salary/1714719207889.png" /></p><h3 id="经验vs薪资">4.2 经验VS薪资</h3><p><strong>1-3年新鲜人儿</strong>：初出茅庐的你，薪资322k，够你喝不少星巴克了！但别停步，未来更精彩！</p><p><strong>1年以内小鲜肉</strong>：应届生们，你们薪资333k，起点不错！不过这只是起点，挑战还在后头哦！</p><p><strong>3-5年小有成就</strong>：404k的薪资，帝都<strong>租房</strong>没问题！继续加油，成为公司顶梁柱！</p><p><strong>5-10年资深玩家</strong>：资深大佬，458k薪资，生活舒适还能追梦！多年打拼，果然值得！</p><p><strong>10年以上大佬级人物</strong>：传奇大佬，467k薪资，人生赢家！人脉经验都丰富，这钱你应得！</p><p><strong>在校/应届小白</strong>：小白们，150k只是开始，努力学习，未来可期！</p><p><strong>经验不限的小伙伴</strong>：无门槛岗位，378k薪资，虽有挑战，但你有实力，定能闯出一片天！</p><p><img src="/images/ai_salary/1714719746033.png" /></p><hr /><p>在北京,人工智能行业以平均年薪<strong>400k</strong>的高薪,<strong>经验不限</strong>的要求,让无数人心生向往。尽管如此,本科学历只是<strong>入行地板砖</strong>,稳妥些确实得硕士学历.但长远来看,AI行业是一个不断发展的增量市场,它注定要成为推动社会变革的新质生产力,你<strong>准备好迎接这个崭新的时代了吗</strong>?</p>]]></content>
    
    
    
    <tags>
      
      <tag>可视化</tag>
      
      <tag>数据分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【B站】从零开始学习大语言模型-Lyi</title>
    <link href="/B%E7%AB%99_%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E4%B9%A0%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-Lyi.html"/>
    <url>/B%E7%AB%99_%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E4%B9%A0%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-Lyi.html</url>
    
    <content type="html"><![CDATA[<h1 id="从零开始学习大语言模型-lyi">从零开始学习大语言模型-Lyi</h1><p>林亦是我比较喜欢的一个UP，视频讲述了他对深度学习基本范式的回顾和梳理。主要介绍了神经网络模型的结构和训练过程，以及当前流行的大语言模型——基于神经网络的技术。视频指出，构建一个能力强、学习效率高的模型是影响学习效果的关键，也是深度学习研究的核心问题。整个视频围绕着数据和模型展开，梳理了深度学习的核心概念和基本流程。</p><span id="more"></span><iframe src="//player.bilibili.com/player.html?isOutside=true&amp;aid=1750586968&amp;bvid=BV1v4421w7pU&amp;cid=1441154247&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe><h3id="机器学习的概念和分类着重讲解了监督学习和无监督学习以及模型的重要性">机器学习的概念和分类,着重讲解了监督学习和无监督学习,以及模型的重要性。</h3><p>00:01机器学习概念与语言任务</p><p>02:01监督学习与无监督学习</p><p>04:00机器学习模型的构建和选择</p><h3id="模型的概念以及神经网络模型的运作原理包括感知机单元和多层神经网络模型">模型的概念,以及神经网络模型的运作原理,包括感知机单元和多层神经网络模型。</h3><p>04:26模型训练与神经网络结构</p><p>06:40感知机的原理和激活函数</p><p>07:45多层神经网络模型和信息归纳能力</p><h3id="神经网络中的函数导数偏导数损失函数和梯度下降等概念以及如何用反向传播算法优化权重值">神经网络中的函数、导数、偏导数、损失函数和梯度下降等概念,以及如何用反向传播算法优化权重值。</h3><p>08:15每个单元之间的连接是一个权重数值，这些数值可以通过反向传播算法进行优化。</p><p>09:27训练神经网络的流程：训练程序会为每个输入变量随机分配一个权重值，然后通过前向传播和梯度下降算法不断优化权重值，直到损失函数最小化。</p><p>11:08梯度下降算法：通过不断更新权重值，沿着梯度下降的方向最快达到最低损失函数值。</p><h3id="如何使用链式法则解决复杂函数的导数计算问题以及模型训练中的收敛和超参数设置">如何使用链式法则解决复杂函数的导数计算问题,以及模型训练中的收敛和超参数设置。</h3><p>12:22链式法则与模型更新</p><p>15:26深度学习中的残差网络</p><p>15:50梯度消失与跳过连接</p><h3id="残差网络的作用和意义以及机器学习中泛化能力的评估和重要性">残差网络的作用和意义,以及机器学习中泛化能力的评估和重要性。</h3><p>17:05模型遇到从未见过的数据时，能不能整明白。</p><p>19:16大模型的转变：从小型专用模型到大型通用模型的转变。</p><h3id="机器如何理解人类语言的基础知识并提到了接下来将进一步探讨这一话题">机器如何理解人类语言的基础知识,并提到了接下来将进一步探讨这一话题。</h3><p>19:45机器理解人类语言的基础知识</p><p>19:52视频结束及作者告别</p>]]></content>
    
    
    <categories>
      
      <category>B站</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
      <tag>视频分享</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ROC曲线原理及绘制</title>
    <link href="/ROC%E6%9B%B2%E7%BA%BF%E7%BB%98%E5%88%B6.html"/>
    <url>/ROC%E6%9B%B2%E7%BA%BF%E7%BB%98%E5%88%B6.html</url>
    
    <content type="html"><![CDATA[<p>ROC曲线（Receiver Operating CharacteristicCurve）是一种用于评价二分类模型性能的图形工具。它展示了模型在不同阈值下的分类性能，通过绘制假阳性率（FPR）和真阳性率（TPR）之间的关系来表现。</p><span id="more"></span><h3 id="混淆矩阵">1.混淆矩阵</h3><p><strong>混淆矩阵</strong>是对<strong>预测正例</strong>样本的进一步分析,而<strong>准确率</strong>是综合了正例与反例的比例.</p><table><thead><tr class="header"><th></th><th>预测值</th><th>预测值</th><th></th></tr></thead><tbody><tr class="odd"><td></td><td><strong>正例</strong> (positive)</td><td><strong>假例</strong> (negtive)</td><td></td></tr><tr class="even"><td><strong>真实正例</strong></td><td>真正例TP</td><td><strong>伪反例</strong>FN</td><td>TPR=TP/真实正例</td></tr><tr class="odd"><td><strong>真实假例</strong></td><td><strong>伪正例</strong> FP</td><td>真反例TN</td><td>FPR=FP/真实假例</td></tr></tbody></table><h3 id="roc曲线的含义">2.ROC曲线的含义</h3><p>ROC曲线（Receiver Operating CharacteristicCurve）是一种用于评价二分类模型性能的图形工具。它展示了模型在不同阈值下的分类性能，通过绘制假阳性率（FPR）和真阳性率（TPR）之间的关系来表现。</p><ul><li><p><strong>真阳性率（TPR, True PositiveRate）</strong>：也称为灵敏度（sensitivity）或召回率（recall），表示真正被分类为正类的比例。公式为：<span class="math display">\[\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}\]</span></p></li><li><p><strong>假阳性率（FPR, False PositiveRate）</strong>：表示被错误分类为正类的负类样本比例。公式为： <spanclass="math display">\[\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}\]</span></p></li></ul><h3 id="roc曲线的绘制步骤">3.ROC曲线的绘制步骤</h3><ol type="1"><li><strong>计算预测概率</strong>：使用二分类模型对数据进行预测，得到每个样本属于正类的概率。</li><li><strong>确定阈值</strong>：从0到1选择一系列阈值，对每个阈值进行如下操作：<ul><li>将预测概率与当前阈值比较，得到分类结果（大于等于阈值为正类，小于阈值为负类）。</li><li>计算对应的TPR和FPR。</li></ul></li><li><strong>绘制曲线</strong>：以FPR为横坐标，TPR为纵坐标，绘制曲线。</li></ol><h3 id="实例说明">4.实例说明</h3><p>假设有一个简单的二分类问题，以下是一些预测结果及对应的实际标签：</p><table><thead><tr class="header"><th>实际标签</th><th>预测概率</th></tr></thead><tbody><tr class="odd"><td>1</td><td>0.9</td></tr><tr class="even"><td>0</td><td>0.8</td></tr><tr class="odd"><td>1</td><td>0.7</td></tr><tr class="even"><td>1</td><td>0.6</td></tr><tr class="odd"><td>0</td><td>0.4</td></tr><tr class="even"><td>1</td><td>0.3</td></tr><tr class="odd"><td>0</td><td>0.2</td></tr><tr class="even"><td>0</td><td>0.1</td></tr></tbody></table><p>我们使用这些数据来绘制ROC曲线。</p><h4 id="步骤1计算tpr和fpr">步骤1：计算TPR和FPR</h4><p>我们选择几个阈值来计算TPR和FPR：</p><ul><li>阈值 = 0.9</li><li>阈值 = 0.7</li><li>阈值 = 0.5</li><li>阈值 = 0.3</li><li>阈值 = 0.1</li></ul><p>对于每个阈值，计算TPR和FPR：</p><ol type="1"><li><p><strong>阈值 = 0.9</strong>:</p><ul><li>预测结果：1 0 0 0 0 0 0 0</li><li>TPR = 1/4 = 0.25</li><li>FPR = 0/4 = 0</li></ul></li><li><p><strong>阈值 = 0.7</strong>:</p><ul><li>预测结果：1 1 1 0 0 0 0 0</li><li>TPR = 2/4 = 0.5</li><li>FPR = 1/4 = 0.25</li></ul></li><li><p><strong>阈值 = 0.4</strong>:</p><ul><li>预测结果：1 1 1 1 1 0 0 0</li><li>TPR = 3/4 = 0.75</li><li>FPR = 2/4 = 0.5</li></ul></li><li><p><strong>阈值 = 0.3</strong>:</p><ul><li>预测结果：1 1 1 1 1 1 0 0</li><li>TPR = 4/4 = 1</li><li>FPR = 2/4 = 0.5</li></ul></li><li><p><strong>阈值 = 0.2</strong>:</p><ul><li>预测结果：1 1 1 1 1 1 1 0</li><li>TPR = 4/4 = 1</li><li>FPR = 3/4 = 0.75</li></ul><p><img src="/images/roc/image-20240526220530303.png" /></p></li></ol><h4 id="python绘制曲线">5.python绘制曲线</h4><p>我们将这些TPR和FPR值在图上绘制出来，得到ROC曲线。</p><p>下面用Python代码实现绘制ROC曲线：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_curve<br><br><span class="hljs-comment"># 实际标签</span><br>y_true = [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># 预测概率</span><br>y_scores = [<span class="hljs-number">0.9</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.1</span>]<br><br><span class="hljs-comment"># 计算FPR和TPR</span><br>fpr, tpr, thresholds = roc_curve(y_true, y_scores)<br><br><span class="hljs-comment"># 绘制ROC曲线</span><br>plt.figure()<br>plt.plot(fpr, tpr, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;False Positive Rate&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;True Positive Rate&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;ROC Curve&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>这段代码将绘制出对应的数据的ROC曲线。ROC曲线越靠近左上角，表示模型性能越好。曲线下面积（AUC,Area Under theCurve）可以用来量化模型的整体性能，AUC值越大表示模型性能越好。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ROC曲线</tag>
      
      <tag>模型评估</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何知道一个大模型是否可以在自己的显卡上运行呢？</title>
    <link href="/%E5%A6%82%E4%BD%95%E7%9F%A5%E9%81%93%E4%B8%80%E4%B8%AA%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E5%9C%A8%E8%87%AA%E5%B7%B1%E7%9A%84%E6%98%BE%E5%8D%A1%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%91%A2%EF%BC%9F.html"/>
    <url>/%E5%A6%82%E4%BD%95%E7%9F%A5%E9%81%93%E4%B8%80%E4%B8%AA%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E5%9C%A8%E8%87%AA%E5%B7%B1%E7%9A%84%E6%98%BE%E5%8D%A1%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%91%A2%EF%BC%9F.html</url>
    
    <content type="html"><![CDATA[<h4 id="经验评估">1.经验评估</h4><ul><li>推理显存估算：7B-float 是 28 GB，7B-BF16 是 14GB，7B-int8 是7GB；其他版本以此类推即可。</li><li>训练的参数类型，只能是 float / BF16</li><li>训练 所需显存 保守估算 是 同参数同类型llm 推理 的 4倍。<ul><li>例子：7B-float 训练 显存：28 * 4 = 112 GB</li></ul></li></ul><table><thead><tr class="header"><th>方法</th><th>bits</th><th>7B</th><th>13B</th><th>30B</th><th>65B</th><th>8*7B</th></tr></thead><tbody><tr class="odd"><td>全参数微调</td><td>16</td><td>160GB</td><td>320GB</td><td>600GB</td><td>1200GB</td><td>900GB</td></tr><tr class="even"><td>Freeze</td><td>16</td><td>20GB</td><td>40GB</td><td>120GB</td><td>240GB</td><td>200GB</td></tr><tr class="odd"><td>LoRA</td><td>16</td><td>16GB</td><td>32GB</td><td>80GB</td><td>160GB</td><td>120GB</td></tr><tr class="even"><td>QLoRA</td><td>8</td><td>10GB</td><td>16GB</td><td>40GB</td><td>80GB</td><td>80GB</td></tr><tr class="odd"><td>QLoRA</td><td>4</td><td>6GB</td><td>12GB</td><td>24GB</td><td>48GB</td><td>32GB</td></tr></tbody></table><h4 id="精确评估">2.精确评估</h4><h5 id="在线评估">2.1 在线评估</h5><p>accelerate estimate-memory 是 huggingface 的 accelerate开发库中提供的一个工具。可网页在线访问</p><p>https://huggingface.co/spaces/hf-accelerate/model-memory-usage选择相应模型进行评估</p><h5 id="本地评估">2.2 本地评估</h5><ul><li>安装 accelerate, transformers</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">pip install accelerate<br>pip install transformers<br></code></pre></td></tr></table></figure><ul><li>使用方法举例</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs text"># 基本使用方法<br>accelerate estimate-memory mistralai/Mistral-7B-v0.1<br><br># 只显示指定的数据类型<br>accelerate estimate-memory mistralai/Mistral-7B-v0.1 --dtypes float16<br><br># 指定开发库(针对本地模型，Hub上存储的模型不需要指定)<br>accelerate estimate-memory mistralai/Mistral-7B-v0.1 --dtypes float32 float16 --library_name transformers<br><br># 设置 trust_remote_code=True<br>accelerate estimate-memory Qwen/Qwen1.5-7B #正常<br>accelerate estimate-memory Qwen/Qwen-7B #报错<br>accelerate estimate-memory Qwen/Qwen-7B --trust_remote_code #可以运行<br><br># 其他模型<br>accelerate estimate-memory google/gemma-7b<br>accelerate estimate-memory baichuan-inc/Baichuan2-7B-Base --trust_remote_code<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>模型训练</tag>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客上云 纵享丝滑</title>
    <link href="/%E5%8D%9A%E5%AE%A2%E4%B8%8A%E4%BA%91%E7%BA%B5%E4%BA%AB%E4%B8%9D%E6%BB%91.html"/>
    <url>/%E5%8D%9A%E5%AE%A2%E4%B8%8A%E4%BA%91%E7%BA%B5%E4%BA%AB%E4%B8%9D%E6%BB%91.html</url>
    
    <content type="html"><![CDATA[<p>让你的博客上云，体验丝般顺滑~</p><span id="more"></span><h1 id="一工作原理">一、工作原理</h1><p>使用Hexo搭建个人博客并自动部署到阿里云ECS服务器的原理如下图所示：</p><p><a href="/images/hexo_aliyun/Hexo_ALiYun.jpg"><imgsrc="/images/hexo_aliyun/Hexo_ALiYun.jpg"alt="基于Hexo的博客搭建和阿里云部署原理" /></a></p><p>简单来说就是在本地计算机搭建Hexo环境，Hexo通过generate命令将*.md文件渲染成静态的html页面，然后Hexo通过deploy命令触发git用户通过公钥免密登陆服务器，进而将静态页面推送到服务器的git仓库（repository）中。然后，服务器再通过钩子（git-hooks）将静态页面checkout到网站的根目录下，进而实现博客的自动部署。具体过程如图中实线箭头所示。</p><h1 id="二搭建步骤">二、搭建步骤</h1><h2 id="在本地计算机安装hexo环境">1、在本地计算机安装Hexo环境</h2><p>首先需要说明的是：我本地使用的是Win10（64位）操作系统。更权威的安装过程可以参照<ahref="https://hexo.io/zh-cn/">Hexo官方主页</a>。</p><h3 id="安装node.js">1.1 安装Node.js</h3><p>去<a href="https://nodejs.org/en/">Node.js官网</a>下载Windows(x64)长期支持版 Long Term Support (LTS)schedule。按提示逐步安装即可，安装完成后打开cmd查看版本号验证是否安装成功。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">C:\Users\tangcl&gt; <span class="hljs-keyword">node</span> <span class="hljs-title">-v</span><br>v12.<span class="hljs-number">13.1</span><br></code></pre></td></tr></table></figure><p>Node.js中自带了npm包管理工具，在cmd中查看npm版本。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">C</span>:\Users\tangcl&gt; npm -v<br><span class="hljs-attribute">6</span>.<span class="hljs-number">12</span>.<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h3 id="安装git">1.2 安装Git</h3><p>git是一个版本控制工具，国外镜像下载巨慢，建议前往<ahref="https://npm.taobao.org/mirrors/git-for-windows/">淘宝 Git forWindows 镜像</a>下载 git安装包。按提示逐步安装即可，安装完成后右键菜单中出现Git Bash和GitGUI菜单表明安装成功，如下图所示。 <ahref="/images/hexo_aliyun/git_menu.png"><imgsrc="/images/hexo_aliyun/git_menu.png" alt="git右键菜单" /></a></p><p>注：git和github是两个东西。github是基于git二次开发的，git是github的核心，git负责与github相关的所有本地工作。</p><h3 id="安装hexo">1.3 安装Hexo</h3><p>在D盘新建MyHexoBlogs文件夹用来存放个人博客，进入该文件夹，右键打开GitBash，使用 npm 安装 Hexo。</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm">npm install -g hexo-<span class="hljs-keyword">cli</span><br></code></pre></td></tr></table></figure><p>运行结果如下： <a href="/images/hexo_aliyun/installHexo.png"><imgsrc="/images/hexo_aliyun/installHexo.png" alt="安装Hexo" /></a>Hexo安装完成后，在MyHexoBlogs文件夹下新建myblogs项目，并对其进行初始化。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo init myblogs<br><span class="hljs-built_in">cd</span> myblogs<br>npm install<br></code></pre></td></tr></table></figure><p>此时，会在MyHexoBlogs文件夹下新建myblogs文件夹，并在其内部生成相应的项目文件。如下图所示：<a href="/images/hexo_aliyun/files.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="文件" /></a> 在myblogs文件夹下启动hexo服务。</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs axapta">hexo <span class="hljs-keyword">server</span><br></code></pre></td></tr></table></figure><p>此时在本地打开浏览器，通过 http://localhost:4000/便可访问基于Hexo的个人博客主页了。如下图所示： <ahref="/images/hexo_aliyun/hexo.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="个人博客本地主页" /></a></p><h2 id="服务端准备工作">2、服务端准备工作</h2><h3 id="域名注册">2.1 域名注册</h3><p>网站搭建之前我们需要注册自己的域名，因为我们不可能让用户通过“公网IP+端口”的方式访问我们的服务器，这样太不方便记忆了。</p><p>因为万网已被阿里收购，所以对于阿里云用户，我们可以直接在<ahref="https://wanwang.aliyun.com/?spm=5176.12825654.eofdhaal5.9.3dbd2c4anS0SLJ&amp;aly_as=SIqz0Gsr">阿里云域名注册官网</a>上直接注册购买。<a href="/images/hexo_aliyun/yuming.png"><imgsrc="/images/hexo_aliyun/yuming.png" alt="域名注册" /></a>由于域名可以交易，所以域名注册应当有点战略性眼光，应简单直观、方便记忆。域名格式参考是<ahref="http://www.xxxxxx.com/">www.xxxxxx.com</a>。</p><h3 id="域名实名认证">2.2 域名实名认证</h3><p>域名注册过程中，必须进行邮箱和身份证实名认证才可以继续购买，我们只需按提示进行操作即可。</p><h3 id="购买阿里云ecs服务器">2.3 购买阿里云ECS服务器</h3><p>阿里的云服务产品有很多种，如阿里云主机、ECS服务器等。我这里购买的是阿里云ECS服务器。所谓ECS，即弹性计算服务。</p><p>进入阿里云官网的ECS专区购买即可。 <ahref="/images/hexo_aliyun/aliyunECS.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="阿里云ECS" /></a>如下图所示，我这里购买的是双十二入门级活动套餐：实例1核1G（预装CentOS7.4） + 40G高效云盘 + 1M带宽，小白用户选择此配置足以。</p><p><a href="/images/hexo_aliyun/myECS.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="我的ECS配置" /></a>付款成功后，你就拥有一个属于自己的ECS服务器实例了。所谓实例，就是一台装了CentOS的电脑。接下来就是对该实例进行设置，并在它上面搭建相应的部署环境了。</p><h3 id="ecs服务器备案">2.4 ECS服务器备案</h3><p>备案需要有服务器和域名。</p><p>国家法律规定，使用中国大陆境内服务器托管你的网站时，你必须对你的网站进行备案申请。当你使用阿里云中国大陆境内节点的服务器时，你可以直接在<ahref="https://beian.aliyun.com/">阿里云备案管理系统</a>中提交ICP备案申请。<a href="/images/hexo_aliyun/beian.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="备案" /></a>ICP备案申请审核通过后，管局（工信部）会给我们一个ICP备案号，我们需要将备案号在网站底部标明。网站在工信部备案成功后，还需要在网站开通之日起30日内登录<ahref="http://beian.gov.cn/portal/index">全国公安机关互联网站安全管理服务平台</a>提交公安联网备案申请。</p><h3 id="阿里云服务器设置">2.5 阿里云服务器设置</h3><h4 id="重置实例密码">（1）重置实例密码</h4><p>点击阿里云首页的控制台按钮，登录到云服务器管理控制台，便可以查看自己购买的实例了。<a href="/images/hexo_aliyun/control.png"><imgsrc="/images/hexo_aliyun/control.png" alt="登录控制台" /></a>新买的ECS服务器实例对root用户是没有设置初始密码的,ECS服务器的root密码需要重置才能用。重置步骤如下：选中ECS服务器实例，点击下面的重置密码按钮即可重置root用户的密码，密码在实例重启后生效。（该密码必须是字母、数字和其它字符组成的8位以上字符串。）</p><p><a href="/images/hexo_aliyun/shili.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="实例" /></a></p><h4 id="远程连接linux实例">（2）远程连接Linux实例</h4><p>远程连接服务器的方法都很多。我们既可以通过阿里云自带的VNC（VirtualNetworkConsole，虚拟网络控制台）远程连接Linux实例，也可以通过远程连接软件（例如PuTTY、Xshell、SecureCRT等）连接Linux实例。</p><p>我这里用到是VNC方法。需要说明的是：使用阿里云自带的VNC远程连接Linux实例，登录VNC窗口时还要输入一个6位数的远程连接密码，用于连接ECS管理控制台的管理终端，注意不要与root密码混淆。</p><p>注：</p><ul><li>远程连接密码用于连接ECS管理控制台的管理终端，而实例登录密码（root密码）用于登录实例。</li><li>远程连接密码仅在第一次连接管理终端时显示一次，建议启用后立即修改远程连接密码。</li></ul><p>具体连接步骤如下： a.在实例列表中选中当前实例，点击右侧按钮：远程连-&gt;VNC。 b.输入远程连接密码。 <a href="/images/hexo_aliyun/connect.png"><imgsrc="/images/hexo_aliyun/connect.png" alt="远程连接密码" /></a> c.在控制台中输入用户名：root，及其root密码（实例密码）。回车即可进入阿里云ECS服务器的后台，如下图所示。<a href="/images/hexo_aliyun/aliyunServer.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="ECS服务器后台" /></a></p><p>后面，我们主要就是利用此终端在ECS上部署网站运行环境了。</p><h4 id="配置安全组">（3）★ 配置安全组</h4><p>由于我们要通过80端口访问nginx服务，而阿里云默认是禁止80端口访问权限的，所以我们要为实例手动添加安全组，让阿里云给相应的端口和IP放行。该步骤非常重要，若不手动配置，我们将无法通过“公网IP+端口”的方式访问我们的ECS服务器。</p><p>具体操作步骤如下： a.打开阿里云服务管理控制台，点击左侧菜单中的“安全组”按钮，查看安全组列表。b. 点击右上角的“创建安全组”按钮，创建一个新的安全组。 c.立即为新建的安全组添加安全组规则，在入方向解除端口和IP限制，具体参数设置如下图所示。<a href="/images/hexo_aliyun/safe.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="添加安全规则" /></a> d. 在实例列表中为实例添加安全组。</p><p><a href="/images/hexo_aliyun/add.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="为实例添加安全组" /></a></p><p>这样就完成了安全组的配置。<em>注：安全组出方向默认允许所有访问，即从安全组内ECS访问外部都是放行的。</em></p><h2 id="hexo博客的阿里云部署">3、Hexo博客的阿里云部署</h2><p><em>该步骤是整个博客搭建过程中最重要的一步，实现过程中一定要注意是在服务端操作还是在本地计算机上操作。若在服务器上操作，还要注意是使用root用户进行操作还是使用git用户进行操作。</em></p><h3 id="安装nginx">3.1 ★ 安装nginx</h3><p>因为我们用nginx作Web服务器，所以我们需要先安装nginx服务。具体步骤如下：</p><p>使用root用户远程登录阿里云服务器，使用yum命令进行安装。</p><ol type="a"><li>安装nginx依赖环境，安装期间有提示一律选yes。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#yum install gcc-c++</span><br><span class="hljs-meta">#yum install -y pcre pcre-devel</span><br><span class="hljs-meta">#yum install -y zlib zlib-devel</span><br><span class="hljs-meta">#yum install -y openssl openssl-devel</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>下载nginx安装包。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#wget -c https:<span class="hljs-comment">//nginx.org/download/nginx-1.10.1.tar.gz</span></span><br></code></pre></td></tr></table></figure><ol start="3" type="a"><li>将安装包解压到/usr/local目录下。</li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-id">#tar</span> -xvf nginx-<span class="hljs-number">1.10</span>.<span class="hljs-number">1</span><span class="hljs-selector-class">.tar</span><span class="hljs-selector-class">.gz</span> -C /usr/local<br></code></pre></td></tr></table></figure><ol start="4" type="a"><li>进入/usr/local目录，确认nginx解压到该目录下。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> /usr/local</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">ls</span></span><br></code></pre></td></tr></table></figure><ol start="5" type="a"><li>进入nginx-1.10.1目录，会发现该目录下有一个configure文件，执行该配置文件。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> nginx-1.10.1/</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">ls</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">./configure</span><br></code></pre></td></tr></table></figure><ol start="6" type="a"><li>编译并安装nginx。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#make</span><br><span class="hljs-meta">#make install</span><br></code></pre></td></tr></table></figure><ol start="7" type="a"><li>查找nginx安装目录。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#whereis nginx</span><br></code></pre></td></tr></table></figure><p>h.进入安装目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> /usr/local/nginx</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">ls</span></span><br></code></pre></td></tr></table></figure><ol type="i"><li>由于nginx默认通过80端口访问，而Linux默认情况下不会开发该端口号，因此需要开放linux的80端口供外部访问。</li></ol><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">#/sbin/iptables -<span class="hljs-selector-tag">I</span> <span class="hljs-selector-tag">INPUT</span> -<span class="hljs-selector-tag">p</span> tcp <span class="hljs-attr">--dport</span> <span class="hljs-number">80</span> -j ACCEPT<br></code></pre></td></tr></table></figure><ol start="10" type="a"><li>进入/usr/local/nginx/sbin目录，启动nginx。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> sbin</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">./nginx</span><br></code></pre></td></tr></table></figure><p>没有任何消息，代表启动成功。此时，便可以通过“公网IP+端口”的方式访问<a href="http://xx.xx.xxx.xxx/">http://xx.xx.xxx.xxx:80/</a>进入nginx欢迎页面了。 <strong>注：</strong> <strong>（1）可以使用./nginx-s stop命令停止服务；</strong><strong>（2）网站搭建成功后，若出现宕机现象，很有可能是nginx服务器挂了，此时应检查nginx服务器状态，并进行重启操作。</strong></p><h3 id="配置nginx服务器路由">3.2 配置nginx服务器路由</h3><ol type="a"><li>专门为hexo创建一个部署目录/home/www/hexo。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">mkdir</span> -p /home/www/hexo</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>进入/usr/local/nginx/conf目录，打开该文件夹下的nginx.conf配置文件。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> /usr/local/nginx/conf</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">ls</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">vim nginx.conf</span><br></code></pre></td></tr></table></figure><p>进入后按i键由命令模式切换到编辑模式。</p><ul><li>将其中的部署根目录（root）修改为/home/www/hexo；</li><li>将域名（server_name）<ahref="http://xn--www-c88dx1fq77c.xxxxxx.com/">修改为www.xxxxxx.com</a>，如果暂时没有域名就填阿里云实例的公网ip，以后有了再改回来；</li><li>查看监听端口（listen）的系统默认值是否为80（不用修改）。</li></ul><p>完成以上修改后，先按Esc由编辑模式切换到命令模式，再输入:wq命令保存并退出编辑器。</p><h3 id="安装node.js-1">3.3 安装node.js</h3><ol type="a"><li>退回根目录，安装node.js。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> ~</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">curl -sL https://rpm.nodesource.com/setup_10.x | bash -</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">yum install -y nodejs</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>查看安装结果，打印版本号即为安装成功。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#node -v</span><br><span class="hljs-meta">#npm -v</span><br></code></pre></td></tr></table></figure><h3 id="安装git-1">3.4 安装Git</h3><ol type="a"><li>使用yum命令安装Git，安装期间有提示一律选yes。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#yum install git</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>安装成功后，查看版本号。</li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-id">#git</span> <span class="hljs-attr">--version</span><br></code></pre></td></tr></table></figure><h3 id="创建git用户">3.5 创建git用户</h3><p>为了实现博客的自动部署，我们后面要使用公钥免密登录服务器。为了安全起见，最好不要使用root用户免密登录。因此，我们要创建一个新的git用户，用于远程公钥免密登录服务器。</p><ol type="a"><li>创建git用户。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#adduser git</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>修改git用户的权限。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">chmod</span> 740 /etc/sudoers</span><br></code></pre></td></tr></table></figure><ol start="3" type="a"><li>打开文件。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">vim /etc/sudoers</span><br></code></pre></td></tr></table></figure><p>进入后按i键由命令模式切换到编辑模式。找到 root ALL=(ALL)ALL，在下面添加一行 <strong>git ALL=(ALL)ALL</strong>。修改完成后，先按Esc由编辑模式切换到命令模式，再输入:wq命令保存并退出编辑器。</p><ol start="4" type="a"><li>保存退出后改回权限。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">chmod</span> 400 /etc/sudoers</span><br></code></pre></td></tr></table></figure><ol start="5" type="a"><li>设置git用户的密码。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#sudo passwd git</span><br></code></pre></td></tr></table></figure><p>设置密码：************，这样我们就可以使用git用户远程登录阿里云服务器了。</p><h3 id="给git用户配置ssh免密公钥登录">3.6 ★给git用户配置ssh免密公钥登录</h3><p>该步骤是基于Hexo搭建个人博客的核心步骤，也是坑我时间最长的地方。它既需要在本地计算机上操作，也需要在服务器上进行操作，新手一定要搞清原理才不会弄错。</p><p>使用git用户免密公钥登录阿里云服务器的原理是：在本地计算机生成一个公钥文件和一个秘钥文件（类似于一个钥匙配一把锁)，然后使用FTP工具将公钥文件上传到阿里云服务器，并公钥安装到authorized_keys列表中去（即：将公钥文件的内容拷贝到authorized_keys文件中去）。这样本地计算机便可以通过ssh方式免密连接我们的阿里云服务器了。</p><p>具体操作步骤如下：</p><ol type="a"><li>在服务器端将登陆用户切换到git用户，然后在~目录(根目录)下创建.ssh文件夹，用来存放公钥。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">su git</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">cd</span> ~</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">mkdir</span> .ssh</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>在本地计算机桌面右键打开GitBash，在本地生成公钥/私钥对。</li></ol><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-meta"><span class="hljs-keyword">$cd</span> ~</span><br><span class="hljs-meta"><span class="hljs-keyword">$cd</span> .ssh</span><br><span class="hljs-meta"><span class="hljs-keyword">$ssh</span>-keygen</span><br></code></pre></td></tr></table></figure><p>接下来，碰见系统询问就直接按回车键。此时便会在本地计算机的用户根目录（C:）下自动生成.ssh（隐藏）文件夹，并在其中创建两个文件，分别为：id_rsa（私钥）和id_rsa.pub（公钥）。</p><ol start="3" type="a"><li>在本地计算机上给私钥设置权限。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">chmod</span> 700 ~/.ssh<br><span class="hljs-built_in">chmod</span> 600 ~/.ssh/id_rsa <br></code></pre></td></tr></table></figure><ol start="4" type="a"><li><p>下载并安装FTP工具，我这里用的是阿里云官方提供的<ahref="https://help.aliyun.com/knowledge_detail/36243.html">FileZilla（Windows版本）</a>。</p></li><li><p>打开FileZilla，使用git用户通过22端口远程连接到阿里云服务器，将客服端生成的公钥上传到服务器的~/.ssh目录下。</p></li></ol><p><a href="/images/hexo_aliyun/ftp.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="FTP" /></a></p><ol start="6" type="a"><li>上传完成后切回服务器端，继续以git用户的身份进入服务器~/.ssh目录，新建一个authorized_keys文件，并将id_rsa.pub文件中公钥的内容拷贝到该文件中。<em>（注：该步骤既可以用命令行操作，也可使用FTP工具操作。）</em></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">cd</span> ~/.ssh</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">cp</span> id_rsa.pub authorized_keys</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">cat</span> id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></code></pre></td></tr></table></figure><ol start="7" type="a"><li>在服务器上设置文件权限：</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">chmod</span> 600 ~/.ssh/authorized_keys</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">chmod</span> 700 ~/.ssh</span><br></code></pre></td></tr></table></figure><ol start="8" type="a"><li>确保设置了正确的SELinux上下文。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">restorecon -Rv ~/.ssh <br></code></pre></td></tr></table></figure><p>现在，当您使用ssh远程登录服务器时，将不会提示您输入密码（除非您在创建密钥对时输入了密码）。i. 接下来在本地计算机上使用ssh方式连接我们的云服务器。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-variable">$ssh</span> -v git@xxx<span class="hljs-selector-class">.xxx</span><span class="hljs-selector-class">.xxx</span>.xxx（阿里云公网IP）<br></code></pre></td></tr></table></figure><p>或</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-variable">$ssh</span> git@xxx<span class="hljs-selector-class">.xxx</span><span class="hljs-selector-class">.xxx</span>.xxx（阿里云公网IP）<br></code></pre></td></tr></table></figure><p>使用git用户ssh免密公钥登录成功界面如下图所示。 <ahref="/images/hexo_aliyun/ssh.png"><imgsrc="/images/hexo_aliyun/ssh.png" alt="ssh" /></a></p><h3 id="配置git仓库">3.7 配置Git仓库</h3><ol type="a"><li>在服务器上使用git用户创建一个Git仓库，并且在该仓库中新建一个post-receive钩子文件。</li></ol><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-meta"><span class="hljs-keyword">$cd</span> ~</span><br><span class="hljs-meta"><span class="hljs-keyword">$git</span> init --bare hexo.git</span><br><span class="hljs-meta"><span class="hljs-keyword">$vi</span> ~/hexo.git/hooks/post-receive</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>进入后按i键由命令模式切换到编辑模式。输入： <strong>git--work-tree=/home/www/hexo --git-dir=/home/git/hexo.git checkout-f</strong></li></ol><p>即：让钩子文件删除/home/www/hexo目录下原有的文件，然后从blog.git仓库clone 新的博客静态文件到/home/www/hexo目录下。</p><p>完成以上修改后，先按Esc由编辑模式切换到命令模式，再输入:wq命令保存并退出编辑器。</p><ol start="3" type="a"><li>授予钩子文件可执行权限。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">chmod</span> +x ~/hexo.git/hooks/post-receive</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">cd</span> ~</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash">sudo <span class="hljs-built_in">chmod</span> -R 777 /home/www/hexo</span><br></code></pre></td></tr></table></figure><ol start="4" type="a"><li>重启ECS服务器实例。</li></ol><p>至此我们就完成了所有关于服务器端的配置。</p><h2 id="其它配置">4、其它配置</h2><h3 id="客服端hexo配置">4.1 客服端hexo配置</h3><ol type="a"><li><p>在本地计算机hexo的工程目录下，找到_config.yml，对deploy参数进行修改，如下图所示。<a href="/images/hexo_aliyun/deploy.png"><imgsrc="/images/hexo_aliyun/deploy.png" alt="deploy" /></a></p></li><li><p>在本地计算机安装插件: hexo-deployer-git 和hexo-server。在myblogs文件夹下右键打开GitBash，输入以下命令：</p></li></ol><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-meta"><span class="hljs-keyword">$npm</span> install hexo-deployer-git --save</span><br><span class="hljs-meta"><span class="hljs-keyword">$npm</span> install hexo-server</span><br></code></pre></td></tr></table></figure><p><em>这俩插件的作用分别是使用Git自动部署，和hexo本地简单的服务器。</em></p><ol start="3" type="a"><li>在本地计算机配置Git全局变量。 输入以下命令：</li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.email</span> <span class="hljs-string">&quot;xxxxxxxxxx@qq.com&quot;</span><br>git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.name</span> “tangcl”<br></code></pre></td></tr></table></figure><ol start="4" type="a"><li>使用Hexo生成、发布个人博客。</li></ol><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs verilog">hexo clean<br>hexo <span class="hljs-keyword">generate</span><br>hexo deploy<br></code></pre></td></tr></table></figure><p>此时，便可以通过浏览器访问<ahref="http://xxx.xxx.xxx.xxx/">http://xxx.xxx.xxx.xxx:80/</a>进入hexo我的博客主页了。</p><h3 id="域名绑定">4.2 域名绑定</h3><p>待ECS服务器备案审核通过，在阿里云后台对域名解析进行设置，将域名的解析值修改为ECS实例的公网IP。进而完成域名与ECS服务器实例的公网IP进行绑定。</p><p><a href="/images/hexo_aliyun/jiexi.png"><imgsrc="/images/hexo_aliyun/jiexi.png" alt="解析" /></a></p><p>十分钟后，我们便可以通过浏览器访问http://www.xxxxx.com/进入hexo的博客主页了。</p><p><a href="/images/hexo_aliyun/success.png"><imgsrc="/images/hexo_aliyun/success.png" alt="success" /></a></p><h1 id="结束语">结束语</h1><p>新手搭建个人博客过程中难免会出一些小问题，千万不要害怕遇到问题。解决它，你就进步了</p>]]></content>
    
    
    <categories>
      
      <category>博客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>服务器</tag>
      
      <tag>博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>半小时速通正则表达式</title>
    <link href="/%E5%8D%8A%E5%B0%8F%E6%97%B6%E9%80%9F%E9%80%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.html"/>
    <url>/%E5%8D%8A%E5%B0%8F%E6%97%B6%E9%80%9F%E9%80%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.html</url>
    
    <content type="html"><![CDATA[<p>正则表达式在文本范式处理时有者非常重要的应用，快来一起巩固一下正则的相关知识吧！<span id="more"></span> ### 正则表达式应用场景(Regular Expression)</p><ul><li>数据验证（表单验证、如手机、邮箱、IP地址）</li><li>爬虫功能</li><li>数据检索（数据检索、数据抓取）</li><li>数据隐藏（135****6235王先生）</li><li>数据过滤（论坛敏感关键词过滤）</li></ul><h3 id="正则--match">正则--match</h3><p><strong>re.match(pattern, string, flags=0)</strong></p><table><thead><tr class="header"><th><strong>参数</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr class="odd"><td>pattern</td><td>匹配的正则表达式</td></tr><tr class="even"><td>string</td><td>要匹配的字符串。</td></tr><tr class="odd"><td>flags</td><td>标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：<ahref="https://www.runoob.com/python/python-reg-expressions.html#flags">正则表达式修饰符- 可选标志</a></td></tr></tbody></table><p>re.match 尝试从字符串的<strong>起始位置匹配</strong>一个模式，匹配成功 re.match 方法返回一个匹配的对象，否则返回 None。</p><h3 id="正则--search">正则--search</h3><p><strong>re.search(pattern, string, flags=0)</strong></p><table><thead><tr class="header"><th><strong>参数</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr class="odd"><td>pattern</td><td>匹配的正则表达式</td></tr><tr class="even"><td>string</td><td>要匹配的字符串。</td></tr><tr class="odd"><td>flags</td><td>标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：<ahref="https://www.runoob.com/python/python-reg-expressions.html#flags">正则表达式修饰符- 可选标志</a></td></tr></tbody></table><p>re.match尝试从字符串的<strong>任意位置匹配</strong>一个模式，(常用于全词匹配)匹配成功 re.search 方法返回一个匹配的对象，否则返回 None。</p><p>re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。</p><h3id="re.compile正则表达式.sub用来替换的内容要被替换的内容">re.compile（正则表达式）.sub（用来替换的内容，要被替换的内容）</h3><p>compile 函数用于编译正则表达式，生成一个正则表达式（ Pattern）对象，供 match() 和 search() 这两个函数使用。 ### ###re.sub(正则,替换字符,被替换的内容)</p><h3 id="正则常用符号释义">正则常用符号释义</h3><table><thead><tr class="header"><th><strong>符号</strong></th><th><strong>解释</strong></th><th><strong>示例</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr class="odd"><td>.</td><td><strong>匹配任意字符</strong></td><td>b.t</td><td>可以匹配bat / but / b#t / b1t等</td></tr><tr class="even"><td><a href="file://w"></a></td><td><strong>匹配字母/数字/下划线/汉字</strong></td><td>b</td><td>可以匹配bat / b1t / b_t等&lt;br&gt;但不能匹配b#t</td></tr><tr class="odd"><td><a href="file://s"></a></td><td>**匹配空白字符（包括、*）</td><td>love</td><td>可以匹配love you</td></tr><tr class="even"><td>[(file://d)</td><td><strong>匹配数字</strong></td><td>[(file://d/d)</td><td>可以匹配01 / 23 / 99等</td></tr><tr class="odd"><td>[(file://b)</td><td>匹配单词的边界</td><td>[(file://bThe/b)</td><td></td></tr><tr class="even"><td>^</td><td><strong>匹配字符串的开始</strong></td><td>^The</td><td>可以匹配The开头的字符串</td></tr><tr class="odd"><td>$</td><td><strong>匹配字符串的结束</strong></td><td>.exe$</td><td>可以匹配.exe结尾的字符串</td></tr><tr class="even"><td><a href="file://W"></a></td><td>匹配非字母/数字/下划线</td><td>b</td><td>可以匹配b#t / b@t等&lt;br&gt;但不能匹配but / b1t / b_t等</td></tr><tr class="odd"><td><a href="file://S"></a></td><td>匹配非空白字符</td><td>love</td><td>可以匹配love#you等&lt;br&gt;但不能匹配love you</td></tr><tr class="even"><td><a href="file://D"></a></td><td>匹配非数字</td><td><a href="file://d/D"></a></td><td>可以匹配9a / 3# / 0F等</td></tr><tr class="odd"><td><a href="file://B"></a></td><td>匹配非单词边界</td><td><a href="file://Bio/B"></a></td><td></td></tr><tr class="even"><td>[]</td><td>匹配来自字符集的任意单一字符</td><td>[aeiou]</td><td>可以匹配任一元音字母字符</td></tr><tr class="odd"><td><strong>[^]</strong></td><td>匹配不在字符集中的任意单一字符</td><td>[^aeiou]</td><td>可以匹配任一非元音字母字符</td></tr><tr class="even"><td><strong>*</strong></td><td><strong>匹配0次或多次</strong></td><td><a href="file://w*">*</a></td><td></td></tr><tr class="odd"><td><strong>+</strong></td><td><strong>匹配1次或多次</strong></td><td><a href="file://w+">+</a></td><td></td></tr><tr class="even"><td><strong>?</strong></td><td><strong>匹配0次或1次</strong></td><td><a href="file://w">?</a></td><td></td></tr><tr class="odd"><td><strong>{N}</strong></td><td><strong>匹配N次</strong></td><td></td><td></td></tr><tr class="even"><td><strong>{M,}</strong></td><td><strong>匹配至少M次</strong></td><td></td><td></td></tr><tr class="odd"><td><strong>{M,N}</strong></td><td><strong>匹配至少M次至多N次</strong></td><td></td><td></td></tr><tr class="even"><td><strong>\</strong></td><td>分支</td><td>foo\bar</td><td>可以匹配foo或者bar</td></tr><tr class="odd"><td><strong>(?#)</strong></td><td>注释</td><td></td><td></td></tr><tr class="even"><td><strong>(exp)</strong></td><td>匹配exp并捕获到自动命名的组中</td><td></td><td></td></tr><tr class="odd"><td><strong>(?&lt;name&gt;exp)</strong></td><td>匹配exp并捕获到名为name的组中</td><td></td><td></td></tr><tr class="even"><td><strong>(?:exp)</strong></td><td>匹配exp但是不捕获匹配的文本</td><td></td><td></td></tr><tr class="odd"><td><strong>(?=exp)</strong></td><td>匹配exp前面的位置</td><td><a href="file://b/w+(%3f=ing)">+(?=ing)</a></td><td>可以匹配I'm dancing中的danc</td></tr><tr class="even"><td><strong>(?&lt;=exp)</strong></td><td>匹配exp后面的位置</td><td>[(?&lt;=)+(file://bdanc)/w+/b)</td><td>可以匹配I love dancing and reading中的第一个ing</td></tr><tr class="odd"><td><strong>(?!exp)</strong></td><td>匹配后面不是exp的位置</td><td></td><td></td></tr><tr class="even"><td><strong>(?&lt;!exp)</strong></td><td>匹配前面不是exp的位置</td><td></td><td></td></tr><tr class="odd"><td><strong>*?</strong></td><td>重复任意次，但尽可能少重复</td><td>a.\b&lt;br&gt;a.\?b</td><td>将正则表达式应用于aabab，前者会匹配整个字符串aabab，后者会匹配aab和ab两个字符串</td></tr><tr class="even"><td><strong>+?</strong></td><td>重复1次或多次，但尽可能少重复</td><td></td><td></td></tr><tr class="odd"><td><strong>??</strong></td><td>重复0次或1次，但尽可能少重复</td><td></td><td></td></tr><tr class="even"><td><strong>{M,N}?</strong></td><td>重复M到N次，但尽可能少重复</td><td></td><td></td></tr><tr class="odd"><td><strong>{M,}?</strong></td><td>重复M次以上，但尽可能少重复</td><td></td><td></td></tr></tbody></table><p>说明：如果需要匹配的字符是正则表达式中的特殊字符，那么可以使用\进行转义处理，例如想匹配小数点可以写成\.就可以了，因为直接写.会匹配任意字符；同理，想匹配圆括号必须写成\(和\)，否则圆括号被视为正则表达式中的分组。</p>]]></content>
    
    
    
    <tags>
      
      <tag>正则表达式</tag>
      
      <tag>基础语法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>用wordcoloud生成超炫酷的词云_内含python源码</title>
    <link href="/%E7%94%A8wordcoloud%E7%94%9F%E6%88%90%E8%B6%85%E7%82%AB%E9%85%B7%E7%9A%84%E8%AF%8D%E4%BA%91-%E5%86%85%E5%90%ABpython%E6%BA%90%E7%A0%81.html"/>
    <url>/%E7%94%A8wordcoloud%E7%94%9F%E6%88%90%E8%B6%85%E7%82%AB%E9%85%B7%E7%9A%84%E8%AF%8D%E4%BA%91-%E5%86%85%E5%90%ABpython%E6%BA%90%E7%A0%81.html</url>
    
    <content type="html"><![CDATA[<p>使用jieba分词，wordcoloud词云可视化</p><span id="more"></span><p><img src="/images/word_cloud/2024年5月10日ai.jpg" /><strong>环境准备</strong> pip 安装jieba库,wordcloud库与scipy库<strong>资料准备</strong></p><ul class="task-list"><li><label><inputtype="checkbox" />用于分词的文本:词频统计_AIjob.csv</label></li><li><label><inputtype="checkbox" />禁止统计词库:stopwords.txt</label></li><li><label><inputtype="checkbox" />自定义分词库:人工智能词汇.txt</label></li><li><label><inputtype="checkbox" />掩膜用的形状图片:mask.jpg</label></li></ul><p>不废话,直接上码 ### 1.用结巴分词,生成字典对象 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">import</span> wordcloud<br><span class="hljs-comment"># 读取文件</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;词频统计_AIjob.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    desc = f.read()<br><br><span class="hljs-comment"># 加载停用词列表</span><br>stop_words = []<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;stopwords.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:<br>        stop_words.append(line.strip())<br><br>jieba.load_userdict(<span class="hljs-string">&quot;人工智能词汇.txt&quot;</span>)<br><span class="hljs-comment"># 分词</span><br>words = jieba.cut(desc, cut_all=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 过滤停用词</span><br>filtered_words = []<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(word) &gt; <span class="hljs-number">1</span>:<br>        filtered_words.append(word)<br><br><span class="hljs-comment"># 统计词频</span><br>word_counts = Counter(filtered_words)<br><br>w100=word_counts.most_common(<span class="hljs-number">500</span>)<br><span class="hljs-comment"># 使用字典推导将列表转换为字典  </span><br>dict_result = &#123;key: value <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> w100&#125; <br></code></pre></td></tr></table></figure> ###2.用wordcloud 生成词云 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> scipy.ndimage <span class="hljs-keyword">import</span> gaussian_gradient_magnitude<br><span class="hljs-keyword">from</span> wordcloud <span class="hljs-keyword">import</span> WordCloud, ImageColorGenerator<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pic_wordcloud</span>(<span class="hljs-params">dict_result,img_path,out_path</span>):<br><br>    <br>    <span class="hljs-comment"># img_path=r&quot;E:\jupyter\spyder\bosszhipin\词频统计\pic\T1.jpg&quot;</span><br>    <br>    parrot_color = np.array(Image.<span class="hljs-built_in">open</span>(img_path))<br>    <br>    parrot_color = parrot_color[::<span class="hljs-number">3</span>, ::<span class="hljs-number">3</span>]<br>    <br>    <span class="hljs-comment"># create mask  white is &quot;masked out&quot;</span><br>    parrot_mask = parrot_color.copy()<br>    parrot_mask[parrot_mask.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">2</span>) == <span class="hljs-number">0</span>] = <span class="hljs-number">255</span><br>    <br>    <br>    edges = np.mean([gaussian_gradient_magnitude(parrot_color[:, :, i] / <span class="hljs-number">255.</span>, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)], axis=<span class="hljs-number">0</span>)<br>    parrot_mask[edges &gt; <span class="hljs-number">.08</span>] = <span class="hljs-number">255</span><br>    <br>    <br>    <span class="hljs-comment"># acurately but it makes a better picture</span><br>    wc = WordCloud(max_words=<span class="hljs-number">1000</span>, mask=parrot_mask, max_font_size=<span class="hljs-number">40</span>, random_state=<span class="hljs-number">42</span>, font_path=<span class="hljs-string">r&quot;C:\Users\10921\AppData\Local\Microsoft\Windows\Fonts\方正正准黑简体.ttf&quot;</span>,relative_scaling=<span class="hljs-number">0</span>,<br>                   <span class="hljs-comment">#width=1920, height=1080</span><br>                   )<br>    <br>    <span class="hljs-comment"># generate word cloud</span><br>    wc.generate_from_frequencies(dict_result)<br>    <span class="hljs-comment"># plt.imshow(wc)</span><br>    <br>    <span class="hljs-comment"># create coloring from image</span><br>    image_colors = ImageColorGenerator(parrot_color)<br>    wc.recolor(color_func=image_colors)<br>    <span class="hljs-comment"># plt.figure(figsize=(10, 10))</span><br>    <span class="hljs-comment"># plt.imshow(wc, interpolation=&quot;bilinear&quot;)</span><br>    <span class="hljs-comment"># wc.to_file(&quot;parrot_new.png&quot;)</span><br>    wc.to_file(out_path)<br>img_path=<span class="hljs-string">&quot;mask.jpg&quot;</span><br>out_path=<span class="hljs-string">&#x27;output.png&#x27;</span><br>pic_wordcloud(dict_result,img_path,out_path)<br></code></pre></td></tr></table></figure> <imgsrc="/images/word_cloud/2024年5月10日color112.png" /> <imgsrc="/images/word_cloud/2024年5月10日parrot_new.png" /></p>]]></content>
    
    
    
    <tags>
      
      <tag>可视化</tag>
      
      <tag>数据分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch VS TensorFlow</title>
    <link href="/TensorFlow%20VS%20Pytorch.html"/>
    <url>/TensorFlow%20VS%20Pytorch.html</url>
    
    <content type="html"><![CDATA[<p>TensorFlow和PyTorch是两种流行的深度学习框架，它们各自具有独特的优缺点和优势领域。以下是对它们的比较：<span id="more"></span></p><h3 id="tensorflow">TensorFlow</h3><h4 id="优点">优点</h4><ol type="1"><li><strong>生态系统完善</strong>：TensorFlow提供了丰富的工具和库，如TensorFlowExtended (TFX)用于生产部署，TensorFlowLite用于移动设备，TensorFlow.js用于JavaScript开发等。</li><li><strong>高性能</strong>：通过TensorFlowServing，可以实现高效的模型部署和推理。此外，TensorFlow有XLA（加速线性代数）编译器，可以进一步优化性能。</li><li><strong>跨平台支持</strong>：TensorFlow可以在多种平台上运行，包括CPU、GPU、TPU，且支持分布式计算。</li><li><strong>社区和企业支持</strong>：TensorFlow由Google开发并维护，拥有广泛的社区支持和大量的企业用户。</li></ol><h4 id="缺点">缺点</h4><ol type="1"><li><strong>学习曲线陡峭</strong>：TensorFlow的API较为复杂，新手入门相对困难。</li><li><strong>调试困难</strong>：虽然TensorFlow 2.x版本引入了EagerExecution模式，提升了可调试性，但总体上调试体验仍不如PyTorch。</li></ol><h3 id="优势领域">优势领域</h3><ul><li><strong>生产环境</strong>：TensorFlow的工具链和生态系统使其非常适合于从研究到生产的全流程。</li><li><strong>大规模分布式训练</strong>：TensorFlow在大规模分布式训练方面具有明显优势。</li><li><strong>移动和嵌入式设备</strong>：TensorFlowLite专门优化了在移动和嵌入式设备上的性能。</li></ul><h3 id="pytorch">PyTorch</h3><h4 id="优点-1">优点</h4><ol type="1"><li><strong>易用性</strong>：PyTorch的API设计直观且友好，非常适合研究和实验。动态图计算模式使得代码更加灵活和易于调试。</li><li><strong>调试便捷</strong>：由于PyTorch使用动态图计算模式，开发者可以使用标准的Python调试工具来调试模型。</li><li><strong>动态计算图</strong>：PyTorch的动态计算图机制使得开发者可以在运行时改变模型结构，非常适合于研究和实验。</li><li><strong>社区支持</strong>：PyTorch由Facebook（现Meta）开发，得到广泛的学术界支持，许多研究论文和前沿技术都首先在PyTorch上实现。</li></ol><h4 id="缺点-1">缺点</h4><ol type="1"><li><strong>生态系统相对较小</strong>：相比TensorFlow，PyTorch的工具链和扩展库较少，虽然近年来有显著改进。</li><li><strong>生产部署工具较少</strong>：虽然PyTorch推出了TorchServe等部署工具，但整体上在生产部署方面的支持不如TensorFlow成熟。</li></ol><h3 id="优势领域-1">优势领域</h3><ul><li><strong>研究和实验</strong>：PyTorch因其灵活性和易用性，非常适合学术研究和快速原型开发。</li><li><strong>调试和开发</strong>：由于其动态计算图机制，开发和调试深度学习模型更加便捷。</li><li><strong>计算机视觉和自然语言处理</strong>：很多前沿的计算机视觉和自然语言处理研究和应用首先在PyTorch上实现。</li></ul><h3 id="总结">总结</h3><p>选择TensorFlow还是PyTorch主要取决于具体需求。如果你需要一个强大的生产环境，广泛的工具链支持，尤其是在大规模分布式训练和部署方面，TensorFlow可能是更好的选择。而如果你更关注研究、实验以及开发过程的灵活性和可调试性，PyTorch则可能更适合你。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>学习框架</tag>
      
      <tag>笔记整理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>光影集-2022</title>
    <link href="/%E5%85%89%E5%BD%B1%E9%9B%862022.html"/>
    <url>/%E5%85%89%E5%BD%B1%E9%9B%862022.html</url>
    
    <content type="html"><![CDATA[<h1 id="难忘那年">难忘那年</h1><figure><img src="/images/光影集2022/image-20240513215734472.png"alt="月色苍茫" /><figcaption aria-hidden="true">月色苍茫</figcaption></figure><figure><img src="/images/光影集2022/image-20240513215908875.png"alt="鱼来鱼往" /><figcaption aria-hidden="true">鱼来鱼往</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220042681.png"alt="花开灿烂" /><figcaption aria-hidden="true">花开灿烂</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220159390.png"alt="绿意盎然" /><figcaption aria-hidden="true">绿意盎然</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220313485.png"alt="雨打蕉叶" /><figcaption aria-hidden="true">雨打蕉叶</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220422759.png"alt="风华正茂" /><figcaption aria-hidden="true">风华正茂</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>生活记录</tag>
      
      <tag>摄影作品</tag>
      
      <tag>难忘那年</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人工智能专业术语汇编</title>
    <link href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E6%B1%87%E7%BC%96.html"/>
    <url>/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E6%B1%87%E7%BC%96.html</url>
    
    <content type="html"><![CDATA[<blockquote><p>本术语库目前拥有专业术语约 2442 个、专项领域篇 2篇，主要为人工智能领域基础概念和术语。</p><p>本术语库前两版主要是将机器之心在编译技术文章和论文过程中所遇到的专业术语记录下来，希望有助于AI从业者的查阅和学习<span id="more"></span> # 人工智能--术语库 引自 github的 <ahref="https://github.com/jiqizhixin/Artificial-Intelligence-Terminology-Database">Artificial-Intelligence-Terminology-Database</a>项目</p></blockquote><table style="width:100%;"><thead><tr class="header"><th>索引编号</th><th>英文术语</th><th>中文翻译</th><th>常用缩写</th><th>来源&amp;扩展</th><th>备注</th></tr></thead><tbody><tr class="odd"><td>AITD-00000</td><td>0-1 Loss Function</td><td>0-1损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00001</td><td>Absolute Loss Function</td><td>绝对损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00002</td><td>Absolute Value Rectification</td><td>绝对值整流</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00003</td><td>Accept-Reject Sampling Method</td><td>接受-拒绝抽样法/接受-拒绝采样法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00004</td><td>Acceptance Distribution</td><td>接受分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00005</td><td>Access Parameters</td><td>访问参数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00006</td><td>Accumulated Error Backpropagation</td><td>累积误差反向传播</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00007</td><td>Accuracy</td><td>准确率</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00008</td><td>Acoustic</td><td>声学</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00009</td><td>Acoustic Modeling</td><td>声学建模</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00010</td><td>Acquisition Function</td><td>采集函数</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-18-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00011</td><td>Action</td><td>动作</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00012</td><td>Action Value Function</td><td>动作价值函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00013</td><td>Actionism</td><td>行为主义</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00014</td><td>Activation</td><td>活性值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00015</td><td>Activation Function</td><td>激活函数</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-06-11-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-18-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[4]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00016</td><td>Active Learning</td><td>主动学习</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00017</td><td>Actor</td><td>演员</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00018</td><td>Actor-Critic Algorithm</td><td>演员-评论员算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00019</td><td>Actor-Critic Method</td><td>演员-评论员法</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-14">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00020</td><td>Adaptive Bitrate Algorithm</td><td>自适应比特率算法</td><td>ABR</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00021</td><td>Adaptive Boosting</td><td>AdaBoost</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00022</td><td>Adaptive Gradient Algorithm</td><td>AdaGrad</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00023</td><td>Adaptive Moment Estimation Algorithm</td><td>Adam算法</td><td>Adam</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00024</td><td>Adaptive Resonance Theory</td><td>自适应谐振理论</td><td>ART</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00025</td><td>Additive Model</td><td>加性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00026</td><td>Adversarial</td><td>对抗</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00027</td><td>Adversarial Example</td><td>对抗样本</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-06-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00028</td><td>Adversarial Networks</td><td>对抗网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-08-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00029</td><td>Adversarial Training</td><td>对抗训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00030</td><td>Affine Layer</td><td>仿射层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00031</td><td>Affine Transformation</td><td>仿射变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00032</td><td>Affinity Matrix</td><td>亲和矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00033</td><td>Agent</td><td>智能体</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-04-06-6">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-15-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-10-2">[3]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-29-5">[4]</a></td><td></td></tr><tr class="odd"><td>AITD-00034</td><td>Agglomerative</td><td>聚合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00035</td><td>Agnostic PAC Learnable</td><td>不可知PAC可学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00036</td><td>Algorithm</td><td>算法</td><td></td><td><ahref="https://jiqizhixin.github.io/AI-Terminology-page/">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-05-23-4">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-04-2">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00037</td><td>Almost Everywhere</td><td>几乎处处</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00038</td><td>Almost Sure</td><td>几乎必然</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00039</td><td>Almost Sure Convergence</td><td>几乎必然收敛</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00040</td><td>Alpha-Beta Pruning</td><td>α-β修剪法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00041</td><td>Alternative Splicing Dataset</td><td>选择性剪接数据集</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00042</td><td>Ambiguity</td><td>分歧</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00043</td><td>Analytic Gradient</td><td>解析梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00044</td><td>Ancestral Sampling</td><td>原始采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00045</td><td>Annealed Importance Sampling</td><td>退火重要采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00046</td><td>Anomaly Detection</td><td>异常检测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00047</td><td>Aperiodic</td><td>非周期的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00048</td><td>Aperiodic Graph</td><td>非周期性图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00049</td><td>Application-Specific Integrated Circuit</td><td>专用集成电路</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00050</td><td>Approximate Bayesian Computation</td><td>近似贝叶斯计算</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00051</td><td>Approximate Dynamic Programming</td><td>近似动态规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00052</td><td>Approximate Inference</td><td>近似推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00053</td><td>Approximation</td><td>近似</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00054</td><td>Approximation Error</td><td>近似误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00055</td><td>Architecture</td><td>架构</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-12">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00056</td><td>Area Under ROC Curve</td><td>AUC（ROC曲线下方面积，度量分类模型好坏的标准）</td><td>AUC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00057</td><td>Arithmetic Coding</td><td>算术编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00058</td><td>Artificial General Intelligence</td><td>通用人工智能</td><td>AGI</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-06-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00059</td><td>Artificial Intelligence</td><td>人工智能</td><td>AI</td><td><a href="https://www.jiqizhixin.com/articles/2017-05-21-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-05-21-7">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-05-17-16">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[5]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00060</td><td>Artificial Neural Network</td><td>人工神经网络</td><td>ANN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00061</td><td>Artificial Neuron</td><td>人工神经元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00062</td><td>Association Analysis</td><td>关联分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00063</td><td>Associative Memory</td><td>联想记忆</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00064</td><td>Associative Memory Model</td><td>联想记忆模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00065</td><td>Asymptotically Unbiased</td><td>渐近无偏</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00066</td><td>Asynchronous Stochastic Gradient Descent</td><td>异步随机梯度下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00067</td><td>Asynchronous</td><td>异步</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00068</td><td>Atrous Convolution</td><td>空洞卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00069</td><td>Attention</td><td>注意力</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00070</td><td>Attention Cue</td><td>注意力提示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00071</td><td>Attention Distribution</td><td>注意力分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00072</td><td>Attention Mechanism</td><td>注意力机制</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-06-19-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-14-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-28-5">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00073</td><td>Attention Model</td><td>注意力模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00074</td><td>Attractor</td><td>吸引点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00075</td><td>Attribute</td><td>属性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00076</td><td>Attribute Conditional Independence Assumption</td><td>属性条件独立性假设</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00077</td><td>Attribute Space</td><td>属性空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00078</td><td>Attribute Value</td><td>属性值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00079</td><td>Augmented Lagrangian</td><td>增广拉格朗日法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00080</td><td>Auto-Regressive Network</td><td>自回归网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00081</td><td>Autoencoder</td><td>自编码器</td><td>AE</td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-26-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00082</td><td>Automatic Differentiation</td><td>自动微分</td><td>AD</td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-07">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00083</td><td>Automatic Speech Recognition</td><td>自动语音识别</td><td>ASR</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00084</td><td>Automatic Summarization</td><td>自动摘要</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00085</td><td>Autoregressive Generative Model</td><td>自回归生成模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00086</td><td>Autoregressive Model</td><td>自回归模型</td><td>AR</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00087</td><td>Autoregressive Process</td><td>自回归过程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00088</td><td>Average Gradient</td><td>平均梯度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00089</td><td>Average Pooling Layer</td><td>平均汇聚层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00090</td><td>Average-Pooling</td><td>平均汇聚</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00091</td><td>Averaged Perceptron</td><td>平均感知器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00092</td><td>Back Propagation</td><td>反向传播</td><td>BP</td><td><a href="https://www.jiqizhixin.com/articles/2016-11-25-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00093</td><td>Back Propagation Algorithm</td><td>反向传播算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00094</td><td>Back Propagation Through Time</td><td>随时间反向传播</td><td>BPTT</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00095</td><td>Back-Off</td><td>回退</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00096</td><td>Backward</td><td>后向</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00097</td><td>Backward Induction</td><td>反向归纳</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00098</td><td>Backward Search</td><td>反向搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00099</td><td>Bag of Words</td><td>词袋</td><td>BOW</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00100</td><td>Bagging</td><td>袋装</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00101</td><td>Bandit</td><td>赌博机/老虎机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00102</td><td>Bandpass Filter</td><td>带通滤波器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00103</td><td>Base</td><td>基</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00104</td><td>Base Classifier</td><td>基分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00105</td><td>Base Learner</td><td>基学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00106</td><td>Base Learning Algorithm</td><td>基学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00107</td><td>Base Vector</td><td>基向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00108</td><td>Baseline</td><td>基准</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00109</td><td>Basin of Attraction</td><td>吸引域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00110</td><td>Batch</td><td>批量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00111</td><td>Batch Gradient Descent</td><td>批量梯度下降法</td><td>BGD</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00112</td><td>Batch Learning</td><td>批量学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00113</td><td>Batch Normalization</td><td>批量规范化</td><td>BN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00114</td><td>Batch Size</td><td>批量大小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00115</td><td>Baum-Welch Algorithm</td><td>Baum-Welch算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00116</td><td>Bayes Classifier</td><td>贝叶斯分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00117</td><td>Bayes Decision Rule</td><td>贝叶斯决策准则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00118</td><td>Bayes Error</td><td>贝叶斯误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00119</td><td>Bayes Model Averaging</td><td>贝叶斯模型平均</td><td>BMA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00120</td><td>Bayes Optimal Classifier</td><td>贝叶斯最优分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00121</td><td>Bayes Risk</td><td>贝叶斯风险</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00122</td><td>Bayes' Rule</td><td>贝叶斯规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00123</td><td>Bayes' Theorem</td><td>贝叶斯定理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00124</td><td>Bayesian Decision Theory</td><td>贝叶斯决策理论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00125</td><td>Bayesian Estimation</td><td>贝叶斯估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00126</td><td>Bayesian Inference</td><td>贝叶斯推断</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-00127</td><td>Bayesian Learning</td><td>贝叶斯学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00128</td><td>Bayesian Linear Regression</td><td>贝叶斯线性回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00129</td><td>Bayesian Network</td><td>贝叶斯网/贝叶斯网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>Network翻译为网或网络皆可，只要统一翻译成网或者统一翻译成网络即可；统计，机器学习</td></tr><tr class="odd"><td>AITD-00130</td><td>Bayesian Optimization</td><td>贝叶斯优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-28">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00131</td><td>Bayesian Probability</td><td>贝叶斯概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00132</td><td>Bayesian Statistics</td><td>贝叶斯统计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00133</td><td>Beam Search</td><td>束搜索</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-31-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00134</td><td>Benchmark</td><td>基准</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00135</td><td>Belief Network</td><td>信念网/信念网络</td><td>BN</td><td>[1]</td><td>Network翻译为网或网络皆可，只要统一翻译成网或者统一翻译成网络即可</td></tr><tr class="odd"><td>AITD-00136</td><td>Belief Propagation</td><td>信念传播</td><td>BP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00137</td><td>Bellman Equation</td><td>贝尔曼方程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00138</td><td>Bellman Optimality Equation</td><td>贝尔曼最优方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00139</td><td>Bernoulli Distribution</td><td>伯努利分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-00140</td><td>Bernoulli Output Distribution</td><td>伯努利输出分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00141</td><td>Best-Arm Problem</td><td>最优臂问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00142</td><td>Beta Distribution</td><td>贝塔分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00143</td><td>Between-Class Scatter Matrix</td><td>类间散度矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00144</td><td>BFGS</td><td>BFGS</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00145</td><td>Bi-Directional Long-Short Term Memory</td><td>双向长短期记忆</td><td>Bi-LSTM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00146</td><td>Bi-Partition</td><td>二分法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00147</td><td>Bias</td><td>偏差/偏置</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[4]</a></td><td>看上下语境；机器学习</td></tr><tr class="odd"><td>AITD-00148</td><td>Bias In Affine Function</td><td>偏置</td><td></td><td>[1]</td><td>看上下语境</td></tr><tr class="even"><td>AITD-00149</td><td>Bias In Statistics</td><td>偏差</td><td></td><td>[1]</td><td>看上下语境</td></tr><tr class="odd"><td>AITD-00150</td><td>Bias Shift</td><td>偏置偏移</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00151</td><td>Bias-Variance Decomposition</td><td>偏差 - 方差分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00152</td><td>Bias-Variance Dilemma</td><td>偏差 - 方差困境</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00153</td><td>Biased</td><td>有偏</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00154</td><td>Biased Importance Sampling</td><td>有偏重要采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00155</td><td>Bidirectional Language Model</td><td>双向语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00156</td><td>Bidirectional Recurrent Neural Network</td><td>双向循环神经网络</td><td>Bi-RNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00157</td><td>Bigram</td><td>二元语法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00158</td><td>Bilingual Evaluation Understudy</td><td>BLEU</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00159</td><td>Binary Classification</td><td>二分类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00160</td><td>Binary Relation</td><td>二元关系</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00161</td><td>Binary Sparse Coding</td><td>二值稀疏编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00162</td><td>Binomial Distribution</td><td>二项分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00163</td><td>Binomial Logistic Regression Model</td><td>二项对数几率回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00164</td><td>Binomial Test</td><td>二项检验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00165</td><td>Biological Plausibility</td><td>生物学合理性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00166</td><td>Bit</td><td>比特</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00167</td><td>Block</td><td>块</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00168</td><td>Block Coordinate Descent</td><td>块坐标下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00169</td><td>Block Gibbs Sampling</td><td>块吉布斯采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00170</td><td>Boilerplate Code</td><td>样板代码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00171</td><td>Boltzmann</td><td>玻尔兹曼</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00172</td><td>Boltzmann Distribution</td><td>玻尔兹曼分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00173</td><td>Boltzmann Factor</td><td>玻尔兹曼因子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00174</td><td>Boltzmann Machine</td><td>玻尔兹曼机</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-08-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00175</td><td>Boosting</td><td>Boosting（一种模型训练加速方式）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00176</td><td>Boosting Tree</td><td>提升树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00177</td><td>Bootstrap Aggregating</td><td>Bagging</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00178</td><td>Bootstrap Sampling</td><td>自助采样法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00179</td><td>Bootstrapping</td><td>自助法/自举法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00180</td><td>Bottleneck Layer</td><td>瓶颈层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00181</td><td>Bottom-Up</td><td>自下而上</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00182</td><td>Bounding Boxes</td><td>边界框</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00183</td><td>Break-Event Point</td><td>平衡点</td><td>BEP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00184</td><td>Bridge Sampling</td><td>桥式采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00185</td><td>Broadcasting</td><td>广播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00186</td><td>Broyden's Algorithm</td><td>Broyden类算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00187</td><td>Bucketing</td><td>分桶</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00188</td><td>Burn-In Period</td><td>预烧期</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00189</td><td>Burning-In</td><td>磨合</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00190</td><td>Calculus</td><td>微积分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00191</td><td>Calculus of Variations</td><td>变分法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00192</td><td>Calibration</td><td>校准</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00193</td><td>Canonical</td><td>正则的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00194</td><td>Canonical Correlation Analysis</td><td>典型相关分析</td><td>CCA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00195</td><td>Capacity</td><td>容量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00196</td><td>Cartesian Coordinate</td><td>笛卡尔坐标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00197</td><td>Cascade</td><td>级联</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00198</td><td>Cascade-Correlation</td><td>级联相关</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00199</td><td>Catastrophic Forgetting</td><td>灾难性遗忘</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00200</td><td>Categorical Attribute</td><td>分类属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00201</td><td>Categorical Distribution</td><td>类别分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00202</td><td>Causal Factor</td><td>因果因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00203</td><td>Causal Modeling</td><td>因果模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00204</td><td>Cell</td><td>单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00205</td><td>Centered Difference</td><td>中心差分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00206</td><td>Central Limit Theorem</td><td>中心极限定理</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00207</td><td>Chain Rule</td><td>链式法则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00208</td><td>Channel</td><td>通道</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00209</td><td>Chaos</td><td>混沌</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00210</td><td>Chebyshev Distance</td><td>切比雪夫距离</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00211</td><td>Chord</td><td>弦</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00212</td><td>Chordal Graph</td><td>弦图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00213</td><td>City Block Distance</td><td>街区距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00214</td><td>Class</td><td>类别</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00215</td><td>Class Label</td><td>类标记</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00216</td><td>Class-Conditional Probability</td><td>类条件概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00217</td><td>Class-Imbalance</td><td>类别不平衡</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00218</td><td>Classification</td><td>分类</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00219</td><td>Classification And Regression Tree</td><td>分类与回归树</td><td>CART</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00220</td><td>Classifier</td><td>分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00221</td><td>Clip Gradient</td><td>梯度截断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00222</td><td>Clipping The Gradient</td><td>截断梯度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00223</td><td>Clique</td><td>团</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00224</td><td>Clique Potential</td><td>团势能</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00225</td><td>Clockwork RNN</td><td>时钟循环神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00226</td><td>Closed Form Solution</td><td>闭式解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00227</td><td>Closed-Form</td><td>闭式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00228</td><td>Cluster</td><td>簇</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00229</td><td>Cluster Analysis</td><td>聚类分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00230</td><td>Cluster Assumption</td><td>聚类假设</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00231</td><td>Clustering</td><td>聚类</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-09">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00232</td><td>Clustering Ensemble</td><td>聚类集成</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00233</td><td>Co-Adapting</td><td>共适应</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00234</td><td>Co-Occurrence</td><td>共现</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00235</td><td>Co-Occurrence Frequency</td><td>共现词频</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00236</td><td>Co-Training</td><td>协同训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00237</td><td>Code</td><td>编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00238</td><td>Codebook Learning</td><td>码书学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00239</td><td>Coding Matrix</td><td>编码矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00240</td><td>Collaborative Filtering</td><td>协同过滤</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-23-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00241</td><td>Collapsed Gibbs Sampling</td><td>收缩的吉布斯抽样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00242</td><td>Collinearity</td><td>共线性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00243</td><td>COLT</td><td>国际学习理论会议</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00244</td><td>Column</td><td>列</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00245</td><td>Column Space</td><td>列空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00246</td><td>Combinatorial Optimization</td><td>组合优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00247</td><td>Committee-Based Learning</td><td>基于委员会的学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00248</td><td>Common Cause</td><td>共因</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00249</td><td>Common Parent</td><td>同父</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00250</td><td>Compact Singular Value Decomposition</td><td>紧奇异值分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00251</td><td>Competitive Learning</td><td>竞争型学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00252</td><td>Complementary Slackness</td><td>互补松弛</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00253</td><td>Complete Graph</td><td>完全图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00254</td><td>Complete Linkage</td><td>完全连接</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00255</td><td>Complete-Data</td><td>完全数据</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00256</td><td>Complex Cell</td><td>复杂细胞</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00257</td><td>Component Learner</td><td>组件学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00258</td><td>Compositionality</td><td>组合性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00259</td><td>Comprehensibility</td><td>可解释性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00260</td><td>Computation Cost</td><td>计算代价</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00261</td><td>Computation Graph</td><td>计算图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00262</td><td>Computational Learning Theory</td><td>计算学习理论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00263</td><td>Computational Linguistics</td><td>计算语言学</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00264</td><td>Computer Vision</td><td>计算机视觉</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00265</td><td>Concatenate</td><td>连结</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00266</td><td>Concept Class</td><td>概念类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00267</td><td>Concept Drift</td><td>概念漂移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00268</td><td>Concept Learning System</td><td>概念学习系统</td><td>CLS</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00269</td><td>Concept Shift</td><td>概念偏移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00270</td><td>Conditional Computation</td><td>条件计算</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00271</td><td>Conditional Entropy</td><td>条件熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00272</td><td>Conditional Independence</td><td>条件独立</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00273</td><td>Conditional Language Model</td><td>条件语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00274</td><td>Conditional Mutual Information</td><td>条件互信息</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00275</td><td>Conditional Probability</td><td>条件概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00276</td><td>Conditional Probability Density Function</td><td>条件概率密度函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00277</td><td>Conditional Probability Distribution</td><td>条件概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00278</td><td>Conditional Probability Table</td><td>条件概率表</td><td>CPT</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00279</td><td>Conditional Random Field</td><td>条件随机场</td><td>CRF</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00280</td><td>Conditional Risk</td><td>条件风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00281</td><td>Conditionally Independent</td><td>条件独立的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00282</td><td>Conference On Neural Information Processing Systems</td><td>国际神经信息处理系统会议</td><td>NeurIPS</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-18-9">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00283</td><td>Confidence</td><td>置信度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00284</td><td>Conflict Resolution</td><td>冲突消解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00285</td><td>Confusion Matrix</td><td>混淆矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00286</td><td>Conjugate</td><td>共轭</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00287</td><td>Conjugate Directions</td><td>共轭方向</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00288</td><td>Conjugate Distribution</td><td>共轭分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00289</td><td>Conjugate Gradient</td><td>共轭梯度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>优化，数学</td></tr><tr class="odd"><td>AITD-00290</td><td>Conjugate Prior</td><td>共轭先验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00291</td><td>Connection Weight</td><td>连接权</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00292</td><td>Connectionism</td><td>连接主义</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00293</td><td>Consistency</td><td>一致性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00294</td><td>Consistency Convergence</td><td>一致性收敛</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00295</td><td>Constrained Optimization</td><td>约束优化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00296</td><td>Content-Addressable Memory</td><td>基于内容寻址的存储</td><td>CAM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00297</td><td>Context Variable</td><td>上下文变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00298</td><td>Context Vector</td><td>上下文向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00299</td><td>Context Window</td><td>上下文窗口</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00300</td><td>Context Word</td><td>上下文词</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00301</td><td>Context-Specific Independences</td><td>特定上下文独立</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00302</td><td>Contextual Bandit</td><td>上下文赌博机/上下文老虎机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00303</td><td>Contextualized Representation</td><td>基于上下文的表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00304</td><td>Contingency Table</td><td>列联表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00305</td><td>Continous Bag-Of-Words Model</td><td>连续词袋模型</td><td>CBOW</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00306</td><td>Continuation Method</td><td>延拓法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00307</td><td>Continuing Task</td><td>持续式任务</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00308</td><td>Continuous Attribute</td><td>连续属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00309</td><td>Continuous Learning</td><td>持续学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00310</td><td>Continuous Optimization</td><td>连续优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00311</td><td>Contractive</td><td>收缩</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00312</td><td>Contractive Autoencoder</td><td>收缩自编码器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00313</td><td>Contractive Neural Network</td><td>收缩神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00314</td><td>Contrastive Divergence</td><td>对比散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00315</td><td>Controller</td><td>控制器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00316</td><td>Convergence</td><td>收敛</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00317</td><td>Conversational Agent</td><td>会话智能体</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00318</td><td>Convex Optimization</td><td>凸优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-29-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00319</td><td>Convex Quadratic Programming</td><td>凸二次规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00320</td><td>Convex Set</td><td>凸集</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00321</td><td>Convexity</td><td>凸性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00322</td><td>Convolution</td><td>卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00323</td><td>Convolutional Boltzmann Machine</td><td>卷积玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00324</td><td>Convolutional Deep Belief Network</td><td>卷积深度信念网络</td><td>CDBN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00325</td><td>Convolutional Kernel</td><td>卷积核</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00326</td><td>Convolutional Network</td><td>卷积网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00327</td><td>Convolutional Neural Network</td><td>卷积神经网络</td><td>CNN</td><td><a href="https://www.jiqizhixin.com/articles/2017-12-19-8">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-08-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-18-2">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-00328</td><td>Coordinate</td><td>坐标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00329</td><td>Coordinate Ascent</td><td>坐标上升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00330</td><td>Coordinate Descent</td><td>坐标下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00331</td><td>Coparent</td><td>共父</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00332</td><td>Corpus</td><td>语料库</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00333</td><td>Correlation</td><td>相关系数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00334</td><td>Correlation Coefficient</td><td>相关系数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00335</td><td>Cosine</td><td>余弦</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00336</td><td>Cosine Decay</td><td>余弦衰减</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00337</td><td>Cosine Similarity</td><td>余弦相似度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00338</td><td>Cost</td><td>代价</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00339</td><td>Cost Curve</td><td>代价曲线</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00340</td><td>Cost Function</td><td>代价函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00341</td><td>Cost Matrix</td><td>代价矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00342</td><td>Cost-Sensitive</td><td>代价敏感</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00343</td><td>Covariance</td><td>协方差</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00344</td><td>Covariance Matrix</td><td>协方差矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00345</td><td>Covariance RBM</td><td>协方差RBM</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00346</td><td>Covariate Shift</td><td>协变量偏移</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00347</td><td>Coverage</td><td>覆盖</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00348</td><td>Credit Assignment Problem</td><td>贡献度分配问题</td><td>CAP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00349</td><td>Criterion</td><td>准则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00350</td><td>Critic</td><td>评论员</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00351</td><td>Critic Network</td><td>评价网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00352</td><td>Critical Point</td><td>临界点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00353</td><td>Critical Temperatures</td><td>临界温度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00354</td><td>Cross Correlation</td><td>互相关</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00355</td><td>Cross Entropy</td><td>交叉熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00356</td><td>Cross Validation</td><td>交叉验证</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-16-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00357</td><td>Cross-Entropy Loss Function</td><td>交叉熵损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00358</td><td>Crowdsourcing</td><td>众包</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-28-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00359</td><td>Cumulative Distribution Function</td><td>累积分布函数</td><td>CDF</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00360</td><td>Cumulative Function</td><td>累积函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00361</td><td>Curriculum Learning</td><td>课程学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00362</td><td>Curse of Dimensionality</td><td>维数灾难</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00363</td><td>Curvature</td><td>曲率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00364</td><td>Curve-Fitting</td><td>曲线拟合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00365</td><td>Cut Point</td><td>截断点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00366</td><td>Cutting Plane Algorithm</td><td>割平面法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00367</td><td>Cybernetics</td><td>控制论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00368</td><td>Cyclic Learning Rate</td><td>循环学习率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00369</td><td>Damping</td><td>衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00370</td><td>Damping Factor</td><td>阻尼因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00371</td><td>Data</td><td>数据</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00372</td><td>Data Augmentation</td><td>数据增强</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00373</td><td>Data Generating Distribution</td><td>数据生成分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00374</td><td>Data Generating Process</td><td>数据生成过程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00375</td><td>Data Instance</td><td>数据样本</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00376</td><td>Data Mining</td><td>数据挖掘</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00377</td><td>Data Parallelism</td><td>数据并行</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00378</td><td>Data Point</td><td>数据点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00379</td><td>Data Preprocessing</td><td>数据预处理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00380</td><td>Data Set</td><td>数据集</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-04-6">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00381</td><td>Data Wrangling</td><td>数据整理</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-25-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00382</td><td>Dataset Augmentation</td><td>数据集增强</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00383</td><td>Davidon-Fletcher-Powell</td><td>DFP</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00384</td><td>Debugging Strategy</td><td>调试策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00385</td><td>Decision Boundary</td><td>决策边界</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00386</td><td>Decision Function</td><td>决策函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00387</td><td>Decision Stump</td><td>决策树桩</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00388</td><td>Decision Surface</td><td>决策平面</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00389</td><td>Decision Tree</td><td>决策树</td><td>DT</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-10">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-29-5">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[4]</a></td><td></td></tr><tr class="odd"><td>AITD-00390</td><td>Decoder</td><td>解码器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00391</td><td>Decoding</td><td>解码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00392</td><td>Decompose</td><td>分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00393</td><td>Deconvolution</td><td>反卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00394</td><td>Deconvolutional Network</td><td>反卷积网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-14">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00395</td><td>Deduction</td><td>演绎</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00396</td><td>Deep Belief Network</td><td>深度信念网络</td><td>DBN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00397</td><td>Deep Boltzmann Machine</td><td>深度玻尔兹曼机</td><td>DBM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00398</td><td>Deep Circuit</td><td>深度回路</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00399</td><td>Deep Convolutional Generative Adversarial Network</td><td>深度卷积生成对抗网络</td><td>DCGAN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00400</td><td>Deep Feedforward Network</td><td>深度前馈网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00401</td><td>Deep Generative Model</td><td>深度生成模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00402</td><td>Deep Learning</td><td>深度学习</td><td>DL</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-17-2">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-15-4">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-15-2">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[4]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[5]</a></td><td></td></tr><tr class="even"><td>AITD-00403</td><td>Deep Model</td><td>深度模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00404</td><td>Deep Network</td><td>深度网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00405</td><td>Deep Neural Network</td><td>深度神经网络</td><td>DNN</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-15-2">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-10">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-07-2">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[5]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[6]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[7]</a></td><td></td></tr><tr class="odd"><td>AITD-00406</td><td>Deep Q-Learning</td><td>深度 Q 学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-10-10-2">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-08-22-8">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00407</td><td>Deep Q-Network</td><td>深度Q网络</td><td>DQN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00408</td><td>Deep Reinforcement Learning</td><td>深度强化学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00409</td><td>Deep Sequence Model</td><td>深度序列模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00410</td><td>Default Rule</td><td>默认规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00411</td><td>Definite Integral</td><td>定积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00412</td><td>Degree Of Belief</td><td>信任度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00413</td><td>Delta-Bar-Delta</td><td>Delta-Bar-Delta</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00414</td><td>Denoising</td><td>去噪</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00415</td><td>Denoising Autoencoder</td><td>去噪自编码器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00416</td><td>Denoising Score Matching</td><td>去躁分数匹配</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00417</td><td>Denominator Layout</td><td>分母布局</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00418</td><td>Dense</td><td>稠密</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00419</td><td>Density Estimation</td><td>密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00420</td><td>Density-Based Clustering</td><td>密度聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00421</td><td>Dependency</td><td>依赖</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00422</td><td>Depth</td><td>深度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00423</td><td>Derivative</td><td>导数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00424</td><td>Description</td><td>描述</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00425</td><td>Design Matrix</td><td>设计矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00426</td><td>Detailed Balance</td><td>细致平衡</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00427</td><td>Detailed Balance Equation</td><td>细致平衡方程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00428</td><td>Detector Stage</td><td>探测级</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00429</td><td>Determinant</td><td>行列式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00430</td><td>Deterministic</td><td>确定性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00431</td><td>Deterministic Model</td><td>确定性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00432</td><td>Deterministic Policy</td><td>确定性策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00433</td><td>Development Set</td><td>开发集</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00434</td><td>Diagonal Matrix</td><td>对角矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00435</td><td>Diameter</td><td>直径</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00436</td><td>Dictionary</td><td>字典</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00437</td><td>Dictionary Learning</td><td>字典学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00438</td><td>Differentiable Function</td><td>可微函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00439</td><td>Differentiable Neural Computer</td><td>可微分神经计算机</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-11-7">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00440</td><td>Differential Entropy</td><td>微分熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00441</td><td>Differential Equation</td><td>微分方程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00442</td><td>Differentiation</td><td>微分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00443</td><td>Dilated Convolution</td><td>膨胀卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00444</td><td>Dimension</td><td>维度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00445</td><td>Dimension Reduction</td><td>降维</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00446</td><td>Dimensionality Reduction Algorithm</td><td>降维算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-08-31-2">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00447</td><td>Dirac Delta Function</td><td>Dirac Delta函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00448</td><td>Dirac Distribution</td><td>Dirac分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00449</td><td>Directed</td><td>有向</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00450</td><td>Directed Acyclic Graph</td><td>有向非循环图</td><td>DAG</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00451</td><td>Directed Edge</td><td>有向边</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00452</td><td>Directed Graph</td><td>有向图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00453</td><td>Directed Graphical Model</td><td>有向图模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00454</td><td>Directed Model</td><td>有向模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00455</td><td>Directed Separation</td><td>有向分离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00456</td><td>Directional Derivative</td><td>方向导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00457</td><td>Dirichlet Distribution</td><td>狄利克雷分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00458</td><td>Disagreement Measure</td><td>不合度量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00459</td><td>Disagreement-Based Methods</td><td>基于分歧的方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00460</td><td>Discount Factor</td><td>衰减系数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00461</td><td>Discounted Return</td><td>折扣回报</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00462</td><td>Discrete Optimization</td><td>离散优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00463</td><td>Discriminant Function</td><td>判别函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00464</td><td>Discriminative Approach</td><td>判别方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00465</td><td>Discriminative Model</td><td>判别式模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00466</td><td>Discriminative RBM</td><td>判别RBM</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00467</td><td>Discriminator</td><td>判别器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00468</td><td>Discriminator Network</td><td>判别网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00469</td><td>Distance</td><td>距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00470</td><td>Distance Measure</td><td>距离度量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00471</td><td>Distance Metric Learning</td><td>距离度量学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00472</td><td>Distributed Representation</td><td>分布式表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00473</td><td>Distribution</td><td>分布</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-09">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00474</td><td>Diverge</td><td>发散</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00475</td><td>Divergence</td><td>散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00476</td><td>Diversity</td><td>多样性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00477</td><td>Diversity Measure</td><td>多样性度量/差异性度量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00478</td><td>Divide-And-Conquer</td><td>分而治之</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00479</td><td>Divisive</td><td>分裂</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00480</td><td>Domain</td><td>领域</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00481</td><td>Domain Adaptation</td><td>领域自适应</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00482</td><td>Dominant Eigenvalue</td><td>主特征值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00483</td><td>Dominant Eigenvector</td><td>主特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00485</td><td>Dominant Strategy</td><td>占优策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00486</td><td>Dot Product</td><td>点积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00487</td><td>Double Backprop</td><td>双反向传播</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00488</td><td>Doubly Block Circulant Matrix</td><td>双重分块循环矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00489</td><td>Down Sampling</td><td>下采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00490</td><td>Downstream Task</td><td>下游任务</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00491</td><td>Dropout</td><td>暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00492</td><td>Dropout Boosting</td><td>暂退Boosting</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00493</td><td>Dropout Mask</td><td>暂退掩码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00494</td><td>Dropout Method</td><td>暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00495</td><td>Dual Algorithm</td><td>对偶算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00496</td><td>Dual Problem</td><td>对偶问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00497</td><td>Dummy Node</td><td>哑结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00498</td><td>Dying ReLU Problem</td><td>死亡ReLU问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00499</td><td>Dynamic Bayesian Network</td><td>动态贝叶斯网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00500</td><td>Dynamic Computational Graph</td><td>动态计算图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00501</td><td>Dynamic Fusion</td><td>动态融合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00502</td><td>Dynamic Programming</td><td>动态规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00503</td><td>Dynamic Structure</td><td>动态结构</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00504</td><td>Dynamical System</td><td>动力系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00505</td><td>Eager Learning</td><td>急切学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00506</td><td>Early Stopping</td><td>早停</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00507</td><td>Earth-Mover's Distance</td><td>推土机距离</td><td>EMD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00508</td><td>Echo State Network</td><td>回声状态网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00509</td><td>Edge</td><td>边</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00510</td><td>Edge Device</td><td>边缘设备</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-24-8">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00511</td><td>Effective Capacity</td><td>有效容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00512</td><td>Eigendecomposition</td><td>特征分解</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-07-05-2">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00513</td><td>Eigenvalue</td><td>特征值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00514</td><td>Eigenvalue Decomposition</td><td>特征值分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00515</td><td>Elastic Net Regularization</td><td>弹性网络正则化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00516</td><td>Elastic Weight Consolidation</td><td>弹性权重巩固</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00517</td><td>Element-Wise Product</td><td>逐元素积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00518</td><td>Elementary Basis Vectors</td><td>基本单位向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00519</td><td>Ellipsoid Method</td><td>椭球法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00520</td><td>Embedding</td><td>嵌入</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-02-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00521</td><td>Embedding Lookup Table</td><td>嵌入表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00522</td><td>Emotional Analysis</td><td>情绪分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00523</td><td>Empirical Conditional Entropy</td><td>经验条件熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00524</td><td>Empirical Distribution</td><td>经验分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00525</td><td>Empirical Entropy</td><td>经验熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00526</td><td>Empirical Error</td><td>经验误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00527</td><td>Empirical Frequency</td><td>经验频率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00528</td><td>Empirical Loss</td><td>经验损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00529</td><td>Empirical Risk</td><td>经验风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00530</td><td>Empirical Risk Minimization</td><td>经验风险最小化</td><td>ERM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00531</td><td>Encoder</td><td>编码器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00532</td><td>Encoder-Decoder</td><td>编码器-解码器（模型）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-00533</td><td>Encoding</td><td>编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00534</td><td>End-To-End</td><td>端到端</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-15">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00535</td><td>End-To-End Learning</td><td>端到端学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00536</td><td>End-To-End Memory Network</td><td>端到端记忆网络</td><td>Memn2N</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00537</td><td>Energy Function</td><td>能量函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00538</td><td>Energy Gap</td><td>能量差异</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00539</td><td>Energy-Based Model</td><td>基于能量的模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00540</td><td>Ensemble</td><td>集成</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00541</td><td>Ensemble Learning</td><td>集成学习</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-14-8">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00542</td><td>Ensemble Pruning</td><td>集成修剪</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00543</td><td>Entropy</td><td>熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00544</td><td>Entropy Encoding</td><td>熵编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00545</td><td>Environment</td><td>环境</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00546</td><td>Episode</td><td>回合</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00547</td><td>Episodic Task</td><td>回合式任务</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00548</td><td>Epoch</td><td>轮</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00549</td><td>Equal-Width Convolution</td><td>等宽卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00550</td><td>Equality Constraint</td><td>等式约束</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00551</td><td>Equilibrium Distribution</td><td>均衡分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00552</td><td>Equivariance</td><td>等变</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00553</td><td>Equivariant Representations</td><td>等变表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00554</td><td>Error</td><td>误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00555</td><td>Error Backpropagation Algorithm</td><td>误差反向传播算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00556</td><td>Error Backpropagation</td><td>误差反向传播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00557</td><td>Error Bar</td><td>误差条</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00558</td><td>Error Correcting Output Codes</td><td>纠错输出编码</td><td>ECOC</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00559</td><td>Error Function</td><td>误差函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00560</td><td>Error Metric</td><td>误差度量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00561</td><td>Error Rate</td><td>错误率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00562</td><td>Error-Ambiguity Decomposition</td><td>误差－分歧分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00563</td><td>Estimation Error</td><td>估计误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00564</td><td>Estimation Of Mathematical Expectation</td><td>数学期望估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00565</td><td>Estimator</td><td>估计/估计量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00566</td><td>Euclidean Distance</td><td>欧氏距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00567</td><td>Euclidean Norm</td><td>欧几里得范数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00568</td><td>Euclidean Space</td><td>欧氏空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00569</td><td>Euler-Lagrange Equation</td><td>欧拉-拉格朗日方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00570</td><td>Evaluation Criterion</td><td>评价准则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00571</td><td>Evidence</td><td>证据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00572</td><td>Evidence Lower Bound</td><td>证据下界</td><td>ELBO</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00573</td><td>Evolution</td><td>演化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00574</td><td>Evolutionary Computation</td><td>演化计算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00575</td><td>Exact</td><td>确切的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00576</td><td>Exact Inference</td><td>精确推断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00577</td><td>Example</td><td>样例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00578</td><td>Excess Error</td><td>额外误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00579</td><td>Exchangeable</td><td>可交换的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00580</td><td>Expectation</td><td>期望</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00581</td><td>Expectation Maximization Algorithm</td><td>期望极大算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00582</td><td>Expectation Maximization</td><td>期望最大化</td><td>EM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00583</td><td>Expectation Step</td><td>E步</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00584</td><td>Expected Error</td><td>期望错误</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00585</td><td>Expected Loss</td><td>期望损失</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00586</td><td>Expected Return</td><td>期望回报</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00587</td><td>Expected Risk</td><td>期望风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00588</td><td>Expected Value</td><td>期望值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00589</td><td>Experience</td><td>经验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00590</td><td>Experience Replay</td><td>经验回放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00591</td><td>Expert Network</td><td>专家网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00592</td><td>Expert System</td><td>专家系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00593</td><td>Explaining Away</td><td>相消解释</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00594</td><td>Explaining Away Effect</td><td>相消解释作用</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00595</td><td>Explanatory Factort</td><td>解释因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00596</td><td>Explicit Density Model</td><td>显式密度模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00597</td><td>Exploding Gradient</td><td>梯度爆炸</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00598</td><td>Exploitation</td><td>利用</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00599</td><td>Exploration</td><td>探索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00600</td><td>Exploration-Exploitation Dilemma</td><td>探索-利用窘境</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00601</td><td>Exponential Decay</td><td>指数衰减</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00602</td><td>Exponential Distribution</td><td>指数分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00603</td><td>Exponential Linear Unit</td><td>指数线性单元</td><td>ELU</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00604</td><td>Exponential Loss</td><td>指数损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00605</td><td>Exponential Loss Function</td><td>指数损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00606</td><td>Exponentially Weighted Moving Average</td><td>指数加权移动平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00607</td><td>Exposure Bias</td><td>曝光偏差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00608</td><td>External Memory</td><td>外部记忆</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00609</td><td>Extreme Learning Machine</td><td>超限学习机</td><td>ELM</td><td><ahref="https://www.jiqizhixin.com/articles/2016-09-30-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00610</td><td>F Measure</td><td>F值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00611</td><td>F-Score</td><td>F分数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00612</td><td>Factor</td><td>因子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00613</td><td>Factor Analysis</td><td>因子分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00614</td><td>Factor Graph</td><td>因子图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00615</td><td>Factor Loading</td><td>因子负荷量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00616</td><td>Factorization</td><td>因子分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00617</td><td>Factorized</td><td>分解的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00618</td><td>Factors of Variation</td><td>变差因素</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00619</td><td>False Negative</td><td>假负例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00620</td><td>False Positive</td><td>假正例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00621</td><td>False Positive Rate</td><td>假正例率</td><td>FPR</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00622</td><td>Fast Dropout</td><td>快速暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00623</td><td>Fast Persistent Contrastive Divergence</td><td>快速持续性对比散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00624</td><td>Fault-Tolerant Asynchronous Training</td><td>容错异步训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00625</td><td>Feasible</td><td>可行</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00626</td><td>Feature</td><td>特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00627</td><td>Feature Engineering</td><td>特征工程</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00628</td><td>Feature Extraction</td><td>特征抽取</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00629</td><td>Feature Extractor</td><td>特征提取器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00630</td><td>Feature Function</td><td>特征函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00631</td><td>Feature Map</td><td>特征图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00632</td><td>Feature Scaling Transform</td><td>特征尺度变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00633</td><td>Feature Selection</td><td>特征选择</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00634</td><td>Feature Space</td><td>特征空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00635</td><td>Feature Vector</td><td>特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00636</td><td>Featured Learning</td><td>特征学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00637</td><td>Feedback</td><td>反馈</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00638</td><td>Feedforward</td><td>前馈</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00639</td><td>Feedforward Classifier</td><td>前馈分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00640</td><td>Feedforward Network</td><td>前馈网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00641</td><td>Feedforward Neural Network</td><td>前馈神经网络</td><td>FNN</td><td><a href="https://www.jiqizhixin.com/articles/2017-09-07-9">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00642</td><td>Few-Shot Learning</td><td>少试学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00643</td><td>Fidelity</td><td>逼真度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00644</td><td>Field Programmable Gated Array</td><td>现场可编程门阵列</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00645</td><td>Filter</td><td>滤波器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00646</td><td>Filter Method</td><td>过滤式方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00647</td><td>Fine-Tuning</td><td>微调</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00648</td><td>Finite Difference</td><td>有限差分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00649</td><td>First Layer</td><td>第一层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00650</td><td>First-Order Method</td><td>一阶方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00651</td><td>First-Order Rule</td><td>一阶规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00652</td><td>Fisher Information Matrix</td><td>Fisher信息矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00653</td><td>Fixed Point Equation</td><td>不动点方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00654</td><td>Fixed-Point Arithmetic</td><td>不动点运算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00655</td><td>Flat Minima</td><td>平坦最小值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00656</td><td>Flip</td><td>翻转</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00657</td><td>Flipping Output</td><td>翻转法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00658</td><td>Float-Point Arithmetic</td><td>浮点运算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00659</td><td>Fluctuation</td><td>振荡</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00660</td><td>Focus Attention</td><td>聚焦式注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00661</td><td>Folk Theorem</td><td>无名氏定理</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00662</td><td>Forget Gate</td><td>遗忘门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00663</td><td>Forward</td><td>前向</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00664</td><td>Forward KL Divergence</td><td>前向KL散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00665</td><td>Forward Mode Accumulation</td><td>前向模式累加</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00666</td><td>Forward Propagation</td><td>前向传播/正向传播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00667</td><td>Forward Search</td><td>前向搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00668</td><td>Forward Stagewise Algorithm</td><td>前向分步算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00669</td><td>Forward-Backward Algorithm</td><td>前向-后向算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00670</td><td>Fourier Transform</td><td>傅立叶变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00671</td><td>Fovea</td><td>中央凹</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00672</td><td>Fractionally Strided Convolution</td><td>微步卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00673</td><td>Free Energy</td><td>自由能</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00674</td><td>Frequentist</td><td>频率主义学派</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00675</td><td>Frequentist Probability</td><td>频率派概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00676</td><td>Frequentist Statistics</td><td>频率派统计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00677</td><td>Frobenius Norm</td><td>Frobenius 范数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00678</td><td>Full</td><td>全</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00679</td><td>Full Conditional Distribution</td><td>满条件分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00680</td><td>Full Conditional Probability</td><td>全条件概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00681</td><td>Full Padding</td><td>全填充</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00682</td><td>Full Singular Value Decomposition</td><td>完全奇异值分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00683</td><td>Full-Rank Matrix</td><td>满秩矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00684</td><td>Fully Connected Layer</td><td>全连接层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00685</td><td>Fully Connected Neural Network</td><td>全连接神经网络</td><td>FCNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00686</td><td>Fully Convolutional Network</td><td>全卷积网络</td><td>FCN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00687</td><td>Function</td><td>函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00688</td><td>Functional</td><td>泛函</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00689</td><td>Functional Derivative</td><td>泛函导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00690</td><td>Functional Margin</td><td>函数间隔</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00691</td><td>Functional Neuron</td><td>功能神经元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00692</td><td>Gabor Function</td><td>Gabor函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00693</td><td>Gain Ratio</td><td>増益率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00694</td><td>Game Payoff</td><td>博弈效用</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00695</td><td>Game Theory</td><td>博弈论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00696</td><td>Gamma Distribution</td><td>Gamma分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00697</td><td>Gate</td><td>门</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00698</td><td>Gate Controlled RNN</td><td>门控循环神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00699</td><td>Gated</td><td>门控</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00700</td><td>Gated Control</td><td>门控</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00701</td><td>Gated Recurrent Net</td><td>门控循环网络</td><td>GRN</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-24">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00702</td><td>Gated Recurrent Unit</td><td>门控循环单元</td><td>GRU</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00703</td><td>Gated RNN</td><td>门控RNN</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00704</td><td>Gater</td><td>选通器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00705</td><td>Gating Mechanism</td><td>门控机制</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00706</td><td>Gaussian Distribution</td><td>高斯分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00707</td><td>Gaussian Error Linear Unit</td><td>高斯误差线性单元</td><td>GELU</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00708</td><td>Gaussian Kernel</td><td>高斯核</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00709</td><td>Gaussian Kernel Function</td><td>高斯核函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00710</td><td>Gaussian Mixture Model</td><td>高斯混合模型</td><td>GMM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00711</td><td>Gaussian Mixtures</td><td>高斯混合（模型）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00712</td><td>Gaussian Output Distribution</td><td>高斯输出分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00713</td><td>Gaussian Process</td><td>高斯过程</td><td>GP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00714</td><td>Gaussian Process Regression</td><td>高斯过程回归</td><td>GPR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00715</td><td>Gaussian RBM</td><td>高斯RBM</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00716</td><td>Gaussian-Bernoulli RBM</td><td>高斯-伯努利RBM</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00717</td><td>General Problem Solving</td><td>通用问题求解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00718</td><td>General Purpose GPU</td><td>通用GPU</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00719</td><td>Generalization Ability</td><td>泛化能力</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00720</td><td>Generalization Error</td><td>泛化误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00721</td><td>Generalization Error Bound</td><td>泛化误差上界</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00722</td><td>Generalize</td><td>泛化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-25-10">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00723</td><td>Generalized Bregman Divergence</td><td>一般化 Bregman 散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00724</td><td>Generalized Expectation Maximization</td><td>广义期望极大</td><td>GEM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00725</td><td>Generalized Function</td><td>广义函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00726</td><td>Generalized Lagrange Function</td><td>广义拉格朗日函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00727</td><td>Generalized Lagrangian</td><td>广义拉格朗日</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00728</td><td>Generalized Linear Model</td><td>广义线性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00729</td><td>Generalized Pseudolikelihood</td><td>广义伪似然</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00730</td><td>Generalized Pseudolikelihood Estimator</td><td>广义伪似然估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00731</td><td>Generalized Rayleigh Quotient</td><td>广义瑞利商</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00732</td><td>Generalized Score Matching</td><td>广义得分匹配</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00733</td><td>Generative Adversarial Framework</td><td>生成式对抗框架</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00734</td><td>Generative Adversarial Network</td><td>生成对抗网络</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-26-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-08-5">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-13-2">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-00735</td><td>Generative Approach</td><td>生成方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00736</td><td>Generative Model</td><td>生成式模型</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-19-7">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-11-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-04-5">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-00737</td><td>Generative Modeling</td><td>生成式建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00738</td><td>Generative Moment Matching Network</td><td>生成矩匹配网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00739</td><td>Generative Pre-Training</td><td>生成式预训练</td><td>GPT</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00740</td><td>Generative Stochastic Network</td><td>生成随机网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00741</td><td>Generative Weight</td><td>生成权重</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00742</td><td>Generator</td><td>生成器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00743</td><td>Generator Network</td><td>生成器网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00744</td><td>Genetic Algorithm</td><td>遗传算法</td><td>GA</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-17-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-22">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-12-2">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[5]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[6]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00745</td><td>Geometric Margin</td><td>几何间隔</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00746</td><td>Giant Magnetoresistance</td><td>巨磁阻</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00747</td><td>Gibbs Distribution</td><td>吉布斯分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00748</td><td>Gibbs Sampling</td><td>吉布斯采样/吉布斯抽样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00749</td><td>Gibbs Steps</td><td>吉布斯步数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00750</td><td>Gini Index</td><td>基尼指数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00751</td><td>Global Contrast Normalization</td><td>全局对比度规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00752</td><td>Global Markov Property</td><td>全局马尔可夫性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00753</td><td>Global Minima</td><td>全局极小值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00754</td><td>Global Minimizer</td><td>全局极小解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00755</td><td>Global Minimum</td><td>全局最小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00756</td><td>Global Optimization</td><td>全局优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-03-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00757</td><td>Gradient</td><td>梯度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00758</td><td>Gradient Ascent</td><td>梯度上升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00759</td><td>Gradient Ascent Method</td><td>梯度上升法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00760</td><td>Gradient Boosting</td><td>梯度提升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00761</td><td>Gradient Boosting Tree</td><td>梯度提升树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00762</td><td>Gradient Clipping</td><td>梯度截断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00763</td><td>Gradient Descent</td><td>梯度下降</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00764</td><td>Gradient Descent In One-Dimensional Space</td><td>一维梯度下降</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00765</td><td>Gradient Descent Method</td><td>梯度下降法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00766</td><td>Gradient Energy Distribution</td><td>梯度能量分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00767</td><td>Gradient Estimation</td><td>梯度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00768</td><td>Gradient Exploding Problem</td><td>梯度爆炸问题</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-21-14">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00769</td><td>Gradient Field</td><td>梯度场</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00770</td><td>Gradual Warmup</td><td>逐渐预热</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00771</td><td>Gram Matrix</td><td>Gram 矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00772</td><td>Graph</td><td>图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00773</td><td>Graph Analytics</td><td>图分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00774</td><td>Graph Attention Network</td><td>图注意力网络</td><td>GAT</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00775</td><td>Graph Convolutional Network</td><td>图卷积神经网络/图卷积网络</td><td>GCN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00776</td><td>Graph Neural Network</td><td>图神经网络</td><td>GNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00777</td><td>Graph Theory</td><td>图论</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-04-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00778</td><td>Graphical Model</td><td>图模型</td><td>GM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00779</td><td>Graphics Processing Unit</td><td>图形处理器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00780</td><td>Greedy</td><td>贪心</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00781</td><td>Greedy Algorithm</td><td>贪心算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00782</td><td>Greedy Layer-Wise Pretraining</td><td>贪心逐层预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00783</td><td>Greedy Layer-Wise Training</td><td>贪心逐层训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00784</td><td>Greedy Layer-Wise Unsupervised Pretraining</td><td>贪心逐层无监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00785</td><td>Greedy Search</td><td>贪心搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00786</td><td>Greedy Supervised Pretraining</td><td>贪心监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00787</td><td>Greedy Unsupervised Pretraining</td><td>贪心无监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00788</td><td>Grid Search</td><td>网格搜索</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00789</td><td>Grid World</td><td>网格世界</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00790</td><td>Ground Truth</td><td>真实值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00791</td><td>Growth Function</td><td>增长函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00792</td><td>Hadamard Product</td><td>Hadamard积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00793</td><td>Hamming Distance</td><td>汉明距离</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00794</td><td>Hard Attention</td><td>硬性注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00795</td><td>Hard Clustering</td><td>硬聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00796</td><td>Hard Margin</td><td>硬间隔</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00797</td><td>Hard Margin Maximization</td><td>硬间隔最大化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00798</td><td>Hard Mixture Of Experts</td><td>硬专家混合体</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00799</td><td>Hard Tanh</td><td>硬双曲正切函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00800</td><td>Hard Target</td><td>硬目标</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00801</td><td>Hard Voting</td><td>硬投票</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00802</td><td>Harmonic Mean</td><td>调和平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00803</td><td>Harmonium</td><td>簧风琴</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00804</td><td>Harmony</td><td>Harmony</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00805</td><td>Harris Chain</td><td>哈里斯链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00806</td><td>Hausdorff Distance</td><td>豪斯多夫距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00807</td><td>Hebbian Rule</td><td>赫布法则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00808</td><td>Hebbian Theory</td><td>赫布理论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00809</td><td>Helmholtz Machine</td><td>Helmholtz机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00810</td><td>Hesse Matrix</td><td>海赛矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00811</td><td>Hessian</td><td>Hessian</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00812</td><td>Hessian Matrix</td><td>黑塞矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00813</td><td>Heterogeneous Information Network</td><td>异质信息网络</td><td>HIN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00814</td><td>Heteroscedastic</td><td>异方差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00815</td><td>Hidden Dynamic Model</td><td>隐动态模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00816</td><td>Hidden Layer</td><td>隐藏层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00817</td><td>Hidden Markov Model</td><td>隐马尔可夫模型</td><td>HMM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-21-8">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00818</td><td>Hidden State</td><td>隐状态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00819</td><td>Hidden Unit</td><td>隐藏单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00820</td><td>Hidden Variable</td><td>隐变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00821</td><td>Hierarchical Clustering</td><td>层次聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00822</td><td>Hierarchical Reinforcement Learning</td><td>分层强化学习</td><td>HRL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00823</td><td>Hierarchical Softmax</td><td>层序Softmax/层序软最大化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00824</td><td>Hilbert Space</td><td>希尔伯特空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00825</td><td>Hill Climbing</td><td>爬山</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00826</td><td>Hinge Loss Function</td><td>合页损失函数/Hinge损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00827</td><td>Histogram Method</td><td>直方图方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00828</td><td>Hold-Out</td><td>留出法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00829</td><td>Homogeneous</td><td>同质</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00830</td><td>Hopfield Network</td><td>Hopfield网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00831</td><td>Huffman Coding</td><td>霍夫曼编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00832</td><td>Hybrid Computing</td><td>混合计算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00833</td><td>Hyperbolic Tangent Function</td><td>双曲正切函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00834</td><td>Hyperparameter</td><td>超参数</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-08-18-5">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-28">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-00835</td><td>Hyperparameter Optimization</td><td>超参数优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00836</td><td>Hyperplane</td><td>超平面</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-00837</td><td>Hypothesis</td><td>假设</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00838</td><td>Hypothesis Space</td><td>假设空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00839</td><td>Hypothesis Test</td><td>假设检验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00840</td><td>I.I.D. Assumption</td><td>独立同分布假设</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00841</td><td>Identically Distributed</td><td>同分布的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00842</td><td>Identifiable</td><td>可辨认的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00843</td><td>Identity Function</td><td>恒等函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00844</td><td>Identity Mapping</td><td>恒等映射</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00845</td><td>Identity Matrix</td><td>单位矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00846</td><td>Ill Conditioning</td><td>病态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00847</td><td>Ill-Formed Problem</td><td>病态问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00848</td><td>Image</td><td>图像</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00849</td><td>Image Restoration</td><td>图像还原</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00850</td><td>Imitation Learning</td><td>模仿学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00851</td><td>Immorality</td><td>不道德</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00852</td><td>Imperfect Information</td><td>不完美信息</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-16-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00853</td><td>Implicit Density Model</td><td>隐式密度模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00854</td><td>Import</td><td>导入</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00855</td><td>Importance Sampling</td><td>重要性采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00856</td><td>Improved Iterative Scaling</td><td>改进的迭代尺度法</td><td>IIS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00857</td><td>Incomplete-Data</td><td>不完全数据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00858</td><td>Incremental Learning</td><td>增量学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00859</td><td>Indefinite Integral</td><td>不定积分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00860</td><td>Independence</td><td>独立</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00861</td><td>Independent</td><td>相互独立的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00862</td><td>Independent and Identically Distributed</td><td>独立同分布</td><td>I.I.D.</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00863</td><td>Independent Component Analysis</td><td>独立成分分析</td><td>ICA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00864</td><td>Independent Subspace Analysis</td><td>独立子空间分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00865</td><td>Index of Matrix</td><td>索引</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00866</td><td>Indicator Function</td><td>指示函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00867</td><td>Individual Learner</td><td>个体学习器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00868</td><td>Induction</td><td>归纳</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00869</td><td>Inductive Bias</td><td>归纳偏好</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00870</td><td>Inductive Learning</td><td>归纳学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00871</td><td>Inductive Logic Programming</td><td>归纳逻辑程序设计</td><td>ILP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00872</td><td>Inductive Transfer Learning</td><td>归纳迁移学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00873</td><td>Inequality Constraint</td><td>不等式约束</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00874</td><td>Inference</td><td>推断</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-14-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00875</td><td>Infinite</td><td>无限</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00876</td><td>Infinitely Exchangeable</td><td>无限可交换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00877</td><td>Information Divergence</td><td>信息散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00878</td><td>Information Entropy</td><td>信息熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00879</td><td>Information Gain</td><td>信息增益</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-00880</td><td>Information Gain Ratio</td><td>信息增益比</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-00881</td><td>Information Retrieval</td><td>信息检索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00882</td><td>Information Theory</td><td>信息论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00883</td><td>Inner Product</td><td>内积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00884</td><td>Input</td><td>输入</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00885</td><td>Input Distribution</td><td>输入分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00886</td><td>Input Gate</td><td>输入门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00887</td><td>Input Layer</td><td>输入层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00888</td><td>Input Space</td><td>输入空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00889</td><td>Insensitive Loss</td><td>不敏感损失</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00890</td><td>Instance</td><td>示例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00891</td><td>Instance Segmentation</td><td>实例分割</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00892</td><td>Integer Linear Programming</td><td>整数线性规划</td><td>ILP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00893</td><td>Integer Programming</td><td>整数规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00894</td><td>Integration</td><td>积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00895</td><td>Inter-Cluster Similarity</td><td>簇间相似度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00896</td><td>Internal Covariate Shift</td><td>内部协变量偏移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00897</td><td>Internal Node</td><td>内部结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00898</td><td>International Conference For Machine Learning</td><td>国际机器学习大会</td><td>ICML</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-31">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00899</td><td>Intervention Query</td><td>干预查询</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00900</td><td>Intra-Attention</td><td>内部注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00901</td><td>Intra-Cluster Similarity</td><td>簇内相似度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00902</td><td>Intrinsic Value</td><td>固有值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00903</td><td>Invariance</td><td>不变性</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-16-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00904</td><td>Invariant</td><td>不变</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00905</td><td>Inverse Matrix</td><td>逆矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00906</td><td>Inverse Reinforcement Learning</td><td>逆强化学习</td><td>IRL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00907</td><td>Inverse Resolution</td><td>逆归结</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00908</td><td>Inverse Time Decay</td><td>逆时衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00909</td><td>Invert</td><td>求逆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00910</td><td>Irreducible</td><td>不可约的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00911</td><td>Irrelevant Feature</td><td>无关特征</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00912</td><td>Isometric Mapping</td><td>等度量映射</td><td>Isomap</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00913</td><td>Isotonic Regression</td><td>等分回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00914</td><td>Isotropic</td><td>各向同性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00915</td><td>Isotropic Gaussian Distribution</td><td>各向同性高斯分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00916</td><td>Iteration</td><td>迭代</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td>数学、机器学习</td></tr><tr class="odd"><td>AITD-00917</td><td>Iterative Dichotomiser</td><td>迭代二分器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00918</td><td>Jacobian</td><td>雅克比</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00919</td><td>Jacobian Matrix</td><td>雅可比矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00920</td><td>Jensen Inequality</td><td>Jensen不等式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00921</td><td>Jensen-Shannon Divergence</td><td>JS散度</td><td>JSD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00922</td><td>Joint Probability Density Function</td><td>联合概率密度函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00923</td><td>Joint Probability Distribution</td><td>联合概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00924</td><td>Junction Tree Algorithm</td><td>联合树算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00925</td><td>K-Armed Bandit Problem</td><td>k-摇臂老虎机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00926</td><td>K-Fold Cross Validation</td><td>k 折交叉验证</td><td>K-FOLD CV</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-00927</td><td>K-Means Clustering</td><td>k-均值聚类</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-11-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00928</td><td>K-Nearest Neighbor Classifier</td><td>k-近邻分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00929</td><td>K-Nearest Neighbor Method</td><td>k-近邻</td><td>K-NN</td><td>[1]</td><td>统计</td></tr><tr class="even"><td>AITD-00930</td><td>Karush-Kuhn-Tucker Condition</td><td>KKT条件</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00931</td><td>Karush–Kuhn–Tucker</td><td>Karush–Kuhn–Tucker</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00932</td><td>Kd Tree</td><td>Kd 树</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00933</td><td>Kernel Density Estimation</td><td>核密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00934</td><td>Kernel Function</td><td>核函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00935</td><td>Kernel Machine</td><td>核机器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00936</td><td>Kernel Matrix</td><td>核矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00937</td><td>Kernel Method</td><td>核方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00938</td><td>Kernel Regression</td><td>核回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00939</td><td>Kernel Trick</td><td>核技巧</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00940</td><td>Kernelized</td><td>核化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00941</td><td>Kernelized Linear Discriminant Analysis</td><td>核线性判别分析</td><td>KLDA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00942</td><td>Kernelized PCA</td><td>核主成分分析</td><td>KPCA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00943</td><td>Key-Value Store</td><td>键-值数据库</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00944</td><td>KL Divergence</td><td>KL散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00945</td><td>Knowledge</td><td>知识</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00946</td><td>Knowledge Base</td><td>知识库</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-21-10">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00947</td><td>Knowledge Distillation</td><td>知识蒸馏</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00948</td><td>Knowledge Engineering</td><td>知识工程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00949</td><td>Knowledge Graph</td><td>知识图谱</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-11-03-5">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-03-24">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-09-26-8">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00950</td><td>Knowledge Representation</td><td>知识表征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00951</td><td>Kronecker Product</td><td>Kronecker积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00952</td><td>Krylov Method</td><td>Krylov方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00953</td><td>L-BFGS</td><td>L-BFGS</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00954</td><td>Label</td><td>标签/标记</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00955</td><td>Label Propagation</td><td>标记传播</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00956</td><td>Label Smoothing</td><td>标签平滑</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00957</td><td>Label Space</td><td>标记空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00958</td><td>Labeled</td><td>标注</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00959</td><td>Lagrange Dual Problem</td><td>拉格朗日对偶问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00960</td><td>Lagrange Duality</td><td>拉格朗日对偶性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00961</td><td>Lagrange Function</td><td>拉格朗日函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00962</td><td>Lagrange Multiplier</td><td>拉格朗日乘子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00963</td><td>Language Model</td><td>语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00964</td><td>Language Modeling</td><td>语言模型化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00965</td><td>Laplace Distribution</td><td>Laplace分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00966</td><td>Laplace Smoothing</td><td>拉普拉斯平滑</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00967</td><td>Laplacian Correction</td><td>拉普拉斯修正</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00968</td><td>Large Learning Step</td><td>大学习步骤</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00969</td><td>Las Vegas Method</td><td>拉斯维加斯方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00970</td><td>Latent</td><td>潜在</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00971</td><td>Latent Dirichlet Allocation</td><td>潜在狄利克雷分配</td><td>LDA</td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-01-7">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00972</td><td>Latent Layer</td><td>潜层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00973</td><td>Latent Semantic Analysis</td><td>潜在语义分析</td><td>LSA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00974</td><td>Latent Semantic Indexing</td><td>潜在语义索引</td><td>LSI</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00975</td><td>Latent Variable</td><td>潜变量/隐变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00976</td><td>Law of Large Numbers</td><td>大数定律</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00977</td><td>Layer</td><td>层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00978</td><td>Layer Normalization</td><td>层规范化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00979</td><td>Layer-Wise</td><td>逐层的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00980</td><td>Layer-Wise Adaptive Rate Scaling</td><td>逐层适应率缩放</td><td>LARS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00981</td><td>Layer-Wise Normalization</td><td>逐层规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00982</td><td>Layer-Wise Pretraining</td><td>逐层预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00983</td><td>Layer-Wise Training</td><td>逐层训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00984</td><td>Lazy Learning</td><td>懒惰学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00985</td><td>Leaf Node</td><td>叶结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00986</td><td>Leaky Lelu Function</td><td>泄漏线性整流函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00987</td><td>Leaky Relu</td><td>泄漏修正线性单元/泄漏整流线性单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00988</td><td>Leaky Unit</td><td>渗漏单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00989</td><td>Learned</td><td>学成</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00990</td><td>Learned Approximate Inference</td><td>学习近似推断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00991</td><td>Learner</td><td>学习器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00992</td><td>Learning</td><td>学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00993</td><td>Learning Algorithm</td><td>学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00994</td><td>Learning By Analogy</td><td>类比学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00995</td><td>Learning Rate</td><td>学习率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00996</td><td>Learning Rate Annealing</td><td>学习率退火</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00997</td><td>Learning Rate Decay</td><td>学习率衰减</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00998</td><td>Learning Rate Warmup</td><td>学习率预热</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00999</td><td>Learning To Learn</td><td>学习的学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01000</td><td>Learning Vector Quantization</td><td>学习向量量化</td><td>LVQ</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01001</td><td>Least General Generalization</td><td>最小一般泛化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01002</td><td>Least Mean Squares</td><td>最小均方</td><td>LMS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01003</td><td>Least Square Method</td><td>最小二乘法</td><td>LSM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-24-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01004</td><td>Least Squares Regression Tree</td><td>最小二乘回归树</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01005</td><td>Leave-One-Out Cross Validation</td><td>留一交叉验证</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01006</td><td>Leave-One-Out</td><td>留一法</td><td>LOO</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01007</td><td>Lebesgue-Integrable</td><td>勒贝格可积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01008</td><td>Left Eigenvector</td><td>左特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01009</td><td>Left Singular Vector</td><td>左奇异向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01010</td><td>Leibniz's Rule</td><td>莱布尼兹法则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01011</td><td>Lifelong Learning</td><td>终身学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01012</td><td>Likelihood</td><td>似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01013</td><td>Line Search</td><td>线搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01014</td><td>Linear Auto-Regressive Network</td><td>线性自回归网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01015</td><td>Linear Chain</td><td>线性链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01016</td><td>Linear Chain Conditional Random Field</td><td>线性链条件随机场</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01017</td><td>Linear Classification Model</td><td>线性分类模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01018</td><td>Linear Classifier</td><td>线性分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01019</td><td>Linear Combination</td><td>线性组合</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="even"><td>AITD-01020</td><td>Linear Dependence</td><td>线性相关</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01021</td><td>Linear Discriminant Analysis</td><td>线性判别分析</td><td>LDA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01022</td><td>Linear Factor Model</td><td>线性因子模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01023</td><td>Linear Mapping</td><td>线性映射</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01024</td><td>Linear Model</td><td>线性模型</td><td>LR</td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01025</td><td>Linear Programming</td><td>线性规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01026</td><td>Linear Regression</td><td>线性回归</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-01">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-17-5">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a></td><td>统计、数学</td></tr><tr class="odd"><td>AITD-01027</td><td>Linear Scaling Rule</td><td>线性缩放规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01028</td><td>Linear Scan</td><td>线性扫描</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01029</td><td>Linear Space</td><td>线性空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01030</td><td>Linear Support Vector Machine</td><td>线性支持向量机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01031</td><td>Linear Support Vector Machine In Linearly Separable Case</td><td>线性可分支持向量机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01032</td><td>Linear Threshold Units</td><td>线性阈值单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01033</td><td>Linear Transformation</td><td>线性变换</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01034</td><td>Linearly Independent</td><td>线性无关</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01035</td><td>Linearly Separable</td><td>线性可分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01036</td><td>Linearly Separable Data Set</td><td>线性可分数据集</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01037</td><td>Link Analysis</td><td>链接分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01038</td><td>Link Function</td><td>联系函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01039</td><td>Link Prediction</td><td>链接预测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01040</td><td>Link Table</td><td>连接表</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01041</td><td>Linkage</td><td>连接</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01042</td><td>Linked Importance Sampling</td><td>链接重要采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01043</td><td>Lipschitz</td><td>Lipschitz</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01044</td><td>Lipschitz Constant</td><td>Lipschitz常数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01045</td><td>Lipschitz Continuous</td><td>Lipschitz连续</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01046</td><td>Liquid State Machine</td><td>流体状态机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01047</td><td>Local Conditional Probability Distribution</td><td>局部条件概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01048</td><td>Local Constancy Prior</td><td>局部不变性先验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01049</td><td>Local Contrast Normalization</td><td>局部对比度规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01050</td><td>Local Curvature</td><td>局部曲率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01051</td><td>Local Descent</td><td>局部下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01052</td><td>Local Invariances</td><td>局部不变性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01053</td><td>Local Kernel</td><td>局部核</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01054</td><td>Local Markov Property</td><td>局部马尔可夫性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01055</td><td>Local Maxima</td><td>局部极大值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01056</td><td>Local Maximum</td><td>局部极大点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01057</td><td>Local Minima</td><td>局部极小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01058</td><td>Local Minimizer</td><td>局部最小解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01059</td><td>Local Minimum</td><td>局部极小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01060</td><td>Local Representation</td><td>局部式表示/局部式表征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01061</td><td>Local Response Normalization</td><td>局部响应规范化</td><td>LRN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01062</td><td>Locally Linear Embedding</td><td>局部线性嵌入</td><td>LLE</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01063</td><td>Log Likelihood</td><td>对数似然函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01064</td><td>Log Linear Model</td><td>对数线性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01065</td><td>Log-Likelihood</td><td>对数似然</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01066</td><td>Log-Likelihood Loss Function</td><td>对数似然损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01067</td><td>Log-Linear Regression</td><td>对数线性回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01068</td><td>Logarithmic Loss Function</td><td>对数损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01069</td><td>Logarithmic Scale</td><td>对数尺度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01070</td><td>Logistic Distribution</td><td>对数几率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01071</td><td>Logistic Function</td><td>对数几率函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01072</td><td>Logistic Loss</td><td>对率损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01073</td><td>Logistic Regression</td><td>对数几率回归(逻辑回归)</td><td>LR</td><td><a href="https://www.jiqizhixin.com/articles/2017-11-23-6">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[2]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01074</td><td>Logistic Sigmoid</td><td>对数几率Sigmoid</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01075</td><td>Logit</td><td>对数几率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01076</td><td>Long Short Term Memory</td><td>长短期记忆</td><td>LSTM</td><td><a href="https://www.jiqizhixin.com/articles/2017-12-18-6">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-10-04-2">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-09-29-7">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[4]</a></td><td></td></tr><tr class="odd"><td>AITD-01077</td><td>Long Short-Term Memory Network</td><td>长短期记忆网络</td><td>LSTM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01078</td><td>Long-Term Dependencies Problem</td><td>长程依赖问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01079</td><td>Long-Term Dependency</td><td>长期依赖</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01080</td><td>Long-Term Memory</td><td>长期记忆</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01081</td><td>Loop</td><td>环</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01082</td><td>Loopy Belief Propagation</td><td>环状信念传播</td><td>LBP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01083</td><td>Loss</td><td>损失</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01084</td><td>Loss Function</td><td>损失函数</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-03-4">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01085</td><td>Low Rank Matrix Approximation</td><td>低秩矩阵近似</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01086</td><td>Lp Distance</td><td>Lp距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01087</td><td>Machine Learning Model</td><td>机器学习模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01088</td><td>Machine Learning</td><td>机器学习</td><td>ML</td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01089</td><td>Machine Translation</td><td>机器翻译</td><td>MT</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-13-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01090</td><td>Macro Average</td><td>宏平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01091</td><td>Macro-F1</td><td>宏F1</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01092</td><td>Macro-P</td><td>宏查准率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01093</td><td>Macron-R</td><td>宏查全率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01094</td><td>Mahalanobis Distance</td><td>马哈拉诺比斯距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01095</td><td>Main Diagonal</td><td>主对角线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01096</td><td>Majority Voting</td><td>绝对多数投票</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01097</td><td>Majority Voting Rule</td><td>多数表决规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01098</td><td>Manhattan Distance</td><td>曼哈顿距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01099</td><td>Manifold</td><td>流形</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01100</td><td>Manifold Assumption</td><td>流形假设</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01101</td><td>Manifold Learning</td><td>流形学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01102</td><td>Manifold Tangent Classifier</td><td>流形正切分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01103</td><td>Margin</td><td>间隔</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01104</td><td>Margin Theory</td><td>间隔理论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01105</td><td>Marginal Distribution</td><td>边缘分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01106</td><td>Marginal Independence</td><td>边缘独立性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01107</td><td>Marginal Likelihood</td><td>边缘似然函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01108</td><td>Marginal Probability Distribution</td><td>边缘概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01109</td><td>Marginalization</td><td>边缘化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01110</td><td>Markov Blanket</td><td>马尔可夫毯</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01111</td><td>Markov Chain</td><td>马尔可夫链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01112</td><td>Markov Chain Monte Carlo</td><td>马尔可夫链蒙特卡罗</td><td>MCMC</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-24-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01113</td><td>Markov Decision Process</td><td>马尔可夫决策过程</td><td>MDP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01114</td><td>Markov Network</td><td>马尔可夫网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01115</td><td>Markov Process</td><td>马尔可夫过程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01116</td><td>Markov Property</td><td>马尔可夫性质</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01117</td><td>Markov Random Field</td><td>马尔可夫随机场</td><td>MRF</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01118</td><td>Mask</td><td>掩码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01119</td><td>Mask Language Modeling</td><td>掩码语言模型化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01120</td><td>Masked Self-Attention</td><td>掩蔽自注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01121</td><td>Mathematical Optimization</td><td>数学优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01122</td><td>Matrix</td><td>矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01123</td><td>Matrix Calculus</td><td>矩阵微积分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01124</td><td>Matrix Completion</td><td>矩阵补全</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01125</td><td>Matrix Decomposition</td><td>矩阵分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01126</td><td>Matrix Inversion</td><td>逆矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01127</td><td>Matrix Product</td><td>矩阵乘积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01128</td><td>Max Norm</td><td>最大范数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01129</td><td>Max Pooling</td><td>最大汇聚</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-02-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01130</td><td>Maxima</td><td>极大值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01131</td><td>Maximal Clique</td><td>最大团</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01132</td><td>Maximization</td><td>极大</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01133</td><td>Maximization Step</td><td>M步</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01134</td><td>Maximization-Maximization Algorithm</td><td>极大-极大算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01135</td><td>Maximum A Posteriori</td><td>最大后验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01136</td><td>Maximum A Posteriori Estimation</td><td>最大后验估计</td><td>MAP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01137</td><td>Maximum Entropy Model</td><td>最大熵模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01138</td><td>Maximum Likelihood</td><td>极大似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01139</td><td>Maximum Likelihood Estimation</td><td>极大似然估计</td><td>MLE</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-09-6">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01140</td><td>Maximum Likelihood Method</td><td>极大似然法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01141</td><td>Maximum Margin</td><td>最大间隔</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01142</td><td>Maximum Mean Discrepancy</td><td>最大平均偏差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01143</td><td>Maximum Posterior Probability Estimation</td><td>最大后验概率估计</td><td>MAP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01144</td><td>Maximum Weighted Spanning Tree</td><td>最大带权生成树</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01145</td><td>Maxout</td><td>Maxout</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01146</td><td>Maxout Unit</td><td>Maxout单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01147</td><td>Mean</td><td>均值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01148</td><td>Mean Absolute Error</td><td>平均绝对误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01149</td><td>Mean And Covariance RBM</td><td>均值和协方差RBM</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01150</td><td>Mean Filed</td><td>平均场</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01151</td><td>Mean Filter</td><td>均值滤波</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01152</td><td>Mean Pooling</td><td>平均汇聚</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01153</td><td>Mean Product of Student t-Distribution</td><td>学生 t 分布均值乘积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01154</td><td>Mean Squared Error</td><td>均方误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01155</td><td>Mean-Covariance Restricted Boltzmann Machine</td><td>均值-协方差受限玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01156</td><td>Mean-Field</td><td>平均场</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01157</td><td>Meanfield</td><td>均匀场</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01158</td><td>Measure Theory</td><td>测度论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01159</td><td>Measure Zero</td><td>零测度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01160</td><td>Median</td><td>中位数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01161</td><td>Memory</td><td>记忆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01162</td><td>Memory Augmented Neural Network</td><td>记忆增强神经网络</td><td>MANN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01163</td><td>Memory Capacity</td><td>记忆容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01164</td><td>Memory Cell</td><td>记忆元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01165</td><td>Memory Network</td><td>记忆网络</td><td>MN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01166</td><td>Memory Segment</td><td>记忆片段</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01167</td><td>Mercer Kernel</td><td>Mercer 核</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01168</td><td>Message</td><td>消息</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01169</td><td>Message Passing</td><td>消息传递</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01170</td><td>Message Passing Neural Network</td><td>消息传递神经网络</td><td>MPNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01171</td><td>Meta-Learner</td><td>元学习器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01172</td><td>Meta-Learning</td><td>元学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01173</td><td>Meta-Optimization</td><td>元优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01174</td><td>Meta-Rule</td><td>元规则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01175</td><td>Metric</td><td>指标</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-01176</td><td>Metric Learning</td><td>度量学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01177</td><td>Micro Average</td><td>微平均</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01178</td><td>Micro-F1</td><td>微F1</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01179</td><td>Micro-P</td><td>微査准率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01180</td><td>Micro-R</td><td>微查全率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01181</td><td>Min-Max Normalization</td><td>最小最大值规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01182</td><td>Mini-Batch Gradient</td><td>小批量梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01183</td><td>Mini-Batch Gradient Descent</td><td>小批量梯度下降法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01184</td><td>Mini-Batch SGD</td><td>小批次随机梯度下降</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01185</td><td>Minibatch</td><td>小批量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01186</td><td>Minibatch Stochastic</td><td>小批量随机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01187</td><td>Minima</td><td>极小值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01188</td><td>Minimal Description Length</td><td>最小描述长度</td><td>MDL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01189</td><td>Minimax Game</td><td>极小极大博弈</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01190</td><td>Minimum</td><td>极小点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01191</td><td>Minkowski Distance</td><td>闵可夫斯基距离</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01192</td><td>Misclassification Cost</td><td>误分类代价</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01193</td><td>Mixing</td><td>混合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01194</td><td>Mixing Time</td><td>混合时间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01195</td><td>Mixture Density Network</td><td>混合密度网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01196</td><td>Mixture Distribution</td><td>混合分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01197</td><td>Mixture of Experts</td><td>混合专家模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01198</td><td>Mixture-of-Gaussian</td><td>高斯混合</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01199</td><td>Modality</td><td>模态</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01200</td><td>Mode</td><td>峰值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01201</td><td>Model</td><td>模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01202</td><td>Model Averaging</td><td>模型平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01203</td><td>Model Collapse</td><td>模型坍塌</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01204</td><td>Model Complexity</td><td>模型复杂度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01205</td><td>Model Compression</td><td>模型压缩</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01206</td><td>Model Identifiability</td><td>模型可辨识性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01207</td><td>Model Parallelism</td><td>模型并行</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01208</td><td>Model Parameter</td><td>模型参数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01209</td><td>Model Predictive Control</td><td>模型预测控制</td><td>MPC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01210</td><td>Model Selection</td><td>模型选择</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01211</td><td>Model-Agnostic Meta-Learning</td><td>模型无关的元学习</td><td>MAML</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01212</td><td>Model-Based Learning</td><td>有模型学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01213</td><td>Model-Based Reinforcement Learning</td><td>基于模型的强化学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01214</td><td>Model-Free Learning</td><td>免模型学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01215</td><td>Model-Free Reinforcement Learning</td><td>模型无关的强化学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01216</td><td>Moment</td><td>矩</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01217</td><td>Moment Matching</td><td>矩匹配</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01218</td><td>Momentum</td><td>动量</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-07-01-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01219</td><td>Momentum Method</td><td>动量法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01220</td><td>Monte Carlo</td><td>蒙特卡罗</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01221</td><td>Monte Carlo Estimate</td><td>蒙特卡罗估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01222</td><td>Monte Carlo Integration</td><td>蒙特卡罗积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01223</td><td>Monte Carlo Method</td><td>蒙特卡罗方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01224</td><td>Moore's Law</td><td>摩尔定律</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01225</td><td>Moore-Penrose Pseudoinverse</td><td>Moore-Penrose 伪逆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01226</td><td>Moral Graph</td><td>端正图/道德图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01227</td><td>Moralization</td><td>道德化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01228</td><td>Most General Unifier</td><td>最一般合一置换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01229</td><td>Moving Average</td><td>移动平均</td><td>MA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01230</td><td>Multi-Armed Bandit Problem</td><td>多臂赌博机问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01231</td><td>Multi-Class Classification</td><td>多分类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01232</td><td>Multi-Classifier System</td><td>多分类器系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01233</td><td>Multi-Document Summarization</td><td>多文档摘要</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01234</td><td>Multi-Head Attention</td><td>多头注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01235</td><td>Multi-Head Self-Attention</td><td>多头自注意力</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01236</td><td>Multi-Hop</td><td>多跳</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01237</td><td>Multi-Kernel Learning</td><td>多核学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01238</td><td>Multi-Label Classification</td><td>多标签分类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01239</td><td>Multi-Label Learning</td><td>多标记学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01240</td><td>Multi-Layer Feedforward Neural Networks</td><td>多层前馈神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01241</td><td>Multi-Layer Perceptron</td><td>多层感知机</td><td>MLP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-01242</td><td>Multi-Nominal Logistic Regression Model</td><td>多项对数几率回归模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01243</td><td>Multi-Prediction Deep Boltzmann Machine</td><td>多预测深度玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01244</td><td>Multi-Response Linear Regression</td><td>多响应线性回归</td><td>MLR</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01245</td><td>Multi-View Learning</td><td>多视图学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01246</td><td>Multicollinearity</td><td>多重共线性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01247</td><td>Multimodal</td><td>多峰值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01248</td><td>Multimodal Learning</td><td>多模态学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01249</td><td>Multinomial Distribution</td><td>多项分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01250</td><td>Multinoulli Distribution</td><td>Multinoulli分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01251</td><td>Multinoulli Output Distribution</td><td>Multinoulli输出分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01252</td><td>Multiple Dimensional Scaling</td><td>多维缩放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01253</td><td>Multiple Linear Regression</td><td>多元线性回归</td><td>MLR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[3]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01254</td><td>Multitask Learning</td><td>多任务学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01255</td><td>Multivariate Decision Tree</td><td>多变量决策树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01256</td><td>Multivariate Gaussian Distribution</td><td>多元高斯分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01257</td><td>Multivariate Normal Distribution</td><td>多元正态分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01258</td><td>Mutual Information</td><td>互信息</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01259</td><td>N-Gram</td><td>N元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01260</td><td>N-Gram Feature</td><td>N元特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01261</td><td>N-Gram Model</td><td>N元模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01262</td><td>Naive Bayes Algorithm</td><td>朴素贝叶斯算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01263</td><td>Naive Bayes Classifier</td><td>朴素贝叶斯分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01264</td><td>Naive Bayes</td><td>朴素贝叶斯</td><td>NB</td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-20-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01265</td><td>Named Entity Recognition</td><td>命名实体识别</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01266</td><td>Narrow Convolution</td><td>窄卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01267</td><td>Nash Equilibrium</td><td>纳什均衡</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01268</td><td>Nash Reversion</td><td>纳什回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01269</td><td>Nats</td><td>奈特</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01270</td><td>Natural Exponential Decay</td><td>自然指数衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01271</td><td>Natural Language Generation</td><td>自然语言生成</td><td>NLG</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01272</td><td>Natural Language Processing</td><td>自然语言处理</td><td>NLP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[4]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-14-5">[5]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-14-4">[6]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-12-3">[7]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01273</td><td>Nearest Neighbor</td><td>最近邻</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01274</td><td>Nearest Neighbor Classifier</td><td>最近邻分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01275</td><td>Nearest Neighbor Graph</td><td>最近邻图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01276</td><td>Nearest Neighbor Regression</td><td>最近邻回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01277</td><td>Nearest-Neighbor Search</td><td>最近邻搜索</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-24-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01278</td><td>Negative Class</td><td>负类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01279</td><td>Negative Correlation</td><td>负相关法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01280</td><td>Negative Definite</td><td>负定</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01281</td><td>Negative Log Likelihood</td><td>负对数似然函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01282</td><td>Negative Part Function</td><td>负部函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01283</td><td>Negative Phase</td><td>负相</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01284</td><td>Negative Sample</td><td>负例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01285</td><td>Negative Sampling</td><td>负采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01286</td><td>Negative Semidefinite</td><td>半负定</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01287</td><td>Neighbourhood Component Analysis</td><td>近邻成分分析</td><td>NCA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01288</td><td>Nesterov Accelerated Gradient</td><td>Nesterov加速梯度</td><td>NAG</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01289</td><td>Nesterov Momentum</td><td>Nesterov动量法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01290</td><td>Net Activation</td><td>净活性值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01291</td><td>Net Input</td><td>净输入</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01292</td><td>Network</td><td>网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01293</td><td>Network Capacity</td><td>网络容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01294</td><td>Neural Architecture Search</td><td>神经架构搜索</td><td>NAS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01295</td><td>Neural Auto-Regressive Density Estimator</td><td>神经自回归密度估计器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01296</td><td>Neural Auto-Regressive Network</td><td>神经自回归网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01297</td><td>Neural Language Model</td><td>神经语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01298</td><td>Neural Machine Translation</td><td>神经机器翻译</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-22-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01299</td><td>Neural Model</td><td>神经模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01300</td><td>Neural Network</td><td>神经网络</td><td>NN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01301</td><td>Neural Turing Machine</td><td>神经图灵机</td><td>NTM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-11-7">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01302</td><td>Neurodynamics</td><td>神经动力学</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01303</td><td>Neuromorphic Computing</td><td>神经形态计算</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-09-26-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-26-2">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-16-6">[3]</a></td><td></td></tr><tr class="even"><td>AITD-01304</td><td>Neuron</td><td>神经元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01305</td><td>Newton Method</td><td>牛顿法</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-03-11-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01306</td><td>No Free Lunch Theorem</td><td>没有免费午餐定理</td><td>NFL</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-03-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01307</td><td>Node</td><td>结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01308</td><td>Noise</td><td>噪声</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01309</td><td>Noise Distribution</td><td>噪声分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01310</td><td>Noise-Contrastive Estimation</td><td>噪声对比估计</td><td>NCE</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01311</td><td>Nominal Attribute</td><td>列名属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01312</td><td>Non-Autoregressive Process</td><td>非自回归过程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01313</td><td>Non-Convex Optimization</td><td>非凸优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-29-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01314</td><td>Non-Informative Prior</td><td>无信息先验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01315</td><td>Non-Linear Model</td><td>非线性模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01316</td><td>Non-Linear Oscillation</td><td>非线性振荡</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01317</td><td>Non-Linear Support Vector Machine</td><td>非线性支持向量机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01318</td><td>Non-Metric Distance</td><td>非度量距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01319</td><td>Non-Negative Matrix Factorization</td><td>非负矩阵分解</td><td>NMF</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01320</td><td>Non-Ordinal Attribute</td><td>无序属性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01321</td><td>Non-Parametric</td><td>非参数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01322</td><td>Non-Parametric Model</td><td>非参数化模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01323</td><td>Non-Probabilistic Model</td><td>非概率模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01324</td><td>Non-Saturating Game</td><td>非饱和博弈</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01325</td><td>Non-Separable</td><td>不可分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01326</td><td>Nonconvex</td><td>非凸</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01327</td><td>Nondistributed</td><td>非分布式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01328</td><td>Nondistributed Representation</td><td>非分布式表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01329</td><td>Nonlinear Autoregressive With Exogenous Inputs Model</td><td>有外部输入的非线性自回归模型</td><td>NARX</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01330</td><td>Nonlinear Conjugate Gradients</td><td>非线性共轭梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01331</td><td>Nonlinear Independent Components Estimation</td><td>非线性独立成分估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01332</td><td>Nonlinear Programming</td><td>非线性规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01333</td><td>Nonparametric Density Estimation</td><td>非参数密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01334</td><td>Norm</td><td>范数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01335</td><td>Norm-Preserving</td><td>范数保持性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01336</td><td>Normal Distribution</td><td>正态分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01337</td><td>Normal Equation</td><td>正规方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01338</td><td>Normalization</td><td>规范化</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01339</td><td>Normalization Factor</td><td>规范化因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01340</td><td>Normalized</td><td>规范化的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01341</td><td>Normalized Initialization</td><td>标准初始化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01342</td><td>Nuclear Norm</td><td>核范数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01343</td><td>Null Space</td><td>零空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01344</td><td>Number of Epochs</td><td>轮数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01345</td><td>Numerator Layout</td><td>分子布局</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01346</td><td>Numeric Value</td><td>数值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01347</td><td>Numerical Attribute</td><td>数值属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01348</td><td>Numerical Differentiation</td><td>数值微分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01349</td><td>Numerical Method</td><td>数值方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01350</td><td>Numerical Optimization</td><td>数值优化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01351</td><td>Object Detection</td><td>目标检测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01352</td><td>Object Recognition</td><td>对象识别</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01353</td><td>Objective</td><td>目标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01354</td><td>Objective Function</td><td>目标函数</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-11-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01355</td><td>Oblique Decision Tree</td><td>斜决策树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01356</td><td>Observable Variable</td><td>观测变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01357</td><td>Observation Sequence</td><td>观测序列</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01358</td><td>Occam's Razor</td><td>奥卡姆剃刀</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01359</td><td>Odds</td><td>几率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01360</td><td>Off-Policy</td><td>异策略</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01361</td><td>Offline Inference</td><td>离线推断</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-06-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01362</td><td>Offset</td><td>偏移量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01363</td><td>Offset Vector</td><td>偏移向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01364</td><td>On-Policy</td><td>同策略</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01365</td><td>One-Shot Learning</td><td>单试学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-03-13-2">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-01366</td><td>One-Dependent Estimator</td><td>独依赖估计</td><td>ODE</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01367</td><td>One-Hot</td><td>独热</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01368</td><td>Online</td><td>在线</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01369</td><td>Online Inference</td><td>在线推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01370</td><td>Online Learning</td><td>在线学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01371</td><td>Operation</td><td>操作</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01372</td><td>Operator</td><td>运算符</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01373</td><td>Optimal Capacity</td><td>最佳容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01374</td><td>Optimization</td><td>最优化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01375</td><td>Optimization Landscape</td><td>优化地形</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01376</td><td>Optimizer</td><td>优化器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01377</td><td>Ordered Rule</td><td>带序规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01378</td><td>Ordinal Attribute</td><td>有序属性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01379</td><td>Origin</td><td>原点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01380</td><td>Orthogonal</td><td>正交</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-01381</td><td>Orthogonal Initialization</td><td>正交初始化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01382</td><td>Orthogonal Matrix</td><td>正交矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01383</td><td>Orthonormal</td><td>标准正交</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01384</td><td>Out-Of-Bag Estimate</td><td>包外估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01385</td><td>Outer Product</td><td>外积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01386</td><td>Outlier</td><td>异常点</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-01387</td><td>Output</td><td>输出</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01388</td><td>Output Gate</td><td>输出门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01389</td><td>Output Layer</td><td>输出层</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01390</td><td>Output Smearing</td><td>输出调制法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01391</td><td>Output Space</td><td>输出空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01392</td><td>Over-Parameterized</td><td>过度参数化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01393</td><td>Overcomplete</td><td>过完备</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01394</td><td>Overestimation</td><td>过估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01395</td><td>Overfitting</td><td>过拟合</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01396</td><td>Overfitting Regime</td><td>过拟合机制</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01397</td><td>Overflow</td><td>上溢</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01398</td><td>Oversampling</td><td>过采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01399</td><td>PAC Learning</td><td>PAC学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01400</td><td>Pac-Learnable</td><td>PAC可学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01401</td><td>Padding</td><td>填充</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01402</td><td>Paired t -Test</td><td>成对 t 检验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01403</td><td>Pairwise</td><td>成对型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01404</td><td>Pairwise Markov Property</td><td>成对马尔可夫性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01405</td><td>Parallel Distributed Processing</td><td>分布式并行处理</td><td>PDP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01406</td><td>Parallel Tempering</td><td>并行回火</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01407</td><td>Parameter</td><td>参数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01408</td><td>Parameter Estimation</td><td>参数估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01409</td><td>Parameter Server</td><td>参数服务器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01410</td><td>Parameter Sharing</td><td>参数共享</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01411</td><td>Parameter Space</td><td>参数空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01412</td><td>Parameter Tuning</td><td>调参</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-03-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01413</td><td>Parametric Case</td><td>有参情况</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01414</td><td>Parametric Density Estimation</td><td>参数密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01415</td><td>Parametric Model</td><td>参数化模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01416</td><td>Parametric ReLU</td><td>参数化修正线性单元/参数化整流线性单元</td><td>PReLU</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01417</td><td>Parse Tree</td><td>解析树</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01418</td><td>Part-Of-Speech Tagging</td><td>词性标注</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01419</td><td>Partial Derivative</td><td>偏导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01420</td><td>Partially Observable Markov Decision Processes</td><td>部分可观测马尔可夫决策过程</td><td>POMDP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01421</td><td>Particle Swarm Optimization</td><td>粒子群优化算法</td><td>PSO</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01422</td><td>Partition</td><td>划分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01423</td><td>Partition Function</td><td>配分函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01424</td><td>Path</td><td>路径</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01425</td><td>Pattern</td><td>模式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01426</td><td>Pattern Recognition</td><td>模式识别</td><td>PR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-01427</td><td>Penalty Term</td><td>罚项</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01428</td><td>Perceptron</td><td>感知机</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-15-2">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01429</td><td>Performance Measure</td><td>性能度量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01430</td><td>Periodic</td><td>周期的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01431</td><td>Permutation Invariant</td><td>置换不变性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01432</td><td>Perplexity</td><td>困惑度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01433</td><td>Persistent Contrastive Divergence</td><td>持续性对比散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01434</td><td>Phoneme</td><td>音素</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01435</td><td>Phonetic</td><td>语音</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01436</td><td>Pictorial Structure</td><td>图形结构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01437</td><td>Piecewise</td><td>分段</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01438</td><td>Piecewise Constant Decay</td><td>分段常数衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01439</td><td>Pipeline</td><td>流水线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01440</td><td>Plate Notation</td><td>板块表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01441</td><td>Plug And Play Generative Network</td><td>即插即用生成网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01442</td><td>Plurality Voting</td><td>相对多数投票</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01443</td><td>Point Estimator</td><td>点估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01444</td><td>Pointer Network</td><td>指针网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01445</td><td>Polarity Detection</td><td>极性检测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01446</td><td>Policy</td><td>策略</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01447</td><td>Policy Evaluation</td><td>策略评估</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01448</td><td>Policy Gradient</td><td>策略梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01449</td><td>Policy Improvement</td><td>策略改进</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01450</td><td>Policy Iteration</td><td>策略迭代</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01451</td><td>Policy Search</td><td>策略搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01452</td><td>Polynomial Basis Function</td><td>多项式基函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01453</td><td>Polynomial Kernel Function</td><td>多项式核函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01454</td><td>Polysemy</td><td>一词多义性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01455</td><td>Pool</td><td>汇聚</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01456</td><td>Pooling</td><td>汇聚</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-02-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01457</td><td>Pooling Function</td><td>汇聚函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01458</td><td>Pooling Layer</td><td>汇聚层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01459</td><td>Poor Conditioning</td><td>病态条件</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01460</td><td>Position Embedding</td><td>位置嵌入</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01461</td><td>Positional Encoding</td><td>位置编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01462</td><td>Positive Class</td><td>正类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01463</td><td>Positive Definite</td><td>正定</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01464</td><td>Positive Definite Kernel Function</td><td>正定核函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01465</td><td>Positive Definite Matrix</td><td>正定矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01466</td><td>Positive Part Function</td><td>正部函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01467</td><td>Positive Phase</td><td>正相</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01468</td><td>Positive Recurrent</td><td>正常返的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01469</td><td>Positive Sample</td><td>正例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01470</td><td>Positive Semidefinite</td><td>半正定</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01471</td><td>Positive-Semidefinite Matrix</td><td>半正定矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01472</td><td>Post-Hoc Test</td><td>后续检验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01473</td><td>Post-Pruning</td><td>后剪枝</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01474</td><td>Posterior Distribution</td><td>后验分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01475</td><td>Posterior Inference</td><td>后验推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01476</td><td>Posterior Probability</td><td>后验概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01477</td><td>Potential Function</td><td>势函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01478</td><td>Power Method</td><td>幂法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01479</td><td>PR Curve</td><td>P-R曲线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01480</td><td>Pre-Trained Initialization</td><td>预训练初始化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01481</td><td>Pre-Training</td><td>预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01482</td><td>Precision</td><td>查准率/准确率</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>数学、HPC</td></tr><tr class="odd"><td>AITD-01483</td><td>Precision Matrix</td><td>精度矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01484</td><td>Predictive Sparse Decomposition</td><td>预测稀疏分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01485</td><td>Prepruning</td><td>预剪枝</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01486</td><td>Pretrained Language Model</td><td>预训练语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01487</td><td>Primal Problem</td><td>主问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01488</td><td>Primary Visual Cortex</td><td>初级视觉皮层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01489</td><td>Principal Component Analysis</td><td>主成分分析</td><td>PCA</td><td><a href="https://www.jiqizhixin.com/articles/2017-12-03-4">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[4]</a></td><td></td></tr><tr class="even"><td>AITD-01490</td><td>Principle Of Multiple Explanations</td><td>多释原则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01491</td><td>Prior</td><td>先验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01492</td><td>Prior Knowledge</td><td>先验知识</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-01493</td><td>Prior Probability</td><td>先验概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01494</td><td>Prior Probability Distribution</td><td>先验概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01495</td><td>Prior Pseudo-Counts</td><td>伪计数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01496</td><td>Prior Shift</td><td>先验偏移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01497</td><td>Priority Rule</td><td>优先级规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01498</td><td>Probabilistic Context-Free Grammar</td><td>概率上下文无关文法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01499</td><td>Probabilistic Density Estimation</td><td>概率密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01500</td><td>Probabilistic Generative Model</td><td>概率生成模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01501</td><td>Probabilistic Graphical Model</td><td>概率图模型</td><td>PGM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-29-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01502</td><td>Probabilistic Latent Semantic Analysis</td><td>概率潜在语义分析</td><td>PLSA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01503</td><td>Probabilistic Latent Semantic Indexing</td><td>概率潜在语义索引</td><td>PLSI</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01504</td><td>Probabilistic Model</td><td>概率模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01505</td><td>Probabilistic PCA</td><td>概率PCA</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01506</td><td>Probabilistic Undirected Graphical Model</td><td>概率无向图模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01507</td><td>Probability</td><td>概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01508</td><td>Probability Density Function</td><td>概率密度函数</td><td>PDF</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01509</td><td>Probability Distribution</td><td>概率分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01510</td><td>Probability Mass Function</td><td>概率质量函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01511</td><td>Probability Model Estimation</td><td>概率模型估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01512</td><td>Probably Approximately Correct</td><td>概率近似正确</td><td>PAC</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01513</td><td>Product of Expert</td><td>专家之积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01514</td><td>Product Rule</td><td>乘法法则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01515</td><td>Properly PAC Learnable</td><td>恰PAC可学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01516</td><td>Proportional</td><td>成比例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01517</td><td>Proposal Distribution</td><td>提议分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01518</td><td>Propositional Atom</td><td>原子命题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01519</td><td>Propositional Rule</td><td>命题规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01520</td><td>Prototype-Based Clustering</td><td>原型聚类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01521</td><td>Proximal Gradient Descent</td><td>近端梯度下降</td><td>PGD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01522</td><td>Pruning</td><td>剪枝</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-26">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01523</td><td>Pseudo-Label</td><td>伪标记</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01524</td><td>Pseudolikelihood</td><td>伪似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01525</td><td>Q Function</td><td>Q函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01526</td><td>Q-Learning</td><td>Q学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01527</td><td>Q-Network</td><td>Q网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01528</td><td>Quadratic Loss Function</td><td>平方损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01529</td><td>Quadratic Programming</td><td>二次规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01530</td><td>Quadrature Pair</td><td>象限对</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01531</td><td>Quantized Neural Network</td><td>量子化神经网络</td><td>QNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01532</td><td>Quantum Computer</td><td>量子计算机</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-13">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-30-5">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-29-5">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-01533</td><td>Quantum Computing</td><td>量子计算</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-13">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-17">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-29-5">[3]</a></td><td></td></tr><tr class="even"><td>AITD-01534</td><td>Quantum Machine Learning</td><td>量子机器学习</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-04-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01535</td><td>Quantum Mechanics</td><td>量子力学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-01536</td><td>Quasi Newton Method</td><td>拟牛顿法</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-16-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01537</td><td>Quasi-Concave</td><td>拟凹</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01538</td><td>Query</td><td>查询</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01539</td><td>Query Vector</td><td>查询向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01540</td><td>Query-Key-Value</td><td>查询-键-值</td><td>QKV</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01541</td><td>Radial Basis Function</td><td>径向基函数</td><td>RBF</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-01542</td><td>Random Access Memory</td><td>随机访问存储</td><td>RAM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01543</td><td>Random Field</td><td>随机场</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01544</td><td>Random Forest Algorithm</td><td>随机森林算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01545</td><td>Random Forest</td><td>随机森林</td><td>RF、RFS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[3]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[5]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01546</td><td>Random Initialization</td><td>随机初始化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01547</td><td>Random Sampling</td><td>随机采样</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[2]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01548</td><td>Random Search</td><td>随机搜索</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01549</td><td>Random Subspace</td><td>随机子空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01550</td><td>Random Variable</td><td>随机变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01551</td><td>Random Walk</td><td>随机游走</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01552</td><td>Range</td><td>值域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01553</td><td>Rank</td><td>秩</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01554</td><td>Ratio Matching</td><td>比率匹配</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01555</td><td>Raw Feature</td><td>原始特征</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01556</td><td>Re-Balance</td><td>再平衡</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01557</td><td>Re-Sampling</td><td>重采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01558</td><td>Re-Weighting</td><td>重赋权</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01559</td><td>Readout Function</td><td>读出函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01560</td><td>Real-Time Recurrent Learning</td><td>实时循环学习</td><td>RTRL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01561</td><td>Recall</td><td>查全率/召回率</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01562</td><td>Recall-Oriented Understudy For Gisting Evaluation</td><td>ROUGE</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01563</td><td>Receiver Operating Characteristic</td><td>受试者工作特征</td><td>ROC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01564</td><td>Receptive Field</td><td>感受野</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01565</td><td>Recirculation</td><td>再循环</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01566</td><td>Recognition Weight</td><td>认知权重</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01567</td><td>Recommender System</td><td>推荐系统</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01568</td><td>Reconstruction</td><td>重构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01569</td><td>Reconstruction Error</td><td>重构误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01570</td><td>Rectangular Diagonal Matrix</td><td>矩形对角矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01571</td><td>Rectified Linear</td><td>整流线性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01572</td><td>Rectified Linear Transformation</td><td>整流线性变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01573</td><td>Rectified Linear Unit</td><td>修正线性单元/整流线性单元</td><td>ReLU</td><td><a href="https://www.jiqizhixin.com/articles/2017-10-21-4">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td>CHAPTER 2</td></tr><tr class="even"><td>AITD-01574</td><td>Rectifier Network</td><td>整流网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01575</td><td>Recurrence</td><td>循环</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01576</td><td>Recurrent Convolutional Network</td><td>循环卷积网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01577</td><td>Recurrent Multi-Layer Perceptron</td><td>循环多层感知器</td><td>RMLP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01578</td><td>Recurrent Network</td><td>循环网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01579</td><td>Recurrent Neural Network</td><td>循环神经网络</td><td>RNN</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-13-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-05-5">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-21-15">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[4]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[5]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[6]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01580</td><td>Recursive Neural Network</td><td>递归神经网络</td><td>RecNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01581</td><td>Reducible</td><td>可约的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01582</td><td>Redundant Feature</td><td>冗余特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01583</td><td>Reference Model</td><td>参考模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01584</td><td>Region</td><td>区域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01585</td><td>Regression</td><td>回归</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-21-13">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[3]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01586</td><td>Regularization</td><td>正则化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-20">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01587</td><td>Regularizer</td><td>正则化项</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01588</td><td>Reinforcement Learning</td><td>强化学习</td><td>RL</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-17-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-28-6">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[5]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01589</td><td>Rejection Sampling</td><td>拒绝采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01590</td><td>Relation</td><td>关系</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01591</td><td>Relational Database</td><td>关系型数据库</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01592</td><td>Relative Entropy</td><td>相对熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01593</td><td>Relevant Feature</td><td>相关特征</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01594</td><td>Reparameterization</td><td>再参数化/重参数化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01595</td><td>Reparametrization Trick</td><td>重参数化技巧</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01596</td><td>Replay Buffer</td><td>经验池</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01597</td><td>Representation</td><td>表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01598</td><td>Representation Learning</td><td>表示学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01599</td><td>Representational Capacity</td><td>表示容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01600</td><td>Representer Theorem</td><td>表示定理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01601</td><td>Reproducing Kernel Hilbert Space</td><td>再生核希尔伯特空间</td><td>RKHS</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01602</td><td>Rescaling</td><td>再缩放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01603</td><td>Reservoir Computing</td><td>储层计算</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01604</td><td>Reset Gate</td><td>重置门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01605</td><td>Residual Blocks</td><td>残差块</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01606</td><td>Residual Connection</td><td>残差连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01607</td><td>Residual Mapping</td><td>残差映射</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01608</td><td>Residual Network</td><td>残差网络</td><td>ResNet</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-18-2">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01609</td><td>Residual Unit</td><td>残差单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01610</td><td>Residue Function</td><td>残差函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01611</td><td>Resolution Quotient</td><td>归结商</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01612</td><td>Restricted Boltzmann Machine</td><td>受限玻尔兹曼机</td><td>RBM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-08-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01613</td><td>Restricted Isometry Property</td><td>限定等距性</td><td>RIP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01614</td><td>Return</td><td>总回报</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01615</td><td>Reverse Correlation</td><td>反向相关</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01616</td><td>Reverse KL Divergence</td><td>逆向KL散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01617</td><td>Reverse Mode Accumulation</td><td>反向模式累加</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01618</td><td>Reversible Markov Chain</td><td>可逆马尔可夫链</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01619</td><td>Reward</td><td>奖励</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01620</td><td>Reward Function</td><td>奖励函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01621</td><td>Ridge Regression</td><td>岭回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01622</td><td>Riemann Integral</td><td>黎曼积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01623</td><td>Right Eigenvector</td><td>右特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01624</td><td>Right Singular Vector</td><td>右奇异向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01625</td><td>Risk</td><td>风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01626</td><td>Risk Function</td><td>风险函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01627</td><td>Robustness</td><td>稳健性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>计算机、机器学习</td></tr><tr class="even"><td>AITD-01628</td><td>Root Node</td><td>根结点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01629</td><td>Round-Off Error</td><td>舍入误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01630</td><td>Row</td><td>行</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01631</td><td>Rule Engine</td><td>规则引擎</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01632</td><td>Rule Learning</td><td>规则学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01633</td><td>S-Fold Cross Validation</td><td>S 折交叉验证</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01634</td><td>Saccade</td><td>扫视</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01635</td><td>Saddle Point</td><td>鞍点</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-08">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01636</td><td>Saddle-Free Newton Method</td><td>无鞍牛顿法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01637</td><td>Saliency Map</td><td>显著图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01638</td><td>Saliency-Based Attention</td><td>基于显著性的注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01639</td><td>Same</td><td>相同</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01640</td><td>Sample</td><td>样本</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01641</td><td>Sample Complexity</td><td>样本复杂度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01642</td><td>Sample Mean</td><td>样本均值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01643</td><td>Sample Space</td><td>样本空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01644</td><td>Sample Variance</td><td>样本方差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01645</td><td>Sampling</td><td>采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01646</td><td>Sampling Method</td><td>采样法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01647</td><td>Saturate</td><td>饱和</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01648</td><td>Saturating Function</td><td>饱和函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01649</td><td>Scalar</td><td>标量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01650</td><td>Scale Invariance</td><td>尺度不变性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01651</td><td>Scatter Matrix</td><td>散布矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01652</td><td>Scheduled Sampling</td><td>计划采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01653</td><td>Score</td><td>得分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01654</td><td>Score Function</td><td>评分函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01655</td><td>Score Matching</td><td>分数匹配</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01656</td><td>Second Derivative</td><td>二阶导数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01657</td><td>Second Derivative Test</td><td>二阶导数测试</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01658</td><td>Second Layer</td><td>第二层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01659</td><td>Second-Order Method</td><td>二阶方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01660</td><td>Selective Attention</td><td>选择性注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01661</td><td>Selective Ensemble</td><td>选择性集成</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01662</td><td>Self Information</td><td>自信息</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01663</td><td>Self-Attention</td><td>自注意力</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01664</td><td>Self-Attention Model</td><td>自注意力模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01665</td><td>Self-Contrastive Estimation</td><td>自对比估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01666</td><td>Self-Driving</td><td>自动驾驶</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-27-7">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-16">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-08-9">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-01667</td><td>Self-Gated</td><td>自门控</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01668</td><td>Self-Organizing Map</td><td>自组织映射网</td><td>SOM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01669</td><td>Self-Taught Learning</td><td>自学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01670</td><td>Self-Training</td><td>自训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01671</td><td>Semantic Gap</td><td>语义鸿沟</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01672</td><td>Semantic Hashing</td><td>语义哈希</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01673</td><td>Semantic Segmentation</td><td>语义分割</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01674</td><td>Semantic Similarity</td><td>语义相似度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01675</td><td>Semi-Definite Programming</td><td>半正定规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01676</td><td>Semi-Naive Bayes Classifiers</td><td>半朴素贝叶斯分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01677</td><td>Semi-Restricted Boltzmann Machine</td><td>半受限玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01678</td><td>Semi-Supervised</td><td>半监督</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01679</td><td>Semi-Supervised Clustering</td><td>半监督聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01680</td><td>Semi-Supervised Learning</td><td>半监督学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-22-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-02">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-07">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-01681</td><td>Semi-Supervised Support Vector Machine</td><td>半监督支持向量机</td><td>S3VM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01682</td><td>Sentiment Analysis</td><td>情感分析</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-07-7">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01683</td><td>Separable</td><td>可分离的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01684</td><td>Separate</td><td>分离的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01685</td><td>Separating Hyperplane</td><td>分离超平面</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01686</td><td>Separation</td><td>分离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01687</td><td>Sequence Labeling</td><td>序列标注</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01688</td><td>Sequence To Sequence Learning</td><td>序列到序列学习</td><td>Seq2Seq</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01689</td><td>Sequence-To-Sequence</td><td>序列到序列</td><td>Seq2Seq</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01690</td><td>Sequential Covering</td><td>序贯覆盖</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01691</td><td>Sequential Minimal Optimization</td><td>序列最小最优化</td><td>SMO</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01692</td><td>Sequential Model-Based Optimization</td><td>时序模型优化</td><td>SMBO</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01693</td><td>Sequential Partitioning</td><td>顺序分区</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01694</td><td>Setting</td><td>情景</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01695</td><td>Shadow Circuit</td><td>浅度回路</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01696</td><td>Shallow Learning</td><td>浅层学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01697</td><td>Shannon Entropy</td><td>香农熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01698</td><td>Shannons</td><td>香农</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01699</td><td>Shaping</td><td>塑造</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01700</td><td>Sharp Minima</td><td>尖锐最小值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01701</td><td>Shattering</td><td>打散</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01702</td><td>Shift Invariance</td><td>平移不变性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01703</td><td>Short-Term Memory</td><td>短期记忆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01704</td><td>Shortcut Connection</td><td>直连边</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01705</td><td>Shortlist</td><td>短列表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01706</td><td>Siamese Network</td><td>孪生网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-02-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01707</td><td>Sigmoid</td><td>Sigmoid（一种激活函数）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01708</td><td>Sigmoid Belief Network</td><td>Sigmoid信念网络</td><td>SBN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01709</td><td>Sigmoid Curve</td><td>S 形曲线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01710</td><td>Sigmoid Function</td><td>Sigmoid函数</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-02-26">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01711</td><td>Sign Function</td><td>符号函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01712</td><td>Signed Distance</td><td>带符号距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01713</td><td>Similarity</td><td>相似度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01714</td><td>Similarity Measure</td><td>相似度度量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01715</td><td>Simple Cell</td><td>简单细胞</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01716</td><td>Simple Recurrent Network</td><td>简单循环网络</td><td>SRN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01717</td><td>Simple Recurrent Neural Network</td><td>简单循环神经网络</td><td>S-RNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01718</td><td>Simplex</td><td>单纯形</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01719</td><td>Simulated Annealing</td><td>模拟退火</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01720</td><td>Simultaneous Localization And Mapping</td><td>即时定位与地图构建</td><td>SLAM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01721</td><td>Single Component Metropolis-Hastings</td><td>单分量Metropolis-Hastings</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01722</td><td>Single Linkage</td><td>单连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01723</td><td>Singular</td><td>奇异的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01724</td><td>Singular Value</td><td>奇异值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01725</td><td>Singular Value Decomposition</td><td>奇异值分解</td><td>SVD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01726</td><td>Singular Vector</td><td>奇异向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01727</td><td>Size</td><td>大小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01728</td><td>Skip Connection</td><td>跳跃连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01729</td><td>Skip-Gram Model</td><td>跳元模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01730</td><td>Skip-Gram Model With Negative Sampling</td><td>跳元模型加负采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01731</td><td>Slack Variable</td><td>松弛变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01732</td><td>Slow Feature Analysis</td><td>慢特征分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01733</td><td>Slowness Principle</td><td>慢性原则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01734</td><td>Smoothing</td><td>平滑</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01735</td><td>Smoothness Prior</td><td>平滑先验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01736</td><td>Soft Attention Mechanism</td><td>软性注意力机制</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01737</td><td>Soft Clustering</td><td>软聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01738</td><td>Soft Margin</td><td>软间隔</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01739</td><td>Soft Margin Maximization</td><td>软间隔最大化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01740</td><td>Soft Target</td><td>软目标</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01741</td><td>Soft Voting</td><td>软投票</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01742</td><td>Softmax</td><td>Softmax/软最大化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01743</td><td>Softmax Function</td><td>Softmax函数/软最大化函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01744</td><td>Softmax Regression</td><td>Softmax回归/软最大化回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01745</td><td>Softmax Unit</td><td>Softmax单元/软最大化单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01746</td><td>Softplus</td><td>Softplus</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01747</td><td>Softplus Function</td><td>Softplus函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01748</td><td>Source Domain</td><td>源领域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01749</td><td>Span</td><td>张成子空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01750</td><td>Sparse</td><td>稀疏</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01751</td><td>Sparse Activation</td><td>稀疏激活</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01752</td><td>Sparse Auto-Encoder</td><td>稀疏自编码器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01753</td><td>Sparse Coding</td><td>稀疏编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01754</td><td>Sparse Connectivity</td><td>稀疏连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01755</td><td>Sparse Initialization</td><td>稀疏初始化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01756</td><td>Sparse Interactions</td><td>稀疏交互</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01757</td><td>Sparse Representation</td><td>稀疏表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01758</td><td>Sparse Weights</td><td>稀疏权重</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01759</td><td>Sparsity</td><td>稀疏性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01760</td><td>Specialization</td><td>特化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01761</td><td>Spectral Clustering</td><td>谱聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01762</td><td>Spectral Radius</td><td>谱半径</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01763</td><td>Speech Recognition</td><td>语音识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-01-3">[4]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-04">[5]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-15">[6]</a></td><td></td></tr><tr class="even"><td>AITD-01764</td><td>Sphering</td><td>Sphering</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01765</td><td>Spike And Slab</td><td>尖峰和平板</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01766</td><td>Spike And Slab RBM</td><td>尖峰和平板RBM</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01767</td><td>Spiking Neural Nets</td><td>脉冲神经网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-13-7">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01768</td><td>Splitting Point</td><td>切分点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01769</td><td>Splitting Variable</td><td>切分变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01770</td><td>Spurious Modes</td><td>虚假模态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01771</td><td>Square</td><td>方阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01772</td><td>Square Loss</td><td>平方损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01773</td><td>Squared Euclidean Distance</td><td>欧氏距离平方</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01774</td><td>Squared Exponential</td><td>平方指数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01775</td><td>Squashing Function</td><td>挤压函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01776</td><td>Stability</td><td>稳定性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01777</td><td>Stability-Plasticity Dilemma</td><td>可塑性-稳定性窘境</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01778</td><td>Stable Base Learner</td><td>稳定基学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01779</td><td>Stacked Auto-Encoder</td><td>堆叠自编码器</td><td>SAE</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01780</td><td>Stacked Deconvolutional Network</td><td>堆叠解卷积网络</td><td>SDN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01781</td><td>Stacked Recurrent Neural Network</td><td>堆叠循环神经网络</td><td>SRNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01782</td><td>Standard Basis</td><td>标准基</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01783</td><td>Standard Deviation</td><td>标准差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01784</td><td>Standard Error</td><td>标准差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01785</td><td>Standard Normal Distribution</td><td>标准正态分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01786</td><td>Standardization</td><td>标准化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01787</td><td>State</td><td>状态</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01788</td><td>State Action Reward State Action</td><td>SARSA算法</td><td>SARSA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01789</td><td>State Sequence</td><td>状态序列</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01790</td><td>State Space</td><td>状态空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01791</td><td>State Value Function</td><td>状态值函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01792</td><td>State-Action Value Function</td><td>状态-动作值函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01793</td><td>Statement</td><td>声明</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01794</td><td>Static Computational Graph</td><td>静态计算图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01795</td><td>Static Game</td><td>静态博弈</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01796</td><td>Stationary</td><td>平稳的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01797</td><td>Stationary Distribution</td><td>平稳分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01798</td><td>Stationary Point</td><td>驻点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01799</td><td>Statistic Efficiency</td><td>统计效率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01800</td><td>Statistical Learning</td><td>统计学习</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-01801</td><td>Statistical Learning Theory</td><td>统计学习理论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01802</td><td>Statistical Machine Learning</td><td>统计机器学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01803</td><td>Statistical Relational Learning</td><td>统计关系学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01804</td><td>Statistical Simulation Method</td><td>统计模拟方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01805</td><td>Statistics</td><td>统计量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01806</td><td>Status Feature Function</td><td>状态特征函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01807</td><td>Steepest Descent</td><td>最速下降法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01808</td><td>Step Decay</td><td>阶梯衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01809</td><td>Stochastic</td><td>随机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01810</td><td>Stochastic Curriculum</td><td>随机课程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01811</td><td>Stochastic Dynamical System</td><td>随机动力系统</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01812</td><td>Stochastic Gradient Ascent</td><td>随机梯度上升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01813</td><td>Stochastic Gradient Descent</td><td>随机梯度下降</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-25-10">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01814</td><td>Stochastic Gradient Descent With Warm Restarts</td><td>带热重启的随机梯度下降</td><td>SGDR</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01815</td><td>Stochastic Matrix</td><td>随机矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01816</td><td>Stochastic Maximum Likelihood</td><td>随机最大似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01817</td><td>Stochastic Neighbor Embedding</td><td>随机近邻嵌入</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01818</td><td>Stochastic Neural Network</td><td>随机神经网络</td><td>SNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01819</td><td>Stochastic Policy</td><td>随机性策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01820</td><td>Stochastic Process</td><td>随机过程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01821</td><td>Stop Words</td><td>停用词</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01822</td><td>Stratified Sampling</td><td>分层采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01823</td><td>Stream</td><td>流</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01824</td><td>Stride</td><td>步幅</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01825</td><td>String Kernel Function</td><td>字符串核函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01826</td><td>Strong Classifier</td><td>强分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01827</td><td>Strong Duality</td><td>强对偶性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01828</td><td>Strongly Connected Graph</td><td>强连通图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01829</td><td>Strongly Learnable</td><td>强可学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01830</td><td>Structural Risk</td><td>结构风险</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01831</td><td>Structural Risk Minimization</td><td>结构风险最小化</td><td>SRM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01832</td><td>Structure Learning</td><td>结构学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01833</td><td>Structured Learning</td><td>结构化学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01834</td><td>Structured Probabilistic Model</td><td>结构化概率模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01835</td><td>Structured Variational Inference</td><td>结构化变分推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01836</td><td>Student Network</td><td>学生网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01837</td><td>Sub-Optimal</td><td>次最优</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01838</td><td>Subatomic</td><td>亚原子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01839</td><td>Subsample</td><td>子采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01840</td><td>Subsampling</td><td>下采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01841</td><td>Subsampling Layer</td><td>子采样层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01842</td><td>Subset Evaluation</td><td>子集评价</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01843</td><td>Subset Search</td><td>子集搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01844</td><td>Subspace</td><td>子空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01845</td><td>Substitution</td><td>置换</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01846</td><td>Successive Halving</td><td>逐次减半</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01847</td><td>Sum Rule</td><td>求和法则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01848</td><td>Sum-Product</td><td>和积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01849</td><td>Sum-Product Network</td><td>和-积网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01850</td><td>Super-Parent</td><td>超父</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01851</td><td>Supervised</td><td>监督</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01852</td><td>Supervised Learning</td><td>监督学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01853</td><td>Supervised Learning Algorithm</td><td>监督学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01854</td><td>Supervised Model</td><td>监督模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01855</td><td>Supervised Pretraining</td><td>监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01856</td><td>Support Vector</td><td>支持向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01857</td><td>Support Vector Expansion</td><td>支持向量展式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01858</td><td>Support Vector Machine</td><td>支持向量机</td><td>SVM</td><td><a href="https://www.jiqizhixin.com/articles/2017-10-08">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[4]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01859</td><td>Support Vector Regression</td><td>支持向量回归</td><td>SVR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[3]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01860</td><td>Surrogat Loss</td><td>替代损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01861</td><td>Surrogate Function</td><td>替代函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01862</td><td>Surrogate Loss Function</td><td>代理损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01863</td><td>Symbol</td><td>符号</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01864</td><td>Symbolic Differentiation</td><td>符号微分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01865</td><td>Symbolic Learning</td><td>符号学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01866</td><td>Symbolic Representation</td><td>符号表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01867</td><td>Symbolism</td><td>符号主义</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01868</td><td>Symmetric</td><td>对称</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01869</td><td>Symmetric Matrix</td><td>对称矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01870</td><td>Synonymy</td><td>多词一义性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01871</td><td>Synset</td><td>同义词集</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01872</td><td>Synthetic Feature</td><td>合成特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01873</td><td>T-Distribution Stochastic Neighbour Embedding</td><td>T分布随机近邻嵌入</td><td>T-SNE</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01874</td><td>Tabular Value Function</td><td>表格值函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01875</td><td>Tagging</td><td>标注</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01876</td><td>Tangent Distance</td><td>切面距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01877</td><td>Tangent Plane</td><td>切平面</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01878</td><td>Tangent Propagation</td><td>正切传播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01879</td><td>Target</td><td>目标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01880</td><td>Target Domain</td><td>目标领域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01881</td><td>Taylor</td><td>泰勒</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01882</td><td>Taylor's Formula</td><td>泰勒公式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01883</td><td>Teacher Forcing</td><td>强制教学</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01884</td><td>Teacher Network</td><td>教师网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01885</td><td>Temperature</td><td>温度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01886</td><td>Tempered Transition</td><td>回火转移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01887</td><td>Tempering</td><td>回火</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01888</td><td>Temporal-Difference Learning</td><td>时序差分学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01889</td><td>Tensor</td><td>张量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01890</td><td>Tensor Processing Units</td><td>张量处理单元</td><td>TPU</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-05-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01891</td><td>Term Frequency-Inverse Document Frequency</td><td>单词频率-逆文本频率</td><td>TF-IDF</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01892</td><td>Terminal State</td><td>终止状态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01893</td><td>Test Data</td><td>测试数据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01894</td><td>Test Error</td><td>测试误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01895</td><td>Test Sample</td><td>测试样本</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01896</td><td>Test Set</td><td>测试集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01897</td><td>The Collider Case</td><td>碰撞情况</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01898</td><td>Threshold</td><td>阈值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-01899</td><td>Threshold Logic Unit</td><td>阈值逻辑单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01900</td><td>Threshold-Moving</td><td>阈值移动</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01901</td><td>Tied Weight</td><td>捆绑权重</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01902</td><td>Tikhonov Regularization</td><td>Tikhonov正则化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01903</td><td>Tiled Convolution</td><td>平铺卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01904</td><td>Time Delay Neural Network</td><td>时延神经网络</td><td>TDNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01905</td><td>Time Homogenous Markov Chain</td><td>时间齐次马尔可夫链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01906</td><td>Time Step</td><td>时间步</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01907</td><td>Toeplitz Matrix</td><td>Toeplitz矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01908</td><td>Token</td><td>词元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01909</td><td>Tokenize</td><td>词元化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01910</td><td>Tokenization</td><td>词元化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01911</td><td>Tokenizer</td><td>词元分析器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01912</td><td>Tolerance</td><td>容差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01913</td><td>Top-Down</td><td>自顶向下</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01914</td><td>Topic</td><td>话题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01915</td><td>Topic Model</td><td>话题模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01916</td><td>Topic Modeling</td><td>话题分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01917</td><td>Topic Vector Space</td><td>话题向量空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01918</td><td>Topic Vector Space Model</td><td>话题向量空间模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01919</td><td>Topic-Document Matrix</td><td>话题-文本矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01920</td><td>Topographic ICA</td><td>地质ICA</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01921</td><td>Total Cost</td><td>总体代价</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01922</td><td>Trace</td><td>迹</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01923</td><td>Tractable</td><td>易处理的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01924</td><td>Training</td><td>训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01925</td><td>Training Data</td><td>训练数据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01926</td><td>Training Error</td><td>训练误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01927</td><td>Training Instance</td><td>训练实例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01928</td><td>Training Sample</td><td>训练样本</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01929</td><td>Training Set</td><td>训练集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01930</td><td>Trajectory</td><td>轨迹</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01931</td><td>Transcribe</td><td>转录</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01932</td><td>Transcription System</td><td>转录系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01933</td><td>Transductive Learning</td><td>直推学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01934</td><td>Transductive Transfer Learning</td><td>直推迁移学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01935</td><td>Transfer Learning</td><td>迁移学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-04-7">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[4]</a></td><td></td></tr><tr class="even"><td>AITD-01936</td><td>Transform</td><td>变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01937</td><td>Transformer</td><td>Transformer</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01938</td><td>Transformer Model</td><td>Transformer模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01939</td><td>Transition</td><td>转移</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01940</td><td>Transition Kernel</td><td>转移核</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01941</td><td>Transition Matrix</td><td>状态转移矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01942</td><td>Transition Probability</td><td>转移概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01943</td><td>Transpose</td><td>转置</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01944</td><td>Transposed Convolution</td><td>转置卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01945</td><td>Tree-Structured LSTM</td><td>树结构的长短期记忆模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01946</td><td>Treebank</td><td>树库</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01947</td><td>Trial</td><td>试验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01948</td><td>Trial And Error</td><td>试错</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01949</td><td>Triangle Inequality</td><td>三角不等式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01950</td><td>Triangular Cyclic Learning Rate</td><td>三角循环学习率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01951</td><td>Triangulate</td><td>三角形化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01952</td><td>Triangulated Graph</td><td>三角形化图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01953</td><td>Trigram</td><td>三元语法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01954</td><td>True Negative</td><td>真负例</td><td>TN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-01955</td><td>True Positive</td><td>真正例</td><td>TP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01956</td><td>True Positive Rate</td><td>真正例率</td><td>TPR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-01957</td><td>Truncated Singular Value Decomposition</td><td>截断奇异值分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01958</td><td>Truncation Error</td><td>截断误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01959</td><td>Turing Completeness</td><td>图灵完备</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01960</td><td>Turing Machine</td><td>图灵机</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-11-7">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01961</td><td>Twice-Learning</td><td>二次学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01962</td><td>Two-Dimensional Array</td><td>二维数组</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01963</td><td>Ugly Duckling Theorem</td><td>丑小鸭定理</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01964</td><td>Unbiased</td><td>无偏</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01965</td><td>Unbiased Estimate</td><td>无偏估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01966</td><td>Unbiased Sample Variance</td><td>无偏样本方差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01967</td><td>Unconstrained Optimization</td><td>无约束优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01968</td><td>Undercomplete</td><td>欠完备</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01969</td><td>Underdetermined</td><td>欠定的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01970</td><td>Underestimation</td><td>欠估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01971</td><td>Underfitting</td><td>欠拟合</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01972</td><td>Underfitting Regime</td><td>欠拟合机制</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01973</td><td>Underflow</td><td>下溢</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01974</td><td>Underlying</td><td>潜在</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01975</td><td>Underlying Cause</td><td>潜在成因</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01976</td><td>Undersampling</td><td>欠采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01977</td><td>Understandability</td><td>可理解性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01978</td><td>Undirected</td><td>无向</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01979</td><td>Undirected Graph</td><td>无向图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01980</td><td>Undirected Graphical Model</td><td>无向图模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01981</td><td>Undirected Model</td><td>无向模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01982</td><td>Unequal Cost</td><td>非均等代价</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01983</td><td>Unfolded Graph</td><td>展开图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01984</td><td>Unfolding</td><td>展开</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01985</td><td>Unidirectional Language Model</td><td>单向语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01986</td><td>Unification</td><td>合一</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01987</td><td>Uniform Distribution</td><td>均匀分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01988</td><td>Uniform Sampling</td><td>均匀采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01989</td><td>Uniform Stability</td><td>均匀稳定性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01990</td><td>Unigram</td><td>一元语法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01991</td><td>Unimodal</td><td>单峰值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01992</td><td>Unit</td><td>单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01993</td><td>Unit Norm</td><td>单位范数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01994</td><td>Unit Test</td><td>单元测试</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01995</td><td>Unit Variance</td><td>单位方差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01996</td><td>Unit Vector</td><td>单位向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01997</td><td>Unit-Step Function</td><td>单位阶跃函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01998</td><td>Unitary Matrix</td><td>酉矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01999</td><td>Univariate Decision Tree</td><td>单变量决策树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02000</td><td>Universal Approximation Theorem</td><td>通用近似定理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02001</td><td>Universal Approximator</td><td>通用近似器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02002</td><td>Universal Function Approximator</td><td>通用函数近似器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02003</td><td>Unknown Token</td><td>未知词元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02004</td><td>Unlabeled</td><td>未标记</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02005</td><td>Unnormalized Probability Function</td><td>未规范化概率函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02006</td><td>Unprojection</td><td>反投影</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02007</td><td>Unshared Convolution</td><td>非共享卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02008</td><td>Unsupervised</td><td>无监督</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02009</td><td>Unsupervised Feature Learning</td><td>无监督特征学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02010</td><td>Unsupervised Layer-Wise Training</td><td>无监督逐层训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02011</td><td>Unsupervised Learning Algorithm</td><td>无监督学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02012</td><td>Unsupervised Learning</td><td>无监督学习</td><td>UL</td><td><a href="https://www.jiqizhixin.com/articles/2017-11-17-5">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-02013</td><td>Unsupervised Pretraining</td><td>无监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02014</td><td>Update Gate</td><td>更新门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02015</td><td>Update Model Parameter</td><td>迭代模型参数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02016</td><td>Upper Confidence Bounds</td><td>上置信界限</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02017</td><td>Upsampling</td><td>上采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02018</td><td>V-Structure</td><td>V型结构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02019</td><td>Valid</td><td>有效</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02020</td><td>Validation Set</td><td>验证集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02021</td><td>Validity Index</td><td>有效性指标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02022</td><td>Value Function</td><td>价值函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02023</td><td>Value Function Approximation</td><td>值函数近似</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02024</td><td>Value Iteration</td><td>值迭代</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02025</td><td>Vanishing And Exploding Gradient Problem</td><td>梯度消失与爆炸问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02026</td><td>Vanishing Gradient</td><td>梯度消失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02027</td><td>Vanishing Gradient Problem</td><td>梯度消失问题</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-07-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02028</td><td>Vapnik-Chervonenkis Dimension</td><td>VC维</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02029</td><td>Variable Elimination</td><td>变量消去</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02030</td><td>Variance</td><td>方差</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02031</td><td>Variance Reduction</td><td>方差减小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02032</td><td>Variance Scaling</td><td>方差缩放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02033</td><td>Variational Autoencoder</td><td>变分自编码器</td><td>VAE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02034</td><td>Variational Bayesian</td><td>变分贝叶斯</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02035</td><td>Variational Derivative</td><td>变分导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02036</td><td>Variational Distribution</td><td>变分分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02037</td><td>Variational Dropout</td><td>变分暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02038</td><td>Variational EM Algorithm</td><td>变分EM算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02039</td><td>Variational Free Energy</td><td>变分自由能</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02040</td><td>Variational Inference</td><td>变分推断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02041</td><td>Vector</td><td>向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02042</td><td>Vector Space</td><td>向量空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02043</td><td>Vector Space Model</td><td>向量空间模型</td><td>VSM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02044</td><td>Vectorization</td><td>向量化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02045</td><td>Version Space</td><td>版本空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02046</td><td>Virtual Adversarial Example</td><td>虚拟对抗样本</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02047</td><td>Virtual Adversarial Training</td><td>虚拟对抗训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02048</td><td>Visible Layer</td><td>可见层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02049</td><td>Visible Variable</td><td>可见变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02050</td><td>Viterbi Algorithm</td><td>维特比算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02051</td><td>Vocabulary</td><td>词表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02052</td><td>Von Neumann Architecture</td><td>冯 · 诺伊曼架构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02053</td><td>Voted Perceptron</td><td>投票感知器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02054</td><td>Wake Sleep</td><td>醒眠</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02055</td><td>Warp</td><td>线程束</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02056</td><td>Wasserstein Distance</td><td>Wasserstein距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02057</td><td>Wasserstein GAN</td><td>Wasserstein生成对抗网络</td><td>WGAN</td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-05">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02058</td><td>Weak Classifier</td><td>弱分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02059</td><td>Weak Duality</td><td>弱对偶性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02060</td><td>Weak Learner</td><td>弱学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02061</td><td>Weakly Learnable</td><td>弱可学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02062</td><td>Weakly Supervised Learning</td><td>弱监督学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02063</td><td>Weight</td><td>权重</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-08-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02064</td><td>Weight Decay</td><td>权重衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02065</td><td>Weight Normalization</td><td>权重规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02066</td><td>Weight Scaling Inference Rule</td><td>权重比例推断规则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02067</td><td>Weight Sharing</td><td>权共享</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02068</td><td>Weight Space Symmetry</td><td>权重空间对称性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02069</td><td>Weight Vector</td><td>权值向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02070</td><td>Weighted Distance</td><td>加权距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02071</td><td>Weighted Voting</td><td>加权投票</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02072</td><td>Whitening</td><td>白化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02073</td><td>Wide Convolution</td><td>宽卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02074</td><td>Width</td><td>宽度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02075</td><td>Winner-Take-All</td><td>胜者通吃</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02076</td><td>Within-Class Scatter Matrix</td><td>类内散度矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02077</td><td>Word Embedding</td><td>词嵌入</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-11-20-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02078</td><td>Word Sense Disambiguation</td><td>词义消歧</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02079</td><td>Word Vector</td><td>词向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02080</td><td>Word Vector Space Model</td><td>单词向量空间模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02081</td><td>Word-Document Matrix</td><td>单词-文本矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02082</td><td>Word-Topic Matrix</td><td>单词-话题矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02083</td><td>Working Memory</td><td>工作记忆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02084</td><td>Wrapper Method</td><td>包裹式方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02085</td><td>Z-Score Normalization</td><td>Z值规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02086</td><td>Zero Mean</td><td>零均值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02087</td><td>Zero Padding</td><td>零填充</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02088</td><td>Zero Tensor</td><td>零张量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02089</td><td>Zero-Centered</td><td>零中心化的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02090</td><td>Zero-Data Learning</td><td>零数据学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02091</td><td>Zero-Shot Learning</td><td>零试学习</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-03-31-6">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02092</td><td>Zipf's Law</td><td>齐普夫定律</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02093</td><td>ε-Greedy Method</td><td>ε-贪心法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02094</td><td>2D Qsar Models</td><td>二维定量构效关系模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="odd"><td>AITD-02095</td><td>3D Cartesian</td><td>三维笛卡尔（坐标）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="even"><td>AITD-02096</td><td>3D Conformation</td><td>三维构象</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>化学、生化</td></tr><tr class="odd"><td>AITD-02097</td><td>3D Grids</td><td>三维（坐标）网格</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02098</td><td>3D Qsar Models</td><td>三维定量构效关系模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="odd"><td>AITD-02099</td><td>Aberration-Corrected</td><td>像差矫正</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-02100</td><td>Active Machine Learning</td><td>主动机器学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02101</td><td>Adaptive Fuzzy Neural Network</td><td>自适应模糊神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02102</td><td>Adaptive Sampling</td><td>自适应采样</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02103</td><td>Admet Evaluation</td><td>毒性评估</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="even"><td>AITD-02104</td><td>Alexnet</td><td>AlexNet</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02105</td><td>Alphago</td><td>阿尔法狗</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02106</td><td>Adaptive Neuro Fuzzy Inference System</td><td>自适应神经模糊推理系统</td><td>ANFIS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02107</td><td>Approximate Probabilistic Models</td><td>近似概率模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02108</td><td>Artificial Neurons</td><td>人工神经元</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02109</td><td>Artificial Synapses</td><td>人工突触</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02110</td><td>Attention-Based</td><td>基于注意力（机制）的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02111</td><td>Automating Synthetic Planning</td><td>自动化综合规划</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02112</td><td>Automation</td><td>自动化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02113</td><td>Autonomous Decision-Making</td><td>自主决策</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02114</td><td>B-Clustering Algorithms</td><td>B树聚类算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02115</td><td>Balanced Accuracy</td><td>平衡精度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02116</td><td>Bandgap Energy</td><td>带隙能量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="odd"><td>AITD-02117</td><td>Baseline Test</td><td>基准测试</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02118</td><td>Basin Hopping</td><td>盆地跳跃（算法）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02119</td><td>Bayesian Approach</td><td>贝叶斯方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-02120</td><td>Bayesian Induction</td><td>贝叶斯归纳</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02121</td><td>Bayesian Mcmc Methods</td><td>贝叶斯马尔可夫链蒙特卡洛方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-02122</td><td>Bayesian Methods</td><td>贝叶斯方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02123</td><td>Bayesian Molecular</td><td>贝叶斯分子（设计方法）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td>统计，机器学习，化学</td></tr><tr class="even"><td>AITD-02124</td><td>Bayesian Prior</td><td>贝叶斯先验</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02125</td><td>Bayesian Program Learning</td><td>贝叶斯程序学习</td><td>BPL</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-02126</td><td>Bayesian Regularized Neural Network</td><td>贝叶斯正则化神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02127</td><td>Beam-Scanning</td><td>波束扫描</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-02128</td><td>Best Separates</td><td>最优分离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02129</td><td>Biased Dataset</td><td>有偏数据集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02130</td><td>Bit Collisions</td><td>字节碰撞/冲突</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>数据库</td></tr><tr class="odd"><td>AITD-02131</td><td>Black Box</td><td>黑盒子</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02132</td><td>Black-Box Attack</td><td>黑盒攻击</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02133</td><td>Bonding Environments</td><td>成键环境</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02134</td><td>Bonferroni Correction</td><td>邦弗朗尼校正</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02135</td><td>Bootstrap Aggregation</td><td>引导聚合</td><td>bagging</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02136</td><td>Broyden–Fletcher–Goldfarb–Shanno</td><td>BFGS（算法）</td><td>BFGS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>一种拟牛顿法，数学计算</td></tr><tr class="odd"><td>AITD-02137</td><td>Buchwald−Hartwig Cross-Coupling</td><td>Buchwald–Hartwig 偶联（反应）</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>化学</td></tr><tr class="even"><td>AITD-02138</td><td>C4.5 Algorithm</td><td>C4.5 算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>一种决策树算法，数据挖掘</td></tr><tr class="odd"><td>AITD-02139</td><td>Calculation Uncertainties</td><td>计算不确定性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02140</td><td>Canonical Ml Methods</td><td>经典机器学习方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02141</td><td>Cartesian Distance Vector</td><td>笛卡尔距离向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02142</td><td>CASP</td><td>国际蛋白质结构预测竞赛</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>生物</td></tr><tr class="odd"><td>AITD-02143</td><td>Categorical Data</td><td>分类数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02144</td><td>Categorization Algorithms</td><td>分类算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02145</td><td>ChemDataExtractor</td><td>化学数据提取器</td><td>CDE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02146</td><td>Chi-Squared</td><td>卡方（分布）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02147</td><td>Classification Model</td><td>分类模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02148</td><td>Cluster Resolution Feature Selection</td><td>聚类分辨率特征选择</td><td>CR-FS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02149</td><td>Cluster-Based Splitting</td><td>基于聚类的分离方法</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02150</td><td>Clustering Methods</td><td>聚类方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02151</td><td>Code Pipeline</td><td>代码流水线</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02152</td><td>Coefficient of Determination</td><td>决定系数</td><td>r^2 or R^2</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02153</td><td>Combined Gradient</td><td>组合梯度（算法）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02154</td><td>Complex Data</td><td>复合数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02155</td><td>Computational Cost</td><td>计算成本</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02156</td><td>Computational Optimisation</td><td>计算优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02157</td><td>Computational Science</td><td>计算科学</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02158</td><td>Computational Toxicology</td><td>计算毒理学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02159</td><td>Computer Science</td><td>计算机科学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02160</td><td>Computer Simulations</td><td>计算机模拟</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02161</td><td>Computer-Aided</td><td>计算机辅助</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02162</td><td>Constraint</td><td>约束</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02163</td><td>Core-Loss Spectrum</td><td>（电子能量损失谱中的）高能区域</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02164</td><td>Coulomb Matrix</td><td>库仑矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02165</td><td>Coupled-Cluster Predictions</td><td>耦合簇预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02166</td><td>Cross-Validated Coefficient of Determination</td><td>交叉验证的决定系数</td><td>q^2 or Q^2</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02167</td><td>Cross-Validation</td><td>交叉验证</td><td>CV</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02168</td><td>Crowd-Sourcing</td><td>众包</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>商业模式</td></tr><tr class="odd"><td>AITD-02169</td><td>Cut-Points</td><td>切点</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02170</td><td>Cutoff Radial Function</td><td>截断径向函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02171</td><td>Data Availability</td><td>数据可用性</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02172</td><td>Data Cleaning</td><td>数据清洗</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02173</td><td>Data Collection</td><td>数据采集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02174</td><td>Data Considerations</td><td>数据注意事项</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02175</td><td>Data Curation</td><td>数据监管</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02176</td><td>Data Disparity</td><td>数据差异</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02177</td><td>Data Dredging</td><td>数据挖掘</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02178</td><td>Data Imputation</td><td>数据填补</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02179</td><td>Data Labels</td><td>数据标签</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02180</td><td>Data Leakage</td><td>数据泄露</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02181</td><td>Data Pre-Processing</td><td>数据预处理</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02182</td><td>Data Processing</td><td>数据处理</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02183</td><td>Data Quality</td><td>数据质量</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02184</td><td>Data Reduction</td><td>数据缩减</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02185</td><td>Data Representation</td><td>数据表示</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02186</td><td>Data Selection</td><td>数据选择</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02187</td><td>Data Sources</td><td>数据源</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02188</td><td>Data Splitting</td><td>数据拆分</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02189</td><td>Data Transformation</td><td>数据转换</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02190</td><td>Data-Driven</td><td>数据驱动</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02191</td><td>Data-Driven Decision-Making</td><td>数据驱动的决策</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02192</td><td>Data-Driven Methods</td><td>数据驱动的方法</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02193</td><td>Data-Driven Spectral Analysis</td><td>数据驱动的光谱分析</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02194</td><td>Data-Mining</td><td>数据挖掘</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02195</td><td>Database</td><td>数据库</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02196</td><td>DE Algorithm</td><td>差分进化算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02197</td><td>Deeplift</td><td>DeepLift模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02198</td><td>Dendrogram</td><td>树状图</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02199</td><td>Density Functional Theory</td><td>密度泛函理论</td><td>DFT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[5]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[6]</a></td><td></td></tr><tr class="even"><td>AITD-02200</td><td>Density-Based Spatial Clustering Of Applications With Noise</td><td>DBSCAN密度聚类</td><td>DBSCAN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02201</td><td>Descriptor</td><td>描述符</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02202</td><td>DFT Calculations</td><td>DFT计算</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02203</td><td>Dice Similarity</td><td>戴斯相似度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02204</td><td>Differential Evolution</td><td>差分进化</td><td>DE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02205</td><td>Dimensionality Reduction</td><td>降维</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02206</td><td>Direct Neural Network Modeling</td><td>正向神经网络建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02207</td><td>Discrete Manner</td><td>离散方式</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02208</td><td>Discrete Quanta</td><td>离散量子</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02209</td><td>Discretization</td><td>离散化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02210</td><td>Distillation</td><td>蒸馏</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02211</td><td>Dynamic Datasets</td><td>动态数据集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02212</td><td>Dynamic Filter Networks</td><td>动态过滤网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02213</td><td>Dynamic Sampling</td><td>动态采样</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02214</td><td>Dynamics Simulations</td><td>动力学模拟</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02215</td><td>Eigenfunction</td><td>特征函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02216</td><td>Electronegativity</td><td>电负性</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02217</td><td>Elman</td><td>埃尔曼</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02218</td><td>Empirical Models</td><td>经验模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02219</td><td>Energy Derivatives</td><td>能源衍生品</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>在DP模型中：能量的导数</td></tr><tr class="even"><td>AITD-02220</td><td>Energy Potentials</td><td>能量潜力</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02221</td><td>Ensemble Methods</td><td>集成方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02222</td><td>Entity Normalisation</td><td>实体规范化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02223</td><td>Ethical Considerations</td><td>道德考虑</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02224</td><td>Euclidean Distances</td><td>欧几里得距离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02225</td><td>Evolutionary Algorithms</td><td>进化算法</td><td>EA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02226</td><td>Evolutionary Method</td><td>进化方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02227</td><td>Exchange–Correlation</td><td>交换关联（的能量/泛函）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02228</td><td>Excited-State Potentials</td><td>激发态能量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02229</td><td>Expected Reduction In Distortion</td><td>符合预期的失真减少</td><td>ERD</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02230</td><td>Experimental Validation Data</td><td>实验验证数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02231</td><td>Expert Systems</td><td>专家系统</td><td>ESS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02232</td><td>Extended-Connectivity Circular Fingerprint</td><td>扩展连接环形指纹</td><td>ECFP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02233</td><td>Extraction Techniques</td><td>提取技术</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02234</td><td>Faber-Christensen-Huang-Lilienfeld</td><td>Faber-Christensen-Huang-Lilienfeld</td><td>FCHL</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>四个人提出的化学结构量子机器学习方法</td></tr><tr class="odd"><td>AITD-02235</td><td>Facial Recognition</td><td>面部识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02236</td><td>FAIR Data Principles</td><td>FAIR数据原则</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>Findability可找寻 Accessibility可访问 Interoperability可交互Reuse可再用</td></tr><tr class="odd"><td>AITD-02237</td><td>False Negatives</td><td>假阴性</td><td>FNs</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02238</td><td>False Positives</td><td>假阳性</td><td>FPs</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02239</td><td>Fchl Representation</td><td>Fchl 表示</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02240</td><td>Feature Binarization</td><td>特征二值化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02241</td><td>Feature Transform</td><td>特征变换</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02242</td><td>Feature Vectors</td><td>特征向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02243</td><td>Features</td><td>特征</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02244</td><td>Feed Back</td><td>反馈</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02245</td><td>Feed-Forward Neural Networks</td><td>前馈神经网络</td><td>FFNN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02246</td><td>Feedback Structure</td><td>反馈结构</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02247</td><td>Final Evaluation</td><td>最终评估</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02248</td><td>Findable, Accessible, Interoperable, Reusable</td><td>可查找、可访问、可互操作、可重用</td><td>FAIR</td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02249</td><td>First-Principles</td><td>第一性原理</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02250</td><td>Flow Rate</td><td>流速</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02251</td><td>Forward Cross-Validation</td><td>前向交叉验证</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02252</td><td>Forward Prediction</td><td>前向预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02253</td><td>Forward Reaction Prediction</td><td>前向反应预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02254</td><td>Fuzzy Logic</td><td>模糊逻辑</td><td>FL</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02255</td><td>Fuzzy Neural Networks</td><td>模糊神经网络</td><td>FNN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02256</td><td>Ga-Based Approaches</td><td>基于遗传算法的方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02257</td><td>Garbage In, Garbage Out</td><td>无用数据入、无用数据出</td><td>GIGO</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02258</td><td>Gas-Phase Networks</td><td>气相网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02259</td><td>Gaussian Kernels</td><td>高斯核</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02260</td><td>Gaussian-Type Structure Descriptors</td><td>高斯型结构描述符</td><td>GTSD</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02261</td><td>General Intelligence</td><td>通用智能</td><td>GI</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02262</td><td>Generalized Gradient Approximation</td><td>广义梯度近似</td><td>GGA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02263</td><td>Generative Adversarial Networks</td><td>生成对抗网络</td><td>GAN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02264</td><td>Gradient Boosting Decision Tree</td><td>梯度提升决策树</td><td>GBDT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02265</td><td>Gradient-Based</td><td>基于梯度的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02266</td><td>Grain-Surface Networks</td><td>粒面网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02267</td><td>Graph Convolutional</td><td>图卷积</td><td>GC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02268</td><td>Graph Models</td><td>图模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02269</td><td>Graph Neural Networks</td><td>图神经网络</td><td>GNNS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02270</td><td>Graph-Based</td><td>基于图形</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02271</td><td>Graph-Based Models</td><td>基于图的模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02272</td><td>Graph-Based Neural Networks</td><td>基于图的神经网络</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02273</td><td>Graph-Based Representation</td><td>基于图的表示</td><td>GB-GA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02274</td><td>Graph-Convolutional Neural Network</td><td>图卷积神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02275</td><td>Graphics Processing Units</td><td>图形处理器</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02276</td><td>Gravimetric Polymerization Degree</td><td>比重聚合度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02277</td><td>Hamiltonian Matrix</td><td>哈密顿矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-02278</td><td>Hamiltonian Operator</td><td>哈密顿算符</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="odd"><td>AITD-02279</td><td>Heterogeneous Data</td><td>异构数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02280</td><td>Hidden Layers</td><td>隐藏层</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02281</td><td>High Data Throughput</td><td>高数据吞吐量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02282</td><td>High Throughput</td><td>高通量</td><td>HT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02283</td><td>High Throughput Screening</td><td>高通量筛选</td><td>HTS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02284</td><td>High Variance Models</td><td>高方差模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02285</td><td>High-Dimensional Data</td><td>高维数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02286</td><td>High-Dimensional NN</td><td>高维神经网络</td><td>HDNN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02287</td><td>High-Dimensional Objects</td><td>高维对象</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02288</td><td>High-Throughput</td><td>高通量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02289</td><td>Higher-Dimensional Space</td><td>高维空间</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="even"><td>AITD-02290</td><td>Higher-Dimensional Spectral Space</td><td>高维光谱空间</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02291</td><td>Homogenization</td><td>同质化</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02292</td><td>Homomorphic Encryption</td><td>同态加密</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02293</td><td>Human Face Recognition</td><td>人脸识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02294</td><td>Human-Encoded</td><td>人工编码的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02295</td><td>Hybrid Model</td><td>混合模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02296</td><td>Hybrid Technique</td><td>混合技术</td><td>HM</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02297</td><td>Hybrid-Neural Model</td><td>混合神经模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02298</td><td>Hyperparameter Opimization</td><td>超参数优化</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02299</td><td>Hyperparameters</td><td>超参数</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02300</td><td>Hyperplanes Separate</td><td>超平面分离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02301</td><td>Id3 Algorithm</td><td>Id3 算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02302</td><td>Image And Speech Recognition</td><td>图像和语音识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02303</td><td>Image Classification</td><td>图像分类</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02304</td><td>Image Classifier</td><td>图像分类器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02305</td><td>Image Recognition</td><td>图像识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02306</td><td>Informative Priors</td><td>信息先验</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02307</td><td>Input-Output Pairs</td><td>输入输出对</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02308</td><td>Instance-Based</td><td>基于实例的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02309</td><td>Intelligent Machine</td><td>智能机器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02310</td><td>Intermediate Neurons</td><td>中间神经元</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02311</td><td>Internet Of Things</td><td>物联网</td><td>IoT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02312</td><td>Interpolation Coordinate</td><td>插值坐标</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02313</td><td>Interpretability</td><td>可解释性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02314</td><td>Inverse Neural Modeling</td><td>逆神经建模</td><td>INN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02315</td><td>Inverse Neural Network Modeling</td><td>逆神经网络建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02316</td><td>Iterative Learning</td><td>迭代学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02317</td><td>Joint Distribution</td><td>联合分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02318</td><td>Jordan-Elman Neural Networks</td><td>Jordan-Elman 神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02319</td><td>K Clusters</td><td>K聚类</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02320</td><td>K Nearest Points</td><td>K 最近点</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02321</td><td>K-1 Folds</td><td>K-1 折</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02322</td><td>K-Edge (O-K Edge)</td><td>K-边缘（O-K 边缘）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02323</td><td>K-Means</td><td>K-均值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02324</td><td>Kendall’S Tau</td><td>肯德尔等级相关系数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02325</td><td>Kernel Ridge Regression</td><td>核岭回归</td><td>KRR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02326</td><td>Kernels</td><td>内核</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02327</td><td>Kinetic Curve</td><td>动力学曲线</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02328</td><td>KNN Model</td><td>K 近邻模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02329</td><td>Knowledge Extraction</td><td>知识提取</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02330</td><td>Knowledge Gradient</td><td>知识梯度</td><td>KG</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02331</td><td>L1 And L2 Regularization</td><td>L1与L2正则化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02332</td><td>Laboratory Level</td><td>实验室级别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02333</td><td>Language Processing</td><td>语言处理</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02334</td><td>Laplacian Prior</td><td>拉普拉斯先验</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02335</td><td>Large-Scale Data Storage</td><td>大规模数据存储</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02336</td><td>Lasers</td><td>激光器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02337</td><td>Lasso Regression</td><td>拉索回归</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02338</td><td>LBP</td><td>局部二值模式</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02339</td><td>Least Absolute Shrinkage And Selection Operator</td><td>Lasso回归</td><td>LASSO</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02340</td><td>Least Square Support Vector Machine</td><td>最小二乘支持向量机</td><td>LSSVM</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02341</td><td>Ligand-Field</td><td>配位场</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02342</td><td>Linear</td><td>线性的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-02343</td><td>Linear Dimension Reduction Methods</td><td>线性降维方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02344</td><td>Linear Vibronic Coupling Model</td><td>线性振子耦合模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02345</td><td>Local Recurrent</td><td>本地卷积</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02346</td><td>Logic And Heuristics Applied To Synthetic Analysis</td><td>LHASA 程序</td><td>LHASA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02347</td><td>Long-Range Prediction</td><td>长期预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02348</td><td>Long-Range Prediction Models</td><td>长期预测模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02349</td><td>Long-Term Planning</td><td>长期规划</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02350</td><td>Long-Term Reward</td><td>长期回报</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02351</td><td>Machine-Readable Data</td><td>机器可读的数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02352</td><td>Mae</td><td>平均绝对误差</td><td>MAE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02353</td><td>Mahalanobis Distances</td><td>马氏距离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02354</td><td>Matrices</td><td>矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-02355</td><td>Matthews Correlation Coefficient</td><td>马修斯相关系数</td><td>MCC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02356</td><td>Maximum Likelihood Methods</td><td>最大似然法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02357</td><td>Maximum Likelihood Procedures</td><td>最大似然估计法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02358</td><td>MCTS Method</td><td>蒙特卡洛树搜索方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02359</td><td>Mean-Squared Error</td><td>均方误差</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-02360</td><td>Mechanical Sympathy</td><td>机械同感，软硬件协同编程</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02361</td><td>Merging</td><td>合并</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02362</td><td>Message Passing Neural Networks</td><td>消息传递神经网络</td><td>MPNNS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02363</td><td>Microarray Data</td><td>微阵列数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02364</td><td>Mini Batch</td><td>小批次</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02365</td><td>Mining</td><td>挖掘</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02366</td><td>Mining Out</td><td>挖掘</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02367</td><td>Missing Values</td><td>缺失值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02368</td><td>ML Algorithm</td><td>机器学习算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02369</td><td>ML Modelling</td><td>机器学习建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02370</td><td>ML Potentials</td><td>机器学习势能</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02371</td><td>ML-Driven</td><td>机器学习驱动的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02372</td><td>ML-Driven Optimization</td><td>机器学习驱动的最优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02373</td><td>MLP Neural Model</td><td>多层感知机神经模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02374</td><td>Model Construction</td><td>模型构建</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02375</td><td>Model Evaluation</td><td>模型评估</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02376</td><td>Model Performance</td><td>模型性能</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02377</td><td>Model Statistics</td><td>模型统计</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02378</td><td>Model Training</td><td>模型训练</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02379</td><td>Model Validation</td><td>模型验证</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02380</td><td>Model-Based Iterative Reconstruction</td><td>基于模型的迭代重建</td><td>MBIR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02381</td><td>Model-Construction</td><td>模型构建</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02382</td><td>Modelling Scenario</td><td>建模场景</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02383</td><td>Molecular Graph Theory</td><td>分子图论</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02384</td><td>Molecular Modelling</td><td>分子建模</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02385</td><td>Monte Carlo Tree Search</td><td>蒙特卡洛树搜索</td><td>MCTS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[3]</a></td><td>数学</td></tr><tr class="even"><td>AITD-02386</td><td>Moore’S Law</td><td>摩尔定律</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a></td><td>计算机</td></tr><tr class="odd"><td>AITD-02387</td><td>ms-QSBER-EL Model</td><td>基于人工神经网络组合的结构生物学效应定量关系多尺度模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02388</td><td>Multi-Agent Control System</td><td>多智能体控制系统</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02389</td><td>Multi-Core Desktop Computer</td><td>多核台式计算机</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>计算机</td></tr><tr class="even"><td>AITD-02390</td><td>Multi-Dimensional Big Data Analysis</td><td>多维度大数据分析</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02391</td><td>Multi-Layer Feed-Forward</td><td>多层前馈</td><td>MLFF</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02392</td><td>Multi-Objective Genetic Algorithm</td><td>多目标遗传算法</td><td>MOGA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02393</td><td>Multi-Objective Optimization</td><td>多目标优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02394</td><td>Multi-Reaction Synthesis</td><td>多反应合成</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02395</td><td>Multilayer Perceptron</td><td>多层感知机</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02396</td><td>Multivariate Regression</td><td>多变量回归</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02397</td><td>N-Dimensional Space</td><td>N维空间</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02398</td><td>Naive Bayesian</td><td>朴素贝叶斯</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02399</td><td>Naive Bayesian Methods</td><td>朴素贝叶斯方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02400</td><td>Named Entity Recognition，NER</td><td>命名实体识别</td><td>NER</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02401</td><td>Nearest Neighbors</td><td>近邻</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02402</td><td>Nearest Neighbour Model</td><td>近邻模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02403</td><td>Negative Predictive Value</td><td>阴性预测值</td><td>NPV</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02404</td><td>Network Architecture</td><td>网络结构</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02405</td><td>Network Geometry</td><td>网络几何</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02406</td><td>Neural Turing Machines</td><td>神经图灵机</td><td>NTM</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02407</td><td>Neural-Network-Based Function</td><td>基于神经网络的函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02408</td><td>Neurons</td><td>神经元</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02409</td><td>Nuclear Magnetic Resonance</td><td>核磁共振</td><td>NMR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02410</td><td>Noise Filters</td><td>噪声过滤器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02411</td><td>Noise-Free</td><td>无噪的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02412</td><td>Non-Linear</td><td>非线性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>数学、统计</td></tr><tr class="odd"><td>AITD-02413</td><td>Non-Linear Correlation</td><td>非线性相关</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02414</td><td>Non-Linearity</td><td>非线性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02415</td><td>Non-Parametric Algorithm</td><td>非参数化学习算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02416</td><td>Non-Safety-Critical Applications</td><td>非安全关键型应用</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02417</td><td>Non-Steady-State</td><td>非稳态</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02418</td><td>Non-Stochastic</td><td>非随机的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02419</td><td>Non-Template</td><td>非模板</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02420</td><td>Non-Template Methods</td><td>非模板方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02421</td><td>Non-Zero Weight</td><td>非零权重</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02422</td><td>On-The-Fly Optimization</td><td>运行中优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>计算机</td></tr><tr class="odd"><td>AITD-02423</td><td>One-Hot Vector</td><td>独热向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>整个矢量中之后一个数为1 其余为0</td></tr><tr class="even"><td>AITD-02424</td><td>Open-Source</td><td>开源</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>软件工程</td></tr><tr class="odd"><td>AITD-02425</td><td>Open-Source Dataset</td><td>开源数据集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02426</td><td>Predicted Label</td><td>预测值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02427</td><td>Prediction</td><td>预测</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02428</td><td>Prediction Accuracy</td><td>预测准确率</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02429</td><td>Predictor</td><td>预测器/决策函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02430</td><td>Protein Folding</td><td>蛋白折叠</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a></td><td>生物</td></tr><tr class="odd"><td>AITD-02431</td><td>Quantum Chemistry</td><td>量子化学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="even"><td>AITD-02432</td><td>Quantum Theory</td><td>量子理论</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="odd"><td>AITD-02433</td><td>Random Selection</td><td>随机选择</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02434</td><td>Raw Datasets</td><td>原始数据集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02435</td><td>Root Mean Square Errors</td><td>均方根</td><td>RMSE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02436</td><td>Scaling</td><td>缩放</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>图像处理</td></tr><tr class="odd"><td>AITD-02437</td><td>Simulation</td><td>仿真</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02438</td><td>The Global Minimum</td><td>全局最小值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02439</td><td>Turing Test</td><td>图灵测试</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>AI，CS</td></tr><tr class="even"><td>AITD-02440</td><td>Version Control</td><td>版本控制</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02441</td><td>Workflow</td><td>工作流</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02442</td><td>Sequence-Function</td><td>序列-功能</td><td></td><td>[1]</td><td></td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>人工智能</tag>
      
      <tag>专业名词</tag>
      
      <tag>基础概念</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>XGBoost参数调优笔记</title>
    <link href="/%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E7%AC%94%E8%AE%B0.html"/>
    <url>/%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E7%AC%94%E8%AE%B0.html</url>
    
    <content type="html"><![CDATA[<p>在机器学习中，参数调优是一门玄学，因为模型的最优参数可能依赖于许多场景。因此，不可能为参数调优创建一个全面的指南。本文尝试为XGBoost中的参数提供一些指导。</p><span id="more"></span><h3id="理解偏差-方差权衡bias-variance-tradeoff">1.理解偏差-方差权衡(Bias-VarianceTradeoff)</h3><p>如果你参加过机器学习或统计学课程，这可能是最重要的概念之一。当我们允许模型变得更加复杂（例如，增加深度）时，模型具有更好的拟合训练数据的能力，从而获得偏差较小的模型。然而，这种复杂的模型需要更多的数据来进行拟合。</p><p>XGBoost中的大多数参数都涉及偏差和方差的权衡。最好的模型应该在模型复杂度和预测能力之间进行仔细权衡。参数文档会告诉你每个参数是否会使模型更保守。这可以帮助你在复杂模型和简单模型之间进行调整。</p><h3 id="控制过拟合">2.控制过拟合</h3><p>当你观察到训练准确率高但测试准确率低时，很可能遇到了过拟合问题。</p><p>在XGBoost中，一般有两种方法可以控制过拟合：</p><blockquote><ul><li><p>第一种方法是直接控制模型复杂度。</p><p><code>这包括max_depth、min_child_weight和gamma。</code></p></li><li><p>第二种方法是增加随机性，使训练对噪声具有鲁棒性。</p></li></ul><p>​ <code>这包括subsample和colsample_bytree。</code></p></blockquote><p>你还可以减少步长eta。记住在这样做时增加num_round。</p><h3 id="处理数据集不平衡">3.处理数据集不平衡</h3><p>对于诸如广告点击日志等常见情况，数据集极度不平衡。这会影响XGBoost模型的训练，有两种方法可以改进。</p><blockquote><p>如果你只关心预测的整体性能指标（AUC）</p><ul><li><p>通过scale_pos_weight平衡正负样本的权重</p></li><li><p>使用AUC作为评估标准</p></li></ul><p>如果你关心预测的正确概率</p><ul><li><p>在这种情况下，你不能重新平衡数据集</p></li><li><p>将参数max_delta_step设置为一个有限的数值（例如1）以帮助收敛</p></li></ul></blockquote><h3 id="减少内存使用">4.减少内存使用</h3><p>如果你使用类似sklearn.model_selection.GridSearchCV的HPO库，请控制它可以使用的线程数。最好让XGBoost并行运行，而不是让GridSearchCV同时运行多个实验。例如，为交叉验证创建一个数据折叠可以消耗大量内存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这会创建数据集的副本。X和X_train同时存在于内存中。</span><br><span class="hljs-comment"># 如果你在n_jobs大于1的情况下运行`GridSearchCV`，每个线程都会同时发生这种情况。</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y)<br><br>df = pd.DataFrame()<br><span class="hljs-comment"># 这会创建数据框的新副本，即使你指定了inplace参数</span><br>new_df = df.drop(...)<br><br>array = np.array(...)<br><span class="hljs-comment"># 这可能会也可能不会复制数据，具体取决于数据类型</span><br>array.astype(np.float32)<br><br><span class="hljs-comment"># np默认使用双精度，你真的需要吗？</span><br>array = np.array(...)<br></code></pre></td></tr></table></figure><p>你可以在文档中找到一些更具体的减少内存使用的实践。例如：与Dask的分布式XGBoost、XGBoostGPU支持。然而，在深入研究这些之前，要意识到数据副本的创建是一个好的起点。它通常消耗的内存比人们预期的要多得多</p>]]></content>
    
    
    <categories>
      
      <category>参数调优</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>回归模型</tag>
      
      <tag>参数调优</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【原创小诗】完整</title>
    <link href="/%E3%80%90%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F%E3%80%91-%20%E5%AE%8C%E6%95%B4.html"/>
    <url>/%E3%80%90%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F%E3%80%91-%20%E5%AE%8C%E6%95%B4.html</url>
    
    <content type="html"><![CDATA[<center><p>我们本是完整的一个，但是分开了<br></p><p>于是开始有了期待，有了思念 <br></p><p>笑声很轻，眼泪也无声 <br></p><p>对天空诉说着爱是永恒 <br></p><p>跌跌撞撞寻找属于我们的完整 <br></p><p>未来的那一天在心里很重 <br></p><p>两只手里藏着全世界的星星 <br></p><p>在彼此的眼睛里看到了 <br></p><p>海洋陆地，银河苍穹 <br></p><p>北极光在脚下划过 <br></p><p>猎户座的虹手中缤纷 <br></p><p>你说那斑驳的星尘好像我们 <br></p><p>后来所有的时间空间都消失了 <br></p><p>拥抱里我们变成宇宙的最初 <br></p><p>完整 <br></p></center>]]></content>
    
    
    <categories>
      
      <category>生活感悟</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活感悟</tag>
      
      <tag>原创诗歌</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>进栈与出栈-递归案例演示</title>
    <link href="/%E8%BF%9B%E6%A0%88%E4%B8%8E%E5%87%BA%E6%A0%88-%E9%80%92%E5%BD%92%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.html"/>
    <url>/%E8%BF%9B%E6%A0%88%E4%B8%8E%E5%87%BA%E6%A0%88-%E9%80%92%E5%BD%92%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.html</url>
    
    <content type="html"><![CDATA[<p>栈作为一种基本的数据结构，其简单性和高效性使得它在各种计算和编程任务中具有广泛的应用。理解和掌握栈的原理和操作，对于编写高效、可靠的代码至关重要。</p><span id="more"></span><h3 id="什么是栈">什么是栈</h3><p>栈是一种特殊的线性表，仅允许在表的一端进行插入和删除运算。这一端被称为栈顶（top），相对地，把另一端称为栈底（bottom）。向一个栈插入新元素又称作进栈、入栈或压栈（push），它是把新元素放到栈顶元素的上面，使之成为新的栈顶元素；从一个栈删除元素又称作出栈或退栈（pop），它是把栈顶元素删除掉，使其相邻的元素成为新的栈顶元素。所以栈具有“后入先出”的特点（LIFO）。</p><h3 id="栈的作用">栈的作用</h3><p>在程序执行过程中，函数调用是通过栈来管理的。每当一个函数被调用时，会将当前的执行环境（例如局部变量、参数和返回地址）压入栈中。当函数执行完毕后，这些信息会从栈中弹出，恢复之前的执行环境。</p><p>同时在一些回溯算法中，如深度优先搜索、迷宫求解等，使用栈来保存回溯路径，便于在需要时返回上一步。</p><h3 id="递归函数中栈的内存演示">递归函数中栈的内存演示</h3><blockquote><p>这里演示了一个爬台阶的案例，题目要求是有jieshu阶台阶，要求每次可以爬1阶或2阶，求解一共有多少爬法。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">stairs</span>(<span class="hljs-params">jieshu</span>):<br>    <span class="hljs-keyword">if</span> jieshu ==<span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">elif</span> jieshu==<span class="hljs-number">2</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">2</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> (jieshu-<span class="hljs-number">1</span>)+stairs(jieshu-<span class="hljs-number">2</span>)<br><br>stairs(<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><p>上述代码中</p><h4 id="进栈的过程图示为">进栈的过程图示为：</h4><figure><img src="/images/递归进出栈内存图/递归可视化_进栈.png"alt="递归可视化_进栈" /><figcaption aria-hidden="true">递归可视化_进栈</figcaption></figure><h4 id="出栈的过程为">出栈的过程为：</h4><figure><img src="/images/递归进出栈内存图/递归可视化_出栈.png"alt="递归可视化_出栈" /><figcaption aria-hidden="true">递归可视化_出栈</figcaption></figure><h4 id="总的流程示意">总的流程示意：</h4><figure><img src="/images/递归进出栈内存图/递归可视化.png" alt="递归可视化" /><figcaption aria-hidden="true">递归可视化</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>数据结构</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>栈</tag>
      
      <tag>递归</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL数据库基础知识</title>
    <link href="/SQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html"/>
    <url>/SQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html</url>
    
    <content type="html"><![CDATA[<p>数据库操作在日常工作非常常见，以下知识点你都掌握了吗？<span id="more"></span></p><h2 id="内容大纲">内容大纲</h2><ul><li>SQL的相关概述</li><li>环境搭建</li><li>SQL语句分类<ul><li>DDL</li><li>DML</li><li>DCL</li><li>DQL</li></ul></li><li>DDL语句之操作数据库</li><li>DDL语句之操作数据表</li><li>DML语句之操作表数据(增删改)</li><li>DQL语句之操作表数据(查)</li></ul><hr /><h2 id="sql概念">1.SQL概念</h2><p>结构化查询语言(Structured QueryLanguage)简称SQL，是<strong>关系型数据库</strong>管理系统都需要遵循的规范，是数据库认识的语句。不同的数据库生产厂商都支持SQL语句，但都有自己特有内容.</p><h3 id="数据库概念">1.1数据库概念</h3><p>数据库就是存储数据的仓库，其本质是一个文件系统，按照特定的格式将数据存储起来，用户可以对数据库中的数据进行增加，修改，删除及查询(<strong>CURD)</strong>操作。</p><ul><li><p>C: Create, 增</p></li><li><p>U: Update, 改</p></li><li><p>R: Read, 查</p></li><li><p>D: Delete, 删</p></li></ul><h3id="关系型数据库与非关系型数据库">1.2关系型数据库与非关系型数据库</h3><h4 id="关系型数据">关系型数据</h4><p>指采用了<strong>关系模型</strong>来组织数据的数据库。关系模型指的就是<strong>二维表格</strong>模型，而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织。</p><h4 id="非关系型数据">非关系型数据</h4><p>又被称为NoSQL（Not Only SQL)，<strong>意为不仅仅是SQL</strong>，对NoSQL最普遍的定义是“非关联型的”，强调 <strong>Key-Value</strong>的方式存储数据。</p><h3 id="sql常用数据类型">1.3 SQL常用数据类型</h3><p>-- SQL 根据每列值的不同, 数据类型也不同, 常用的如下.</p><ul><li><p>整数: int</p></li><li><p>小数; decimal, float, double</p></li><li><p>字符串: varchar(长度), char(长度)</p></li><li><p>日期: date, datetime</p></li></ul><h2 id="mysql基础语法">2.MySql基础语法</h2><ul><li><p>建议先通过小皮安装MySql数据库,并将mysql.exe的路径添加到path</p></li><li><p>建议通过Pycharm专业版或DataGrip运行MySql相关命令及可视化</p></li></ul><h3 id="sql通用语法">2.1 SQL通用语法</h3><ul><li><ol type="1"><li>SQL语句可以写单行, 也可以写多行, 最后以 分号; 结尾.</li></ol></li><li><ol start="2" type="1"><li>为了阅读方便, 我们可以用 者 空格来隔开SQL语句.</li></ol></li><li><ol start="3" type="1"><li>SQL语句不区分大小写, 为了阅读方便, 建议: 关键字大写, 其它小写.</li></ol></li><li><ol start="4" type="1"><li>SQL的注释写法如下 -- 单行注释 '# 单行注释' /<em> 多行 注释</em>/</li></ol></li></ul><p>-- 5. 我们目前在PyCharm或者DataGrip中写SQL语句, 是选中执行的, 即:不要漏选, 防止出错.</p><h3 id="sql语句分类">2.2 SQL语句分类</h3><ul><li><p><strong>DDL</strong>语句, DataBase Definition Language,数据定义语言</p><blockquote><p>作用对象: <strong>数据库, 数据表, 列的</strong>, 进行: CURD.</p><p><strong>关键字: create, drop, alter, show</strong></p></blockquote></li><li><p><strong>DML</strong>语句, DataBase Manipulation Language,数据操作语言.</p><blockquote><p>作用对象: <strong>表数据的, 进行: 增删改操作</strong>, 统称为:<strong>更新语句</strong></p><p><strong>关键字: insert, delete, update</strong></p></blockquote></li><li><p><strong>DQL</strong>语句, DataBase Query Language,数据查询语言.</p><blockquote><p>作用对象: <strong>表数据的, 进行: 查询操作</strong>.</p><p><strong>关键字: select, from, where...</strong></p></blockquote></li><li><p><strong>DCL</strong>语句, DataBase Control Language,数据控制语言.</p><blockquote><p>作用对象: 设置权限, 访问级别(隔离级别), 创建用户等的...</p></blockquote></li></ul><h2 id="ddl语句">3.DDL语句</h2><h3 id="ddl操作数据库">3.1DDL操作数据库</h3><ul><li><ol type="1"><li><strong>查看</strong>所有的<strong>数据库</strong>.<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">show databases;         # ctrl + 回车, 执行该行代.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li><strong>创建</strong>数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">create database day01 character set &#x27;utf8&#x27;;                 # 创建day01数据库, 采用: utf8 码表.  库不存在就创建, 存在就: 报错.<br>create database if not exists day01 character set &#x27;utf8&#x27;;   # 创建day01数据库, 采用: utf8 码表.  库不存在就创建, 存在就: 啥也不做.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">上述格式, 语法糖1: character <span class="hljs-built_in">set</span> =&gt; 可以写成 charset</span><br>create database day02 charset &#x27;utf8&#x27;;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li><strong>查看</strong>对象数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">show create database day01;     # utf8<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li><strong>修改</strong>数据库码表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">alter database day03 charset =&#x27;gbk&#x27;;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="5" type="1"><li><strong>删除</strong>数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">drop database day01;            # 删除数据库, 如果数据库存在就删除, 不存在就: 报错.<br>drop database if exists day01;  # 删除数据库, 如果数据库存在就删除, 不存在就: 啥也不做.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="6" type="1"><li><strong>应用</strong>数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">use day01; #之后: 建表, 查表, 查数据等操作, 都是基于数据库完成的.<br></code></pre></td></tr></table></figure> ### 3.2DDL操作数据表</li></ol></li><li><ol type="1"><li>查看当前库中, 所有的数据表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">show tables;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>查看表结构. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">show create table student;      # 查看建表的详细过程.<br>describe student;               # 语法糖,  desc student;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li>创建数据表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">create table if not exists student(<br>    sid int primary key,        # 学生id, primary key: 主键约束, 特点为: 唯一, 非空.<br>    name varchar(20) not null,  # 学生姓名, 非空约束(即: 不能为空)<br>    gender varchar(2),          # 学生性别<br>    age int                     # 学生年龄, 整数.<br>);<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li>删除数据表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">drop table if exists student;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="5" type="1"><li>修改表(名字) <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式: rename table 旧表名 to 新表名;</span><br>rename table student to stu;<br></code></pre></td></tr></table></figure> ### 3.3 DDL操作列</li></ol></li><li><ol type="1"><li>查看表结构. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">desc stu;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>给表新增一列, desc varchar(200), 非空约束. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">alter table stu add `desc` varchar(200) not null;       # 如果列名和关键字重名, 记得用 反引号包裹.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li>修改表的字段(列), 只修改: 数据类型, 约束. 将desc列改为: int类型..<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">alter table stu modify `desc` int;      # 因为没有加非空约束, 所以本次会认为, 不要非空约束了, 即: 会删除它.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li>修改表的字段(列), 修改: 列名, 数据类型, 约束. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">-- 格式: alter table 表名 change 旧列名 新列名 数据类型 约束;<br>alter table stu change `desc` address varchar(10) not null;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="5" type="1"><li>删除表的字段 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式: alter table 表名 drop 旧列名;</span><br>alter table stu drop address;<br></code></pre></td></tr></table></figure> ## 4 DML语句 ### 4.1添加数据</li></ol></li><li><ol type="1"><li>查看表数据, 这个数据DQL语句, 先用一下, 稍后详解. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">select * from stu;<br><span class="hljs-meta prompt_"># </span><span class="language-bash">查看表结构.</span><br>desc stu;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>添加表数据. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">场景1: 添加单条数据, 格式为: insert into 表名(列名1, 列名2, 列名3...) values(值1, 值2, 值3...);</span><br>insert into stu(sid, name, gender, age) values (1, &#x27;乔峰&#x27;, null, 38);<br><br>insert into stu(sid, name, gender, age) values (2, null, null, 38);     # 报错, name列有非空约束, 不能为null<br>场景2:添加多条数据, 格式为: insert into 表名(列名1, 列名2, 列名3...) values(值1, 值2, 值3...), (...), (...);<br>insert into stu(sid, name, gender, age)<br>values<br>    (2, &#x27;虚竹&#x27;, null, 26),<br>    (3, &#x27;段誉&#x27;, &#x27;男&#x27;, 21),<br>    (4, &#x27;阿朱&#x27;, &#x27;女&#x27;, 35),<br>    (5, &#x27;梦姑&#x27;, &#x27;女&#x27;, 23),<br>    (6, &#x27;钟灵儿&#x27;, &#x27;女&#x27;, 19);<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li>上述格式的变形版. 不一定非得是全列名, 只要值的个数, 类型 和列名的个数, 类型保持一致即可. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">insert into stu(sid, name) values (7, &#x27;木婉清&#x27;);<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li>上述格式的语法糖, 掌握, 实际开发一般是用这个.. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">insert into stu values (8, &#x27;鸠摩智&#x27;, &#x27;男&#x27;, 49);      # 如果不写列名, 则默认是: 全列名, 需要给每一个列都要传入值.<br></code></pre></td></tr></table></figure> ###4.2DML修改数据</li></ol></li><li><ol type="1"><li>修改 sid为3的数据, 姓名为: 段氏小王子, 渣男 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">update stu set name=&#x27;段氏小王子&#x27;, gender=&#x27;渣男&#x27; where sid = 3;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>危险操作, 修改数据时, 没有写 where条件,则会一次性修改表中所有的数据. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">update stu set name=&#x27;段氏小王子&#x27;, gender=&#x27;渣男&#x27;;<br></code></pre></td></tr></table></figure> ### 4.3DML删除数据</li></ol></li><li><ol type="1"><li>正常删除数据, 删除id &gt; 3的数据.(主键ID不变) <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">delete from stu where sid &gt; 3;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>删除数据, 删除id &gt; 3的数据.(主键ID改变) <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">truncate table stu where sid &gt; 3;<br></code></pre></td></tr></table></figure> ##5.备份表数据</li></ol></li><li><ol type="1"><li>场景1: 备份表不存在. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">create table stu_tmp select * from stu;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>场景1: 备份表存在. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">insert into hero_tmp select * from hero;<br></code></pre></td></tr></table></figure></li></ol></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL之DQL详解</title>
    <link href="/SQL%E4%B9%8BDQL%E8%AF%A6%E8%A7%A3.html"/>
    <url>/SQL%E4%B9%8BDQL%E8%AF%A6%E8%A7%A3.html</url>
    
    <content type="html"><![CDATA[<p>SQL数据库中的查询语句整理 <span id="more"></span></p><img src="/images/00194-3462269573.jpeg" /><table><thead><tr class="header"><th><h3 id="基础查询">基础查询</h3></th><th></th></tr></thead><tbody><tr class="odd"><td>Select * from 数据表;</td><td>查看所有数据</td></tr><tr class="even"><td>Select 字段1,字段2 from 数据表;</td><td>查看字段</td></tr><tr class="odd"><td>select 地段名 as 字段别名 from 数据表;</td><td>别名查询</td></tr><tr class="even"><td>select distinct 地段名 from 数据表;</td><td>去重查询</td></tr><tr class="odd"><td><h3 id="运算符筛选">运算符筛选</h3></td><td></td></tr><tr class="even"><td>select * from 表名 where 字段名 = '字段值';</td><td>筛选某字段值的行信息</td></tr><tr class="odd"><td>select * from 表名 where 字段名 != '字段值';</td><td>筛选不含某字段值的行信息</td></tr><tr class="even"><td>select * from 表名 where 字段名 &gt; 字段值;</td><td>筛选显示大于字段值的行</td></tr><tr class="odd"><td>select * from 表名 where 字段名 in (a,b);</td><td>筛选显示包含a,b值的行</td></tr><tr class="even"><td>select * from 表名 where 字段名 <strong>between</strong> 20<strong>and</strong> 80;</td><td>筛选显示介于20 - 80之间的行</td></tr><tr class="odd"><td>select * from 表名 where 字段名=a <strong>and</strong> 字段名&gt;b;</td><td>交集条件</td></tr><tr class="even"><td>select * from 表名 where 字段名=a <strong>or</strong> 字段名&gt;b;</td><td>并集条件</td></tr><tr class="odd"><td>select * from 表名 where 字段名 like '_值%';</td><td>近似查询,_占位符,%任意字符</td></tr><tr class="even"><td>select * from 表名 where 字段名 is null;</td><td>空字段查询</td></tr><tr class="odd"><td>select * from 表名 where 字段名 is not null;</td><td>非空字段查询</td></tr><tr class="even"><td><h3 id="排序">排序</h3></td><td></td></tr><tr class="odd"><td>select * from 表名 order by 字段名 asc | desc;</td><td>排序</td></tr><tr class="even"><td>select * from 表名 order by 字段名1 asc,字段名2 desc</td><td>多重排序</td></tr><tr class="odd"><td><h3 id="聚合与分组">聚合与分组</h3></td><td></td></tr><tr class="even"><td><p>select 聚合函数 from 表名 where 字段名xxx;</p><p>聚合函数为:count(),max(),min(),sum(),avg()</p><p>xxx为筛选条件</p></td><td>相关条件值下的统计值</td></tr><tr class="odd"><td><p></p><p>select</p><p><strong>分组字段</strong>, 聚合函数(<strong>count(*)</strong>)...</p><p>from</p><p>数据表名</p><p>where</p><p>组前筛选</p><p><strong>group by</strong></p><p><strong>分组字段</strong></p><p>having</p><p>组后筛选;</p></td><td><p>分组查询</p><p>一般结合聚合函数一起用, 否则: 无意义</p><p></p><p>where: 组前筛选, 后边不能跟: 聚合函数.</p><p>having: 组后筛选, 后边可以跟: 聚合函数.</p></td></tr><tr class="even"><td><h3 id="分页查询">分页查询</h3></td><td></td></tr><tr class="odd"><td><p>select * from 表名 limit 起始索引, 数据条数;</p><p>注:索引从0开始,从0开始则0可以省略不写</p><p>经验:总页数:=(总条数 + 每页的数据条数 - 1) // 每页的数据条数</p></td><td>分页查询较为常用,有效减少服务器/用户压力</td></tr><tr class="even"><td><h3 id="重分类查询">重分类查询</h3></td><td></td></tr><tr class="odd"><td><p>select case</p><p>when 条件1 then 重命名值</p><p>when 条件2 then 重命名值2</p><p>.....</p><p>else 重命名值3</p><p>end as 字段名,</p><p>from 数据表</p></td><td></td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>查询语句</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo-fluid数学公式显示问题处理</title>
    <link href="/hexo_fluid%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86.html"/>
    <url>/hexo_fluid%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86.html</url>
    
    <content type="html"><![CDATA[<p>在此补充一下之前公式不显示的问题。</p><span id="more"></span><p>虽然<ahref="https://hexo.fluid-dev.com/docs/">Fluid</a>主题支持<strong>LaTeX数学公式</strong>，但是需要手动操作，而且我按照<ahref="https://hexo.fluid-dev.com/docs/guide/#latex-数学公式">教程</a>开启本功能<code>mathjax</code>没有成功，即公式在网页里并没有被渲染和转换。通过网上查找，发现解决这类问题的思路主要是换渲染引擎，例如<code>pandoc</code>、<code>mathjax</code>、<code>katex</code>。我目前使用<code>mathjax</code>，操作如下：</p><ul><li><p><strong>卸载</strong>默认引擎，并<strong>安装</strong>这个新的渲染引擎</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ada">$ npm uninstall hexo-renderer-marked <span class="hljs-comment">--save </span><br>$ npm install hexo-renderer-kramed <span class="hljs-comment">--saveCopy</span><br></code></pre></td></tr></table></figure></li><li><p>修改<code>/node_modules/hexo-renderer-kramed/lib/renderer.js</code></p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs arcade"><span class="hljs-comment">// Change inline math rule</span><br><span class="hljs-keyword">function</span> <span class="hljs-title function_">formatText</span>(<span class="hljs-params">text</span>) &#123;<br>  <span class="hljs-comment">// Fit kramed&#x27;s rule: $$ + \1 + $$</span><br>  <span class="hljs-comment">// 直接返回text</span><br>  <span class="hljs-comment">// return text.replace(/`\$(.*?)\$`/g, &#x27;$$$$$1$$$$&#x27;);</span><br>  <span class="hljs-keyword">return</span> <span class="hljs-built_in">text</span>;<br>&#125;Copy<br></code></pre></td></tr></table></figure></li><li><p>修改hexo的渲染源码<code>/node_modules/kramed/lib/rules/inline.js</code></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs r"><span class="hljs-operator">/</span><span class="hljs-operator">/</span> 去掉`\\`的额外转义，第<span class="hljs-number">11</span>行，将其修改为<br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> escape<span class="hljs-operator">:</span> <span class="hljs-operator">/</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">\</span>`*&#123;&#125;\[\]()# +\-.!_&gt;])/, <br>escape: /^\\([`<span class="hljs-operator">*</span><span class="hljs-punctuation">&#123;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">)</span><span class="hljs-comment"># +\-.!_&gt;])/,</span><br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> 将em标签对应的符号中，去掉`_`，第<span class="hljs-number">20</span>行，将其修改为<br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> em<span class="hljs-operator">:</span> <span class="hljs-operator">/</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span>b_<span class="hljs-punctuation">(</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">:</span>__<span class="hljs-operator">|</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span>s<span class="hljs-punctuation">\</span>S<span class="hljs-punctuation">]</span><span class="hljs-punctuation">)</span><span class="hljs-operator">+</span><span class="hljs-operator">?</span><span class="hljs-punctuation">)</span>_<span class="hljs-punctuation">\</span>b<span class="hljs-operator">|</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">:</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-operator">|</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span>s<span class="hljs-punctuation">\</span>S<span class="hljs-punctuation">]</span><span class="hljs-punctuation">)</span><span class="hljs-operator">+</span><span class="hljs-operator">?</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">!</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">)</span><span class="hljs-operator">/</span><span class="hljs-punctuation">,</span>    <br>em<span class="hljs-operator">:</span> <span class="hljs-operator">/</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">:</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-operator">|</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span>s<span class="hljs-punctuation">\</span>S<span class="hljs-punctuation">]</span><span class="hljs-punctuation">)</span><span class="hljs-operator">+</span><span class="hljs-operator">?</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">!</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">)</span><span class="hljs-operator">/</span><span class="hljs-punctuation">,</span>Copy<br></code></pre></td></tr></table></figure></li><li><p>停止使用 <code>hexo-math</code>，安装<code>hexo-renderer-mathjax</code></p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-symbol">$</span> npm uninstall hexo-math --save<br><span class="hljs-comment">// 不知道是不是必要的</span><br><span class="hljs-symbol">$</span> npm install hexo-renderer-mathjax --saveCopy<br></code></pre></td></tr></table></figure></li><li><p>更新 <code>Mathjax</code> 的 <code>CDN</code>链接，打开<code>/node_modules/hexo-renderer-mathjax/mathjax.html</code>，在最后一行添加js：</p><ul><li>网上推荐的上面这个，但我使用失败了</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml">// <span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span>Copy<br></code></pre></td></tr></table></figure><ul><li>推荐下面这个，亲测可行，不过偶尔出问题，需要多部署几次就ok</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span>Copy<br></code></pre></td></tr></table></figure><ul><li><strong>更新于2020年6月6日</strong>：如果有人看到这，可以注意下<code>MathJax.js</code>版本已经到3.0.5了，参照mathjax<ahref="https://www.npmjs.com/package/mathjax#installation-and-use">文档</a>，那么现在的上面的一步可以自行修改，如果控制台报错可以到<ahref="https://cdn.jsdelivr.net/npm/mathjax@3/es5/">mathjax CDNfiles</a>下找到合适的js代替</li></ul><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs applescript">&lt;<span class="hljs-keyword">script</span> <span class="hljs-built_in">id</span>=<span class="hljs-string">&quot;MathJax-script&quot;</span> async<br>  src=<span class="hljs-string">&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js&quot;</span>&gt;<br>&lt;/<span class="hljs-keyword">script</span>&gt;Copy<br></code></pre></td></tr></table></figure><ul><li>当然，如果博客的<strong>内部静态</strong>文件<strong>第三方库</strong>包含了mathjax，上面的<code>MathJax.js</code>不用导入都行，导的不对甚至有冲突，虽然不影响公式的显示，但会在控制台报错。</li></ul></li></ul><p>经过<strong><ahref="https://github.com/Ningsir">Ningsir</a></strong>提醒，删除掉hexo-renderer-mathjax就行了，简单省事。</p><ul><li><p>按照<a href="https://hexo.fluid-dev.com/docs/">Fluid</a>的<ahref="https://hexo.fluid-dev.com/docs/guide/#快速开始">快速开始</a>，需要修改<strong>主题配置</strong>，打开<code>/source/_data/fluid_config.yml</code>文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">post:</span><br>  <span class="hljs-attr">math:</span>  <br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span>  <br>    <span class="hljs-attr">specific:</span> <span class="hljs-literal">false</span>   <br>    <span class="hljs-attr">engine:</span> <span class="hljs-string">mathjaxCopy</span><br></code></pre></td></tr></table></figure></li><li><p>在根目录下修改<code>_config.yml</code>，添加</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">mathjax: <span class="hljs-literal">true</span>Copy<br></code></pre></td></tr></table></figure></li><li><p>在<code>Front-matter</code>中打开<code>MathJax</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br>  <span class="hljs-attr">layout:</span> <span class="hljs-string">post</span><br>  <span class="hljs-attr">title:</span> <span class="hljs-string">title</span><br>  <span class="hljs-attr">date:</span> <span class="hljs-string">date</span><br>  <span class="hljs-attr">categories:</span> <br>  <span class="hljs-bullet">-</span> <span class="hljs-string">categories</span><br>  <span class="hljs-attr">tags:</span> <br>  <span class="hljs-bullet">-</span> <span class="hljs-string">tags1</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">tags2</span><br>  <span class="hljs-attr">mathjax:</span> <span class="hljs-literal">true</span><br><span class="hljs-string">---Copy</span><br></code></pre></td></tr></table></figure></li><li><p>显示数学公式 <span class="math display">\[\Sigma({n} ;{p})=\left\{\left(\zeta_{1}, \ldots, \zeta_{r}\right) \in\mathbb{C}^{n_{1}} \times \cdots \times \mathbb{C}^{n_{r}}:\sum_{k=1}^{r}\left\|{\zeta}_{k}\right\|^{2 p_{k}} &lt;1\right\}\]</span></p></li></ul><p>最后如果公式还是乱码可以尝试重启电脑，然后先尝试部署一下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hexo clean&amp;&amp;hexo g&amp;&amp;hexo d&amp;&amp;hexo s<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客维护</tag>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux常用命令整理</title>
    <link href="/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86.html"/>
    <url>/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86.html</url>
    
    <content type="html"><![CDATA[<p>整理了一下linux常用的一些命令 <span id="more"></span> ## 基本格式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>命令名 [-选项] [参数]# 有些命令要选项和参数, 有些不需要. 这里的[]表示可选项. <br></code></pre></td></tr></table></figure><h2 id="文件目录操作">文件目录操作</h2><h4 id="ls命令">2.ls命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">ls</span>命令, 来源于: list(列表)  即: 查看指定目录下所有的子级(不包括子级的子级)</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>ls [-a -l -h] [Linux的路径]<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">参数解释</span><br>-a显示所有(包括隐藏的) all<br>-l以行的形式展示详细信息 line<br>-h以人性化的方式展示.   human<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">例如:</span> <br>ls# 查看当前目录的子级, 不包括隐藏.<br>ls /# 查看根目录(/)下的内容.<br>ls -a # 查看当前目录的子级, 包括隐藏.<br>ls -l# 以行的方式, 查看当前目录的子级. 简写形式: ll<br>ls -h# 以人性化的方式展示当前目录的内容, 但是: 无效果.<br>ls -lh# 行的方式, 人性化展示当前目录下的内容. 简写形式:  ll -h<br>ls -al# 以行的形式, 展示当前目录下所有子级(包括 隐藏)<br>ls -alh # 以行, 人性化的方式展示当前目录下所有子级(包括 隐藏)<br></code></pre></td></tr></table></figure></p><h4 id="cd命令">3.cd命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">cd</span>命令, 来源于: change directory, 改变目录</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>cd 要切换到的目录的路径<br></code></pre></td></tr></table></figure></p><h4 id="pwd命令">4.pwd命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">来源于 Print Work Directory</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>pwd # 查看当前所在的工作目录,  即: 当前在Linux的哪个路径下. <br></code></pre></td></tr></table></figure></p><h4 id="linux中的路径写法">5.Linux中的路径写法</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">路径介绍</span><br>就是用来描述文件 或者 文件夹(目录)的路径的, 有: 绝对路径 和 相对路径两种写法.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">绝对路径</span><br>以 / 根目录开头.   <br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">相对路径</span><br>默认是相对于当前路径来写的. <br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">例如: 当前目录是在 /aa/bb  如果相切换到 /aa/bb/cc目录, 有如下两种写法.</span><br>绝对路径:   cd /aa/bb/cc<br>相对路径:   cd cc<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">几个特殊的路径写法</span><br>./# 代表当前路径, 例如: 上述的 cd cc 还可以写成 cd ./cc<br>..# 代表上级路径<br>../..# 代表上上级路径<br>~# 代表: 回到家目录, root账号的家 /root,  其它账号的家 /home/账号名<br><span class="hljs-meta prompt_"># </span><span class="language-bash">语法糖, 可以直接写 <span class="hljs-built_in">cd</span> 也是回家命令.</span><br>-# 代表: 在最近的两个目录之间做切换.<br></code></pre></td></tr></table></figure></p><h4 id="mkdir命令">6.mkdir命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">来源于 make directory, 创建目录(文件夹)的.</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>mkdir [-p] 文件夹路径# -p表示parent, 即: 父目录不存在, 也会自动创建.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">简单理解, 假设: 目前只有 /root/aa 文件夹</span><br>mkdir /root/aa/bb/cc# 报错, 因为不写-p, 只能创建单级文件夹.<br>mkdir -p /root/aa/bb/cc# 不报错, 加上-p可以创建多级目录.<br></code></pre></td></tr></table></figure></p><h4 id="文件相关">7.文件相关</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">touch</span>创建文件的.</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>touch 文件路径1 文件路径2...# 可以同时创建多个文件.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">cat</span>查看文件内容的</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>cat文件路径# 一次性查看文件所有内容, 如果内容较多, 会翻页, 只留最后一页.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">more查看文件内容的, 可以分页查看.</span><br>more 文件路径# 以分页的形式查看文件内容.<br><span class="hljs-meta prompt_"># </span><span class="language-bash">空格向下翻一页</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">d  down的意思, 向下翻半页</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">enter 向下翻一行</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">b  back, 向上翻一页.</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">q     quit, 表示退出.  也可以按下 ctrl + 字母C</span><br></code></pre></td></tr></table></figure></p><h2 id="文件和文件夹相关命令">文件和文件夹相关命令</h2><h4 id="cp命令-来源于-copy单词-可以拷贝-文件-文件夹">8.cp命令, 来源于copy单词, 可以拷贝 文件, 文件夹</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"> # </span><span class="language-bash">格式</span><br>cp [-r] 数据源 目的地# -r表示recursive(递归), 即: 拷贝文件夹时, 要写. <br>cp -r /root/aa /root/test<br><br>[root@linxkon ~]# cd /root/<br>[root@linxkon ~]# ls<br>2.avi  3.jpg  4.mp3  aa  anaconda-ks.cfg  a.txt<br>[root@linxkon ~]# mkdir lk<br>[root@linxkon ~]# <br>[root@linxkon ~]# cp a.txt lk# 拷贝<br>[root@linxkon ~]# ls<br>2.avi  3.jpg  4.mp3  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# ls lk/<br>a.txt<br>[root@linxkon ~]# <br>[root@linxkon ~]# cp 2.avi lk/abc.avi# 拷贝, 并改名<br>[root@linxkon ~]# ls lk/<br>abc.avi  a.txt<br>[root@linxkon ~]# cp aa lk# 报错, 拷贝文件夹必须夹-r, 递归拷贝.<br>cp: 略过目录&quot;aa&quot;<br>[root@linxkon ~]# cp -r aa lk# 拷贝文件夹<br>[root@linxkon ~]# ls lk/<br>aa  abc.avi  a.txt<br></code></pre></td></tr></table></figure></p><h4 id="mvmove剪切移动重命名">9.mv（move）剪切移动/重命名</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>mv 数据源 目的地# 注意: 如果是同级路径, 就是改名.<br><br>[root@linxkon ~]# ls<br>2.avi  3.jpg  4.mp3  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# ls lk/<br>aa  abc.avi  a.txt<br>[root@linxkon ~]# <br>[root@linxkon ~]# mv 3.jpg lk/# 剪切文件<br>[root@linxkon ~]# ls lk/<br>3.jpg  aa  abc.avi  a.txt<br>[root@linxkon ~]# ls<br>2.avi  4.mp3  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# <br>[root@linxkon ~]# <br>[root@linxkon ~]# mv 4.mp3 lk/好日子.xyz# 剪切(文件)并改名<br>[root@linxkon ~]# ls<br>2.avi  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# ls lk/<br>3.jpg  aa  abc.avi  a.txt  好日子.xyz<br><br>[root@linxkon ~]# mkdir xyz<br>[root@linxkon ~]# ls<br>2.avi  aa  anaconda-ks.cfg  a.txt  lk  xyz<br>[root@linxkon ~]# mv aa xyz# 剪切文件夹, 无需加: -r<br>[root@linxkon ~]# ls<br>2.avi  anaconda-ks.cfg  a.txt  lk  xyz<br>[root@linxkon ~]# ls xyz/<br>aa<br><br><br>[root@linxkon ~]# ls<br>[root@linxkon ~]# touch 1.txt<br>[root@linxkon ~]# <br>[root@linxkon ~]# mv 1.txt abc.txt# 改名操作<br>[root@linxkon ~]# ls<br>abc.txt<br></code></pre></td></tr></table></figure><h4 id="rm命令-来源于-remove单词-可以删除-文件-文件夹">10.rm命令, 来源于remove单词, 可以删除 文件, 文件夹</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">rm [-r -f] 要删除的文件或者文件夹路径# -r:递归,  -f: force(强制)<br><br>[root@linxkon ~]# rm -rf lk# 强制删除 lk文件夹, 且不询问<br>[root@linxkon ~]# ls<br>anaconda-ks.cfg  xyz<br>[root@linxkon ~]# touch 1.txt 2.txt 3.avi 4.avi 5.jpg<br>[root@linxkon ~]# ls<br>1.txt  2.txt  3.avi  4.avi  5.jpg  anaconda-ks.cfg  xyz<br>[root@linxkon ~]# rm -rf *.txt<br>[root@linxkon ~]# ls<br>3.avi  4.avi  5.jpg  anaconda-ks.cfg  xyz<br>[root@linxkon ~]# rm -rf *# 清空当前文件夹<br>[root@linxkon ~]# ls<br>[root@linxkon ~]# rm -rf /*  ^C# 慎用<br></code></pre></td></tr></table></figure><h4 id="一个坐牢命令">11.一个坐牢命令</h4><figure class="highlight shell"><figcaption><span>rm -rf</span><a href="/*">link</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">rm -rf /* #删除系统<br></code></pre></td></tr></table></figure><h2 id="查找命令">查找命令</h2><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">### 12.which命令,  查找Linux可执行命令 的路径的.</span></span> <br>  which ls# /usr/bin/ls<br>  which pwd# /usr/bin/pwd<br>  <br>  which ifconfig# /usr/sbin/ifconfig<br>  <br><span class="hljs-meta prompt_">  </span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">### 13.find命令, 根据文件名, 或者 文件大小查找指定文件.</span></span><br><span class="hljs-meta prompt_">  # </span><span class="language-bash">格式</span><br>  find 要被检索的目录路径 -name &#x27;要检索的文件名&#x27;<br>  <br>  find / -name &#x27;abc*&#x27;# 查找Linux中, 以abc开头的内容.<br>  <br><span class="hljs-meta prompt_">  # </span><span class="language-bash">格式</span><br>  find 要被检索的目录路径 -size +100M# 超过100MB,  -10K, 小于10KB<br>  <br>  find / -size +100M# 查找Linux中, 文件大小超过100M的文件.<br></code></pre></td></tr></table></figure></p><p>—————————————————华丽的分割线—————————————————<img src="/images/2024年5月10日genk.jpg" title="毕加索" alt="dolor"></p>]]></content>
    
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>基础命令</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo博客搭建教程</title>
    <link href="/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B.html"/>
    <url>/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B.html</url>
    
    <content type="html"><![CDATA[<p>hexo + github 搭建你的静态博客 <span id="more"></span> #一，搭建前的软件准备（git，node）</p><blockquote><p>搭建之前需要准备的软件： Git：官网下载：https://git-scm.com/ Node.js官网下载：http://nodejs.cn/</p></blockquote><h1 id="二-安装hexo完成简单本地页面展示">二，安装hexo，完成简单本地页面展示</h1><p>1.进入cmd窗口输入指令：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm">npm install -g hexo-<span class="hljs-keyword">cli</span><br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/d6d9c791b8f449fea0b64b1a72bd32b2.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>2.你可以先创建一个文件夹myblog，然后cd到这个文件夹下（或者在这个文件夹下直接右键gitbash打开）。 <imgsrc="/images/hexo博客搭建教程/5b5554d54f20471098768040624585ba.png"alt="在这里插入图片描述" /></p><p>接下来初始化一下hexo</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs csharp">hexo <span class="hljs-keyword">init</span><br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/6010660448004f25871ce6b5a479f832.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>3.查看是否能启动成功</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clike">hexo s<br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/4df56b5dc3ce40a2a2c9dc99585fd8ed.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><blockquote><p>新建完成后，指定文件夹目录下有： node_modules: 依赖包public：存放生成的页面 scaffolds：生成文章的一些模板source：用来存放你的文章 themes：主题 **_config.yml:博客的配置文件**</p></blockquote><p>4.复制网址打开</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clike">http://localhost:4000/<br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/2bcb9a6a09024ce68d095901dc1ca203.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>这是初始界面，我们需要部署到github上。</p><p>ctrl+C可以停止；</p><h1 id="三将hexo部署到github">三，将Hexo部署到Github</h1><h2 id="github创建个人仓库">1.Github创建个人仓库</h2><blockquote><p>首先，需要有一个github账号。登上账号后建一个仓库：仓库名为你的用户名.github.io，举例如下： 创建一个和你用户名相同的仓库，后面加.github.io，只有这样，将来要部署到GitHub的时候，才会被识别，也就是xxxx.github.io，其中xxx就是你注册GitHub的用户名.</p></blockquote><figure><img src="/images/hexo博客搭建教程/a697d02a363e48e08d07854051642860.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h2 id="生成ssh添加到github">2.生成ssh添加到Github</h2><blockquote><p>在Github上创建仓库完成之后，需要设置ssh免密登录</p></blockquote><p>1.打开cmd窗口：执行如下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clike">git config --global user.name &quot;yourname&quot;<br>git config --global user.email &quot;youremail&quot;<br></code></pre></td></tr></table></figure><p>这里的yourname输入你的GitHub用户名，youremail输入你GitHub的邮箱。这样GitHub才能知道你是不是对应它的账户。用户名为仓库的名称，邮箱为注册github的邮箱，举例如下：</p><figure><img src="/images/hexo博客搭建教程/ef3ce50ba8ce4e39bb4dd1387a9da316.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>防止输错可以检查：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clike">git config user.name<br>git config user.email<br></code></pre></td></tr></table></figure><p>2.接着进入到家目录：C:，右击打开git bash 。 输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clike">ssh-keygen -t rsa -C 2412757158@qq.com<br></code></pre></td></tr></table></figure><p>后面是自己注册github的邮箱，然后敲三次回车，</p><figure><img src="/images/hexo博客搭建教程/b07cadba4a484a7eac9c19884ea6f3b5.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>3.接着就会发现C:.ssh目录，打开后有一个公钥，一个私钥。id_rsa.pub是公钥，我们需要打开它，复制里面的内容。</p><p>然后进入github：</p><p>点击setings <imgsrc="/images/hexo博客搭建教程/2f3217c541b94d59bc17c3d8119e8801.png"alt="在这里插入图片描述" /></p><p>进行以下操作</p><p><imgsrc="/images/hexo博客搭建教程/a1def242038c4c77b125d0c0b597f987.png"alt="在这里插入图片描述" />发现我们需要一个密钥，把我们刚刚复制的密钥粘进去，title随便起</p><p><imgsrc="/images/hexo博客搭建教程/821106b4621d4a1a91cfc2f1510abd99.png"alt="在这里插入图片描述" /> 点击 Add SSH Key</p><h2 id="进行部署">3.进行部署</h2><blockquote><p>这一步，我们就可以将hexo和GitHub关联起来，也就是将hexo生成的文章部署到GitHub上，打开站点配置文件_config.yml，翻到最后，修改为 YourgithubName就是你的GitHub账户</p></blockquote><p>1.修改配置文件 <imgsrc="/images/hexo博客搭建教程/c87432a9b49c4552b931c51e0e94e61d.png"alt="在这里插入图片描述" /></p><p>修改内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs clike">deploy:<br>  type: git<br>  repo: git@github.com:goubin18/goubin18.github.io.git<br>  branch: main<br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/7955e295748647388285871fcf65b511.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p><strong>注意：后面有空格</strong></p><p><strong>repo：获取步骤如下</strong></p><p><strong>点进自己刚刚创建的仓库，复制</strong></p><p><strong><imgsrc="/images/hexo博客搭建教程/a8b5f30ed44448b88f759faf8f104ecb.png"alt="在这里插入图片描述" /></strong></p><p><strong>2.找到自己的博客路径打开</strong></p><p><strong><imgsrc="/images/hexo博客搭建教程/fa09bd6a0d7448deb1ce16a424e0c987.png"alt="在这里插入图片描述" /></strong></p><p><strong>这个时候需要先安装deploy-git，也就是部署的命令,这样你才能用命令部署到GitHub。</strong></p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">npm install hexo-deployer-git <span class="hljs-comment">--save</span><br></code></pre></td></tr></table></figure><p><strong><imgsrc="/images/hexo博客搭建教程/a4ff5a3aed8443d6b98b366fa63e724d.png"alt="在这里插入图片描述" /></strong></p><p><strong>2.然后依次执行以下命令：</strong></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs verilog">hexo c   #清除缓存文件 db<span class="hljs-variable">.json</span> 和已生成的静态文件 public<br>hexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo <span class="hljs-keyword">generate</span> 的缩写)<br>hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)<br></code></pre></td></tr></table></figure><p><strong>注意deploy时会让输个yes</strong></p><p><strong><em>*最后回到github上查看自己的仓库：*</em></strong></p><p><imgsrc="/images/hexo博客搭建教程/5a62c4630f164385831ad449065b5b03.png"alt="在这里插入图片描述" /> 这就表示上传成功。</p><p>现在就可以使用xxx.github.io来访问你的博客啦例如：我的用户名是linxkon，那么我的博客地址就是<code>linxkon.github.io</code></p><p>举例如下：</p><figure><img src="/images/hexo博客搭建教程/9eefc08e36464040bc8fbe8d9716073b.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h1 id="写在最后">写在最后：</h1><blockquote><p>现在简单的博客已经搭建完成了 现在你的个人网站的地址是xxx.github.io，如果觉得这个网址配不上帅气多金的你，你就可以设置个人域名了。但是需要花钱。小提示： 操作要细心，如果出现了问题可以私信留言，大家一起想办法！</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客搭建</tag>
      
      <tag>知识管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>集成学习常见模型比对</title>
    <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A6%81%E7%82%B9%E5%AF%B9%E6%AF%94.html"/>
    <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A6%81%E7%82%B9%E5%AF%B9%E6%AF%94.html</url>
    
    <content type="html"><![CDATA[<p>集成学习的基础思想是通过组合多个基学习器形成整体强学习器,使基学习器在预测准确性、降低过拟合风险、增强模型的鲁棒性等方面获得明显提升,集成学习主要包含Bagging和Boosting两大分类。本文对比总结了四种集成学习常见模型。</p><span id="more"></span><p>Bagging是一种并行式的集成学习方法，其特点包括通过有放回的抽样产生不同的训练集，从而训练多个不同的学习器，并通过平均投票或多数表决的方式决定预测结果。此外，Bagging允许弱学习器并行训练，代表算法包括随机森林算法。</p><p>Boosting是一种串行式的集成学习方法，其特点是随着学习的积累从弱到强，每加入一个弱学习器，整体能力会得到提升。Boosting对学习器进行加权投票，采用串行方式进行学习，具有明确的先后顺序。代表算法包括Adaboost、GBDT、XGBoost以及LightGBM。</p><p><img src="/images/集成学习常见模型对比/集成学习对比示意.png" /></p><table><thead><tr class="header"><th>模型</th><th>核心要点</th><th>模型优缺点</th><th>模型应用</th></tr></thead><tbody><tr class="odd"><td>Bagging随机森林</td><td>1. 随机有放回的抽样产生不同的训练集(boostrap)<br>2.基于不同抽样训练多个基学习器（如决策树）<br>3.通过投票或平均组合预测结果</td><td>优点:<br>- 泛化错误率低<br>-易于并行训练<br>缺点:<br>-性能上限低</td><td>1. 分类问题<br>2. 回归问题</td></tr><tr class="even"><td>Adaptive Boosting</td><td>1. 迭代构建弱学习器<br>2.聚焦错误样本,每轮根据分类结果调整样本及模型权重<br>3.组合加权弱学习器成强学习器</td><td>优点:<br>- 泛化能力强<br>- 易于处理多种数据<br>缺点:<br>-对离群点敏感<br>- 需要预处理高维或不平衡数据</td><td>1. 分类问题<br>2. 图像识别</td></tr><tr class="odd"><td>GBDT (梯度提升树)</td><td>1. 迭代构建决策树<br>2. 拟合损失函数的负梯度训练新树<br>3.累加方式构建最终模型</td><td>优点:<br>- 准确性高<br>- 可以适应多种损失函数<br>缺点:<br>-容易过拟合<br>- 计算量大</td><td>1. 回归问题<br>2. 排名问题<br>3. 分类问题</td></tr><tr class="even"><td>XGBoost</td><td>1. 基于GBDT的高效实现<br>2. 加入正则化项解决GBDT过拟合问题<br>3.损失函数泰勒二阶近似优化拟合函数<br/>4.支持并行化和缺失值处理</td><td>优点:<br>- 速度快<br>- 准确性高,防止过拟合<br>-支持多种目标函数和评估指标<br>缺点:<br>- 参数调整复杂<br>-可能需要更多的内存</td><td>1. 赢取竞赛的首选算法<br>2. 排名问题<br>3. 分类和回归问题</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>机器学习</tag>
      
      <tag>集成学习</tag>
      
      <tag>总结归纳</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模型调优指南--过拟合</title>
    <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97--%E8%BF%87%E6%8B%9F%E5%90%88.html"/>
    <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97--%E8%BF%87%E6%8B%9F%E5%90%88.html</url>
    
    <content type="html"><![CDATA[<p>过拟合是机器学习模型在训练数据上表现很好，但在新数据上表现不佳的一种现象。它的发生通常是由于模型过于复杂，以至于能够记住训练数据的噪声和细节，而不是学习到数据的普遍模式和特征。以下是导致过拟合的常见原因以及相应的解决方法：<span id="more"></span></p><h3 id="过拟合的原因">过拟合的原因</h3><ol type="1"><li><strong>模型复杂度过高</strong>：模型参数过多（如深层神经网络的层数和节点数过多），导致模型具有很强的表达能力，能够拟合训练数据中的噪声。</li><li><strong>训练数据不足</strong>：训练数据量太少，使得模型只能依赖于有限的数据，容易记住而不是泛化。</li><li><strong>数据噪声</strong>：训练数据中包含大量噪声或异常值，模型在训练时会把这些噪声也当作有效模式来学习。</li><li><strong>训练次数过多</strong>：模型在训练数据上迭代次数过多，导致模型对训练数据的拟合过于精细。</li></ol><h3 id="解决过拟合的方法">解决过拟合的方法</h3><ol type="1"><li><strong>增加训练数据量</strong>：通过收集更多的数据或使用数据增强技术来扩展训练集，可以帮助模型学习到更加普遍的特征。<ul><li><strong>数据增强</strong>：对于图像数据，可以使用翻转、旋转、缩放等技术来生成更多的训练样本。</li></ul></li><li><strong>简化模型</strong>：减少模型的参数数量，选择一个较为简单的模型结构。<ul><li><strong>正则化</strong>：在损失函数中加入正则化项，如L1正则化（Lasso）和L2正则化（Ridge），可以防止模型参数过大，减小模型的复杂度。</li></ul></li><li><strong>使用交叉验证</strong>：将数据集划分为多个子集，进行交叉验证，以确保模型在不同数据子集上的表现一致，帮助发现和防止过拟合。<ul><li><strong>K折交叉验证</strong>：将数据集分成K个子集，每次用K-1个子集训练模型，剩下的一个子集测试，循环K次，综合评估模型表现。</li></ul></li><li><strong>提前停止（EarlyStopping）</strong>：在训练过程中监控验证集的误差，当验证误差不再降低时，停止训练，避免模型在训练集上过度拟合。</li><li><strong>集成方法</strong>：使用多种模型的组合来降低单一模型过拟合的风险。<ul><li><strong>袋装（Bagging）</strong>：如随机森林，通过对数据进行多次采样并训练多个模型，最后进行投票或平均来得到最终结果。</li><li><strong>提升（Boosting）</strong>：如梯度提升决策树（GradientBoosting Decision Trees,GBDT），通过逐步训练多个弱模型，每次针对前一轮模型的错误进行改进。</li></ul></li><li><strong>正则化技术</strong>：<ul><li><strong>Dropout</strong>：在神经网络训练中随机将一部分神经元输出设置为0，以防止模型过于依赖某些特定的路径。</li><li><strong>数据标准化</strong>：对输入数据进行归一化处理，使其均值为0，标准差为1，帮助模型更快收敛，并减少过拟合的可能。</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>AI基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>机器学习</tag>
      
      <tag>模型训练</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git的原理及常用指令</title>
    <link href="/Git%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8.html"/>
    <url>/Git%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8.html</url>
    
    <content type="html"><![CDATA[<p>Git是一种分布式版本控制系统，用于跟踪项目中的更改，并允许多个开发者协作，本文介绍了Git的版本管理特点及常用指令。</p><span id="more"></span><h2 id="git的版本控制要点">Git的版本控制要点</h2><p>git与传统的集中式版本控制系统（如 Subversion 和CVS）有显著不同。分布式版本控制系统（DVCS）提供了更高的灵活性和可靠性。下面是关于Git 分布式管理的一些关键点：</p><p><img src="/images/git/git版本管理模式.png" /></p><h3 id="分布式架构">1. 分布式架构</h3><p>在 Git中，每个开发者的工作目录都包含了整个项目的完整版本库。这意味着每个开发者都有一个项目的完整副本，包括所有的历史记录和分支。这样，即使中央服务器出现问题，开发者仍然可以继续工作并且不会丢失任何数据。</p><h3 id="本地操作">2. 本地操作</h3><p>Git的大部分操作都是在本地完成的，例如提交（commit）、创建分支（branching）、合并（merging）等。这使得Git 操作非常快速，因为不需要与远程仓库通信。</p><h3 id="分支和合并">3. 分支和合并</h3><p>Git的分支和合并功能非常强大且灵活。创建和合并分支的操作都是本地的，效率高并且不会影响其他开发者的工作。分支在Git 中是轻量级的，这鼓励开发者频繁使用分支来进行独立开发和实验。</p><h3 id="协作工作流">4. 协作工作流</h3><p>Git 支持多种协作工作流，例如：</p><ul><li><strong>集中式工作流</strong>：所有的开发者都从中央仓库中拉取（pull）和推送（push）代码。</li><li><strong>功能分支工作流</strong>：每个新功能都有一个单独的分支，开发完成后合并回主分支。</li><li><strong>Forking 工作流</strong>：开发者从主仓库 fork出自己的仓库，在自己的仓库中工作，完成后向主仓库提交 pull request。</li></ul><h3 id="远程仓库">5. 远程仓库</h3><p>虽然 Git是分布式的，但它仍然支持通过远程仓库来进行团队协作。远程仓库通常托管在GitHub、GitLab 或 Bitbucket等平台上。开发者可以将本地的更改推送到远程仓库，也可以从远程仓库拉取其他开发者的更改。</p><h3 id="数据完整性">6. 数据完整性</h3><p>Git 使用 SHA-1哈希函数来确保数据的完整性。每一个文件、提交和标记都由一个唯一的哈希值标识，这些哈希值在版本库中是唯一的，可以确保数据不会被意外篡改。</p><h3 id="离线工作">7. 离线工作</h3><p>由于 Git的分布式特性，开发者可以在没有网络连接的情况下进行大部分操作。所有的操作都是在本地完成的，等到有网络连接时，再将更改推送到远程仓库。</p><h2 id="git的常用命令">Git的常用命令</h2><h3 id="配置">配置</h3><ol type="1"><li><p><strong>配置用户信息</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">git config --global user.name <span class="hljs-string">&quot;Your Name&quot;</span><br>git config --global user.email <span class="hljs-string">&quot;your.email@example.com&quot;</span><br></code></pre></td></tr></table></figure></p></li><li><p><strong>查看配置</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git config --list<br></code></pre></td></tr></table></figure></p></li></ol><h3 id="基本操作">基本操作</h3><ol type="1"><li><p><strong>初始化仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git init<br></code></pre></td></tr></table></figure>在当前目录中创建一个新的 Git 仓库。</p></li><li><p><strong>克隆仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> &lt;repository_url&gt;<br></code></pre></td></tr></table></figure>从远程仓库克隆一个副本到本地。</p></li><li><p><strong>查看状态</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git status<br></code></pre></td></tr></table></figure>显示工作目录和暂存区的状态。</p></li><li><p><strong>添加文件到暂存区</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git add &lt;file&gt;<br></code></pre></td></tr></table></figure>将文件添加到暂存区。使用 <code>git add .</code>可以添加所有更改的文件。</p></li><li><p><strong>提交更改</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git commit -m <span class="hljs-string">&quot;Commit message&quot;</span><br></code></pre></td></tr></table></figure>提交暂存区的更改并附带提交信息。</p></li><li><p><strong>查看日志</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git <span class="hljs-built_in">log</span><br></code></pre></td></tr></table></figure>查看提交历史记录。</p></li></ol><h3 id="分支操作">分支操作</h3><ol type="1"><li><p><strong>创建新分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git branch &lt;branch_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>切换分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git checkout &lt;branch_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>创建并切换到新分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git checkout -b &lt;branch_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>合并分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git merge &lt;branch_name&gt;<br></code></pre></td></tr></table></figure>将指定分支合并到当前分支。</p></li><li><p><strong>删除分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git branch -d &lt;branch_name&gt;<br></code></pre></td></tr></table></figure>删除指定的分支。</p></li></ol><h3 id="远程操作">远程操作</h3><ol type="1"><li><p><strong>添加远程仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git remote add &lt;remote_name&gt; &lt;url&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>查看远程仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git remote -v<br></code></pre></td></tr></table></figure></p></li><li><p><strong>从远程仓库拉取更改</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git pull &lt;remote_name&gt; &lt;branch_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>推送更改到远程仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git push &lt;remote_name&gt; &lt;branch_name&gt;<br></code></pre></td></tr></table></figure></p></li></ol><h3 id="撤销操作">撤销操作</h3><ol type="1"><li><p><strong>撤销工作目录中的更改</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git checkout -- &lt;file&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>重置暂存区的更改</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git reset &lt;file&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>撤销提交</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git revert &lt;commit&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>强制重置分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git reset --hard &lt;commit&gt;<br></code></pre></td></tr></table></figure></p></li></ol><h3 id="标签">标签</h3><ol type="1"><li><p><strong>创建标签</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git tag &lt;tag_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>查看标签</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git tag<br></code></pre></td></tr></table></figure></p></li><li><p><strong>推送标签到远程仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git push &lt;remote_name&gt; &lt;tag_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>删除标签</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git tag -d &lt;tag_name&gt;<br></code></pre></td></tr></table></figure></p></li></ol><h3 id="比较">比较</h3><ol type="1"><li><p><strong>比较文件</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git diff &lt;file&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>比较分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git diff &lt;branch1&gt; &lt;branch2&gt;<br></code></pre></td></tr></table></figure></p></li></ol><p>这些 Git的基本操作，熟练使用可以极大地提升管理代码和协作开发的工作效率。</p>]]></content>
    
    
    <categories>
      
      <category>编程基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>git</tag>
      
      <tag>代码管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性回归知识点梳理</title>
    <link href="/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86.html"/>
    <url>/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86.html</url>
    
    <content type="html"><![CDATA[<p>🏠线性回归可以称得上最经典的回归模型，从房子值多少钱，再到股票价格的涨跌🌈，疾病与各种因素的关联，广告投放的收益等，都使它擅长的领域💼接下来让我们走进线性回归💖</p><span id="more"></span><h3 id="线性回归的概念">1.线性回归的概念</h3><h5id="利用-回归方程函数-对-一个或多个自变量特征值和因变量目标值之间-关系进行建模的一种分析方式.">利用回归方程(函数) 对 一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式.</h5><h5 id="yw1x1w2x2....wnb-wtb">y=w1x1+w2x2+....+w(n)+b= wT+b</h5><ul><li>w=weight</li><li>b=bias wT x为将权重系数转置并与x相乘,矩阵的乘法</li></ul><h5 id="常用于分类与回归问题">常用于分类与回归问题</h5><h3 id="损失函数">2.损失函数</h3><h5 id="loss-functioncost-function目标函数成本函数">loss function/costfunction/目标函数/成本函数</h5><h5 id="最小二乘损失计算">最小二乘损失计算:</h5><p><span class="math display">\[J(w,b)=\sum_{i=0}^m\bigl(h\bigl(x^i\bigr)-y^i\bigr)^2\]</span></p><h5 id="均方误差mse">均方误差MSE</h5><p><span class="math display">\[J(w,b)=\frac1m\sum_{i=0}^m\left(h\left(x^i\right)-y^i\right)^2\]</span></p><h5 id="平均绝对误差mae">平均绝对误差MAE</h5><h5id="注-mse与mae既能在模型训练阶段作为损失函数求解拟合函数最优解又可作为模型评估阶段衡量已有模型误差大小">注:MSE与MAE既能在模型训练阶段作为损失函数求解拟合函数最优解,又可作为模型评估阶段,衡量已有模型误差大小</h5><h3 id="损失函数推导_正规方程法">3.损失函数推导_正规方程法</h3><p><span class="math display">\[J(w) =∥Xw−y∥_2^2\]</span></p><p><span class="math display">\[w=(X^TX)^{-1}*X^Ty\]</span></p><h5 id="优点可以精确求解">优点:可以精确求解</h5><h5 id="缺点计算量极大xt-x的逆不存在时无解">缺点:计算量极大,X^TX的逆不存在时无解</h5><h3 id="梯度与导数">4.梯度与导数</h3><h5id="导数表征了函数在某点处的变化速率.">导数表征了函数在某点处的变化速率.</h5><h5 id="梯度gradient">梯度（gradient）</h5><ul><li>多元函数中，导数不再是一个单一的数值，而是一个向量，因为它涉及到函数对多个自变量的变化率。这个向量被称为梯度（gradient），它表示了函数在某一点上沿着各个自变量方向的变化率。</li></ul><h5 id="梯度的性质">梯度的性质</h5><ul><li>梯度的方向是函数在给定点增长最快的方向。</li><li>梯度的模（长度）是函数在给定点处沿最大增长方向的增长率</li><li>梯度是垂直于等值面的</li><li>对函数求导</li></ul><p><span class="math display">\[f(\theta)=\theta_0x_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\cdots+\theta_nx_n\]</span></p><p><span class="math display">\[\nabla f=\begin{bmatrix}\frac{\partialf}{\partial\theta_0}\\\frac{\partialf}{\partial\theta_1}\\\vdots\\\frac{\partialf}{\partial\theta_n}\end{bmatrix}=\begin{bmatrix}x_0\\x_1\\\vdots\\x_n\end{bmatrix}\]</span></p><h3 id="梯度下降">5.梯度下降</h3><h5 id="梯度下降公式"><strong>梯度下降公式</strong></h5><ul><li>循环迭代求当前点的梯度，更新当前的权重参数</li><li></li></ul><p><span class="math display">\[\theta_{i+1}=\theta_i-\alpha\frac\partial{\partial\theta_i}J(\theta)\]</span> - θ_i:初始位置 α:学习率(步长),一般取值范围0.001 ~ 0.01 ∂/(∂θ_i) J(θ) :损失函数在i处的导数</p><h5 id="梯度下降优化过程"><strong>梯度下降优化过程 </strong></h5><ul><li><p>给定学习率,步长,初始位置</p></li><li><p>计算该点梯度方向并取反</p></li><li><p>向梯度反方向移动</p></li><li><p>重复以上步骤</p></li><li><p>达到收敛条件</p><ul><li>两次差距小于指定的阈值 •</li><li>达到指定的迭代次数</li></ul></li></ul><h5 id="学习率"><strong>学习率</strong></h5><ul><li>步长决定了在梯度下降迭代过程中</li><li>学习率太小，下降的速度会慢</li><li>学习率太大：容易造成错过最低点、产生下降过程中的震荡、甚至梯度爆炸</li></ul><h5 id="推导过程">推导过程</h5><ul><li>已知</li></ul><p><span class="math display">\[h_{(\theta)}=\theta_{1}x_{1}+\theta_{2}x_{2}+\cdots+\theta_{\mathrm{m}}x_{\mathrm{m}}+b\\=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\cdots+\theta_{\mathrm{m}}x_{\mathrm{m}}\]</span> - 损失函数</p><p><span class="math display">\[J_{(\theta)}=\frac1{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2\]</span> - 梯度下降公式</p><p><span class="math display">\[\theta_{i+1}=\theta_i-\alpha\frac\partial{\partial\theta_i}J(\theta)\]</span></p><ul><li>对损失函数求导</li></ul><p><span class="math display">\[J&#39;_{(\theta)}=\frac{\partial\mathrm{J}(\theta)}{\partial\theta}=\frac{2*1}{2m}\sum_{i=1}^{m}(h_{\theta}\big(x^{i}\big)-y^{i})^{2-1}*h_{\theta}\big(x^{i}\big)^{\prime}\]</span> - <strong>带入梯度下降公式</strong></p><p><span class="math display">\[\theta_j=\theta_j-\alpha\frac1m\sum_{i=1}^m(h_\theta(x^i)-y^i)*x_j^i\]</span> - 参数说明</p><p>θ_j:当前损失函数的梯度位置/原函数的特征权重<br />m,n:行数,列数<br />i,j:列索引,行索引<br />x,y:特征向量与目标向量</p><h3 id="常见梯度下降算法">6.常见梯度下降算法</h3><h5 id="全梯度下降算法-fgd">全梯度下降算法 FGD</h5><ul><li>每次迭代使用全样本梯度<br /></li><li>(硬件要求极高,数据量大时无法实现)</li></ul><h5 id="随机梯度下降-sgd">随机梯度下降 SGD</h5><ul><li>每次迭代随机选择并使用一个样本梯度<br /></li><li>(容易受异常值影响)</li></ul><h5id="小批量梯度下降算法-mini-bantch-最常用"><strong>小批量梯度下降算法mini-bantch 最常用√</strong></h5><ul><li>每次迭代随机选择并使用小批量的样本梯度<br /></li><li>(在硬件性能满足的情况下,每批的量应该尽可能大)</li></ul><h5 id="随机平均梯度下降-sag">随机平均梯度下降 SAG</h5><ul><li>每次迭代随机选择并使用一个样本梯度并和以往样本梯度值做平均</li><li>(解决异常值影响问题,但训练初期受异常值影响较大)</li></ul><h3 id="回归问题的评估方法">7.回归问题的评估方法</h3><h5 id="平方误差mse">平方误差MSE</h5><p><span class="math display">\[=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}\]</span></p><h5 id="平均绝对误差mae-1">平均绝对误差MAE</h5><p><span class="math display">\[=\frac{1}{n}\sum_{i=1}^{n}|y_{i}-\hat{y}_{i}|\]</span></p><h5 id="均方根误差">均方根误差</h5><p><span class="math display">\[RMSE=\sqrt{\frac1n\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2}\]</span></p><h3 id="模型拟合">8.模型拟合</h3><h5 id="过拟合"><strong>过拟合</strong></h5><ul><li><p>原因</p><ul><li>模型过于复杂,学习到了过多异常特征</li><li>数据噪声大</li></ul></li><li><p>解决方案</p><ul><li>数据清洗</li><li>正则化</li><li>精简特征维度</li><li>增加数据量</li></ul></li></ul><h5 id="欠拟合"><strong>欠拟合</strong></h5><ul><li><p>原因</p><ul><li>模型复杂度低</li><li>特征选择不当</li><li>数据量不足</li><li>正则化过度</li></ul></li><li><p>解决方案</p><ul><li>添加多项式特征项</li><li>添加其它特征</li><li>增加训练量</li></ul></li></ul><h3 id="正则化">9.正则化</h3><h5id="概念在模型训练时数据中有些特征影响模型复杂度或者某个特征的异常值较多-所以要尽量减少这个特征的影响甚至删除某个特征的影响这就是正则化正则化是添加在损失函数中的特殊项.">概念:在模型训练时，数据中有些特征影响模型复杂度、或者某个特征的异常值较多，所以要尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化。正则化是添加在损失函数中的特殊项.</h5><h5 id="l1正则化"><strong>L1正则化:</strong></h5><p><span class="math display">\[J(w)=\mathrm{MSE}(w)+\alpha\sum_{i=1}^{n}\mid w_{i}\mid\]</span> - • α叫做惩罚系数，该值越大则权重调整的幅度就越大，即：表示对特征权重惩罚力度就越大- L1 正则化会使得权重趋向于 0，甚至等于0，使得某些特征失效，达到特征筛选的目的 - from sklearn.linear_modelimport Lasso</p><h5 id="l2正则化"><strong>L2正则化</strong></h5><ul><li></li></ul><p><span class="math display">\[J(w)=\mathrm{MSE}(w)+\alpha\sum_{i=1}^nw_i^2\]</span> - L2 正则化会使得权重趋向于 0，一般不等于 0 - fromsklearn.linear_model import Ridge - L2的线性回归又称为岭回归</p>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>机器学习</tag>
      
      <tag>回归模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
