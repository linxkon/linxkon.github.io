<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>大模型训练Guidelines</title>
    <link href="/2024/05/18/%E5%A6%82%E4%BD%95%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/05/18/%E5%A6%82%E4%BD%95%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="背景">1 背景</h2><p>根据scaling law，模型越大，高质量数据越多，效果越好。</p><p>但还有一个很直观的情况，随着预训练样本的质量不断提升，训练手段的优化。新的模型，往往效果能轻松反超参数量两倍于它的模型。</p><p>例如，最新出的minicpm，微信内部评测效果也是非常棒的。跟规模相对接近的2b、7b模型比，得分比qwen2b高，和qwen7b比有的高有的低。</p><p>这个是minicpm的详细技术文档。</p><p>[https://shengdinghu.notion.site/MiniCPM-c805a17c5c8046398914e47f0542095a]</p><p>这说明，现有参数量情况下，哪怕是2B尺度，也并没有得到充分训练。</p><h2 id="样本">2 样本</h2><h3 id="样本构成">2.1 样本构成</h3><p>大家已经达成一些基础的共识。</p><p>如中英混合比例大家都大差不差。</p><p>逻辑推理比较强的样本，像代码，数学。这种就是模型越大，混合的比例反而可以越高。</p><p>跟SFT是类似的，越大的模型，越聪明的模型，需要的SFT数据就越少。同理，越大的模型，越聪明，复杂样本混合比例就可以越高。</p><h3 id="样本质量">2.2 样本质量</h3><h3 id="基本清洗">2.1.1 基本清洗</h3><p>导致ppl崩掉的，都要清洗掉，政治敏感数据清洗，去重等，肯定是一个很长的pipeline。</p><p>大家比较一致的结论是，天工开源的那份预训练数据，是一个比较好的满足基础清洗要求的数据。</p><h3 id="进阶清洗">2.1.2 进阶清洗</h3><p>大家都不太方便展开，但可以透露的信息。</p><p>跟SFT一样，产出各种各样的label来刻画数据，有的公司实习生就优化几个label。</p><p>不过随着优化的往后拓展，这些label的投入产出比越来越难以评估。</p><h3 id="phi式的生成synthetic数据">2.1.3 PHI式的生成(synthetic)数据</h3><p>预训练清洗的pipeline搭建，对于开源团队，小公司来讲，成本其实还是蛮高的。</p><p>所以，基于开源数据，做一些聚类的topic。然后基于这些topic，丢到更大的模型，来构建一批高质量的数据，是一个反而比较低成本的方案。</p><h3 id="买数据">2.1.4 买数据</h3><p>嗯，这次大模型，除了李一舟。卖数据的公司，也是真的赚到钱了。</p><h3 id="不同训练阶段的训练样本">2.3 不同训练阶段的训练样本</h3><p>经过讨论，发现有三种方案。</p><h3 id="末期高质量样本minicpm">2.3.1 末期高质量样本（minicpm)</h3><p>快速收敛阶段和平稳阶段，都采用普通样本。</p><p>退火阶段，混入高质量样本来做教科书式的学习。</p><h3 id="初期高质量样本">2.3.2 初期高质量样本</h3><p>快速收敛阶段，以高质量样本为主，让模型快速收敛。</p><p>平稳阶段，逐步调整比例，增加更多的普通样本。</p><p>退火阶段，跟平稳阶段一致</p><h3 id="全程高质量样本phil方式">2.3.3全程高质量样本（PHIL方式）</h3><p>全程都是高质量样本</p><p><strong>这里大家讨论的蛮激烈的，有这么几点。</strong></p><p>第一，初期就加入高质量样本，初期收敛的更快。但高质量样本少，不断的重复学习高质量样本，会不会导致过拟合？但反方认为，人类的本质上就是复读机，特别对于小模型，不断的重复学习，好像也没太大问题。</p><p>第二，初期学习高质量样本，会不会导致初期模型的初始化，局限在特定的区域，后面的普通样本学了之后，也不一定能很好的跳出来，会不会影响泛化？但反方认为，末期加入高质量样本，反而会不会因为最后加入高质量样本，导致泛化能力受损，集中在高质量样本的领域？</p><p>第三，PHIL方式，大家很快达成一致，PHIL就是探索小模型能不能在特定领域达到SOTA。好处，特定榜单/领域效果会特别好。坏处，模型泛化能力会很差（但PHIL从来没说要做世界模型。</p><h3 id="小模型样本的极限在哪">2.4 小模型样本的极限在哪？</h3><p>到底喂了多少tokens，小模型参数才算是充分得到训练？</p><p>当天讨论，并没有一个很好的结论。</p><p>最近YI-9B的公开技术文档，做了一个有趣的尝试。把每层的输入和输出算cos，来评估模型是否训练的非常充分。</p><p>但内部讨论后，发现这种尝试有一个巨大的遗漏点。</p><p>前段时间，我们做longcontext调研，也是把每层也都单独做了一个分析。结论是，如果模型深度足够的话，有些层其实区分度是在降低的，相当于几层做了一层做的事情。</p><p>以及，另外一个可能，小模型每一层cos都小，有可能每一层在干不同的事，或者每一层都会注意到新的东西。大模型某些层cos大，有可能是因为句子太简单，大模型对结果更加肯定，靠后的层的功能没有被激活。</p><p>感觉这种评估方式，仍旧有一定的优化空间，也期待业内能公开更多好用的评估方式。</p><h2 id="训练">3 训练</h2><h3 id="tokenizer">3.1 tokenizer</h3><p>小模型过大的tokenizer的确是一种浪费。很多小模型有大tokenizer，一个潜在的可能性，作者人力不足，直接是把大模型的tokenizer拿来复用了。</p><h3 id="阶段">3.2 阶段</h3><p>现在大家预训练分成三个阶段。</p><p>快速收敛阶段，稳定阶段，<strong>退火阶段(minicpm比较显式的提出这个阶段）</strong></p><h3 id="为什么要分阶段">3.2.1 为什么要分阶段</h3><p>这个阶段来自于大家对loss曲线的观察，发现loss曲线的收敛就是这么一个特点。</p><p>然后，大家发现不同的loss曲线阶段，做一些针对性样本和参数的调整，能带来更好的效果，于是就这么分了。</p><h3 id="不同阶段学的是什么东西">3.2.2 不同阶段学的是什么东西？</h3><p>首先，我们现在的评估手段还是比较粗糙的，假如有了更细的评估手段，可能能观测到更多有趣的东西。</p><p>例如之前俊林做过关于涌现的分享，涌现从指标观测来看，就是突然出现的。但当把指标细化后，可以发现这个涌现好像也没那么突然，这个可以参考<ahref="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2310.03262">https://arxiv.org/abs/2310.03262</a>把原本离散的准确率在1e-5级别的时候的值估计出来了。</p><p>但反方这里又有不同的观点，我们用物理学的一个理论来解释涌现。</p><p>我们可以把涌现替换成相变来聊一聊它和指标突变的辩证关系：当我们谈论相变时，我们指的是物质从一种状态转变为另一种状态的过程，比如水从液态变成固态的过程（冰冻）或者从液态变成气态的过程（蒸发）。而指标突变通常指的是某种性质或者物理量在某个条件下突然发生明显的变化，比如在某个温度或者压力下，某种物质的导电性、磁性或者其他性质突然发生变化。</p><p>相变与指标突变之间存在着密切的关系，因为相变往往伴随着物质性质的突变。当物质经历相变时，它的一些性质会突然改变，这些性质就是我们所说的指标。举个例子，当水温降到0摄氏度以下时，水会由液态变成固态，这就是相变，同时水的密度也会突然增加，导致它的体积变小，这就是指标突变。</p><p>虽然相变往往伴随着物质性质的指标突破，但是不意味着不突变就不是相变，指标的突变不是相变的重点，相变的重点在于从一个状态/性质，变成另外一个状态/性质，这两种状态有着很不一样的差别。</p><p>尽管可以使用一些技巧方法来构造一些看起来特别平滑的指标来反对大模型涌现这个词汇，但是不可否认的事实是，在不同的尺寸变化或者数据量、计算量变化之后，人们可以非常明显地感知到大模型表现的巨大差异，这就是一个相变的结果，就像是炼制一门18连环刃的法器，从第一把的炼制到第18把，从个数的指标上来说是非常平滑的，但是从威力上来说，18把可以构建一个法阵，极大地增加了武器的威力，与之前不可同日而语。</p><h3 id="batch-size">3.3 batch size</h3><p>老调重弹的问题。</p><p>2020年，transformer出来后，当时大家就碰到这么一个问题。模型太大了，用尽可能能塞进内存的batchsize去train模型，来提升速度。</p><p>很快，大家发现batch size有个trade off。</p><p>batchsize过小，波动会比较大，不太容易收敛。但这种波峰，也有助于跳出局部最优，模型更容易有更好的泛化能力。</p><p>batchsize变大，步数整体变少，训练的步数更少，本来就波动就小，步数也少，同样本的情况下，你收敛的会更慢。</p><p>2020年其实有人就研究，如何用大batchsize，更快的训练的同时，也能收敛的更好。一个解决思路是优化了优化器，例如谷歌当年出的LAMB，就把batchsize从512扩充到了6w，收敛效果也不错。</p><h3 id="lr-scheduler">3.4 LR scheduler</h3><p>机器学习的目标，都是为了收敛loss，让学习的target和我们预测的target的loss尽可能低。</p><p>学习的过程，就是基于样本，分批（batchsize）丢进去。根据过去，现在学习的效果，来决定参数更新的方向和大小。</p><p>batch size这里是很清晰的。比较纠结的点是，优化器和LRscheduler这俩好像边界不是很清晰。</p><h3 id="lr-scheduler是什么">3.4.1 LR scheduler是什么</h3><p>假如我们要下山，山脚就是我们的目标，learningrate就是我们每一步要走多远。如果速度太快，可能开到山脚后，发现刹不住车，还会往山上多开一会，于是这样反复在目标处来回震荡。如果太小的话，到山脚的速度又太慢了。</p><p>现在主流的就是cosine，初期warmup learning rate线性增长，然后learningrate就是以余弦函数的周期方式周期性变换。</p><h3 id="优化器做什么">3.4.2 优化器做什么？</h3><p>优化器核心要解决的问题，初期怎么更好的学，那些地方要加速学，那些地方容易陷入局部最优，要如何跳出来。</p><p>现在的主流做法都是基于历史的反馈。</p><p>类似于爬山，某个地方你发现爬的很慢，那么就加下油门。有的地方你发现是下坡路，跑的贼快，那就就要松下油门，免得油门太快，直接从主路跑出去了。</p><p>从momentum，到adagrad，再到adam，这两年还有人在各种折腾。</p><h3 id="优化器和lr-scheduler如何协同工作">3.4.3 优化器和LRscheduler如何协同工作？</h3><p>问题就来了，LR scheduler决定了learningrate的大小。优化器也会根据历史情况来自动调整。</p><p>这俩会不会冲突？</p><p>优化器的优点刚刚说了，但它的缺点就是无论优化器怎么说的高大上，它本质上还规则，是大家基于调参经验，或者一些假设，定的规则。</p><p>规则就很难完美适配所有任务，事实上2020年左右，大家就发现不同的任务上不同的优化器效果是不同的。例如当年的炼丹经验，计算机视觉优先使用SGD(withMomentum)，NLP（特别是用Transformer）优先使用Adam，现在CV都上transformer了，那么就又切到了AdamW。</p><p>除此之外，还有一个learning ratedecay的问题，但这个偏经验，并不一定完全solid！</p><p>用CIFAR或者ImageNet跑一跑常见的模型，就会发现，最后如果不把learningrate降低下去，loss很难收敛到一个很小的数字。</p><p>SGD和Adam的收敛性证明也都是要求learningrate最后会降到足够低的。但自适应优化器的学习率不会在训练中自动降到很低。</p><p>现在大模型预训练，大家其实最关注的就是这个loss的收敛效果。</p><p>这个时候，LRschedule的出现就是一个比较好的补充，能够补足优化器的一些问题。</p><p>所以，你可以理解为，现在我们没有一个完美的油门，所以搞了俩油门，互相辅助。</p><p>优化器是个老司机的油门，好用，但人类的经验是有局限性的，很容易陷入局部最优跑不出来。</p><p>LR schedule像是一个全局的油门，定期更新，帮助老司机跳出局部最优。</p><h3 id="w-s-d的讨论和优化方案">3.4.4 W-S-D的讨论和优化方案</h3><p>minicpm提出了W-S-D LR scheduler，但stable阶段高learningrate，相当于把调节油门的压力全给到优化器了。</p><p>但S-D的确也有很多好处，例如我想train到什么时候就train到什么时候。</p><p>这里提出了一个解决思路，W-S-D是不是可以改成，warm-cosine-stable-decay，cosine占据训练阶段大头，甚至多个余弦波段，余弦波段多了，如上文所说，是不是能更好的跳出局部最优？</p><p>快要结束训练的时候，把cosine的learningrate给升上去，走一段stable+decay。</p><h3 id="退火加sft-和面">3.5 退火加sft &amp;“和面”</h3><p>前段时间，业界流行一个说法，你发现某块效果差，在预训练最后阶段补充一些类似的数据，效果就会蹭蹭的往上涨。</p><p>简称，和面——水多了加面，面多了加水。</p><p>刚开始，大家都很鄙视这种行为，觉得这种行为不就是刷榜么？</p><p>但现在我们来探讨这块的合理性，minicpm可以算是更进一步的“作弊”了，如果按照之前的观点。他都直接把sft数据混入了预训练数据里面，更加明目张胆的“刷榜”。</p><p>我个人觉得这里可以用两个角度去理解:</p><p>角度一，模型学习的训练动态角度，在退火的时候loss下降较stable和正常的cosine都要快，证明这里的学习效率在提升(最聪明的时候?)，而此时在这一刻使用高质量数据来喂给模型,可以尽可能发挥高质量数据的作用;</p><p>角度二， SFT数据较正常的文本数据，我猜测会更偏向于benchmark一些，因为SFT很多都是"QA型"结构的数据,对于模型认识bechmark有一定的改善。</p><p>之前预训练完毕后，直接上SFT数据，语料分布差距很大，其实天然是不太好的。这种作弊的行为，只要控制样本比例得当，反而能保证后面通用对其的一致性。</p><h2 id="再看scaling-law"><strong>4 再看scaling law</strong></h2><p>随着一些common sense的建立，scalinglaw的指导意义的确是在不断下降的。</p><p>举个例子，假如我有300张卡，我要train多大的模型？</p><p>计算方式，往往就变成，我大致知道训练1T-2Ttokens效果往往就不错了，这个模型两个月后我想赶一个发布会。那么就来反推，1T的tokens训练2个月，300张卡能train多大的。</p><p>但我们回到2020年，当大部分人都在基于bert做各种魔改的时候。</p><p>OpenAI发现了这么一个规律。数据，训练，参数一直增长下去，好像loss的确是在不断的下降哎？</p><p>于是，他们拿着这个paper去问微软的CTO，你想不想看看这个loss下降到一定程度会发生什么？</p><p>会发生什么？</p><p>chatgpt就出来了</p><blockquote><p>转载自:<ahref="https://zhuanlan.zhihu.com/p/686664720">如何从零开始训练大模型（minicpm分享&amp;讨论）- 知乎 (zhihu.com)</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>大模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>模型训练</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI行业技能点含金量统计分析</title>
    <link href="/2024/05/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E7%9A%84%E4%B8%8D%E5%90%8C%E6%8A%80%E8%83%BD%E6%A0%91%E5%90%AB%E9%87%91%E9%87%8F%E5%88%86%E6%9E%90/"/>
    <url>/2024/05/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E7%9A%84%E4%B8%8D%E5%90%8C%E6%8A%80%E8%83%BD%E6%A0%91%E5%90%AB%E9%87%91%E9%87%8F%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>在人工智能领域，有很多热议的技能选择，如CV还是NLP，TensorFlow还是PyTorch，python还是C++……<br />话题有争议，但是数据非常客观，让我们拭目以待吧~ <span id="more"></span></p><h3 id="数据说明">1.数据说明</h3><p><img src="/images/AI哪个技能点最值钱/原始数据.png" /></p><p><strong>数据来源：</strong>为BOSS直聘北京地区算法工程师相关岗位,数据时间是2024年五月,有效样本总量为2200个.<br /><strong>数据内容：</strong>招聘标题,薪资,福利待遇,关键词,岗位描述,公司,地址,招聘网址<br /><strong>统计方法：</strong>对样本数据通过相关的岗位关键词进行筛选,并统计筛选结果的薪资均值,岗位数量等信息,汇总为下表。</p><h3 id="ai技能点含金量排名">2.AI技能点含金量排名</h3><div id="container0" style="height: 500px;"></div><script type="text/javascript" src="https://registry.npmmirror.com/echarts/5.5.0/files/dist/echarts.min.js"></script><script type="text/javascript">var dom = document.getElementById('container0');var myChart = echarts.init(dom, null, {  renderer: 'canvas',  useDirtyRect: false});var app = {};var option;option = {  xAxis: {  type: 'category',  data: [    "视觉|cv|视频|图像",    "NLP|自然语言|LLM",    "多模态|大模型",    "机器学习",    "深度学习",    "软件开发",    "控制算法",    "推荐算法",    "数据挖掘|数据分析",    "地图|路径算法"  ],  axisLabel: {    rotate: 45, // 旋转标签以适应显示    interval: 0 // 设置为0表示显示全部标签  }  },  yAxis: {  type: 'value'  },  series: [  {    data: [378.43, 414.87, 439.42, 396.63, 395.89, 359.30, 379.71, 435.72, 358.50, 415.83],    type: 'bar',    showBackground: true,    backgroundStyle: {      color: 'rgba(180, 180, 180, 0.2)'    },  label: {      show: true, // 开启数据标签显示      position: 'top', // 数据标签的位置，这里是顶部      formatter: '{c}' // 格式化函数，这里使用默认的'{c}'表示显示数值    }}  ]  };if (option && typeof option === 'object') {  myChart.setOption(option);}window.addEventListener('resize', myChart.resize);</script><p><strong>多模态大模型</strong>以均值439k的平均年薪拔得头筹，<strong>推荐算法</strong>以436k紧随其后，<strong>NLP自然语言处理与地图路径算法工程师</strong>以415K的薪资并列第三。说起薪资较低的，分别是<strong>AI软件开发岗、数据挖掘分析岗</strong>，将近360K，也是难能可贵了。</p><h3id="ai技能点的需求量与含金量分布">3.AI技能点的需求量与含金量分布</h3><div id="container" style="height: 500px;"></div><script type="text/javascript" src="https://registry.npmmirror.com/echarts/5.5.0/files/dist/echarts.min.js"></script><script type="text/javascript">  var dom = document.getElementById('container');  var myChart = echarts.init(dom, null, {    renderer: 'canvas',    useDirtyRect: false  });  var app = {};  var option;  option = {    dataset: {      source: [        ['平均年薪(k)', '岗位数量', '职业技能'],        [378.43, 708, "视觉|cv|视频|图像"],        [414.87, 350, "NLP|自然语言|LLM"],        [439.42, 545, "多模态|大模型"],        [396.63, 691, "机器学习"],        [395.89, 785, "深度学习"],        [359.3, 154, "软件开发"],        [379.71, 98, "控制算法"],        [435.72, 171, "推荐算法"],        [358.5, 100, "数据挖掘|数据分析"],        [415.83, 36, "地图|路径算法"],        [384.27, 1032, "python"],        [396.68, 1225, "C++"],        [384.61, 206, "TensorFlow"],        [390.4, 193, "PyTorch"],      ]    },    grid: { containLabel: true },    xAxis: { name: '岗位数量' },    yAxis: { type: 'category' },    visualMap: {      orient: 'horizontal',      left: 'center',      min: 350,      max: 450,      text: ['High 平均年薪(k)', 'Low 平均年薪(k)'],      // Map the 平均年薪(k) column to color      dimension: 0,      inRange: {        color: ['#65B581', '#FFCE34', '#FD665F']      }    },    series: [      {        type: 'bar',        encode: {          // Map the "岗位数量" column to X axis.          x: '岗位数量',          // Map the "职业技能" column to Y axis          y: '职业技能'        }      }    ]  };  if (option && typeof option === 'object') {    myChart.setOption(option);  }  window.addEventListener('resize', myChart.resize);</script><p><strong>计算机视觉CV与自然语言NLP</strong>：视觉（CV）与图像、视频打交道，年薪约378k，岗位多；而自然语言处理（NLP）年薪诱人达415k，但岗位少。想多赚钱选NLP，想稳就业选CV。</p><p><strong>多模态与大模型</strong>：新兴的多模态与大模型领域，年薪高达439k，岗位也不少。想站风口就选它！</p><p><strong>机器学习与深度学习</strong>：机器学习年薪约397k，岗位稳定；深度学习略低但需求多。两者薪资相近，看需求选。</p><p><strong>软件开发与控制算法</strong>：软件开发年薪359k但岗位少；控制算法稍好，年薪380k。两者传统但重要。</p><p><strong>推荐算法与数据挖掘</strong>：推荐算法年薪高达436k但岗位少；数据挖掘年薪359k。喜欢数据处理就选它们。</p><p><strong>地图与路径算法</strong>：小众但高薪的地图与路径算法，年薪416k但竞争大。适合专长者挑战。</p><p><strong>编程语言：Python与C++</strong>Python年薪384k岗位多，C++年薪略高且岗位更多。两者都是AI开发利器。</p><p><strong>框架选择：TensorFlow与PyTorch</strong>TensorFlow年薪385k，PyTorch年薪390k。两者差距小，选谁看心情和项目需求。</p><h3id="技能点与教育程度工作经验及薪资的关系">4.技能点与教育程度，工作经验及薪资的关系</h3><h4id="技能点和教育程度对薪资的影响单位k">4.1技能点和教育程度对薪资的影响（单位：K）</h4><table><thead><tr class="header"><th></th><th>专科</th><th>本科</th><th>985/211</th><th>硕士</th><th>博士</th></tr></thead><tbody><tr class="odd"><td>地图/路径</td><td>-</td><td>481</td><td>330</td><td>470</td><td>1260</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>-</td><td>394</td><td>383</td><td>358</td><td>277</td></tr><tr class="odd"><td>推荐算法</td><td>-</td><td>378</td><td>456</td><td>397</td><td>397</td></tr><tr class="even"><td>PyTorch</td><td>360</td><td>370</td><td>427</td><td>383</td><td>365</td></tr><tr class="odd"><td>控制算法</td><td>-</td><td>364</td><td>390</td><td>400</td><td>-</td></tr><tr class="even"><td>c++/C++</td><td>315</td><td>359</td><td>395</td><td>418</td><td>420</td></tr><tr class="odd"><td>多模态/大模型</td><td>-</td><td>352</td><td>367</td><td>465</td><td>459</td></tr><tr class="even"><td>TensorFlow</td><td>360</td><td>343</td><td>367</td><td>383</td><td>365</td></tr><tr class="odd"><td>机器学习</td><td>360</td><td>343</td><td>395</td><td>434</td><td>433</td></tr><tr class="even"><td>深度学习</td><td>315</td><td>339</td><td>408</td><td>423</td><td>429</td></tr><tr class="odd"><td>python</td><td>315</td><td>330</td><td>361</td><td>411</td><td>394</td></tr><tr class="even"><td>NLP/自然语言/LLM</td><td>-</td><td>328</td><td>373</td><td>480</td><td>556</td></tr><tr class="odd"><td>软件/开发</td><td>360</td><td>327</td><td>252</td><td>394</td><td>393</td></tr><tr class="even"><td>视觉/cv/视频/图像</td><td>360</td><td>320</td><td>434</td><td>411</td><td>514</td></tr></tbody></table><h4id="技能点和教育程度对工作岗位数量的影响单位个">4.2技能点和教育程度对工作岗位数量的影响（单位：个）</h4><table><thead><tr class="header"><th></th><th>专科</th><th>本科</th><th>985/211</th><th>硕士</th><th>博士</th></tr></thead><tbody><tr class="odd"><td>c++/C++</td><td>2</td><td>248</td><td>23</td><td>255</td><td>42</td></tr><tr class="even"><td>python</td><td>2</td><td>223</td><td>22</td><td>212</td><td>39</td></tr><tr class="odd"><td>深度学习</td><td>2</td><td>157</td><td>25</td><td>166</td><td>25</td></tr><tr class="even"><td>机器学习</td><td>1</td><td>144</td><td>23</td><td>117</td><td>29</td></tr><tr class="odd"><td>视觉/cv/视频/图像</td><td>1</td><td>126</td><td>16</td><td>159</td><td>29</td></tr><tr class="even"><td>多模态/大模型</td><td>-</td><td>115</td><td>21</td><td>111</td><td>26</td></tr><tr class="odd"><td>NLP/自然语言/LLM</td><td>-</td><td>91</td><td>21</td><td>82</td><td>11</td></tr><tr class="even"><td>推荐算法</td><td>-</td><td>50</td><td>11</td><td>31</td><td>5</td></tr><tr class="odd"><td>TensorFlow</td><td>1</td><td>41</td><td>7</td><td>40</td><td>9</td></tr><tr class="even"><td>PyTorch</td><td>1</td><td>39</td><td>7</td><td>38</td><td>9</td></tr><tr class="odd"><td>软件/开发</td><td>1</td><td>38</td><td>3</td><td>20</td><td>3</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>-</td><td>21</td><td>5</td><td>17</td><td>4</td></tr><tr class="odd"><td>地图/路径</td><td>-</td><td>17</td><td>1</td><td>11</td><td>1</td></tr><tr class="even"><td>控制算法</td><td>-</td><td>13</td><td>3</td><td>19</td><td>-</td></tr></tbody></table><h4id="技能点和工作经验对平均年薪的影响单位k">4.3技能点和工作经验对平均年薪的影响（单位：K）</h4><table><thead><tr class="header"><th></th><th>应届</th><th>一年</th><th>两年</th><th>三年</th><th>四年</th><th>五年及以上</th></tr></thead><tbody><tr class="odd"><td>推荐算法</td><td>351</td><td>261</td><td>433</td><td>433</td><td>-</td><td>462</td></tr><tr class="even"><td>TensorFlow</td><td>272</td><td>343</td><td>324</td><td>415</td><td>360</td><td>427</td></tr><tr class="odd"><td>PyTorch</td><td>285</td><td>350</td><td>318</td><td>409</td><td>360</td><td>517</td></tr><tr class="even"><td>c++/C++</td><td>373</td><td>356</td><td>389</td><td>407</td><td>383</td><td>438</td></tr><tr class="odd"><td>多模态/大模型</td><td>409</td><td>355</td><td>427</td><td>397</td><td>423</td><td>428</td></tr><tr class="even"><td>控制算法</td><td>360</td><td>-</td><td>402</td><td>397</td><td>-</td><td>338</td></tr><tr class="odd"><td>NLP/自然语言/LLM</td><td>544</td><td>359</td><td>392</td><td>392</td><td>442</td><td>416</td></tr><tr class="even"><td>软件/开发</td><td>-</td><td>339</td><td>325</td><td>386</td><td>216</td><td>344</td></tr><tr class="odd"><td>机器学习</td><td>330</td><td>392</td><td>383</td><td>383</td><td>454</td><td>472</td></tr><tr class="even"><td>python</td><td>304</td><td>356</td><td>396</td><td>382</td><td>425</td><td>366</td></tr><tr class="odd"><td>深度学习</td><td>357</td><td>368</td><td>399</td><td>381</td><td>442</td><td>420</td></tr><tr class="even"><td>视觉/cv/视频/图像</td><td>406</td><td>360</td><td>454</td><td>379</td><td>397</td><td>505</td></tr><tr class="odd"><td>地图/路径</td><td>-</td><td>406</td><td>375</td><td>350</td><td>927</td><td>-</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>242</td><td>415</td><td>294</td><td>308</td><td>-</td><td>530</td></tr></tbody></table><h4id="技能点和工作经验对工作岗位数量的影响单位个">4.4技能点和工作经验对工作岗位数量的影响（单位：个）</h4><table><thead><tr class="header"><th></th><th>应届</th><th>一年</th><th>两年</th><th>三年</th><th>四年</th><th>五年及以上</th></tr></thead><tbody><tr class="odd"><td>c++/C++</td><td>38</td><td>70</td><td>142</td><td>211</td><td>17</td><td>92</td></tr><tr class="even"><td>python</td><td>35</td><td>49</td><td>143</td><td>187</td><td>14</td><td>70</td></tr><tr class="odd"><td>深度学习</td><td>19</td><td>44</td><td>99</td><td>146</td><td>9</td><td>58</td></tr><tr class="even"><td>视觉/cv/视频/图像</td><td>18</td><td>32</td><td>89</td><td>125</td><td>8</td><td>59</td></tr><tr class="odd"><td>机器学习</td><td>26</td><td>32</td><td>85</td><td>120</td><td>10</td><td>41</td></tr><tr class="even"><td>多模态/大模型</td><td>13</td><td>25</td><td>71</td><td>116</td><td>9</td><td>39</td></tr><tr class="odd"><td>NLP/自然语言/LLM</td><td>7</td><td>16</td><td>58</td><td>78</td><td>4</td><td>42</td></tr><tr class="even"><td>推荐算法</td><td>6</td><td>4</td><td>40</td><td>33</td><td>-</td><td>14</td></tr><tr class="odd"><td>PyTorch</td><td>5</td><td>16</td><td>19</td><td>32</td><td>1</td><td>21</td></tr><tr class="even"><td>TensorFlow</td><td>7</td><td>18</td><td>21</td><td>32</td><td>1</td><td>19</td></tr><tr class="odd"><td>软件/开发</td><td>-</td><td>6</td><td>22</td><td>25</td><td>1</td><td>11</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>3</td><td>4</td><td>18</td><td>16</td><td>-</td><td>6</td></tr><tr class="odd"><td>控制算法</td><td>1</td><td>-</td><td>13</td><td>13</td><td>-</td><td>8</td></tr><tr class="even"><td>地图/路径</td><td>-</td><td>9</td><td>5</td><td>9</td><td>7</td><td>-</td></tr></tbody></table><h3 id="高性价比工作岗位排名">5高性价比工作岗位排名</h3><h4id="加权工作经验工作岗位数量与薪资计算技能点的得分及排名情况">加权工作经验,工作岗位数量与薪资,计算技能点的得分及排名情况</h4><ul><li>基本原则：学历要求越低，工作经验要求越低，平均年薪越高的技能点越好，其评分会越高</li><li>学历，工作经验数据划分五个等级，然后将三个特征数据归一化处理，彼此相乘，再乘上百分系数，得到最终评分</li></ul><div id="container2" style="height: 400px;"></div><script type="text/javascript" src="https://registry.npmmirror.com/echarts/5.5.0/files/dist/echarts.min.js"></script><script type="text/javascript">  var dom = document.getElementById('container2');  var myChart = echarts.init(dom, null, {    renderer: 'canvas',    useDirtyRect: false  });  var app = {};  var option;  option = {    dataset: [      {        dimensions: ['name','score'],        source: [          ["NLP/自然语言/LLM",30.16],          ["PyTorch",14.78],          ["TensorFlow",15.56],          ["c++/C++",83.54],          ["python",74.84],          ["地图/路径",15.25],          ["多模态/大模型",45.56],          ["控制算法",10.59],          ["推荐算法",29.53],          ["数据挖掘/数据分析",5.26],          ["机器学习",46.19],          ["深度学习",52.13],          ["视觉/cv/视频/图像",42.48],          ["软件/开发",13.56]        ]      },      {        transform: {          type: 'sort',          config: { dimension: 'score', order: 'desc' }        }      }    ],    xAxis: {      type: 'category',      axisLabel: { interval: 0, rotate: 30 }    },    yAxis: {},    series: {      type: 'bar',      encode: { x: 'name', y: 'score' },            datasetIndex: 1    }  };  if (option && typeof option === 'object') {    myChart.setOption(option);  }  window.addEventListener('resize', myChart.resize);</script>]]></content>
    
    
    <categories>
      
      <category>Data Visualization</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ECharts</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一首歌的时间部署本地Llama3大模型</title>
    <link href="/2024/05/11/%E4%B8%80%E9%A6%96%E6%AD%8C%E7%9A%84%E6%97%B6%E9%97%B4-%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E4%B8%93%E5%B1%9Ellama3%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/05/11/%E4%B8%80%E9%A6%96%E6%AD%8C%E7%9A%84%E6%97%B6%E9%97%B4-%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E4%B8%93%E5%B1%9Ellama3%E5%A4%A7%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>LLaMA3真的是相当相当炸裂啊！远超过去的体验！看数据Llama3-8B超过Mistra-7BMMLU10分；70B超过Claude3Sonet3分。这是一个惊人的成绩，一个开源模型超过闭源模型这样多。我只能说Meta是真正的OpenAI。自从它从Meta这个邪路上转正后，在OpenAI的路上一骑绝尘了！不废话，动手来给自己的电脑部署下吧。 <span id="more"></span></p><h2 id="有什么硬件要求"><strong>有什么硬件要求</strong></h2><p>N卡独占，起步4G显存，建议8G＋。纯CPU也能跑，如果你不嫌慢的话。</p><h2 id="安装lm-studio"><strong>1. 安装LM studio</strong></h2><p>就这个软件(<a href="https://lmstudio.ai/">LM Studio - Discover,download, and run local LLMs</a>)</p><figure><imgsrc="/images/本地部署Llama3大模型/v2-3a61b06246c57b88fcd83f17062c10df_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>安装成功，打开后应该出现如下界面</p><figure><img src="/images/本地部署Llama3大模型/image-20240514080559847.png"alt="image-20240514080559847" /><figcaption aria-hidden="true">image-20240514080559847</figcaption></figure><h2 id="选择llama3-8b模型"><strong>2. 选择llama3-8B模型</strong></h2><p>我们直接搜索llama 3-8B，找到该模型</p><figure><img src="/images/本地部署Llama3大模型/image-20240514081038674.png"alt="image-20240514081038674" /><figcaption aria-hidden="true">image-20240514081038674</figcaption></figure><p>当然我们也可以选择其他模型，模型选择的重要因素是大小，也就是参数量。模型参数量一般写在名字上，比如Dolphin 2.6 Mistral 7b – DPO Laser就是7B大小，也就是70亿参数。根据自己的电脑内存和显存容量选（CPU运行就看内存，GPU运行就看显存，混合运行就两个加起来），我电脑是8G显存，用的7B模型。</p><p>然后就是模型指标，现在huggingface上有成百上千个LLM，可以根据benchmark的成绩选，排名网页在此：<ahref="https://link.zhihu.com/?target=https%3A//huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">OpenLLM Leaderboard - a Hugging Face Space by HuggingFaceH4</a> 。</p><p>还有就是模型特性，比如是否经过审查，适合于什么类型的工作等。</p><h2 id="下载gguf文件"><strong>3. 下载gguf文件</strong></h2><h3 id="在lm-studio内部下载需要配置网络"><strong>1. 在LMStudio内部下载，需要配置网络</strong></h3><p>如果有国际互联网连接就可以直接下载。如果没有见下一步。</p><h3 id="在huggingface下载并转移到lm-studio中"><strong>2.在huggingface下载并转移到LM Studio中</strong></h3><h3 id="下载"><strong>1. 下载</strong></h3><p>手动将网址复制到浏览器下载。</p><figure><img src="/images/本地部署Llama3大模型/image-20240514081632465.png"alt="image-20240514081632465" /><figcaption aria-hidden="true">image-20240514081632465</figcaption></figure><h3 id="移动下载的gguf文件到lm-studio识别的位置"><strong>2.移动下载的gguf文件到LM studio识别的位置</strong></h3><figure><img src="/images/本地部署Llama3大模型/image-20240514081756888.png"alt="image-20240514081756888" /><figcaption aria-hidden="true">image-20240514081756888</figcaption></figure><p>打开My models,找到gguf文件位置，然后在系统文件管理器中管理好你下载的gguf文件路径，格式为models/A/B/xxx.gguf。再重启LMstudio就能看到它。</p><h2 id="运行"><strong>4. 运行</strong></h2><h3 id="cpu运行"><strong>1.CPU运行</strong></h3><p>同GPU运行，但不用改settings 中的 GPU 参数。</p><h3 id="gpu运行"><strong>2.GPU运行</strong></h3><figure><img src="/images/本地部署Llama3大模型/image-20240514082144052.png"alt="image-20240514082144052" /><figcaption aria-hidden="true">image-20240514082144052</figcaption></figure><p>然后点击窗口上方的Select a model toload，加载上一步下载的模型就可以了。任务管理器中可以监视显存占用。</p><p>如果成功加载到显卡，就可以在下方与其对话了。</p><figure><img src="/images/本地部署Llama3大模型/image-20240514082345176.png"alt="image-20240514082345176" /><figcaption aria-hidden="true">image-20240514082345176</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>项目部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北京人工智能行业薪资大揭秘</title>
    <link href="/2024/05/03/%E5%8C%97%E4%BA%AC%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E8%96%AA%E8%B5%84%E5%A4%A7%E6%8F%AD%E7%A7%98/"/>
    <url>/2024/05/03/%E5%8C%97%E4%BA%AC%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E8%96%AA%E8%B5%84%E5%A4%A7%E6%8F%AD%E7%A7%98/</url>
    
    <content type="html"><![CDATA[<p><img src="/images/1714717993428.png" />BOSS直聘数据，含北京市各区[<code>算法工程师|人工智能</code>]岗位数据7534条<span id="more"></span> # <strong>北京2024人工智能行业薪资大揭秘</strong></p><h3 id="数据说明">1.数据说明</h3><p>数据时间:<code>2024年5月</code></p><p>数据来源:**BOSS直聘,爬取北京市各区[<code>算法工程师|人工智能</code>]岗位数据7534条(多次爬取结果)</p><p><strong>数据清洗:</strong>提取岗位内容中包含<code>[人工智能|算法|nlp|cv]</code>的内容,并执行去重操作,得到岗位数据2208个.</p><p><img src="/images/1714721994550.png" /></p><h3 id="薪资分析">2.薪资分析</h3><p><img src="/images/1714716697686.png" /></p><p><strong>薪资上限，星辰大海般的梦想</strong>：首先，让人眼前一亮的是薪资上限——竟然高达<strong>1800k（年薪）</strong>！这意味着在这个行业，如果你拥有出色的才华和丰富的经验，那么年薪百万的梦想并非遥不可及。当然，这样的高薪也对应着极高的工作要求和挑战。</p><p><strong>薪资下限，无良公司真没下限</strong>：而对于那些刚刚步入人工智能行业的新人或者初入这个领域的小伙伴们来说，薪资下限为24k（年薪）,在北京还不够房租的,试问这些无良公司,你们的良心不会痛吗。</p><p><strong>平均年薪，舒适圈的魅力</strong>：说到最吸引人的部分，莫过于平均年薪了。北京人工智能行业的平均年薪高达<strong>400k</strong>左右，这真是一个赏心悦目的数字,另外年薪的众数和中位数都是<strong>360k</strong>,不知道屏幕前的你达到平均水平没有。</p><p><strong>人工智能岗位平均年薪与下限年薪对比:</strong></p><p><img src="/images/1714717993428.png" /></p><h3 id="岗位要求">3.岗位要求</h3><p>人工智能行业这么卷,是不是得<code>985\211起步,研究生占半数</code>呢?我们用数据来说话:</p><p><img src="/images/1714721621417.png" /></p><p>根据统计的2208个岗位数据来看,研究生占比30.4%,反而是<strong>本科生占据了大多数</strong>,占比达<strong>64.9%</strong>,本科生才是人工智能产业的中坚力量.不过AI行业的起步门槛是真高,大专和学历不限的岗位占比仅<strong>2.4%.</strong></p><p>在岗位经验来看,<strong>人工智能行业的包容性还是比较大的</strong>,经验不限的占到了17.91%(越缺人才的行业,这个指标越高),3-5年的岗位占比超过一半(鲜明的新兴行业).现在来看,又是招兵买马又是百模大战,<strong>人工智能的时代才刚开始.</strong></p><h3 id="薪资与学历和经验的关系">4.薪资与学历和经验的关系</h3><h4 id="学历vs薪资">4.1 学历VS薪资</h4><p><strong>大专小鲜肉</strong>：虽然起步稍低，但凭借着一股不服输的劲头，也能拿到251k的薪资，证明了在人工智能领域，实力非常的重要。</p><p><strong>本科高手</strong>：他们像是中流砥柱，稳稳地占据了薪资的中上游，379k的薪资，是对他们扎实基础和广泛知识的认可。</p><p><strong>硕士精英</strong>：他们在学历上更上一层楼，薪资也随之水涨船高，416k的薪资，是他们辛勤付出的回报。</p><p><strong>博士大佬</strong>：一出场就自带光环，稳稳地站在了薪资的金字塔尖，462k的薪资，仿佛在告诉大家：“知识就是力量，学历就是金钱！”</p><p><strong>学历不限</strong>：这个神秘的角色，似乎不受学历的束缚，凭借着自己的独特技能和经验，也能轻松拿到385k的薪资，可谓是“英雄不问出处”。</p><p><img src="/images/1714719207889.png" /></p><h3 id="经验vs薪资">4.2 经验VS薪资</h3><p><strong>1-3年新鲜人儿</strong>：初出茅庐的你，薪资322k，够你喝不少星巴克了！但别停步，未来更精彩！</p><p><strong>1年以内小鲜肉</strong>：应届生们，你们薪资333k，起点不错！不过这只是起点，挑战还在后头哦！</p><p><strong>3-5年小有成就</strong>：404k的薪资，帝都<strong>租房</strong>没问题！继续加油，成为公司顶梁柱！</p><p><strong>5-10年资深玩家</strong>：资深大佬，458k薪资，生活舒适还能追梦！多年打拼，果然值得！</p><p><strong>10年以上大佬级人物</strong>：传奇大佬，467k薪资，人生赢家！人脉经验都丰富，这钱你应得！</p><p><strong>在校/应届小白</strong>：小白们，150k只是开始，努力学习，未来可期！</p><p><strong>经验不限的小伙伴</strong>：无门槛岗位，378k薪资，虽有挑战，但你有实力，定能闯出一片天！</p><p><img src="/images/1714719746033.png" /></p><hr /><p>在北京,人工智能行业以平均年薪<strong>400k</strong>的高薪,<strong>经验不限</strong>的要求,让无数人心生向往。尽管如此,本科学历只是<strong>入行地板砖</strong>,稳妥些确实得硕士学历.但长远来看,AI行业是一个不断发展的增量市场,它注定要成为推动社会变革的新质生产力,你<strong>准备好迎接这个崭新的时代了吗</strong>?</p>]]></content>
    
    
    
    <tags>
      
      <tag>可视化</tag>
      
      <tag>数据分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>半小时速通正则表达式</title>
    <link href="/2023/05/10/%E5%8D%8A%E5%B0%8F%E6%97%B6%E9%80%9F%E9%80%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <url>/2023/05/10/%E5%8D%8A%E5%B0%8F%E6%97%B6%E9%80%9F%E9%80%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<p>正则表达式在文本范式处理时有者非常重要的应用，快来一起巩固一下正则的相关知识吧！<span id="more"></span> ### 正则表达式应用场景(Regular Expression)</p><ul><li>数据验证（表单验证、如手机、邮箱、IP地址）</li><li>爬虫功能</li><li>数据检索（数据检索、数据抓取）</li><li>数据隐藏（135****6235王先生）</li><li>数据过滤（论坛敏感关键词过滤）</li></ul><h3 id="正则--match">正则--match</h3><p><strong>re.match(pattern, string, flags=0)</strong></p><table><thead><tr class="header"><th><strong>参数</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr class="odd"><td>pattern</td><td>匹配的正则表达式</td></tr><tr class="even"><td>string</td><td>要匹配的字符串。</td></tr><tr class="odd"><td>flags</td><td>标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：<ahref="https://www.runoob.com/python/python-reg-expressions.html#flags">正则表达式修饰符- 可选标志</a></td></tr></tbody></table><p>re.match 尝试从字符串的<strong>起始位置匹配</strong>一个模式，匹配成功 re.match 方法返回一个匹配的对象，否则返回 None。</p><h3 id="正则--search">正则--search</h3><p><strong>re.search(pattern, string, flags=0)</strong></p><table><thead><tr class="header"><th><strong>参数</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr class="odd"><td>pattern</td><td>匹配的正则表达式</td></tr><tr class="even"><td>string</td><td>要匹配的字符串。</td></tr><tr class="odd"><td>flags</td><td>标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：<ahref="https://www.runoob.com/python/python-reg-expressions.html#flags">正则表达式修饰符- 可选标志</a></td></tr></tbody></table><p>re.match尝试从字符串的<strong>任意位置匹配</strong>一个模式，(常用于全词匹配)匹配成功 re.search 方法返回一个匹配的对象，否则返回 None。</p><p>re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。</p><h3id="re.compile正则表达式.sub用来替换的内容要被替换的内容">re.compile（正则表达式）.sub（用来替换的内容，要被替换的内容）</h3><p>compile 函数用于编译正则表达式，生成一个正则表达式（ Pattern）对象，供 match() 和 search() 这两个函数使用。 ### ###re.sub(正则,替换字符,被替换的内容)</p><h3 id="正则常用符号释义">正则常用符号释义</h3><table><thead><tr class="header"><th><strong>符号</strong></th><th><strong>解释</strong></th><th><strong>示例</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr class="odd"><td>.</td><td><strong>匹配任意字符</strong></td><td>b.t</td><td>可以匹配bat / but / b#t / b1t等</td></tr><tr class="even"><td><a href="file://w"></a></td><td><strong>匹配字母/数字/下划线/汉字</strong></td><td>b</td><td>可以匹配bat / b1t / b_t等&lt;br&gt;但不能匹配b#t</td></tr><tr class="odd"><td><a href="file://s"></a></td><td>**匹配空白字符（包括、*）</td><td>love</td><td>可以匹配love you</td></tr><tr class="even"><td>[(file://d)</td><td><strong>匹配数字</strong></td><td>[(file://d/d)</td><td>可以匹配01 / 23 / 99等</td></tr><tr class="odd"><td>[(file://b)</td><td>匹配单词的边界</td><td>[(file://bThe/b)</td><td></td></tr><tr class="even"><td>^</td><td><strong>匹配字符串的开始</strong></td><td>^The</td><td>可以匹配The开头的字符串</td></tr><tr class="odd"><td>$</td><td><strong>匹配字符串的结束</strong></td><td>.exe$</td><td>可以匹配.exe结尾的字符串</td></tr><tr class="even"><td><a href="file://W"></a></td><td>匹配非字母/数字/下划线</td><td>b</td><td>可以匹配b#t / b@t等&lt;br&gt;但不能匹配but / b1t / b_t等</td></tr><tr class="odd"><td><a href="file://S"></a></td><td>匹配非空白字符</td><td>love</td><td>可以匹配love#you等&lt;br&gt;但不能匹配love you</td></tr><tr class="even"><td><a href="file://D"></a></td><td>匹配非数字</td><td><a href="file://d/D"></a></td><td>可以匹配9a / 3# / 0F等</td></tr><tr class="odd"><td><a href="file://B"></a></td><td>匹配非单词边界</td><td><a href="file://Bio/B"></a></td><td></td></tr><tr class="even"><td>[]</td><td>匹配来自字符集的任意单一字符</td><td>[aeiou]</td><td>可以匹配任一元音字母字符</td></tr><tr class="odd"><td><strong>[^]</strong></td><td>匹配不在字符集中的任意单一字符</td><td>[^aeiou]</td><td>可以匹配任一非元音字母字符</td></tr><tr class="even"><td><strong>*</strong></td><td><strong>匹配0次或多次</strong></td><td><a href="file://w*">*</a></td><td></td></tr><tr class="odd"><td><strong>+</strong></td><td><strong>匹配1次或多次</strong></td><td><a href="file://w+">+</a></td><td></td></tr><tr class="even"><td><strong>?</strong></td><td><strong>匹配0次或1次</strong></td><td><a href="file://w">?</a></td><td></td></tr><tr class="odd"><td><strong>{N}</strong></td><td><strong>匹配N次</strong></td><td></td><td></td></tr><tr class="even"><td><strong>{M,}</strong></td><td><strong>匹配至少M次</strong></td><td></td><td></td></tr><tr class="odd"><td><strong>{M,N}</strong></td><td><strong>匹配至少M次至多N次</strong></td><td></td><td></td></tr><tr class="even"><td><strong>\</strong></td><td>分支</td><td>foo\bar</td><td>可以匹配foo或者bar</td></tr><tr class="odd"><td><strong>(?#)</strong></td><td>注释</td><td></td><td></td></tr><tr class="even"><td><strong>(exp)</strong></td><td>匹配exp并捕获到自动命名的组中</td><td></td><td></td></tr><tr class="odd"><td><strong>(?&lt;name&gt;exp)</strong></td><td>匹配exp并捕获到名为name的组中</td><td></td><td></td></tr><tr class="even"><td><strong>(?:exp)</strong></td><td>匹配exp但是不捕获匹配的文本</td><td></td><td></td></tr><tr class="odd"><td><strong>(?=exp)</strong></td><td>匹配exp前面的位置</td><td><a href="file://b/w+(%3f=ing)">+(?=ing)</a></td><td>可以匹配I'm dancing中的danc</td></tr><tr class="even"><td><strong>(?&lt;=exp)</strong></td><td>匹配exp后面的位置</td><td>[(?&lt;=)+(file://bdanc)/w+/b)</td><td>可以匹配I love dancing and reading中的第一个ing</td></tr><tr class="odd"><td><strong>(?!exp)</strong></td><td>匹配后面不是exp的位置</td><td></td><td></td></tr><tr class="even"><td><strong>(?&lt;!exp)</strong></td><td>匹配前面不是exp的位置</td><td></td><td></td></tr><tr class="odd"><td><strong>*?</strong></td><td>重复任意次，但尽可能少重复</td><td>a.\b&lt;br&gt;a.\?b</td><td>将正则表达式应用于aabab，前者会匹配整个字符串aabab，后者会匹配aab和ab两个字符串</td></tr><tr class="even"><td><strong>+?</strong></td><td>重复1次或多次，但尽可能少重复</td><td></td><td></td></tr><tr class="odd"><td><strong>??</strong></td><td>重复0次或1次，但尽可能少重复</td><td></td><td></td></tr><tr class="even"><td><strong>{M,N}?</strong></td><td>重复M到N次，但尽可能少重复</td><td></td><td></td></tr><tr class="odd"><td><strong>{M,}?</strong></td><td>重复M次以上，但尽可能少重复</td><td></td><td></td></tr></tbody></table><p>说明：如果需要匹配的字符是正则表达式中的特殊字符，那么可以使用\进行转义处理，例如想匹配小数点可以写成\.就可以了，因为直接写.会匹配任意字符；同理，想匹配圆括号必须写成\(和\)，否则圆括号被视为正则表达式中的分组。</p>]]></content>
    
    
    
    <tags>
      
      <tag>正则表达式</tag>
      
      <tag>基础语法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>用wordcoloud生成超炫酷的词云_内含python源码</title>
    <link href="/2023/05/10/%E7%94%A8wordcoloud%E7%94%9F%E6%88%90%E8%B6%85%E7%82%AB%E9%85%B7%E7%9A%84%E8%AF%8D%E4%BA%91-%E5%86%85%E5%90%ABpython%E6%BA%90%E7%A0%81/"/>
    <url>/2023/05/10/%E7%94%A8wordcoloud%E7%94%9F%E6%88%90%E8%B6%85%E7%82%AB%E9%85%B7%E7%9A%84%E8%AF%8D%E4%BA%91-%E5%86%85%E5%90%ABpython%E6%BA%90%E7%A0%81/</url>
    
    <content type="html"><![CDATA[<p>使用jieba分词，wordcoloud词云可视化</p><span id="more"></span><p><img src="/images/2024年5月10日ai.jpg" /> <strong>环境准备</strong>pip 安装jieba库,wordcloud库与scipy库 <strong>资料准备</strong></p><ul class="task-list"><li><label><inputtype="checkbox" />用于分词的文本:词频统计_AIjob.csv</label></li><li><label><inputtype="checkbox" />禁止统计词库:stopwords.txt</label></li><li><label><inputtype="checkbox" />自定义分词库:人工智能词汇.txt</label></li><li><label><inputtype="checkbox" />掩膜用的形状图片:mask.jpg</label></li></ul><p>不废话,直接上码 ### 1.用结巴分词,生成字典对象 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">import</span> wordcloud<br><span class="hljs-comment"># 读取文件</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;词频统计_AIjob.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    desc = f.read()<br><br><span class="hljs-comment"># 加载停用词列表</span><br>stop_words = []<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;stopwords.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:<br>        stop_words.append(line.strip())<br><br>jieba.load_userdict(<span class="hljs-string">&quot;人工智能词汇.txt&quot;</span>)<br><span class="hljs-comment"># 分词</span><br>words = jieba.cut(desc, cut_all=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 过滤停用词</span><br>filtered_words = []<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(word) &gt; <span class="hljs-number">1</span>:<br>        filtered_words.append(word)<br><br><span class="hljs-comment"># 统计词频</span><br>word_counts = Counter(filtered_words)<br><br>w100=word_counts.most_common(<span class="hljs-number">500</span>)<br><span class="hljs-comment"># 使用字典推导将列表转换为字典  </span><br>dict_result = &#123;key: value <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> w100&#125; <br></code></pre></td></tr></table></figure> ###2.用wordcloud 生成词云 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> scipy.ndimage <span class="hljs-keyword">import</span> gaussian_gradient_magnitude<br><span class="hljs-keyword">from</span> wordcloud <span class="hljs-keyword">import</span> WordCloud, ImageColorGenerator<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pic_wordcloud</span>(<span class="hljs-params">dict_result,img_path,out_path</span>):<br><br>    <br>    <span class="hljs-comment"># img_path=r&quot;E:\jupyter\spyder\bosszhipin\词频统计\pic\T1.jpg&quot;</span><br>    <br>    parrot_color = np.array(Image.<span class="hljs-built_in">open</span>(img_path))<br>    <br>    parrot_color = parrot_color[::<span class="hljs-number">3</span>, ::<span class="hljs-number">3</span>]<br>    <br>    <span class="hljs-comment"># create mask  white is &quot;masked out&quot;</span><br>    parrot_mask = parrot_color.copy()<br>    parrot_mask[parrot_mask.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">2</span>) == <span class="hljs-number">0</span>] = <span class="hljs-number">255</span><br>    <br>    <br>    edges = np.mean([gaussian_gradient_magnitude(parrot_color[:, :, i] / <span class="hljs-number">255.</span>, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)], axis=<span class="hljs-number">0</span>)<br>    parrot_mask[edges &gt; <span class="hljs-number">.08</span>] = <span class="hljs-number">255</span><br>    <br>    <br>    <span class="hljs-comment"># acurately but it makes a better picture</span><br>    wc = WordCloud(max_words=<span class="hljs-number">1000</span>, mask=parrot_mask, max_font_size=<span class="hljs-number">40</span>, random_state=<span class="hljs-number">42</span>, font_path=<span class="hljs-string">r&quot;C:\Users\10921\AppData\Local\Microsoft\Windows\Fonts\方正正准黑简体.ttf&quot;</span>,relative_scaling=<span class="hljs-number">0</span>,<br>                   <span class="hljs-comment">#width=1920, height=1080</span><br>                   )<br>    <br>    <span class="hljs-comment"># generate word cloud</span><br>    wc.generate_from_frequencies(dict_result)<br>    <span class="hljs-comment"># plt.imshow(wc)</span><br>    <br>    <span class="hljs-comment"># create coloring from image</span><br>    image_colors = ImageColorGenerator(parrot_color)<br>    wc.recolor(color_func=image_colors)<br>    <span class="hljs-comment"># plt.figure(figsize=(10, 10))</span><br>    <span class="hljs-comment"># plt.imshow(wc, interpolation=&quot;bilinear&quot;)</span><br>    <span class="hljs-comment"># wc.to_file(&quot;parrot_new.png&quot;)</span><br>    wc.to_file(out_path)<br>img_path=<span class="hljs-string">&quot;mask.jpg&quot;</span><br>out_path=<span class="hljs-string">&#x27;output.png&#x27;</span><br>pic_wordcloud(dict_result,img_path,out_path)<br></code></pre></td></tr></table></figure> <imgsrc="/images/2024年5月10日color112.png" /> <imgsrc="/images/2024年5月10日parrot_new.png" /></p>]]></content>
    
    
    
    <tags>
      
      <tag>可视化</tag>
      
      <tag>数据分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>光影集-2022</title>
    <link href="/2022/12/13/%E5%85%89%E5%BD%B1%E9%9B%862022/"/>
    <url>/2022/12/13/%E5%85%89%E5%BD%B1%E9%9B%862022/</url>
    
    <content type="html"><![CDATA[<h1 id="难忘那年">难忘那年</h1><figure><img src="/images/光影集2022/image-20240513215734472.png"alt="月色苍茫" /><figcaption aria-hidden="true">月色苍茫</figcaption></figure><figure><img src="/images/光影集2022/image-20240513215908875.png"alt="鱼来鱼往" /><figcaption aria-hidden="true">鱼来鱼往</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220042681.png"alt="花开灿烂" /><figcaption aria-hidden="true">花开灿烂</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220159390.png"alt="绿意盎然" /><figcaption aria-hidden="true">绿意盎然</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220313485.png"alt="雨打蕉叶" /><figcaption aria-hidden="true">雨打蕉叶</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220422759.png"alt="风华正茂" /><figcaption aria-hidden="true">风华正茂</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>生活记录</tag>
      
      <tag>摄影作品</tag>
      
      <tag>难忘那年</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人工智能专业术语汇编</title>
    <link href="/2022/09/11/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E6%B1%87%E7%BC%96/"/>
    <url>/2022/09/11/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E6%B1%87%E7%BC%96/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本术语库目前拥有专业术语约 2442 个、专项领域篇 2篇，主要为人工智能领域基础概念和术语。</p><p>本术语库前两版主要是将机器之心在编译技术文章和论文过程中所遇到的专业术语记录下来，希望有助于AI从业者的查阅和学习<span id="more"></span> # 人工智能--术语库 引自 github的 <ahref="https://github.com/jiqizhixin/Artificial-Intelligence-Terminology-Database">Artificial-Intelligence-Terminology-Database</a>项目</p></blockquote><table style="width:100%;"><thead><tr class="header"><th>索引编号</th><th>英文术语</th><th>中文翻译</th><th>常用缩写</th><th>来源&amp;扩展</th><th>备注</th></tr></thead><tbody><tr class="odd"><td>AITD-00000</td><td>0-1 Loss Function</td><td>0-1损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00001</td><td>Absolute Loss Function</td><td>绝对损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00002</td><td>Absolute Value Rectification</td><td>绝对值整流</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00003</td><td>Accept-Reject Sampling Method</td><td>接受-拒绝抽样法/接受-拒绝采样法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00004</td><td>Acceptance Distribution</td><td>接受分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00005</td><td>Access Parameters</td><td>访问参数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00006</td><td>Accumulated Error Backpropagation</td><td>累积误差反向传播</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00007</td><td>Accuracy</td><td>准确率</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00008</td><td>Acoustic</td><td>声学</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00009</td><td>Acoustic Modeling</td><td>声学建模</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00010</td><td>Acquisition Function</td><td>采集函数</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-18-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00011</td><td>Action</td><td>动作</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00012</td><td>Action Value Function</td><td>动作价值函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00013</td><td>Actionism</td><td>行为主义</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00014</td><td>Activation</td><td>活性值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00015</td><td>Activation Function</td><td>激活函数</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-06-11-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-18-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[4]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00016</td><td>Active Learning</td><td>主动学习</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00017</td><td>Actor</td><td>演员</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00018</td><td>Actor-Critic Algorithm</td><td>演员-评论员算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00019</td><td>Actor-Critic Method</td><td>演员-评论员法</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-14">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00020</td><td>Adaptive Bitrate Algorithm</td><td>自适应比特率算法</td><td>ABR</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00021</td><td>Adaptive Boosting</td><td>AdaBoost</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00022</td><td>Adaptive Gradient Algorithm</td><td>AdaGrad</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00023</td><td>Adaptive Moment Estimation Algorithm</td><td>Adam算法</td><td>Adam</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00024</td><td>Adaptive Resonance Theory</td><td>自适应谐振理论</td><td>ART</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00025</td><td>Additive Model</td><td>加性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00026</td><td>Adversarial</td><td>对抗</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00027</td><td>Adversarial Example</td><td>对抗样本</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-06-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00028</td><td>Adversarial Networks</td><td>对抗网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-08-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00029</td><td>Adversarial Training</td><td>对抗训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00030</td><td>Affine Layer</td><td>仿射层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00031</td><td>Affine Transformation</td><td>仿射变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00032</td><td>Affinity Matrix</td><td>亲和矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00033</td><td>Agent</td><td>智能体</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-04-06-6">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-15-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-10-2">[3]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-29-5">[4]</a></td><td></td></tr><tr class="odd"><td>AITD-00034</td><td>Agglomerative</td><td>聚合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00035</td><td>Agnostic PAC Learnable</td><td>不可知PAC可学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00036</td><td>Algorithm</td><td>算法</td><td></td><td><ahref="https://jiqizhixin.github.io/AI-Terminology-page/">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-05-23-4">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-04-2">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00037</td><td>Almost Everywhere</td><td>几乎处处</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00038</td><td>Almost Sure</td><td>几乎必然</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00039</td><td>Almost Sure Convergence</td><td>几乎必然收敛</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00040</td><td>Alpha-Beta Pruning</td><td>α-β修剪法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00041</td><td>Alternative Splicing Dataset</td><td>选择性剪接数据集</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00042</td><td>Ambiguity</td><td>分歧</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00043</td><td>Analytic Gradient</td><td>解析梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00044</td><td>Ancestral Sampling</td><td>原始采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00045</td><td>Annealed Importance Sampling</td><td>退火重要采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00046</td><td>Anomaly Detection</td><td>异常检测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00047</td><td>Aperiodic</td><td>非周期的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00048</td><td>Aperiodic Graph</td><td>非周期性图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00049</td><td>Application-Specific Integrated Circuit</td><td>专用集成电路</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00050</td><td>Approximate Bayesian Computation</td><td>近似贝叶斯计算</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00051</td><td>Approximate Dynamic Programming</td><td>近似动态规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00052</td><td>Approximate Inference</td><td>近似推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00053</td><td>Approximation</td><td>近似</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00054</td><td>Approximation Error</td><td>近似误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00055</td><td>Architecture</td><td>架构</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-12">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00056</td><td>Area Under ROC Curve</td><td>AUC（ROC曲线下方面积，度量分类模型好坏的标准）</td><td>AUC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00057</td><td>Arithmetic Coding</td><td>算术编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00058</td><td>Artificial General Intelligence</td><td>通用人工智能</td><td>AGI</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-06-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00059</td><td>Artificial Intelligence</td><td>人工智能</td><td>AI</td><td><a href="https://www.jiqizhixin.com/articles/2017-05-21-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-05-21-7">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-05-17-16">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[5]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00060</td><td>Artificial Neural Network</td><td>人工神经网络</td><td>ANN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00061</td><td>Artificial Neuron</td><td>人工神经元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00062</td><td>Association Analysis</td><td>关联分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00063</td><td>Associative Memory</td><td>联想记忆</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00064</td><td>Associative Memory Model</td><td>联想记忆模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00065</td><td>Asymptotically Unbiased</td><td>渐近无偏</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00066</td><td>Asynchronous Stochastic Gradient Descent</td><td>异步随机梯度下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00067</td><td>Asynchronous</td><td>异步</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00068</td><td>Atrous Convolution</td><td>空洞卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00069</td><td>Attention</td><td>注意力</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00070</td><td>Attention Cue</td><td>注意力提示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00071</td><td>Attention Distribution</td><td>注意力分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00072</td><td>Attention Mechanism</td><td>注意力机制</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-06-19-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-14-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-28-5">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00073</td><td>Attention Model</td><td>注意力模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00074</td><td>Attractor</td><td>吸引点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00075</td><td>Attribute</td><td>属性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00076</td><td>Attribute Conditional Independence Assumption</td><td>属性条件独立性假设</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00077</td><td>Attribute Space</td><td>属性空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00078</td><td>Attribute Value</td><td>属性值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00079</td><td>Augmented Lagrangian</td><td>增广拉格朗日法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00080</td><td>Auto-Regressive Network</td><td>自回归网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00081</td><td>Autoencoder</td><td>自编码器</td><td>AE</td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-26-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00082</td><td>Automatic Differentiation</td><td>自动微分</td><td>AD</td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-07">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00083</td><td>Automatic Speech Recognition</td><td>自动语音识别</td><td>ASR</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00084</td><td>Automatic Summarization</td><td>自动摘要</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00085</td><td>Autoregressive Generative Model</td><td>自回归生成模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00086</td><td>Autoregressive Model</td><td>自回归模型</td><td>AR</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00087</td><td>Autoregressive Process</td><td>自回归过程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00088</td><td>Average Gradient</td><td>平均梯度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00089</td><td>Average Pooling Layer</td><td>平均汇聚层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00090</td><td>Average-Pooling</td><td>平均汇聚</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00091</td><td>Averaged Perceptron</td><td>平均感知器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00092</td><td>Back Propagation</td><td>反向传播</td><td>BP</td><td><a href="https://www.jiqizhixin.com/articles/2016-11-25-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00093</td><td>Back Propagation Algorithm</td><td>反向传播算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00094</td><td>Back Propagation Through Time</td><td>随时间反向传播</td><td>BPTT</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00095</td><td>Back-Off</td><td>回退</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00096</td><td>Backward</td><td>后向</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00097</td><td>Backward Induction</td><td>反向归纳</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00098</td><td>Backward Search</td><td>反向搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00099</td><td>Bag of Words</td><td>词袋</td><td>BOW</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00100</td><td>Bagging</td><td>袋装</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00101</td><td>Bandit</td><td>赌博机/老虎机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00102</td><td>Bandpass Filter</td><td>带通滤波器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00103</td><td>Base</td><td>基</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00104</td><td>Base Classifier</td><td>基分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00105</td><td>Base Learner</td><td>基学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00106</td><td>Base Learning Algorithm</td><td>基学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00107</td><td>Base Vector</td><td>基向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00108</td><td>Baseline</td><td>基准</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00109</td><td>Basin of Attraction</td><td>吸引域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00110</td><td>Batch</td><td>批量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00111</td><td>Batch Gradient Descent</td><td>批量梯度下降法</td><td>BGD</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00112</td><td>Batch Learning</td><td>批量学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00113</td><td>Batch Normalization</td><td>批量规范化</td><td>BN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00114</td><td>Batch Size</td><td>批量大小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00115</td><td>Baum-Welch Algorithm</td><td>Baum-Welch算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00116</td><td>Bayes Classifier</td><td>贝叶斯分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00117</td><td>Bayes Decision Rule</td><td>贝叶斯决策准则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00118</td><td>Bayes Error</td><td>贝叶斯误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00119</td><td>Bayes Model Averaging</td><td>贝叶斯模型平均</td><td>BMA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00120</td><td>Bayes Optimal Classifier</td><td>贝叶斯最优分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00121</td><td>Bayes Risk</td><td>贝叶斯风险</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00122</td><td>Bayes' Rule</td><td>贝叶斯规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00123</td><td>Bayes' Theorem</td><td>贝叶斯定理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00124</td><td>Bayesian Decision Theory</td><td>贝叶斯决策理论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00125</td><td>Bayesian Estimation</td><td>贝叶斯估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00126</td><td>Bayesian Inference</td><td>贝叶斯推断</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-00127</td><td>Bayesian Learning</td><td>贝叶斯学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00128</td><td>Bayesian Linear Regression</td><td>贝叶斯线性回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00129</td><td>Bayesian Network</td><td>贝叶斯网/贝叶斯网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>Network翻译为网或网络皆可，只要统一翻译成网或者统一翻译成网络即可；统计，机器学习</td></tr><tr class="odd"><td>AITD-00130</td><td>Bayesian Optimization</td><td>贝叶斯优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-28">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00131</td><td>Bayesian Probability</td><td>贝叶斯概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00132</td><td>Bayesian Statistics</td><td>贝叶斯统计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00133</td><td>Beam Search</td><td>束搜索</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-31-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00134</td><td>Benchmark</td><td>基准</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00135</td><td>Belief Network</td><td>信念网/信念网络</td><td>BN</td><td>[1]</td><td>Network翻译为网或网络皆可，只要统一翻译成网或者统一翻译成网络即可</td></tr><tr class="odd"><td>AITD-00136</td><td>Belief Propagation</td><td>信念传播</td><td>BP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00137</td><td>Bellman Equation</td><td>贝尔曼方程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00138</td><td>Bellman Optimality Equation</td><td>贝尔曼最优方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00139</td><td>Bernoulli Distribution</td><td>伯努利分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-00140</td><td>Bernoulli Output Distribution</td><td>伯努利输出分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00141</td><td>Best-Arm Problem</td><td>最优臂问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00142</td><td>Beta Distribution</td><td>贝塔分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00143</td><td>Between-Class Scatter Matrix</td><td>类间散度矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00144</td><td>BFGS</td><td>BFGS</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00145</td><td>Bi-Directional Long-Short Term Memory</td><td>双向长短期记忆</td><td>Bi-LSTM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00146</td><td>Bi-Partition</td><td>二分法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00147</td><td>Bias</td><td>偏差/偏置</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[4]</a></td><td>看上下语境；机器学习</td></tr><tr class="odd"><td>AITD-00148</td><td>Bias In Affine Function</td><td>偏置</td><td></td><td>[1]</td><td>看上下语境</td></tr><tr class="even"><td>AITD-00149</td><td>Bias In Statistics</td><td>偏差</td><td></td><td>[1]</td><td>看上下语境</td></tr><tr class="odd"><td>AITD-00150</td><td>Bias Shift</td><td>偏置偏移</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00151</td><td>Bias-Variance Decomposition</td><td>偏差 - 方差分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00152</td><td>Bias-Variance Dilemma</td><td>偏差 - 方差困境</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00153</td><td>Biased</td><td>有偏</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00154</td><td>Biased Importance Sampling</td><td>有偏重要采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00155</td><td>Bidirectional Language Model</td><td>双向语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00156</td><td>Bidirectional Recurrent Neural Network</td><td>双向循环神经网络</td><td>Bi-RNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00157</td><td>Bigram</td><td>二元语法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00158</td><td>Bilingual Evaluation Understudy</td><td>BLEU</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00159</td><td>Binary Classification</td><td>二分类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00160</td><td>Binary Relation</td><td>二元关系</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00161</td><td>Binary Sparse Coding</td><td>二值稀疏编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00162</td><td>Binomial Distribution</td><td>二项分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00163</td><td>Binomial Logistic Regression Model</td><td>二项对数几率回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00164</td><td>Binomial Test</td><td>二项检验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00165</td><td>Biological Plausibility</td><td>生物学合理性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00166</td><td>Bit</td><td>比特</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00167</td><td>Block</td><td>块</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00168</td><td>Block Coordinate Descent</td><td>块坐标下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00169</td><td>Block Gibbs Sampling</td><td>块吉布斯采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00170</td><td>Boilerplate Code</td><td>样板代码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00171</td><td>Boltzmann</td><td>玻尔兹曼</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00172</td><td>Boltzmann Distribution</td><td>玻尔兹曼分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00173</td><td>Boltzmann Factor</td><td>玻尔兹曼因子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00174</td><td>Boltzmann Machine</td><td>玻尔兹曼机</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-08-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00175</td><td>Boosting</td><td>Boosting（一种模型训练加速方式）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00176</td><td>Boosting Tree</td><td>提升树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00177</td><td>Bootstrap Aggregating</td><td>Bagging</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00178</td><td>Bootstrap Sampling</td><td>自助采样法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00179</td><td>Bootstrapping</td><td>自助法/自举法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00180</td><td>Bottleneck Layer</td><td>瓶颈层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00181</td><td>Bottom-Up</td><td>自下而上</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00182</td><td>Bounding Boxes</td><td>边界框</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00183</td><td>Break-Event Point</td><td>平衡点</td><td>BEP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00184</td><td>Bridge Sampling</td><td>桥式采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00185</td><td>Broadcasting</td><td>广播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00186</td><td>Broyden's Algorithm</td><td>Broyden类算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00187</td><td>Bucketing</td><td>分桶</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00188</td><td>Burn-In Period</td><td>预烧期</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00189</td><td>Burning-In</td><td>磨合</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00190</td><td>Calculus</td><td>微积分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00191</td><td>Calculus of Variations</td><td>变分法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00192</td><td>Calibration</td><td>校准</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00193</td><td>Canonical</td><td>正则的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00194</td><td>Canonical Correlation Analysis</td><td>典型相关分析</td><td>CCA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00195</td><td>Capacity</td><td>容量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00196</td><td>Cartesian Coordinate</td><td>笛卡尔坐标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00197</td><td>Cascade</td><td>级联</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00198</td><td>Cascade-Correlation</td><td>级联相关</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00199</td><td>Catastrophic Forgetting</td><td>灾难性遗忘</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00200</td><td>Categorical Attribute</td><td>分类属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00201</td><td>Categorical Distribution</td><td>类别分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00202</td><td>Causal Factor</td><td>因果因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00203</td><td>Causal Modeling</td><td>因果模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00204</td><td>Cell</td><td>单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00205</td><td>Centered Difference</td><td>中心差分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00206</td><td>Central Limit Theorem</td><td>中心极限定理</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00207</td><td>Chain Rule</td><td>链式法则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00208</td><td>Channel</td><td>通道</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00209</td><td>Chaos</td><td>混沌</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00210</td><td>Chebyshev Distance</td><td>切比雪夫距离</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00211</td><td>Chord</td><td>弦</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00212</td><td>Chordal Graph</td><td>弦图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00213</td><td>City Block Distance</td><td>街区距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00214</td><td>Class</td><td>类别</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00215</td><td>Class Label</td><td>类标记</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00216</td><td>Class-Conditional Probability</td><td>类条件概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00217</td><td>Class-Imbalance</td><td>类别不平衡</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00218</td><td>Classification</td><td>分类</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00219</td><td>Classification And Regression Tree</td><td>分类与回归树</td><td>CART</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00220</td><td>Classifier</td><td>分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00221</td><td>Clip Gradient</td><td>梯度截断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00222</td><td>Clipping The Gradient</td><td>截断梯度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00223</td><td>Clique</td><td>团</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00224</td><td>Clique Potential</td><td>团势能</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00225</td><td>Clockwork RNN</td><td>时钟循环神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00226</td><td>Closed Form Solution</td><td>闭式解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00227</td><td>Closed-Form</td><td>闭式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00228</td><td>Cluster</td><td>簇</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00229</td><td>Cluster Analysis</td><td>聚类分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00230</td><td>Cluster Assumption</td><td>聚类假设</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00231</td><td>Clustering</td><td>聚类</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-09">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00232</td><td>Clustering Ensemble</td><td>聚类集成</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00233</td><td>Co-Adapting</td><td>共适应</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00234</td><td>Co-Occurrence</td><td>共现</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00235</td><td>Co-Occurrence Frequency</td><td>共现词频</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00236</td><td>Co-Training</td><td>协同训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00237</td><td>Code</td><td>编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00238</td><td>Codebook Learning</td><td>码书学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00239</td><td>Coding Matrix</td><td>编码矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00240</td><td>Collaborative Filtering</td><td>协同过滤</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-23-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00241</td><td>Collapsed Gibbs Sampling</td><td>收缩的吉布斯抽样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00242</td><td>Collinearity</td><td>共线性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00243</td><td>COLT</td><td>国际学习理论会议</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00244</td><td>Column</td><td>列</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00245</td><td>Column Space</td><td>列空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00246</td><td>Combinatorial Optimization</td><td>组合优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00247</td><td>Committee-Based Learning</td><td>基于委员会的学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00248</td><td>Common Cause</td><td>共因</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00249</td><td>Common Parent</td><td>同父</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00250</td><td>Compact Singular Value Decomposition</td><td>紧奇异值分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00251</td><td>Competitive Learning</td><td>竞争型学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00252</td><td>Complementary Slackness</td><td>互补松弛</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00253</td><td>Complete Graph</td><td>完全图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00254</td><td>Complete Linkage</td><td>完全连接</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00255</td><td>Complete-Data</td><td>完全数据</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00256</td><td>Complex Cell</td><td>复杂细胞</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00257</td><td>Component Learner</td><td>组件学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00258</td><td>Compositionality</td><td>组合性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00259</td><td>Comprehensibility</td><td>可解释性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00260</td><td>Computation Cost</td><td>计算代价</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00261</td><td>Computation Graph</td><td>计算图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00262</td><td>Computational Learning Theory</td><td>计算学习理论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00263</td><td>Computational Linguistics</td><td>计算语言学</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00264</td><td>Computer Vision</td><td>计算机视觉</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00265</td><td>Concatenate</td><td>连结</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00266</td><td>Concept Class</td><td>概念类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00267</td><td>Concept Drift</td><td>概念漂移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00268</td><td>Concept Learning System</td><td>概念学习系统</td><td>CLS</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00269</td><td>Concept Shift</td><td>概念偏移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00270</td><td>Conditional Computation</td><td>条件计算</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00271</td><td>Conditional Entropy</td><td>条件熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00272</td><td>Conditional Independence</td><td>条件独立</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00273</td><td>Conditional Language Model</td><td>条件语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00274</td><td>Conditional Mutual Information</td><td>条件互信息</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00275</td><td>Conditional Probability</td><td>条件概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00276</td><td>Conditional Probability Density Function</td><td>条件概率密度函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00277</td><td>Conditional Probability Distribution</td><td>条件概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00278</td><td>Conditional Probability Table</td><td>条件概率表</td><td>CPT</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00279</td><td>Conditional Random Field</td><td>条件随机场</td><td>CRF</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00280</td><td>Conditional Risk</td><td>条件风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00281</td><td>Conditionally Independent</td><td>条件独立的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00282</td><td>Conference On Neural Information Processing Systems</td><td>国际神经信息处理系统会议</td><td>NeurIPS</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-18-9">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00283</td><td>Confidence</td><td>置信度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00284</td><td>Conflict Resolution</td><td>冲突消解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00285</td><td>Confusion Matrix</td><td>混淆矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00286</td><td>Conjugate</td><td>共轭</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00287</td><td>Conjugate Directions</td><td>共轭方向</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00288</td><td>Conjugate Distribution</td><td>共轭分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00289</td><td>Conjugate Gradient</td><td>共轭梯度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>优化，数学</td></tr><tr class="odd"><td>AITD-00290</td><td>Conjugate Prior</td><td>共轭先验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00291</td><td>Connection Weight</td><td>连接权</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00292</td><td>Connectionism</td><td>连接主义</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00293</td><td>Consistency</td><td>一致性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00294</td><td>Consistency Convergence</td><td>一致性收敛</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00295</td><td>Constrained Optimization</td><td>约束优化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00296</td><td>Content-Addressable Memory</td><td>基于内容寻址的存储</td><td>CAM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00297</td><td>Context Variable</td><td>上下文变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00298</td><td>Context Vector</td><td>上下文向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00299</td><td>Context Window</td><td>上下文窗口</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00300</td><td>Context Word</td><td>上下文词</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00301</td><td>Context-Specific Independences</td><td>特定上下文独立</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00302</td><td>Contextual Bandit</td><td>上下文赌博机/上下文老虎机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00303</td><td>Contextualized Representation</td><td>基于上下文的表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00304</td><td>Contingency Table</td><td>列联表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00305</td><td>Continous Bag-Of-Words Model</td><td>连续词袋模型</td><td>CBOW</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00306</td><td>Continuation Method</td><td>延拓法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00307</td><td>Continuing Task</td><td>持续式任务</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00308</td><td>Continuous Attribute</td><td>连续属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00309</td><td>Continuous Learning</td><td>持续学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00310</td><td>Continuous Optimization</td><td>连续优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00311</td><td>Contractive</td><td>收缩</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00312</td><td>Contractive Autoencoder</td><td>收缩自编码器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00313</td><td>Contractive Neural Network</td><td>收缩神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00314</td><td>Contrastive Divergence</td><td>对比散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00315</td><td>Controller</td><td>控制器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00316</td><td>Convergence</td><td>收敛</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00317</td><td>Conversational Agent</td><td>会话智能体</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00318</td><td>Convex Optimization</td><td>凸优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-29-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00319</td><td>Convex Quadratic Programming</td><td>凸二次规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00320</td><td>Convex Set</td><td>凸集</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00321</td><td>Convexity</td><td>凸性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00322</td><td>Convolution</td><td>卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00323</td><td>Convolutional Boltzmann Machine</td><td>卷积玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00324</td><td>Convolutional Deep Belief Network</td><td>卷积深度信念网络</td><td>CDBN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00325</td><td>Convolutional Kernel</td><td>卷积核</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00326</td><td>Convolutional Network</td><td>卷积网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00327</td><td>Convolutional Neural Network</td><td>卷积神经网络</td><td>CNN</td><td><a href="https://www.jiqizhixin.com/articles/2017-12-19-8">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-08-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-18-2">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-00328</td><td>Coordinate</td><td>坐标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00329</td><td>Coordinate Ascent</td><td>坐标上升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00330</td><td>Coordinate Descent</td><td>坐标下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00331</td><td>Coparent</td><td>共父</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00332</td><td>Corpus</td><td>语料库</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00333</td><td>Correlation</td><td>相关系数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00334</td><td>Correlation Coefficient</td><td>相关系数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00335</td><td>Cosine</td><td>余弦</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00336</td><td>Cosine Decay</td><td>余弦衰减</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00337</td><td>Cosine Similarity</td><td>余弦相似度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00338</td><td>Cost</td><td>代价</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00339</td><td>Cost Curve</td><td>代价曲线</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00340</td><td>Cost Function</td><td>代价函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00341</td><td>Cost Matrix</td><td>代价矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00342</td><td>Cost-Sensitive</td><td>代价敏感</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00343</td><td>Covariance</td><td>协方差</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00344</td><td>Covariance Matrix</td><td>协方差矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00345</td><td>Covariance RBM</td><td>协方差RBM</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00346</td><td>Covariate Shift</td><td>协变量偏移</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00347</td><td>Coverage</td><td>覆盖</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00348</td><td>Credit Assignment Problem</td><td>贡献度分配问题</td><td>CAP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00349</td><td>Criterion</td><td>准则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00350</td><td>Critic</td><td>评论员</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00351</td><td>Critic Network</td><td>评价网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00352</td><td>Critical Point</td><td>临界点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00353</td><td>Critical Temperatures</td><td>临界温度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00354</td><td>Cross Correlation</td><td>互相关</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00355</td><td>Cross Entropy</td><td>交叉熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00356</td><td>Cross Validation</td><td>交叉验证</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-16-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00357</td><td>Cross-Entropy Loss Function</td><td>交叉熵损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00358</td><td>Crowdsourcing</td><td>众包</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-28-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00359</td><td>Cumulative Distribution Function</td><td>累积分布函数</td><td>CDF</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00360</td><td>Cumulative Function</td><td>累积函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00361</td><td>Curriculum Learning</td><td>课程学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00362</td><td>Curse of Dimensionality</td><td>维数灾难</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00363</td><td>Curvature</td><td>曲率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00364</td><td>Curve-Fitting</td><td>曲线拟合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00365</td><td>Cut Point</td><td>截断点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00366</td><td>Cutting Plane Algorithm</td><td>割平面法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00367</td><td>Cybernetics</td><td>控制论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00368</td><td>Cyclic Learning Rate</td><td>循环学习率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00369</td><td>Damping</td><td>衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00370</td><td>Damping Factor</td><td>阻尼因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00371</td><td>Data</td><td>数据</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00372</td><td>Data Augmentation</td><td>数据增强</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00373</td><td>Data Generating Distribution</td><td>数据生成分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00374</td><td>Data Generating Process</td><td>数据生成过程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00375</td><td>Data Instance</td><td>数据样本</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00376</td><td>Data Mining</td><td>数据挖掘</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00377</td><td>Data Parallelism</td><td>数据并行</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00378</td><td>Data Point</td><td>数据点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00379</td><td>Data Preprocessing</td><td>数据预处理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00380</td><td>Data Set</td><td>数据集</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-04-6">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00381</td><td>Data Wrangling</td><td>数据整理</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-25-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00382</td><td>Dataset Augmentation</td><td>数据集增强</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00383</td><td>Davidon-Fletcher-Powell</td><td>DFP</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00384</td><td>Debugging Strategy</td><td>调试策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00385</td><td>Decision Boundary</td><td>决策边界</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00386</td><td>Decision Function</td><td>决策函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00387</td><td>Decision Stump</td><td>决策树桩</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00388</td><td>Decision Surface</td><td>决策平面</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00389</td><td>Decision Tree</td><td>决策树</td><td>DT</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-10">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-29-5">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[4]</a></td><td></td></tr><tr class="odd"><td>AITD-00390</td><td>Decoder</td><td>解码器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00391</td><td>Decoding</td><td>解码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00392</td><td>Decompose</td><td>分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00393</td><td>Deconvolution</td><td>反卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00394</td><td>Deconvolutional Network</td><td>反卷积网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-14">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00395</td><td>Deduction</td><td>演绎</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00396</td><td>Deep Belief Network</td><td>深度信念网络</td><td>DBN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00397</td><td>Deep Boltzmann Machine</td><td>深度玻尔兹曼机</td><td>DBM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00398</td><td>Deep Circuit</td><td>深度回路</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00399</td><td>Deep Convolutional Generative Adversarial Network</td><td>深度卷积生成对抗网络</td><td>DCGAN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00400</td><td>Deep Feedforward Network</td><td>深度前馈网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00401</td><td>Deep Generative Model</td><td>深度生成模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00402</td><td>Deep Learning</td><td>深度学习</td><td>DL</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-17-2">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-15-4">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-15-2">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[4]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[5]</a></td><td></td></tr><tr class="even"><td>AITD-00403</td><td>Deep Model</td><td>深度模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00404</td><td>Deep Network</td><td>深度网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00405</td><td>Deep Neural Network</td><td>深度神经网络</td><td>DNN</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-15-2">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-10">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-07-2">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[5]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[6]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[7]</a></td><td></td></tr><tr class="odd"><td>AITD-00406</td><td>Deep Q-Learning</td><td>深度 Q 学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-10-10-2">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-08-22-8">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00407</td><td>Deep Q-Network</td><td>深度Q网络</td><td>DQN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00408</td><td>Deep Reinforcement Learning</td><td>深度强化学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00409</td><td>Deep Sequence Model</td><td>深度序列模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00410</td><td>Default Rule</td><td>默认规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00411</td><td>Definite Integral</td><td>定积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00412</td><td>Degree Of Belief</td><td>信任度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00413</td><td>Delta-Bar-Delta</td><td>Delta-Bar-Delta</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00414</td><td>Denoising</td><td>去噪</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00415</td><td>Denoising Autoencoder</td><td>去噪自编码器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00416</td><td>Denoising Score Matching</td><td>去躁分数匹配</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00417</td><td>Denominator Layout</td><td>分母布局</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00418</td><td>Dense</td><td>稠密</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00419</td><td>Density Estimation</td><td>密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00420</td><td>Density-Based Clustering</td><td>密度聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00421</td><td>Dependency</td><td>依赖</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00422</td><td>Depth</td><td>深度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00423</td><td>Derivative</td><td>导数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00424</td><td>Description</td><td>描述</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00425</td><td>Design Matrix</td><td>设计矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00426</td><td>Detailed Balance</td><td>细致平衡</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00427</td><td>Detailed Balance Equation</td><td>细致平衡方程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00428</td><td>Detector Stage</td><td>探测级</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00429</td><td>Determinant</td><td>行列式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00430</td><td>Deterministic</td><td>确定性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00431</td><td>Deterministic Model</td><td>确定性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00432</td><td>Deterministic Policy</td><td>确定性策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00433</td><td>Development Set</td><td>开发集</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00434</td><td>Diagonal Matrix</td><td>对角矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00435</td><td>Diameter</td><td>直径</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00436</td><td>Dictionary</td><td>字典</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00437</td><td>Dictionary Learning</td><td>字典学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00438</td><td>Differentiable Function</td><td>可微函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00439</td><td>Differentiable Neural Computer</td><td>可微分神经计算机</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-11-7">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00440</td><td>Differential Entropy</td><td>微分熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00441</td><td>Differential Equation</td><td>微分方程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00442</td><td>Differentiation</td><td>微分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00443</td><td>Dilated Convolution</td><td>膨胀卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00444</td><td>Dimension</td><td>维度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00445</td><td>Dimension Reduction</td><td>降维</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00446</td><td>Dimensionality Reduction Algorithm</td><td>降维算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-08-31-2">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00447</td><td>Dirac Delta Function</td><td>Dirac Delta函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00448</td><td>Dirac Distribution</td><td>Dirac分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00449</td><td>Directed</td><td>有向</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00450</td><td>Directed Acyclic Graph</td><td>有向非循环图</td><td>DAG</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00451</td><td>Directed Edge</td><td>有向边</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00452</td><td>Directed Graph</td><td>有向图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00453</td><td>Directed Graphical Model</td><td>有向图模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00454</td><td>Directed Model</td><td>有向模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00455</td><td>Directed Separation</td><td>有向分离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00456</td><td>Directional Derivative</td><td>方向导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00457</td><td>Dirichlet Distribution</td><td>狄利克雷分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00458</td><td>Disagreement Measure</td><td>不合度量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00459</td><td>Disagreement-Based Methods</td><td>基于分歧的方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00460</td><td>Discount Factor</td><td>衰减系数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00461</td><td>Discounted Return</td><td>折扣回报</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00462</td><td>Discrete Optimization</td><td>离散优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00463</td><td>Discriminant Function</td><td>判别函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00464</td><td>Discriminative Approach</td><td>判别方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00465</td><td>Discriminative Model</td><td>判别式模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00466</td><td>Discriminative RBM</td><td>判别RBM</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00467</td><td>Discriminator</td><td>判别器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00468</td><td>Discriminator Network</td><td>判别网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00469</td><td>Distance</td><td>距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00470</td><td>Distance Measure</td><td>距离度量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00471</td><td>Distance Metric Learning</td><td>距离度量学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00472</td><td>Distributed Representation</td><td>分布式表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00473</td><td>Distribution</td><td>分布</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-09">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00474</td><td>Diverge</td><td>发散</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00475</td><td>Divergence</td><td>散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00476</td><td>Diversity</td><td>多样性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00477</td><td>Diversity Measure</td><td>多样性度量/差异性度量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00478</td><td>Divide-And-Conquer</td><td>分而治之</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00479</td><td>Divisive</td><td>分裂</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00480</td><td>Domain</td><td>领域</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00481</td><td>Domain Adaptation</td><td>领域自适应</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00482</td><td>Dominant Eigenvalue</td><td>主特征值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00483</td><td>Dominant Eigenvector</td><td>主特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00485</td><td>Dominant Strategy</td><td>占优策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00486</td><td>Dot Product</td><td>点积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00487</td><td>Double Backprop</td><td>双反向传播</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00488</td><td>Doubly Block Circulant Matrix</td><td>双重分块循环矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00489</td><td>Down Sampling</td><td>下采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00490</td><td>Downstream Task</td><td>下游任务</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00491</td><td>Dropout</td><td>暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00492</td><td>Dropout Boosting</td><td>暂退Boosting</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00493</td><td>Dropout Mask</td><td>暂退掩码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00494</td><td>Dropout Method</td><td>暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00495</td><td>Dual Algorithm</td><td>对偶算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00496</td><td>Dual Problem</td><td>对偶问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00497</td><td>Dummy Node</td><td>哑结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00498</td><td>Dying ReLU Problem</td><td>死亡ReLU问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00499</td><td>Dynamic Bayesian Network</td><td>动态贝叶斯网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00500</td><td>Dynamic Computational Graph</td><td>动态计算图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00501</td><td>Dynamic Fusion</td><td>动态融合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00502</td><td>Dynamic Programming</td><td>动态规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00503</td><td>Dynamic Structure</td><td>动态结构</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00504</td><td>Dynamical System</td><td>动力系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00505</td><td>Eager Learning</td><td>急切学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00506</td><td>Early Stopping</td><td>早停</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00507</td><td>Earth-Mover's Distance</td><td>推土机距离</td><td>EMD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00508</td><td>Echo State Network</td><td>回声状态网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00509</td><td>Edge</td><td>边</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00510</td><td>Edge Device</td><td>边缘设备</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-24-8">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00511</td><td>Effective Capacity</td><td>有效容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00512</td><td>Eigendecomposition</td><td>特征分解</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-07-05-2">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00513</td><td>Eigenvalue</td><td>特征值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00514</td><td>Eigenvalue Decomposition</td><td>特征值分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00515</td><td>Elastic Net Regularization</td><td>弹性网络正则化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00516</td><td>Elastic Weight Consolidation</td><td>弹性权重巩固</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00517</td><td>Element-Wise Product</td><td>逐元素积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00518</td><td>Elementary Basis Vectors</td><td>基本单位向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00519</td><td>Ellipsoid Method</td><td>椭球法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00520</td><td>Embedding</td><td>嵌入</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-02-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00521</td><td>Embedding Lookup Table</td><td>嵌入表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00522</td><td>Emotional Analysis</td><td>情绪分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00523</td><td>Empirical Conditional Entropy</td><td>经验条件熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00524</td><td>Empirical Distribution</td><td>经验分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00525</td><td>Empirical Entropy</td><td>经验熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00526</td><td>Empirical Error</td><td>经验误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00527</td><td>Empirical Frequency</td><td>经验频率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00528</td><td>Empirical Loss</td><td>经验损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00529</td><td>Empirical Risk</td><td>经验风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00530</td><td>Empirical Risk Minimization</td><td>经验风险最小化</td><td>ERM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00531</td><td>Encoder</td><td>编码器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00532</td><td>Encoder-Decoder</td><td>编码器-解码器（模型）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-00533</td><td>Encoding</td><td>编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00534</td><td>End-To-End</td><td>端到端</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-15">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00535</td><td>End-To-End Learning</td><td>端到端学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00536</td><td>End-To-End Memory Network</td><td>端到端记忆网络</td><td>Memn2N</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00537</td><td>Energy Function</td><td>能量函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00538</td><td>Energy Gap</td><td>能量差异</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00539</td><td>Energy-Based Model</td><td>基于能量的模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00540</td><td>Ensemble</td><td>集成</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00541</td><td>Ensemble Learning</td><td>集成学习</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-14-8">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00542</td><td>Ensemble Pruning</td><td>集成修剪</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00543</td><td>Entropy</td><td>熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00544</td><td>Entropy Encoding</td><td>熵编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00545</td><td>Environment</td><td>环境</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00546</td><td>Episode</td><td>回合</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00547</td><td>Episodic Task</td><td>回合式任务</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00548</td><td>Epoch</td><td>轮</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00549</td><td>Equal-Width Convolution</td><td>等宽卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00550</td><td>Equality Constraint</td><td>等式约束</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00551</td><td>Equilibrium Distribution</td><td>均衡分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00552</td><td>Equivariance</td><td>等变</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00553</td><td>Equivariant Representations</td><td>等变表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00554</td><td>Error</td><td>误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00555</td><td>Error Backpropagation Algorithm</td><td>误差反向传播算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00556</td><td>Error Backpropagation</td><td>误差反向传播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00557</td><td>Error Bar</td><td>误差条</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00558</td><td>Error Correcting Output Codes</td><td>纠错输出编码</td><td>ECOC</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00559</td><td>Error Function</td><td>误差函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00560</td><td>Error Metric</td><td>误差度量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00561</td><td>Error Rate</td><td>错误率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00562</td><td>Error-Ambiguity Decomposition</td><td>误差－分歧分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00563</td><td>Estimation Error</td><td>估计误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00564</td><td>Estimation Of Mathematical Expectation</td><td>数学期望估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00565</td><td>Estimator</td><td>估计/估计量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00566</td><td>Euclidean Distance</td><td>欧氏距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00567</td><td>Euclidean Norm</td><td>欧几里得范数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00568</td><td>Euclidean Space</td><td>欧氏空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00569</td><td>Euler-Lagrange Equation</td><td>欧拉-拉格朗日方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00570</td><td>Evaluation Criterion</td><td>评价准则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00571</td><td>Evidence</td><td>证据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00572</td><td>Evidence Lower Bound</td><td>证据下界</td><td>ELBO</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00573</td><td>Evolution</td><td>演化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00574</td><td>Evolutionary Computation</td><td>演化计算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00575</td><td>Exact</td><td>确切的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00576</td><td>Exact Inference</td><td>精确推断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00577</td><td>Example</td><td>样例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00578</td><td>Excess Error</td><td>额外误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00579</td><td>Exchangeable</td><td>可交换的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00580</td><td>Expectation</td><td>期望</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00581</td><td>Expectation Maximization Algorithm</td><td>期望极大算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00582</td><td>Expectation Maximization</td><td>期望最大化</td><td>EM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00583</td><td>Expectation Step</td><td>E步</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00584</td><td>Expected Error</td><td>期望错误</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00585</td><td>Expected Loss</td><td>期望损失</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00586</td><td>Expected Return</td><td>期望回报</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00587</td><td>Expected Risk</td><td>期望风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00588</td><td>Expected Value</td><td>期望值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00589</td><td>Experience</td><td>经验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00590</td><td>Experience Replay</td><td>经验回放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00591</td><td>Expert Network</td><td>专家网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00592</td><td>Expert System</td><td>专家系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00593</td><td>Explaining Away</td><td>相消解释</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00594</td><td>Explaining Away Effect</td><td>相消解释作用</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00595</td><td>Explanatory Factort</td><td>解释因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00596</td><td>Explicit Density Model</td><td>显式密度模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00597</td><td>Exploding Gradient</td><td>梯度爆炸</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00598</td><td>Exploitation</td><td>利用</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00599</td><td>Exploration</td><td>探索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00600</td><td>Exploration-Exploitation Dilemma</td><td>探索-利用窘境</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00601</td><td>Exponential Decay</td><td>指数衰减</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00602</td><td>Exponential Distribution</td><td>指数分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00603</td><td>Exponential Linear Unit</td><td>指数线性单元</td><td>ELU</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00604</td><td>Exponential Loss</td><td>指数损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00605</td><td>Exponential Loss Function</td><td>指数损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00606</td><td>Exponentially Weighted Moving Average</td><td>指数加权移动平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00607</td><td>Exposure Bias</td><td>曝光偏差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00608</td><td>External Memory</td><td>外部记忆</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00609</td><td>Extreme Learning Machine</td><td>超限学习机</td><td>ELM</td><td><ahref="https://www.jiqizhixin.com/articles/2016-09-30-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00610</td><td>F Measure</td><td>F值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00611</td><td>F-Score</td><td>F分数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00612</td><td>Factor</td><td>因子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00613</td><td>Factor Analysis</td><td>因子分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00614</td><td>Factor Graph</td><td>因子图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00615</td><td>Factor Loading</td><td>因子负荷量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00616</td><td>Factorization</td><td>因子分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00617</td><td>Factorized</td><td>分解的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00618</td><td>Factors of Variation</td><td>变差因素</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00619</td><td>False Negative</td><td>假负例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00620</td><td>False Positive</td><td>假正例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00621</td><td>False Positive Rate</td><td>假正例率</td><td>FPR</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00622</td><td>Fast Dropout</td><td>快速暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00623</td><td>Fast Persistent Contrastive Divergence</td><td>快速持续性对比散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00624</td><td>Fault-Tolerant Asynchronous Training</td><td>容错异步训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00625</td><td>Feasible</td><td>可行</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00626</td><td>Feature</td><td>特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00627</td><td>Feature Engineering</td><td>特征工程</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00628</td><td>Feature Extraction</td><td>特征抽取</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00629</td><td>Feature Extractor</td><td>特征提取器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00630</td><td>Feature Function</td><td>特征函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00631</td><td>Feature Map</td><td>特征图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00632</td><td>Feature Scaling Transform</td><td>特征尺度变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00633</td><td>Feature Selection</td><td>特征选择</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00634</td><td>Feature Space</td><td>特征空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00635</td><td>Feature Vector</td><td>特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00636</td><td>Featured Learning</td><td>特征学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00637</td><td>Feedback</td><td>反馈</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00638</td><td>Feedforward</td><td>前馈</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00639</td><td>Feedforward Classifier</td><td>前馈分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00640</td><td>Feedforward Network</td><td>前馈网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00641</td><td>Feedforward Neural Network</td><td>前馈神经网络</td><td>FNN</td><td><a href="https://www.jiqizhixin.com/articles/2017-09-07-9">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00642</td><td>Few-Shot Learning</td><td>少试学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00643</td><td>Fidelity</td><td>逼真度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00644</td><td>Field Programmable Gated Array</td><td>现场可编程门阵列</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00645</td><td>Filter</td><td>滤波器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00646</td><td>Filter Method</td><td>过滤式方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00647</td><td>Fine-Tuning</td><td>微调</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00648</td><td>Finite Difference</td><td>有限差分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00649</td><td>First Layer</td><td>第一层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00650</td><td>First-Order Method</td><td>一阶方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00651</td><td>First-Order Rule</td><td>一阶规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00652</td><td>Fisher Information Matrix</td><td>Fisher信息矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00653</td><td>Fixed Point Equation</td><td>不动点方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00654</td><td>Fixed-Point Arithmetic</td><td>不动点运算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00655</td><td>Flat Minima</td><td>平坦最小值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00656</td><td>Flip</td><td>翻转</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00657</td><td>Flipping Output</td><td>翻转法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00658</td><td>Float-Point Arithmetic</td><td>浮点运算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00659</td><td>Fluctuation</td><td>振荡</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00660</td><td>Focus Attention</td><td>聚焦式注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00661</td><td>Folk Theorem</td><td>无名氏定理</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00662</td><td>Forget Gate</td><td>遗忘门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00663</td><td>Forward</td><td>前向</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00664</td><td>Forward KL Divergence</td><td>前向KL散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00665</td><td>Forward Mode Accumulation</td><td>前向模式累加</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00666</td><td>Forward Propagation</td><td>前向传播/正向传播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00667</td><td>Forward Search</td><td>前向搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00668</td><td>Forward Stagewise Algorithm</td><td>前向分步算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00669</td><td>Forward-Backward Algorithm</td><td>前向-后向算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00670</td><td>Fourier Transform</td><td>傅立叶变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00671</td><td>Fovea</td><td>中央凹</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00672</td><td>Fractionally Strided Convolution</td><td>微步卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00673</td><td>Free Energy</td><td>自由能</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00674</td><td>Frequentist</td><td>频率主义学派</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00675</td><td>Frequentist Probability</td><td>频率派概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00676</td><td>Frequentist Statistics</td><td>频率派统计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00677</td><td>Frobenius Norm</td><td>Frobenius 范数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00678</td><td>Full</td><td>全</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00679</td><td>Full Conditional Distribution</td><td>满条件分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00680</td><td>Full Conditional Probability</td><td>全条件概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00681</td><td>Full Padding</td><td>全填充</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00682</td><td>Full Singular Value Decomposition</td><td>完全奇异值分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00683</td><td>Full-Rank Matrix</td><td>满秩矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00684</td><td>Fully Connected Layer</td><td>全连接层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00685</td><td>Fully Connected Neural Network</td><td>全连接神经网络</td><td>FCNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00686</td><td>Fully Convolutional Network</td><td>全卷积网络</td><td>FCN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00687</td><td>Function</td><td>函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00688</td><td>Functional</td><td>泛函</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00689</td><td>Functional Derivative</td><td>泛函导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00690</td><td>Functional Margin</td><td>函数间隔</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00691</td><td>Functional Neuron</td><td>功能神经元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00692</td><td>Gabor Function</td><td>Gabor函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00693</td><td>Gain Ratio</td><td>増益率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00694</td><td>Game Payoff</td><td>博弈效用</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00695</td><td>Game Theory</td><td>博弈论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00696</td><td>Gamma Distribution</td><td>Gamma分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00697</td><td>Gate</td><td>门</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00698</td><td>Gate Controlled RNN</td><td>门控循环神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00699</td><td>Gated</td><td>门控</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00700</td><td>Gated Control</td><td>门控</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00701</td><td>Gated Recurrent Net</td><td>门控循环网络</td><td>GRN</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-24">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00702</td><td>Gated Recurrent Unit</td><td>门控循环单元</td><td>GRU</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00703</td><td>Gated RNN</td><td>门控RNN</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00704</td><td>Gater</td><td>选通器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00705</td><td>Gating Mechanism</td><td>门控机制</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00706</td><td>Gaussian Distribution</td><td>高斯分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00707</td><td>Gaussian Error Linear Unit</td><td>高斯误差线性单元</td><td>GELU</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00708</td><td>Gaussian Kernel</td><td>高斯核</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00709</td><td>Gaussian Kernel Function</td><td>高斯核函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00710</td><td>Gaussian Mixture Model</td><td>高斯混合模型</td><td>GMM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00711</td><td>Gaussian Mixtures</td><td>高斯混合（模型）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00712</td><td>Gaussian Output Distribution</td><td>高斯输出分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00713</td><td>Gaussian Process</td><td>高斯过程</td><td>GP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00714</td><td>Gaussian Process Regression</td><td>高斯过程回归</td><td>GPR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00715</td><td>Gaussian RBM</td><td>高斯RBM</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00716</td><td>Gaussian-Bernoulli RBM</td><td>高斯-伯努利RBM</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00717</td><td>General Problem Solving</td><td>通用问题求解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00718</td><td>General Purpose GPU</td><td>通用GPU</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00719</td><td>Generalization Ability</td><td>泛化能力</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00720</td><td>Generalization Error</td><td>泛化误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00721</td><td>Generalization Error Bound</td><td>泛化误差上界</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00722</td><td>Generalize</td><td>泛化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-25-10">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00723</td><td>Generalized Bregman Divergence</td><td>一般化 Bregman 散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00724</td><td>Generalized Expectation Maximization</td><td>广义期望极大</td><td>GEM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00725</td><td>Generalized Function</td><td>广义函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00726</td><td>Generalized Lagrange Function</td><td>广义拉格朗日函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00727</td><td>Generalized Lagrangian</td><td>广义拉格朗日</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00728</td><td>Generalized Linear Model</td><td>广义线性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00729</td><td>Generalized Pseudolikelihood</td><td>广义伪似然</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00730</td><td>Generalized Pseudolikelihood Estimator</td><td>广义伪似然估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00731</td><td>Generalized Rayleigh Quotient</td><td>广义瑞利商</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00732</td><td>Generalized Score Matching</td><td>广义得分匹配</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00733</td><td>Generative Adversarial Framework</td><td>生成式对抗框架</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00734</td><td>Generative Adversarial Network</td><td>生成对抗网络</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-26-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-08-5">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-13-2">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-00735</td><td>Generative Approach</td><td>生成方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00736</td><td>Generative Model</td><td>生成式模型</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-19-7">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-11-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-04-5">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-00737</td><td>Generative Modeling</td><td>生成式建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00738</td><td>Generative Moment Matching Network</td><td>生成矩匹配网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00739</td><td>Generative Pre-Training</td><td>生成式预训练</td><td>GPT</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00740</td><td>Generative Stochastic Network</td><td>生成随机网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00741</td><td>Generative Weight</td><td>生成权重</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00742</td><td>Generator</td><td>生成器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00743</td><td>Generator Network</td><td>生成器网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00744</td><td>Genetic Algorithm</td><td>遗传算法</td><td>GA</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-17-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-22">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-12-2">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[5]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[6]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00745</td><td>Geometric Margin</td><td>几何间隔</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00746</td><td>Giant Magnetoresistance</td><td>巨磁阻</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00747</td><td>Gibbs Distribution</td><td>吉布斯分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00748</td><td>Gibbs Sampling</td><td>吉布斯采样/吉布斯抽样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00749</td><td>Gibbs Steps</td><td>吉布斯步数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00750</td><td>Gini Index</td><td>基尼指数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00751</td><td>Global Contrast Normalization</td><td>全局对比度规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00752</td><td>Global Markov Property</td><td>全局马尔可夫性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00753</td><td>Global Minima</td><td>全局极小值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00754</td><td>Global Minimizer</td><td>全局极小解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00755</td><td>Global Minimum</td><td>全局最小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00756</td><td>Global Optimization</td><td>全局优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-03-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00757</td><td>Gradient</td><td>梯度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00758</td><td>Gradient Ascent</td><td>梯度上升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00759</td><td>Gradient Ascent Method</td><td>梯度上升法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00760</td><td>Gradient Boosting</td><td>梯度提升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00761</td><td>Gradient Boosting Tree</td><td>梯度提升树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00762</td><td>Gradient Clipping</td><td>梯度截断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00763</td><td>Gradient Descent</td><td>梯度下降</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00764</td><td>Gradient Descent In One-Dimensional Space</td><td>一维梯度下降</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00765</td><td>Gradient Descent Method</td><td>梯度下降法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00766</td><td>Gradient Energy Distribution</td><td>梯度能量分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00767</td><td>Gradient Estimation</td><td>梯度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00768</td><td>Gradient Exploding Problem</td><td>梯度爆炸问题</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-21-14">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00769</td><td>Gradient Field</td><td>梯度场</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00770</td><td>Gradual Warmup</td><td>逐渐预热</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00771</td><td>Gram Matrix</td><td>Gram 矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00772</td><td>Graph</td><td>图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00773</td><td>Graph Analytics</td><td>图分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00774</td><td>Graph Attention Network</td><td>图注意力网络</td><td>GAT</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00775</td><td>Graph Convolutional Network</td><td>图卷积神经网络/图卷积网络</td><td>GCN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00776</td><td>Graph Neural Network</td><td>图神经网络</td><td>GNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00777</td><td>Graph Theory</td><td>图论</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-04-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00778</td><td>Graphical Model</td><td>图模型</td><td>GM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00779</td><td>Graphics Processing Unit</td><td>图形处理器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00780</td><td>Greedy</td><td>贪心</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00781</td><td>Greedy Algorithm</td><td>贪心算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00782</td><td>Greedy Layer-Wise Pretraining</td><td>贪心逐层预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00783</td><td>Greedy Layer-Wise Training</td><td>贪心逐层训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00784</td><td>Greedy Layer-Wise Unsupervised Pretraining</td><td>贪心逐层无监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00785</td><td>Greedy Search</td><td>贪心搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00786</td><td>Greedy Supervised Pretraining</td><td>贪心监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00787</td><td>Greedy Unsupervised Pretraining</td><td>贪心无监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00788</td><td>Grid Search</td><td>网格搜索</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00789</td><td>Grid World</td><td>网格世界</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00790</td><td>Ground Truth</td><td>真实值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00791</td><td>Growth Function</td><td>增长函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00792</td><td>Hadamard Product</td><td>Hadamard积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00793</td><td>Hamming Distance</td><td>汉明距离</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00794</td><td>Hard Attention</td><td>硬性注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00795</td><td>Hard Clustering</td><td>硬聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00796</td><td>Hard Margin</td><td>硬间隔</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00797</td><td>Hard Margin Maximization</td><td>硬间隔最大化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00798</td><td>Hard Mixture Of Experts</td><td>硬专家混合体</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00799</td><td>Hard Tanh</td><td>硬双曲正切函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00800</td><td>Hard Target</td><td>硬目标</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00801</td><td>Hard Voting</td><td>硬投票</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00802</td><td>Harmonic Mean</td><td>调和平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00803</td><td>Harmonium</td><td>簧风琴</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00804</td><td>Harmony</td><td>Harmony</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00805</td><td>Harris Chain</td><td>哈里斯链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00806</td><td>Hausdorff Distance</td><td>豪斯多夫距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00807</td><td>Hebbian Rule</td><td>赫布法则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00808</td><td>Hebbian Theory</td><td>赫布理论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00809</td><td>Helmholtz Machine</td><td>Helmholtz机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00810</td><td>Hesse Matrix</td><td>海赛矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00811</td><td>Hessian</td><td>Hessian</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00812</td><td>Hessian Matrix</td><td>黑塞矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00813</td><td>Heterogeneous Information Network</td><td>异质信息网络</td><td>HIN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00814</td><td>Heteroscedastic</td><td>异方差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00815</td><td>Hidden Dynamic Model</td><td>隐动态模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00816</td><td>Hidden Layer</td><td>隐藏层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00817</td><td>Hidden Markov Model</td><td>隐马尔可夫模型</td><td>HMM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-21-8">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00818</td><td>Hidden State</td><td>隐状态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00819</td><td>Hidden Unit</td><td>隐藏单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00820</td><td>Hidden Variable</td><td>隐变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00821</td><td>Hierarchical Clustering</td><td>层次聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00822</td><td>Hierarchical Reinforcement Learning</td><td>分层强化学习</td><td>HRL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00823</td><td>Hierarchical Softmax</td><td>层序Softmax/层序软最大化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00824</td><td>Hilbert Space</td><td>希尔伯特空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00825</td><td>Hill Climbing</td><td>爬山</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00826</td><td>Hinge Loss Function</td><td>合页损失函数/Hinge损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00827</td><td>Histogram Method</td><td>直方图方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00828</td><td>Hold-Out</td><td>留出法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00829</td><td>Homogeneous</td><td>同质</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00830</td><td>Hopfield Network</td><td>Hopfield网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00831</td><td>Huffman Coding</td><td>霍夫曼编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00832</td><td>Hybrid Computing</td><td>混合计算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00833</td><td>Hyperbolic Tangent Function</td><td>双曲正切函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00834</td><td>Hyperparameter</td><td>超参数</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-08-18-5">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-28">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-00835</td><td>Hyperparameter Optimization</td><td>超参数优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00836</td><td>Hyperplane</td><td>超平面</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-00837</td><td>Hypothesis</td><td>假设</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00838</td><td>Hypothesis Space</td><td>假设空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00839</td><td>Hypothesis Test</td><td>假设检验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00840</td><td>I.I.D. Assumption</td><td>独立同分布假设</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00841</td><td>Identically Distributed</td><td>同分布的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00842</td><td>Identifiable</td><td>可辨认的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00843</td><td>Identity Function</td><td>恒等函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00844</td><td>Identity Mapping</td><td>恒等映射</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00845</td><td>Identity Matrix</td><td>单位矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00846</td><td>Ill Conditioning</td><td>病态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00847</td><td>Ill-Formed Problem</td><td>病态问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00848</td><td>Image</td><td>图像</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00849</td><td>Image Restoration</td><td>图像还原</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00850</td><td>Imitation Learning</td><td>模仿学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00851</td><td>Immorality</td><td>不道德</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00852</td><td>Imperfect Information</td><td>不完美信息</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-16-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00853</td><td>Implicit Density Model</td><td>隐式密度模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00854</td><td>Import</td><td>导入</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00855</td><td>Importance Sampling</td><td>重要性采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00856</td><td>Improved Iterative Scaling</td><td>改进的迭代尺度法</td><td>IIS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00857</td><td>Incomplete-Data</td><td>不完全数据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00858</td><td>Incremental Learning</td><td>增量学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00859</td><td>Indefinite Integral</td><td>不定积分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00860</td><td>Independence</td><td>独立</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00861</td><td>Independent</td><td>相互独立的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00862</td><td>Independent and Identically Distributed</td><td>独立同分布</td><td>I.I.D.</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00863</td><td>Independent Component Analysis</td><td>独立成分分析</td><td>ICA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00864</td><td>Independent Subspace Analysis</td><td>独立子空间分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00865</td><td>Index of Matrix</td><td>索引</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00866</td><td>Indicator Function</td><td>指示函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00867</td><td>Individual Learner</td><td>个体学习器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00868</td><td>Induction</td><td>归纳</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00869</td><td>Inductive Bias</td><td>归纳偏好</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00870</td><td>Inductive Learning</td><td>归纳学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00871</td><td>Inductive Logic Programming</td><td>归纳逻辑程序设计</td><td>ILP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00872</td><td>Inductive Transfer Learning</td><td>归纳迁移学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00873</td><td>Inequality Constraint</td><td>不等式约束</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00874</td><td>Inference</td><td>推断</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-14-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00875</td><td>Infinite</td><td>无限</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00876</td><td>Infinitely Exchangeable</td><td>无限可交换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00877</td><td>Information Divergence</td><td>信息散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00878</td><td>Information Entropy</td><td>信息熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00879</td><td>Information Gain</td><td>信息增益</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-00880</td><td>Information Gain Ratio</td><td>信息增益比</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-00881</td><td>Information Retrieval</td><td>信息检索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00882</td><td>Information Theory</td><td>信息论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00883</td><td>Inner Product</td><td>内积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00884</td><td>Input</td><td>输入</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00885</td><td>Input Distribution</td><td>输入分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00886</td><td>Input Gate</td><td>输入门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00887</td><td>Input Layer</td><td>输入层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00888</td><td>Input Space</td><td>输入空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00889</td><td>Insensitive Loss</td><td>不敏感损失</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00890</td><td>Instance</td><td>示例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00891</td><td>Instance Segmentation</td><td>实例分割</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00892</td><td>Integer Linear Programming</td><td>整数线性规划</td><td>ILP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00893</td><td>Integer Programming</td><td>整数规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00894</td><td>Integration</td><td>积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00895</td><td>Inter-Cluster Similarity</td><td>簇间相似度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00896</td><td>Internal Covariate Shift</td><td>内部协变量偏移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00897</td><td>Internal Node</td><td>内部结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00898</td><td>International Conference For Machine Learning</td><td>国际机器学习大会</td><td>ICML</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-31">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00899</td><td>Intervention Query</td><td>干预查询</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00900</td><td>Intra-Attention</td><td>内部注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00901</td><td>Intra-Cluster Similarity</td><td>簇内相似度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00902</td><td>Intrinsic Value</td><td>固有值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00903</td><td>Invariance</td><td>不变性</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-16-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00904</td><td>Invariant</td><td>不变</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00905</td><td>Inverse Matrix</td><td>逆矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00906</td><td>Inverse Reinforcement Learning</td><td>逆强化学习</td><td>IRL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00907</td><td>Inverse Resolution</td><td>逆归结</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00908</td><td>Inverse Time Decay</td><td>逆时衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00909</td><td>Invert</td><td>求逆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00910</td><td>Irreducible</td><td>不可约的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00911</td><td>Irrelevant Feature</td><td>无关特征</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00912</td><td>Isometric Mapping</td><td>等度量映射</td><td>Isomap</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00913</td><td>Isotonic Regression</td><td>等分回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00914</td><td>Isotropic</td><td>各向同性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00915</td><td>Isotropic Gaussian Distribution</td><td>各向同性高斯分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00916</td><td>Iteration</td><td>迭代</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td>数学、机器学习</td></tr><tr class="odd"><td>AITD-00917</td><td>Iterative Dichotomiser</td><td>迭代二分器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00918</td><td>Jacobian</td><td>雅克比</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00919</td><td>Jacobian Matrix</td><td>雅可比矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00920</td><td>Jensen Inequality</td><td>Jensen不等式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00921</td><td>Jensen-Shannon Divergence</td><td>JS散度</td><td>JSD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00922</td><td>Joint Probability Density Function</td><td>联合概率密度函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00923</td><td>Joint Probability Distribution</td><td>联合概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00924</td><td>Junction Tree Algorithm</td><td>联合树算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00925</td><td>K-Armed Bandit Problem</td><td>k-摇臂老虎机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00926</td><td>K-Fold Cross Validation</td><td>k 折交叉验证</td><td>K-FOLD CV</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-00927</td><td>K-Means Clustering</td><td>k-均值聚类</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-11-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00928</td><td>K-Nearest Neighbor Classifier</td><td>k-近邻分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00929</td><td>K-Nearest Neighbor Method</td><td>k-近邻</td><td>K-NN</td><td>[1]</td><td>统计</td></tr><tr class="even"><td>AITD-00930</td><td>Karush-Kuhn-Tucker Condition</td><td>KKT条件</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00931</td><td>Karush–Kuhn–Tucker</td><td>Karush–Kuhn–Tucker</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00932</td><td>Kd Tree</td><td>Kd 树</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00933</td><td>Kernel Density Estimation</td><td>核密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00934</td><td>Kernel Function</td><td>核函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00935</td><td>Kernel Machine</td><td>核机器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00936</td><td>Kernel Matrix</td><td>核矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00937</td><td>Kernel Method</td><td>核方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00938</td><td>Kernel Regression</td><td>核回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00939</td><td>Kernel Trick</td><td>核技巧</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00940</td><td>Kernelized</td><td>核化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00941</td><td>Kernelized Linear Discriminant Analysis</td><td>核线性判别分析</td><td>KLDA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00942</td><td>Kernelized PCA</td><td>核主成分分析</td><td>KPCA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00943</td><td>Key-Value Store</td><td>键-值数据库</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00944</td><td>KL Divergence</td><td>KL散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00945</td><td>Knowledge</td><td>知识</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00946</td><td>Knowledge Base</td><td>知识库</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-21-10">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00947</td><td>Knowledge Distillation</td><td>知识蒸馏</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00948</td><td>Knowledge Engineering</td><td>知识工程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00949</td><td>Knowledge Graph</td><td>知识图谱</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-11-03-5">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-03-24">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-09-26-8">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00950</td><td>Knowledge Representation</td><td>知识表征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00951</td><td>Kronecker Product</td><td>Kronecker积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00952</td><td>Krylov Method</td><td>Krylov方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00953</td><td>L-BFGS</td><td>L-BFGS</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00954</td><td>Label</td><td>标签/标记</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00955</td><td>Label Propagation</td><td>标记传播</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00956</td><td>Label Smoothing</td><td>标签平滑</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00957</td><td>Label Space</td><td>标记空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00958</td><td>Labeled</td><td>标注</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00959</td><td>Lagrange Dual Problem</td><td>拉格朗日对偶问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00960</td><td>Lagrange Duality</td><td>拉格朗日对偶性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00961</td><td>Lagrange Function</td><td>拉格朗日函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00962</td><td>Lagrange Multiplier</td><td>拉格朗日乘子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00963</td><td>Language Model</td><td>语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00964</td><td>Language Modeling</td><td>语言模型化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00965</td><td>Laplace Distribution</td><td>Laplace分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00966</td><td>Laplace Smoothing</td><td>拉普拉斯平滑</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00967</td><td>Laplacian Correction</td><td>拉普拉斯修正</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00968</td><td>Large Learning Step</td><td>大学习步骤</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00969</td><td>Las Vegas Method</td><td>拉斯维加斯方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00970</td><td>Latent</td><td>潜在</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00971</td><td>Latent Dirichlet Allocation</td><td>潜在狄利克雷分配</td><td>LDA</td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-01-7">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00972</td><td>Latent Layer</td><td>潜层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00973</td><td>Latent Semantic Analysis</td><td>潜在语义分析</td><td>LSA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00974</td><td>Latent Semantic Indexing</td><td>潜在语义索引</td><td>LSI</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00975</td><td>Latent Variable</td><td>潜变量/隐变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00976</td><td>Law of Large Numbers</td><td>大数定律</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00977</td><td>Layer</td><td>层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00978</td><td>Layer Normalization</td><td>层规范化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00979</td><td>Layer-Wise</td><td>逐层的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00980</td><td>Layer-Wise Adaptive Rate Scaling</td><td>逐层适应率缩放</td><td>LARS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00981</td><td>Layer-Wise Normalization</td><td>逐层规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00982</td><td>Layer-Wise Pretraining</td><td>逐层预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00983</td><td>Layer-Wise Training</td><td>逐层训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00984</td><td>Lazy Learning</td><td>懒惰学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00985</td><td>Leaf Node</td><td>叶结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00986</td><td>Leaky Lelu Function</td><td>泄漏线性整流函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00987</td><td>Leaky Relu</td><td>泄漏修正线性单元/泄漏整流线性单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00988</td><td>Leaky Unit</td><td>渗漏单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00989</td><td>Learned</td><td>学成</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00990</td><td>Learned Approximate Inference</td><td>学习近似推断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00991</td><td>Learner</td><td>学习器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00992</td><td>Learning</td><td>学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00993</td><td>Learning Algorithm</td><td>学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00994</td><td>Learning By Analogy</td><td>类比学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00995</td><td>Learning Rate</td><td>学习率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00996</td><td>Learning Rate Annealing</td><td>学习率退火</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00997</td><td>Learning Rate Decay</td><td>学习率衰减</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00998</td><td>Learning Rate Warmup</td><td>学习率预热</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00999</td><td>Learning To Learn</td><td>学习的学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01000</td><td>Learning Vector Quantization</td><td>学习向量量化</td><td>LVQ</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01001</td><td>Least General Generalization</td><td>最小一般泛化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01002</td><td>Least Mean Squares</td><td>最小均方</td><td>LMS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01003</td><td>Least Square Method</td><td>最小二乘法</td><td>LSM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-24-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01004</td><td>Least Squares Regression Tree</td><td>最小二乘回归树</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01005</td><td>Leave-One-Out Cross Validation</td><td>留一交叉验证</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01006</td><td>Leave-One-Out</td><td>留一法</td><td>LOO</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01007</td><td>Lebesgue-Integrable</td><td>勒贝格可积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01008</td><td>Left Eigenvector</td><td>左特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01009</td><td>Left Singular Vector</td><td>左奇异向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01010</td><td>Leibniz's Rule</td><td>莱布尼兹法则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01011</td><td>Lifelong Learning</td><td>终身学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01012</td><td>Likelihood</td><td>似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01013</td><td>Line Search</td><td>线搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01014</td><td>Linear Auto-Regressive Network</td><td>线性自回归网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01015</td><td>Linear Chain</td><td>线性链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01016</td><td>Linear Chain Conditional Random Field</td><td>线性链条件随机场</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01017</td><td>Linear Classification Model</td><td>线性分类模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01018</td><td>Linear Classifier</td><td>线性分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01019</td><td>Linear Combination</td><td>线性组合</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="even"><td>AITD-01020</td><td>Linear Dependence</td><td>线性相关</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01021</td><td>Linear Discriminant Analysis</td><td>线性判别分析</td><td>LDA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01022</td><td>Linear Factor Model</td><td>线性因子模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01023</td><td>Linear Mapping</td><td>线性映射</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01024</td><td>Linear Model</td><td>线性模型</td><td>LR</td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01025</td><td>Linear Programming</td><td>线性规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01026</td><td>Linear Regression</td><td>线性回归</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-01">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-17-5">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a></td><td>统计、数学</td></tr><tr class="odd"><td>AITD-01027</td><td>Linear Scaling Rule</td><td>线性缩放规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01028</td><td>Linear Scan</td><td>线性扫描</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01029</td><td>Linear Space</td><td>线性空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01030</td><td>Linear Support Vector Machine</td><td>线性支持向量机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01031</td><td>Linear Support Vector Machine In Linearly Separable Case</td><td>线性可分支持向量机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01032</td><td>Linear Threshold Units</td><td>线性阈值单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01033</td><td>Linear Transformation</td><td>线性变换</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01034</td><td>Linearly Independent</td><td>线性无关</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01035</td><td>Linearly Separable</td><td>线性可分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01036</td><td>Linearly Separable Data Set</td><td>线性可分数据集</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01037</td><td>Link Analysis</td><td>链接分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01038</td><td>Link Function</td><td>联系函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01039</td><td>Link Prediction</td><td>链接预测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01040</td><td>Link Table</td><td>连接表</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01041</td><td>Linkage</td><td>连接</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01042</td><td>Linked Importance Sampling</td><td>链接重要采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01043</td><td>Lipschitz</td><td>Lipschitz</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01044</td><td>Lipschitz Constant</td><td>Lipschitz常数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01045</td><td>Lipschitz Continuous</td><td>Lipschitz连续</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01046</td><td>Liquid State Machine</td><td>流体状态机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01047</td><td>Local Conditional Probability Distribution</td><td>局部条件概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01048</td><td>Local Constancy Prior</td><td>局部不变性先验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01049</td><td>Local Contrast Normalization</td><td>局部对比度规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01050</td><td>Local Curvature</td><td>局部曲率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01051</td><td>Local Descent</td><td>局部下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01052</td><td>Local Invariances</td><td>局部不变性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01053</td><td>Local Kernel</td><td>局部核</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01054</td><td>Local Markov Property</td><td>局部马尔可夫性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01055</td><td>Local Maxima</td><td>局部极大值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01056</td><td>Local Maximum</td><td>局部极大点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01057</td><td>Local Minima</td><td>局部极小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01058</td><td>Local Minimizer</td><td>局部最小解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01059</td><td>Local Minimum</td><td>局部极小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01060</td><td>Local Representation</td><td>局部式表示/局部式表征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01061</td><td>Local Response Normalization</td><td>局部响应规范化</td><td>LRN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01062</td><td>Locally Linear Embedding</td><td>局部线性嵌入</td><td>LLE</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01063</td><td>Log Likelihood</td><td>对数似然函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01064</td><td>Log Linear Model</td><td>对数线性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01065</td><td>Log-Likelihood</td><td>对数似然</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01066</td><td>Log-Likelihood Loss Function</td><td>对数似然损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01067</td><td>Log-Linear Regression</td><td>对数线性回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01068</td><td>Logarithmic Loss Function</td><td>对数损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01069</td><td>Logarithmic Scale</td><td>对数尺度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01070</td><td>Logistic Distribution</td><td>对数几率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01071</td><td>Logistic Function</td><td>对数几率函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01072</td><td>Logistic Loss</td><td>对率损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01073</td><td>Logistic Regression</td><td>对数几率回归(逻辑回归)</td><td>LR</td><td><a href="https://www.jiqizhixin.com/articles/2017-11-23-6">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[2]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01074</td><td>Logistic Sigmoid</td><td>对数几率Sigmoid</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01075</td><td>Logit</td><td>对数几率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01076</td><td>Long Short Term Memory</td><td>长短期记忆</td><td>LSTM</td><td><a href="https://www.jiqizhixin.com/articles/2017-12-18-6">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-10-04-2">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-09-29-7">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[4]</a></td><td></td></tr><tr class="odd"><td>AITD-01077</td><td>Long Short-Term Memory Network</td><td>长短期记忆网络</td><td>LSTM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01078</td><td>Long-Term Dependencies Problem</td><td>长程依赖问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01079</td><td>Long-Term Dependency</td><td>长期依赖</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01080</td><td>Long-Term Memory</td><td>长期记忆</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01081</td><td>Loop</td><td>环</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01082</td><td>Loopy Belief Propagation</td><td>环状信念传播</td><td>LBP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01083</td><td>Loss</td><td>损失</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01084</td><td>Loss Function</td><td>损失函数</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-03-4">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01085</td><td>Low Rank Matrix Approximation</td><td>低秩矩阵近似</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01086</td><td>Lp Distance</td><td>Lp距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01087</td><td>Machine Learning Model</td><td>机器学习模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01088</td><td>Machine Learning</td><td>机器学习</td><td>ML</td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01089</td><td>Machine Translation</td><td>机器翻译</td><td>MT</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-13-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01090</td><td>Macro Average</td><td>宏平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01091</td><td>Macro-F1</td><td>宏F1</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01092</td><td>Macro-P</td><td>宏查准率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01093</td><td>Macron-R</td><td>宏查全率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01094</td><td>Mahalanobis Distance</td><td>马哈拉诺比斯距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01095</td><td>Main Diagonal</td><td>主对角线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01096</td><td>Majority Voting</td><td>绝对多数投票</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01097</td><td>Majority Voting Rule</td><td>多数表决规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01098</td><td>Manhattan Distance</td><td>曼哈顿距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01099</td><td>Manifold</td><td>流形</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01100</td><td>Manifold Assumption</td><td>流形假设</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01101</td><td>Manifold Learning</td><td>流形学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01102</td><td>Manifold Tangent Classifier</td><td>流形正切分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01103</td><td>Margin</td><td>间隔</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01104</td><td>Margin Theory</td><td>间隔理论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01105</td><td>Marginal Distribution</td><td>边缘分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01106</td><td>Marginal Independence</td><td>边缘独立性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01107</td><td>Marginal Likelihood</td><td>边缘似然函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01108</td><td>Marginal Probability Distribution</td><td>边缘概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01109</td><td>Marginalization</td><td>边缘化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01110</td><td>Markov Blanket</td><td>马尔可夫毯</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01111</td><td>Markov Chain</td><td>马尔可夫链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01112</td><td>Markov Chain Monte Carlo</td><td>马尔可夫链蒙特卡罗</td><td>MCMC</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-24-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01113</td><td>Markov Decision Process</td><td>马尔可夫决策过程</td><td>MDP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01114</td><td>Markov Network</td><td>马尔可夫网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01115</td><td>Markov Process</td><td>马尔可夫过程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01116</td><td>Markov Property</td><td>马尔可夫性质</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01117</td><td>Markov Random Field</td><td>马尔可夫随机场</td><td>MRF</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01118</td><td>Mask</td><td>掩码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01119</td><td>Mask Language Modeling</td><td>掩码语言模型化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01120</td><td>Masked Self-Attention</td><td>掩蔽自注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01121</td><td>Mathematical Optimization</td><td>数学优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01122</td><td>Matrix</td><td>矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01123</td><td>Matrix Calculus</td><td>矩阵微积分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01124</td><td>Matrix Completion</td><td>矩阵补全</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01125</td><td>Matrix Decomposition</td><td>矩阵分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01126</td><td>Matrix Inversion</td><td>逆矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01127</td><td>Matrix Product</td><td>矩阵乘积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01128</td><td>Max Norm</td><td>最大范数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01129</td><td>Max Pooling</td><td>最大汇聚</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-02-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01130</td><td>Maxima</td><td>极大值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01131</td><td>Maximal Clique</td><td>最大团</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01132</td><td>Maximization</td><td>极大</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01133</td><td>Maximization Step</td><td>M步</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01134</td><td>Maximization-Maximization Algorithm</td><td>极大-极大算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01135</td><td>Maximum A Posteriori</td><td>最大后验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01136</td><td>Maximum A Posteriori Estimation</td><td>最大后验估计</td><td>MAP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01137</td><td>Maximum Entropy Model</td><td>最大熵模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01138</td><td>Maximum Likelihood</td><td>极大似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01139</td><td>Maximum Likelihood Estimation</td><td>极大似然估计</td><td>MLE</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-09-6">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01140</td><td>Maximum Likelihood Method</td><td>极大似然法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01141</td><td>Maximum Margin</td><td>最大间隔</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01142</td><td>Maximum Mean Discrepancy</td><td>最大平均偏差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01143</td><td>Maximum Posterior Probability Estimation</td><td>最大后验概率估计</td><td>MAP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01144</td><td>Maximum Weighted Spanning Tree</td><td>最大带权生成树</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01145</td><td>Maxout</td><td>Maxout</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01146</td><td>Maxout Unit</td><td>Maxout单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01147</td><td>Mean</td><td>均值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01148</td><td>Mean Absolute Error</td><td>平均绝对误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01149</td><td>Mean And Covariance RBM</td><td>均值和协方差RBM</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01150</td><td>Mean Filed</td><td>平均场</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01151</td><td>Mean Filter</td><td>均值滤波</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01152</td><td>Mean Pooling</td><td>平均汇聚</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01153</td><td>Mean Product of Student t-Distribution</td><td>学生 t 分布均值乘积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01154</td><td>Mean Squared Error</td><td>均方误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01155</td><td>Mean-Covariance Restricted Boltzmann Machine</td><td>均值-协方差受限玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01156</td><td>Mean-Field</td><td>平均场</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01157</td><td>Meanfield</td><td>均匀场</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01158</td><td>Measure Theory</td><td>测度论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01159</td><td>Measure Zero</td><td>零测度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01160</td><td>Median</td><td>中位数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01161</td><td>Memory</td><td>记忆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01162</td><td>Memory Augmented Neural Network</td><td>记忆增强神经网络</td><td>MANN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01163</td><td>Memory Capacity</td><td>记忆容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01164</td><td>Memory Cell</td><td>记忆元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01165</td><td>Memory Network</td><td>记忆网络</td><td>MN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01166</td><td>Memory Segment</td><td>记忆片段</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01167</td><td>Mercer Kernel</td><td>Mercer 核</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01168</td><td>Message</td><td>消息</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01169</td><td>Message Passing</td><td>消息传递</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01170</td><td>Message Passing Neural Network</td><td>消息传递神经网络</td><td>MPNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01171</td><td>Meta-Learner</td><td>元学习器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01172</td><td>Meta-Learning</td><td>元学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01173</td><td>Meta-Optimization</td><td>元优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01174</td><td>Meta-Rule</td><td>元规则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01175</td><td>Metric</td><td>指标</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-01176</td><td>Metric Learning</td><td>度量学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01177</td><td>Micro Average</td><td>微平均</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01178</td><td>Micro-F1</td><td>微F1</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01179</td><td>Micro-P</td><td>微査准率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01180</td><td>Micro-R</td><td>微查全率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01181</td><td>Min-Max Normalization</td><td>最小最大值规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01182</td><td>Mini-Batch Gradient</td><td>小批量梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01183</td><td>Mini-Batch Gradient Descent</td><td>小批量梯度下降法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01184</td><td>Mini-Batch SGD</td><td>小批次随机梯度下降</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01185</td><td>Minibatch</td><td>小批量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01186</td><td>Minibatch Stochastic</td><td>小批量随机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01187</td><td>Minima</td><td>极小值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01188</td><td>Minimal Description Length</td><td>最小描述长度</td><td>MDL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01189</td><td>Minimax Game</td><td>极小极大博弈</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01190</td><td>Minimum</td><td>极小点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01191</td><td>Minkowski Distance</td><td>闵可夫斯基距离</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01192</td><td>Misclassification Cost</td><td>误分类代价</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01193</td><td>Mixing</td><td>混合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01194</td><td>Mixing Time</td><td>混合时间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01195</td><td>Mixture Density Network</td><td>混合密度网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01196</td><td>Mixture Distribution</td><td>混合分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01197</td><td>Mixture of Experts</td><td>混合专家模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01198</td><td>Mixture-of-Gaussian</td><td>高斯混合</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01199</td><td>Modality</td><td>模态</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01200</td><td>Mode</td><td>峰值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01201</td><td>Model</td><td>模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01202</td><td>Model Averaging</td><td>模型平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01203</td><td>Model Collapse</td><td>模型坍塌</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01204</td><td>Model Complexity</td><td>模型复杂度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01205</td><td>Model Compression</td><td>模型压缩</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01206</td><td>Model Identifiability</td><td>模型可辨识性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01207</td><td>Model Parallelism</td><td>模型并行</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01208</td><td>Model Parameter</td><td>模型参数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01209</td><td>Model Predictive Control</td><td>模型预测控制</td><td>MPC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01210</td><td>Model Selection</td><td>模型选择</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01211</td><td>Model-Agnostic Meta-Learning</td><td>模型无关的元学习</td><td>MAML</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01212</td><td>Model-Based Learning</td><td>有模型学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01213</td><td>Model-Based Reinforcement Learning</td><td>基于模型的强化学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01214</td><td>Model-Free Learning</td><td>免模型学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01215</td><td>Model-Free Reinforcement Learning</td><td>模型无关的强化学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01216</td><td>Moment</td><td>矩</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01217</td><td>Moment Matching</td><td>矩匹配</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01218</td><td>Momentum</td><td>动量</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-07-01-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01219</td><td>Momentum Method</td><td>动量法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01220</td><td>Monte Carlo</td><td>蒙特卡罗</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01221</td><td>Monte Carlo Estimate</td><td>蒙特卡罗估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01222</td><td>Monte Carlo Integration</td><td>蒙特卡罗积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01223</td><td>Monte Carlo Method</td><td>蒙特卡罗方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01224</td><td>Moore's Law</td><td>摩尔定律</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01225</td><td>Moore-Penrose Pseudoinverse</td><td>Moore-Penrose 伪逆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01226</td><td>Moral Graph</td><td>端正图/道德图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01227</td><td>Moralization</td><td>道德化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01228</td><td>Most General Unifier</td><td>最一般合一置换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01229</td><td>Moving Average</td><td>移动平均</td><td>MA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01230</td><td>Multi-Armed Bandit Problem</td><td>多臂赌博机问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01231</td><td>Multi-Class Classification</td><td>多分类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01232</td><td>Multi-Classifier System</td><td>多分类器系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01233</td><td>Multi-Document Summarization</td><td>多文档摘要</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01234</td><td>Multi-Head Attention</td><td>多头注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01235</td><td>Multi-Head Self-Attention</td><td>多头自注意力</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01236</td><td>Multi-Hop</td><td>多跳</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01237</td><td>Multi-Kernel Learning</td><td>多核学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01238</td><td>Multi-Label Classification</td><td>多标签分类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01239</td><td>Multi-Label Learning</td><td>多标记学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01240</td><td>Multi-Layer Feedforward Neural Networks</td><td>多层前馈神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01241</td><td>Multi-Layer Perceptron</td><td>多层感知机</td><td>MLP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-01242</td><td>Multi-Nominal Logistic Regression Model</td><td>多项对数几率回归模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01243</td><td>Multi-Prediction Deep Boltzmann Machine</td><td>多预测深度玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01244</td><td>Multi-Response Linear Regression</td><td>多响应线性回归</td><td>MLR</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01245</td><td>Multi-View Learning</td><td>多视图学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01246</td><td>Multicollinearity</td><td>多重共线性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01247</td><td>Multimodal</td><td>多峰值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01248</td><td>Multimodal Learning</td><td>多模态学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01249</td><td>Multinomial Distribution</td><td>多项分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01250</td><td>Multinoulli Distribution</td><td>Multinoulli分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01251</td><td>Multinoulli Output Distribution</td><td>Multinoulli输出分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01252</td><td>Multiple Dimensional Scaling</td><td>多维缩放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01253</td><td>Multiple Linear Regression</td><td>多元线性回归</td><td>MLR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[3]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01254</td><td>Multitask Learning</td><td>多任务学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01255</td><td>Multivariate Decision Tree</td><td>多变量决策树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01256</td><td>Multivariate Gaussian Distribution</td><td>多元高斯分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01257</td><td>Multivariate Normal Distribution</td><td>多元正态分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01258</td><td>Mutual Information</td><td>互信息</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01259</td><td>N-Gram</td><td>N元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01260</td><td>N-Gram Feature</td><td>N元特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01261</td><td>N-Gram Model</td><td>N元模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01262</td><td>Naive Bayes Algorithm</td><td>朴素贝叶斯算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01263</td><td>Naive Bayes Classifier</td><td>朴素贝叶斯分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01264</td><td>Naive Bayes</td><td>朴素贝叶斯</td><td>NB</td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-20-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01265</td><td>Named Entity Recognition</td><td>命名实体识别</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01266</td><td>Narrow Convolution</td><td>窄卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01267</td><td>Nash Equilibrium</td><td>纳什均衡</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01268</td><td>Nash Reversion</td><td>纳什回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01269</td><td>Nats</td><td>奈特</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01270</td><td>Natural Exponential Decay</td><td>自然指数衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01271</td><td>Natural Language Generation</td><td>自然语言生成</td><td>NLG</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01272</td><td>Natural Language Processing</td><td>自然语言处理</td><td>NLP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[4]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-14-5">[5]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-14-4">[6]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-12-3">[7]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01273</td><td>Nearest Neighbor</td><td>最近邻</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01274</td><td>Nearest Neighbor Classifier</td><td>最近邻分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01275</td><td>Nearest Neighbor Graph</td><td>最近邻图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01276</td><td>Nearest Neighbor Regression</td><td>最近邻回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01277</td><td>Nearest-Neighbor Search</td><td>最近邻搜索</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-24-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01278</td><td>Negative Class</td><td>负类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01279</td><td>Negative Correlation</td><td>负相关法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01280</td><td>Negative Definite</td><td>负定</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01281</td><td>Negative Log Likelihood</td><td>负对数似然函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01282</td><td>Negative Part Function</td><td>负部函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01283</td><td>Negative Phase</td><td>负相</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01284</td><td>Negative Sample</td><td>负例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01285</td><td>Negative Sampling</td><td>负采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01286</td><td>Negative Semidefinite</td><td>半负定</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01287</td><td>Neighbourhood Component Analysis</td><td>近邻成分分析</td><td>NCA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01288</td><td>Nesterov Accelerated Gradient</td><td>Nesterov加速梯度</td><td>NAG</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01289</td><td>Nesterov Momentum</td><td>Nesterov动量法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01290</td><td>Net Activation</td><td>净活性值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01291</td><td>Net Input</td><td>净输入</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01292</td><td>Network</td><td>网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01293</td><td>Network Capacity</td><td>网络容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01294</td><td>Neural Architecture Search</td><td>神经架构搜索</td><td>NAS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01295</td><td>Neural Auto-Regressive Density Estimator</td><td>神经自回归密度估计器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01296</td><td>Neural Auto-Regressive Network</td><td>神经自回归网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01297</td><td>Neural Language Model</td><td>神经语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01298</td><td>Neural Machine Translation</td><td>神经机器翻译</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-22-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01299</td><td>Neural Model</td><td>神经模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01300</td><td>Neural Network</td><td>神经网络</td><td>NN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01301</td><td>Neural Turing Machine</td><td>神经图灵机</td><td>NTM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-11-7">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01302</td><td>Neurodynamics</td><td>神经动力学</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01303</td><td>Neuromorphic Computing</td><td>神经形态计算</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-09-26-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-26-2">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-16-6">[3]</a></td><td></td></tr><tr class="even"><td>AITD-01304</td><td>Neuron</td><td>神经元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01305</td><td>Newton Method</td><td>牛顿法</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-03-11-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01306</td><td>No Free Lunch Theorem</td><td>没有免费午餐定理</td><td>NFL</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-03-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01307</td><td>Node</td><td>结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01308</td><td>Noise</td><td>噪声</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01309</td><td>Noise Distribution</td><td>噪声分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01310</td><td>Noise-Contrastive Estimation</td><td>噪声对比估计</td><td>NCE</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01311</td><td>Nominal Attribute</td><td>列名属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01312</td><td>Non-Autoregressive Process</td><td>非自回归过程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01313</td><td>Non-Convex Optimization</td><td>非凸优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-29-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01314</td><td>Non-Informative Prior</td><td>无信息先验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01315</td><td>Non-Linear Model</td><td>非线性模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01316</td><td>Non-Linear Oscillation</td><td>非线性振荡</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01317</td><td>Non-Linear Support Vector Machine</td><td>非线性支持向量机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01318</td><td>Non-Metric Distance</td><td>非度量距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01319</td><td>Non-Negative Matrix Factorization</td><td>非负矩阵分解</td><td>NMF</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01320</td><td>Non-Ordinal Attribute</td><td>无序属性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01321</td><td>Non-Parametric</td><td>非参数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01322</td><td>Non-Parametric Model</td><td>非参数化模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01323</td><td>Non-Probabilistic Model</td><td>非概率模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01324</td><td>Non-Saturating Game</td><td>非饱和博弈</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01325</td><td>Non-Separable</td><td>不可分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01326</td><td>Nonconvex</td><td>非凸</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01327</td><td>Nondistributed</td><td>非分布式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01328</td><td>Nondistributed Representation</td><td>非分布式表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01329</td><td>Nonlinear Autoregressive With Exogenous Inputs Model</td><td>有外部输入的非线性自回归模型</td><td>NARX</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01330</td><td>Nonlinear Conjugate Gradients</td><td>非线性共轭梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01331</td><td>Nonlinear Independent Components Estimation</td><td>非线性独立成分估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01332</td><td>Nonlinear Programming</td><td>非线性规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01333</td><td>Nonparametric Density Estimation</td><td>非参数密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01334</td><td>Norm</td><td>范数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01335</td><td>Norm-Preserving</td><td>范数保持性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01336</td><td>Normal Distribution</td><td>正态分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01337</td><td>Normal Equation</td><td>正规方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01338</td><td>Normalization</td><td>规范化</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01339</td><td>Normalization Factor</td><td>规范化因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01340</td><td>Normalized</td><td>规范化的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01341</td><td>Normalized Initialization</td><td>标准初始化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01342</td><td>Nuclear Norm</td><td>核范数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01343</td><td>Null Space</td><td>零空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01344</td><td>Number of Epochs</td><td>轮数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01345</td><td>Numerator Layout</td><td>分子布局</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01346</td><td>Numeric Value</td><td>数值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01347</td><td>Numerical Attribute</td><td>数值属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01348</td><td>Numerical Differentiation</td><td>数值微分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01349</td><td>Numerical Method</td><td>数值方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01350</td><td>Numerical Optimization</td><td>数值优化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01351</td><td>Object Detection</td><td>目标检测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01352</td><td>Object Recognition</td><td>对象识别</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01353</td><td>Objective</td><td>目标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01354</td><td>Objective Function</td><td>目标函数</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-11-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01355</td><td>Oblique Decision Tree</td><td>斜决策树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01356</td><td>Observable Variable</td><td>观测变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01357</td><td>Observation Sequence</td><td>观测序列</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01358</td><td>Occam's Razor</td><td>奥卡姆剃刀</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01359</td><td>Odds</td><td>几率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01360</td><td>Off-Policy</td><td>异策略</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01361</td><td>Offline Inference</td><td>离线推断</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-06-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01362</td><td>Offset</td><td>偏移量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01363</td><td>Offset Vector</td><td>偏移向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01364</td><td>On-Policy</td><td>同策略</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01365</td><td>One-Shot Learning</td><td>单试学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-03-13-2">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-01366</td><td>One-Dependent Estimator</td><td>独依赖估计</td><td>ODE</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01367</td><td>One-Hot</td><td>独热</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01368</td><td>Online</td><td>在线</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01369</td><td>Online Inference</td><td>在线推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01370</td><td>Online Learning</td><td>在线学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01371</td><td>Operation</td><td>操作</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01372</td><td>Operator</td><td>运算符</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01373</td><td>Optimal Capacity</td><td>最佳容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01374</td><td>Optimization</td><td>最优化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01375</td><td>Optimization Landscape</td><td>优化地形</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01376</td><td>Optimizer</td><td>优化器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01377</td><td>Ordered Rule</td><td>带序规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01378</td><td>Ordinal Attribute</td><td>有序属性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01379</td><td>Origin</td><td>原点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01380</td><td>Orthogonal</td><td>正交</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-01381</td><td>Orthogonal Initialization</td><td>正交初始化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01382</td><td>Orthogonal Matrix</td><td>正交矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01383</td><td>Orthonormal</td><td>标准正交</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01384</td><td>Out-Of-Bag Estimate</td><td>包外估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01385</td><td>Outer Product</td><td>外积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01386</td><td>Outlier</td><td>异常点</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-01387</td><td>Output</td><td>输出</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01388</td><td>Output Gate</td><td>输出门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01389</td><td>Output Layer</td><td>输出层</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01390</td><td>Output Smearing</td><td>输出调制法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01391</td><td>Output Space</td><td>输出空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01392</td><td>Over-Parameterized</td><td>过度参数化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01393</td><td>Overcomplete</td><td>过完备</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01394</td><td>Overestimation</td><td>过估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01395</td><td>Overfitting</td><td>过拟合</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01396</td><td>Overfitting Regime</td><td>过拟合机制</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01397</td><td>Overflow</td><td>上溢</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01398</td><td>Oversampling</td><td>过采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01399</td><td>PAC Learning</td><td>PAC学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01400</td><td>Pac-Learnable</td><td>PAC可学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01401</td><td>Padding</td><td>填充</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01402</td><td>Paired t -Test</td><td>成对 t 检验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01403</td><td>Pairwise</td><td>成对型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01404</td><td>Pairwise Markov Property</td><td>成对马尔可夫性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01405</td><td>Parallel Distributed Processing</td><td>分布式并行处理</td><td>PDP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01406</td><td>Parallel Tempering</td><td>并行回火</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01407</td><td>Parameter</td><td>参数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01408</td><td>Parameter Estimation</td><td>参数估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01409</td><td>Parameter Server</td><td>参数服务器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01410</td><td>Parameter Sharing</td><td>参数共享</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01411</td><td>Parameter Space</td><td>参数空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01412</td><td>Parameter Tuning</td><td>调参</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-03-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01413</td><td>Parametric Case</td><td>有参情况</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01414</td><td>Parametric Density Estimation</td><td>参数密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01415</td><td>Parametric Model</td><td>参数化模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01416</td><td>Parametric ReLU</td><td>参数化修正线性单元/参数化整流线性单元</td><td>PReLU</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01417</td><td>Parse Tree</td><td>解析树</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01418</td><td>Part-Of-Speech Tagging</td><td>词性标注</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01419</td><td>Partial Derivative</td><td>偏导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01420</td><td>Partially Observable Markov Decision Processes</td><td>部分可观测马尔可夫决策过程</td><td>POMDP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01421</td><td>Particle Swarm Optimization</td><td>粒子群优化算法</td><td>PSO</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01422</td><td>Partition</td><td>划分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01423</td><td>Partition Function</td><td>配分函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01424</td><td>Path</td><td>路径</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01425</td><td>Pattern</td><td>模式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01426</td><td>Pattern Recognition</td><td>模式识别</td><td>PR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-01427</td><td>Penalty Term</td><td>罚项</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01428</td><td>Perceptron</td><td>感知机</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-15-2">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01429</td><td>Performance Measure</td><td>性能度量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01430</td><td>Periodic</td><td>周期的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01431</td><td>Permutation Invariant</td><td>置换不变性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01432</td><td>Perplexity</td><td>困惑度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01433</td><td>Persistent Contrastive Divergence</td><td>持续性对比散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01434</td><td>Phoneme</td><td>音素</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01435</td><td>Phonetic</td><td>语音</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01436</td><td>Pictorial Structure</td><td>图形结构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01437</td><td>Piecewise</td><td>分段</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01438</td><td>Piecewise Constant Decay</td><td>分段常数衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01439</td><td>Pipeline</td><td>流水线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01440</td><td>Plate Notation</td><td>板块表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01441</td><td>Plug And Play Generative Network</td><td>即插即用生成网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01442</td><td>Plurality Voting</td><td>相对多数投票</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01443</td><td>Point Estimator</td><td>点估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01444</td><td>Pointer Network</td><td>指针网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01445</td><td>Polarity Detection</td><td>极性检测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01446</td><td>Policy</td><td>策略</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01447</td><td>Policy Evaluation</td><td>策略评估</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01448</td><td>Policy Gradient</td><td>策略梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01449</td><td>Policy Improvement</td><td>策略改进</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01450</td><td>Policy Iteration</td><td>策略迭代</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01451</td><td>Policy Search</td><td>策略搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01452</td><td>Polynomial Basis Function</td><td>多项式基函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01453</td><td>Polynomial Kernel Function</td><td>多项式核函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01454</td><td>Polysemy</td><td>一词多义性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01455</td><td>Pool</td><td>汇聚</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01456</td><td>Pooling</td><td>汇聚</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-02-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01457</td><td>Pooling Function</td><td>汇聚函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01458</td><td>Pooling Layer</td><td>汇聚层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01459</td><td>Poor Conditioning</td><td>病态条件</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01460</td><td>Position Embedding</td><td>位置嵌入</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01461</td><td>Positional Encoding</td><td>位置编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01462</td><td>Positive Class</td><td>正类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01463</td><td>Positive Definite</td><td>正定</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01464</td><td>Positive Definite Kernel Function</td><td>正定核函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01465</td><td>Positive Definite Matrix</td><td>正定矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01466</td><td>Positive Part Function</td><td>正部函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01467</td><td>Positive Phase</td><td>正相</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01468</td><td>Positive Recurrent</td><td>正常返的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01469</td><td>Positive Sample</td><td>正例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01470</td><td>Positive Semidefinite</td><td>半正定</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01471</td><td>Positive-Semidefinite Matrix</td><td>半正定矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01472</td><td>Post-Hoc Test</td><td>后续检验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01473</td><td>Post-Pruning</td><td>后剪枝</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01474</td><td>Posterior Distribution</td><td>后验分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01475</td><td>Posterior Inference</td><td>后验推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01476</td><td>Posterior Probability</td><td>后验概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01477</td><td>Potential Function</td><td>势函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01478</td><td>Power Method</td><td>幂法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01479</td><td>PR Curve</td><td>P-R曲线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01480</td><td>Pre-Trained Initialization</td><td>预训练初始化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01481</td><td>Pre-Training</td><td>预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01482</td><td>Precision</td><td>查准率/准确率</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>数学、HPC</td></tr><tr class="odd"><td>AITD-01483</td><td>Precision Matrix</td><td>精度矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01484</td><td>Predictive Sparse Decomposition</td><td>预测稀疏分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01485</td><td>Prepruning</td><td>预剪枝</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01486</td><td>Pretrained Language Model</td><td>预训练语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01487</td><td>Primal Problem</td><td>主问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01488</td><td>Primary Visual Cortex</td><td>初级视觉皮层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01489</td><td>Principal Component Analysis</td><td>主成分分析</td><td>PCA</td><td><a href="https://www.jiqizhixin.com/articles/2017-12-03-4">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[4]</a></td><td></td></tr><tr class="even"><td>AITD-01490</td><td>Principle Of Multiple Explanations</td><td>多释原则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01491</td><td>Prior</td><td>先验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01492</td><td>Prior Knowledge</td><td>先验知识</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-01493</td><td>Prior Probability</td><td>先验概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01494</td><td>Prior Probability Distribution</td><td>先验概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01495</td><td>Prior Pseudo-Counts</td><td>伪计数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01496</td><td>Prior Shift</td><td>先验偏移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01497</td><td>Priority Rule</td><td>优先级规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01498</td><td>Probabilistic Context-Free Grammar</td><td>概率上下文无关文法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01499</td><td>Probabilistic Density Estimation</td><td>概率密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01500</td><td>Probabilistic Generative Model</td><td>概率生成模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01501</td><td>Probabilistic Graphical Model</td><td>概率图模型</td><td>PGM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-29-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01502</td><td>Probabilistic Latent Semantic Analysis</td><td>概率潜在语义分析</td><td>PLSA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01503</td><td>Probabilistic Latent Semantic Indexing</td><td>概率潜在语义索引</td><td>PLSI</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01504</td><td>Probabilistic Model</td><td>概率模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01505</td><td>Probabilistic PCA</td><td>概率PCA</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01506</td><td>Probabilistic Undirected Graphical Model</td><td>概率无向图模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01507</td><td>Probability</td><td>概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01508</td><td>Probability Density Function</td><td>概率密度函数</td><td>PDF</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01509</td><td>Probability Distribution</td><td>概率分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01510</td><td>Probability Mass Function</td><td>概率质量函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01511</td><td>Probability Model Estimation</td><td>概率模型估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01512</td><td>Probably Approximately Correct</td><td>概率近似正确</td><td>PAC</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01513</td><td>Product of Expert</td><td>专家之积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01514</td><td>Product Rule</td><td>乘法法则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01515</td><td>Properly PAC Learnable</td><td>恰PAC可学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01516</td><td>Proportional</td><td>成比例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01517</td><td>Proposal Distribution</td><td>提议分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01518</td><td>Propositional Atom</td><td>原子命题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01519</td><td>Propositional Rule</td><td>命题规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01520</td><td>Prototype-Based Clustering</td><td>原型聚类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01521</td><td>Proximal Gradient Descent</td><td>近端梯度下降</td><td>PGD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01522</td><td>Pruning</td><td>剪枝</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-26">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01523</td><td>Pseudo-Label</td><td>伪标记</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01524</td><td>Pseudolikelihood</td><td>伪似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01525</td><td>Q Function</td><td>Q函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01526</td><td>Q-Learning</td><td>Q学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01527</td><td>Q-Network</td><td>Q网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01528</td><td>Quadratic Loss Function</td><td>平方损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01529</td><td>Quadratic Programming</td><td>二次规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01530</td><td>Quadrature Pair</td><td>象限对</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01531</td><td>Quantized Neural Network</td><td>量子化神经网络</td><td>QNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01532</td><td>Quantum Computer</td><td>量子计算机</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-13">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-30-5">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-29-5">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-01533</td><td>Quantum Computing</td><td>量子计算</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-13">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-17">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-29-5">[3]</a></td><td></td></tr><tr class="even"><td>AITD-01534</td><td>Quantum Machine Learning</td><td>量子机器学习</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-04-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01535</td><td>Quantum Mechanics</td><td>量子力学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-01536</td><td>Quasi Newton Method</td><td>拟牛顿法</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-16-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01537</td><td>Quasi-Concave</td><td>拟凹</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01538</td><td>Query</td><td>查询</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01539</td><td>Query Vector</td><td>查询向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01540</td><td>Query-Key-Value</td><td>查询-键-值</td><td>QKV</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01541</td><td>Radial Basis Function</td><td>径向基函数</td><td>RBF</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-01542</td><td>Random Access Memory</td><td>随机访问存储</td><td>RAM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01543</td><td>Random Field</td><td>随机场</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01544</td><td>Random Forest Algorithm</td><td>随机森林算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01545</td><td>Random Forest</td><td>随机森林</td><td>RF、RFS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[3]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[5]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01546</td><td>Random Initialization</td><td>随机初始化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01547</td><td>Random Sampling</td><td>随机采样</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[2]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01548</td><td>Random Search</td><td>随机搜索</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01549</td><td>Random Subspace</td><td>随机子空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01550</td><td>Random Variable</td><td>随机变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01551</td><td>Random Walk</td><td>随机游走</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01552</td><td>Range</td><td>值域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01553</td><td>Rank</td><td>秩</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01554</td><td>Ratio Matching</td><td>比率匹配</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01555</td><td>Raw Feature</td><td>原始特征</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01556</td><td>Re-Balance</td><td>再平衡</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01557</td><td>Re-Sampling</td><td>重采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01558</td><td>Re-Weighting</td><td>重赋权</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01559</td><td>Readout Function</td><td>读出函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01560</td><td>Real-Time Recurrent Learning</td><td>实时循环学习</td><td>RTRL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01561</td><td>Recall</td><td>查全率/召回率</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01562</td><td>Recall-Oriented Understudy For Gisting Evaluation</td><td>ROUGE</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01563</td><td>Receiver Operating Characteristic</td><td>受试者工作特征</td><td>ROC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01564</td><td>Receptive Field</td><td>感受野</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01565</td><td>Recirculation</td><td>再循环</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01566</td><td>Recognition Weight</td><td>认知权重</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01567</td><td>Recommender System</td><td>推荐系统</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01568</td><td>Reconstruction</td><td>重构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01569</td><td>Reconstruction Error</td><td>重构误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01570</td><td>Rectangular Diagonal Matrix</td><td>矩形对角矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01571</td><td>Rectified Linear</td><td>整流线性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01572</td><td>Rectified Linear Transformation</td><td>整流线性变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01573</td><td>Rectified Linear Unit</td><td>修正线性单元/整流线性单元</td><td>ReLU</td><td><a href="https://www.jiqizhixin.com/articles/2017-10-21-4">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td>CHAPTER 2</td></tr><tr class="even"><td>AITD-01574</td><td>Rectifier Network</td><td>整流网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01575</td><td>Recurrence</td><td>循环</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01576</td><td>Recurrent Convolutional Network</td><td>循环卷积网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01577</td><td>Recurrent Multi-Layer Perceptron</td><td>循环多层感知器</td><td>RMLP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01578</td><td>Recurrent Network</td><td>循环网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01579</td><td>Recurrent Neural Network</td><td>循环神经网络</td><td>RNN</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-13-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-05-5">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-21-15">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[4]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[5]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[6]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01580</td><td>Recursive Neural Network</td><td>递归神经网络</td><td>RecNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01581</td><td>Reducible</td><td>可约的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01582</td><td>Redundant Feature</td><td>冗余特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01583</td><td>Reference Model</td><td>参考模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01584</td><td>Region</td><td>区域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01585</td><td>Regression</td><td>回归</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-21-13">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[3]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01586</td><td>Regularization</td><td>正则化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-20">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01587</td><td>Regularizer</td><td>正则化项</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01588</td><td>Reinforcement Learning</td><td>强化学习</td><td>RL</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-17-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-28-6">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[5]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01589</td><td>Rejection Sampling</td><td>拒绝采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01590</td><td>Relation</td><td>关系</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01591</td><td>Relational Database</td><td>关系型数据库</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01592</td><td>Relative Entropy</td><td>相对熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01593</td><td>Relevant Feature</td><td>相关特征</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01594</td><td>Reparameterization</td><td>再参数化/重参数化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01595</td><td>Reparametrization Trick</td><td>重参数化技巧</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01596</td><td>Replay Buffer</td><td>经验池</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01597</td><td>Representation</td><td>表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01598</td><td>Representation Learning</td><td>表示学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01599</td><td>Representational Capacity</td><td>表示容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01600</td><td>Representer Theorem</td><td>表示定理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01601</td><td>Reproducing Kernel Hilbert Space</td><td>再生核希尔伯特空间</td><td>RKHS</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01602</td><td>Rescaling</td><td>再缩放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01603</td><td>Reservoir Computing</td><td>储层计算</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01604</td><td>Reset Gate</td><td>重置门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01605</td><td>Residual Blocks</td><td>残差块</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01606</td><td>Residual Connection</td><td>残差连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01607</td><td>Residual Mapping</td><td>残差映射</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01608</td><td>Residual Network</td><td>残差网络</td><td>ResNet</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-18-2">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01609</td><td>Residual Unit</td><td>残差单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01610</td><td>Residue Function</td><td>残差函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01611</td><td>Resolution Quotient</td><td>归结商</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01612</td><td>Restricted Boltzmann Machine</td><td>受限玻尔兹曼机</td><td>RBM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-08-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01613</td><td>Restricted Isometry Property</td><td>限定等距性</td><td>RIP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01614</td><td>Return</td><td>总回报</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01615</td><td>Reverse Correlation</td><td>反向相关</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01616</td><td>Reverse KL Divergence</td><td>逆向KL散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01617</td><td>Reverse Mode Accumulation</td><td>反向模式累加</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01618</td><td>Reversible Markov Chain</td><td>可逆马尔可夫链</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01619</td><td>Reward</td><td>奖励</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01620</td><td>Reward Function</td><td>奖励函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01621</td><td>Ridge Regression</td><td>岭回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01622</td><td>Riemann Integral</td><td>黎曼积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01623</td><td>Right Eigenvector</td><td>右特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01624</td><td>Right Singular Vector</td><td>右奇异向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01625</td><td>Risk</td><td>风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01626</td><td>Risk Function</td><td>风险函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01627</td><td>Robustness</td><td>稳健性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>计算机、机器学习</td></tr><tr class="even"><td>AITD-01628</td><td>Root Node</td><td>根结点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01629</td><td>Round-Off Error</td><td>舍入误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01630</td><td>Row</td><td>行</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01631</td><td>Rule Engine</td><td>规则引擎</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01632</td><td>Rule Learning</td><td>规则学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01633</td><td>S-Fold Cross Validation</td><td>S 折交叉验证</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01634</td><td>Saccade</td><td>扫视</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01635</td><td>Saddle Point</td><td>鞍点</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-08">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01636</td><td>Saddle-Free Newton Method</td><td>无鞍牛顿法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01637</td><td>Saliency Map</td><td>显著图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01638</td><td>Saliency-Based Attention</td><td>基于显著性的注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01639</td><td>Same</td><td>相同</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01640</td><td>Sample</td><td>样本</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01641</td><td>Sample Complexity</td><td>样本复杂度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01642</td><td>Sample Mean</td><td>样本均值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01643</td><td>Sample Space</td><td>样本空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01644</td><td>Sample Variance</td><td>样本方差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01645</td><td>Sampling</td><td>采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01646</td><td>Sampling Method</td><td>采样法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01647</td><td>Saturate</td><td>饱和</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01648</td><td>Saturating Function</td><td>饱和函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01649</td><td>Scalar</td><td>标量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01650</td><td>Scale Invariance</td><td>尺度不变性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01651</td><td>Scatter Matrix</td><td>散布矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01652</td><td>Scheduled Sampling</td><td>计划采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01653</td><td>Score</td><td>得分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01654</td><td>Score Function</td><td>评分函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01655</td><td>Score Matching</td><td>分数匹配</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01656</td><td>Second Derivative</td><td>二阶导数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01657</td><td>Second Derivative Test</td><td>二阶导数测试</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01658</td><td>Second Layer</td><td>第二层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01659</td><td>Second-Order Method</td><td>二阶方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01660</td><td>Selective Attention</td><td>选择性注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01661</td><td>Selective Ensemble</td><td>选择性集成</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01662</td><td>Self Information</td><td>自信息</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01663</td><td>Self-Attention</td><td>自注意力</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01664</td><td>Self-Attention Model</td><td>自注意力模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01665</td><td>Self-Contrastive Estimation</td><td>自对比估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01666</td><td>Self-Driving</td><td>自动驾驶</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-27-7">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-16">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-08-9">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-01667</td><td>Self-Gated</td><td>自门控</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01668</td><td>Self-Organizing Map</td><td>自组织映射网</td><td>SOM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01669</td><td>Self-Taught Learning</td><td>自学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01670</td><td>Self-Training</td><td>自训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01671</td><td>Semantic Gap</td><td>语义鸿沟</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01672</td><td>Semantic Hashing</td><td>语义哈希</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01673</td><td>Semantic Segmentation</td><td>语义分割</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01674</td><td>Semantic Similarity</td><td>语义相似度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01675</td><td>Semi-Definite Programming</td><td>半正定规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01676</td><td>Semi-Naive Bayes Classifiers</td><td>半朴素贝叶斯分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01677</td><td>Semi-Restricted Boltzmann Machine</td><td>半受限玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01678</td><td>Semi-Supervised</td><td>半监督</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01679</td><td>Semi-Supervised Clustering</td><td>半监督聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01680</td><td>Semi-Supervised Learning</td><td>半监督学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-22-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-02">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-07">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-01681</td><td>Semi-Supervised Support Vector Machine</td><td>半监督支持向量机</td><td>S3VM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01682</td><td>Sentiment Analysis</td><td>情感分析</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-07-7">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01683</td><td>Separable</td><td>可分离的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01684</td><td>Separate</td><td>分离的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01685</td><td>Separating Hyperplane</td><td>分离超平面</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01686</td><td>Separation</td><td>分离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01687</td><td>Sequence Labeling</td><td>序列标注</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01688</td><td>Sequence To Sequence Learning</td><td>序列到序列学习</td><td>Seq2Seq</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01689</td><td>Sequence-To-Sequence</td><td>序列到序列</td><td>Seq2Seq</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01690</td><td>Sequential Covering</td><td>序贯覆盖</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01691</td><td>Sequential Minimal Optimization</td><td>序列最小最优化</td><td>SMO</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01692</td><td>Sequential Model-Based Optimization</td><td>时序模型优化</td><td>SMBO</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01693</td><td>Sequential Partitioning</td><td>顺序分区</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01694</td><td>Setting</td><td>情景</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01695</td><td>Shadow Circuit</td><td>浅度回路</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01696</td><td>Shallow Learning</td><td>浅层学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01697</td><td>Shannon Entropy</td><td>香农熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01698</td><td>Shannons</td><td>香农</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01699</td><td>Shaping</td><td>塑造</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01700</td><td>Sharp Minima</td><td>尖锐最小值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01701</td><td>Shattering</td><td>打散</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01702</td><td>Shift Invariance</td><td>平移不变性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01703</td><td>Short-Term Memory</td><td>短期记忆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01704</td><td>Shortcut Connection</td><td>直连边</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01705</td><td>Shortlist</td><td>短列表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01706</td><td>Siamese Network</td><td>孪生网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-02-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01707</td><td>Sigmoid</td><td>Sigmoid（一种激活函数）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01708</td><td>Sigmoid Belief Network</td><td>Sigmoid信念网络</td><td>SBN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01709</td><td>Sigmoid Curve</td><td>S 形曲线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01710</td><td>Sigmoid Function</td><td>Sigmoid函数</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-02-26">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01711</td><td>Sign Function</td><td>符号函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01712</td><td>Signed Distance</td><td>带符号距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01713</td><td>Similarity</td><td>相似度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01714</td><td>Similarity Measure</td><td>相似度度量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01715</td><td>Simple Cell</td><td>简单细胞</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01716</td><td>Simple Recurrent Network</td><td>简单循环网络</td><td>SRN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01717</td><td>Simple Recurrent Neural Network</td><td>简单循环神经网络</td><td>S-RNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01718</td><td>Simplex</td><td>单纯形</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01719</td><td>Simulated Annealing</td><td>模拟退火</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01720</td><td>Simultaneous Localization And Mapping</td><td>即时定位与地图构建</td><td>SLAM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01721</td><td>Single Component Metropolis-Hastings</td><td>单分量Metropolis-Hastings</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01722</td><td>Single Linkage</td><td>单连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01723</td><td>Singular</td><td>奇异的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01724</td><td>Singular Value</td><td>奇异值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01725</td><td>Singular Value Decomposition</td><td>奇异值分解</td><td>SVD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01726</td><td>Singular Vector</td><td>奇异向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01727</td><td>Size</td><td>大小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01728</td><td>Skip Connection</td><td>跳跃连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01729</td><td>Skip-Gram Model</td><td>跳元模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01730</td><td>Skip-Gram Model With Negative Sampling</td><td>跳元模型加负采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01731</td><td>Slack Variable</td><td>松弛变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01732</td><td>Slow Feature Analysis</td><td>慢特征分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01733</td><td>Slowness Principle</td><td>慢性原则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01734</td><td>Smoothing</td><td>平滑</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01735</td><td>Smoothness Prior</td><td>平滑先验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01736</td><td>Soft Attention Mechanism</td><td>软性注意力机制</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01737</td><td>Soft Clustering</td><td>软聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01738</td><td>Soft Margin</td><td>软间隔</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01739</td><td>Soft Margin Maximization</td><td>软间隔最大化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01740</td><td>Soft Target</td><td>软目标</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01741</td><td>Soft Voting</td><td>软投票</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01742</td><td>Softmax</td><td>Softmax/软最大化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01743</td><td>Softmax Function</td><td>Softmax函数/软最大化函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01744</td><td>Softmax Regression</td><td>Softmax回归/软最大化回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01745</td><td>Softmax Unit</td><td>Softmax单元/软最大化单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01746</td><td>Softplus</td><td>Softplus</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01747</td><td>Softplus Function</td><td>Softplus函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01748</td><td>Source Domain</td><td>源领域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01749</td><td>Span</td><td>张成子空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01750</td><td>Sparse</td><td>稀疏</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01751</td><td>Sparse Activation</td><td>稀疏激活</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01752</td><td>Sparse Auto-Encoder</td><td>稀疏自编码器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01753</td><td>Sparse Coding</td><td>稀疏编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01754</td><td>Sparse Connectivity</td><td>稀疏连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01755</td><td>Sparse Initialization</td><td>稀疏初始化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01756</td><td>Sparse Interactions</td><td>稀疏交互</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01757</td><td>Sparse Representation</td><td>稀疏表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01758</td><td>Sparse Weights</td><td>稀疏权重</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01759</td><td>Sparsity</td><td>稀疏性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01760</td><td>Specialization</td><td>特化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01761</td><td>Spectral Clustering</td><td>谱聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01762</td><td>Spectral Radius</td><td>谱半径</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01763</td><td>Speech Recognition</td><td>语音识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-01-3">[4]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-04">[5]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-15">[6]</a></td><td></td></tr><tr class="even"><td>AITD-01764</td><td>Sphering</td><td>Sphering</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01765</td><td>Spike And Slab</td><td>尖峰和平板</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01766</td><td>Spike And Slab RBM</td><td>尖峰和平板RBM</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01767</td><td>Spiking Neural Nets</td><td>脉冲神经网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-13-7">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01768</td><td>Splitting Point</td><td>切分点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01769</td><td>Splitting Variable</td><td>切分变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01770</td><td>Spurious Modes</td><td>虚假模态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01771</td><td>Square</td><td>方阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01772</td><td>Square Loss</td><td>平方损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01773</td><td>Squared Euclidean Distance</td><td>欧氏距离平方</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01774</td><td>Squared Exponential</td><td>平方指数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01775</td><td>Squashing Function</td><td>挤压函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01776</td><td>Stability</td><td>稳定性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01777</td><td>Stability-Plasticity Dilemma</td><td>可塑性-稳定性窘境</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01778</td><td>Stable Base Learner</td><td>稳定基学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01779</td><td>Stacked Auto-Encoder</td><td>堆叠自编码器</td><td>SAE</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01780</td><td>Stacked Deconvolutional Network</td><td>堆叠解卷积网络</td><td>SDN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01781</td><td>Stacked Recurrent Neural Network</td><td>堆叠循环神经网络</td><td>SRNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01782</td><td>Standard Basis</td><td>标准基</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01783</td><td>Standard Deviation</td><td>标准差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01784</td><td>Standard Error</td><td>标准差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01785</td><td>Standard Normal Distribution</td><td>标准正态分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01786</td><td>Standardization</td><td>标准化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01787</td><td>State</td><td>状态</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01788</td><td>State Action Reward State Action</td><td>SARSA算法</td><td>SARSA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01789</td><td>State Sequence</td><td>状态序列</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01790</td><td>State Space</td><td>状态空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01791</td><td>State Value Function</td><td>状态值函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01792</td><td>State-Action Value Function</td><td>状态-动作值函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01793</td><td>Statement</td><td>声明</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01794</td><td>Static Computational Graph</td><td>静态计算图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01795</td><td>Static Game</td><td>静态博弈</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01796</td><td>Stationary</td><td>平稳的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01797</td><td>Stationary Distribution</td><td>平稳分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01798</td><td>Stationary Point</td><td>驻点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01799</td><td>Statistic Efficiency</td><td>统计效率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01800</td><td>Statistical Learning</td><td>统计学习</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-01801</td><td>Statistical Learning Theory</td><td>统计学习理论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01802</td><td>Statistical Machine Learning</td><td>统计机器学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01803</td><td>Statistical Relational Learning</td><td>统计关系学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01804</td><td>Statistical Simulation Method</td><td>统计模拟方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01805</td><td>Statistics</td><td>统计量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01806</td><td>Status Feature Function</td><td>状态特征函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01807</td><td>Steepest Descent</td><td>最速下降法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01808</td><td>Step Decay</td><td>阶梯衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01809</td><td>Stochastic</td><td>随机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01810</td><td>Stochastic Curriculum</td><td>随机课程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01811</td><td>Stochastic Dynamical System</td><td>随机动力系统</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01812</td><td>Stochastic Gradient Ascent</td><td>随机梯度上升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01813</td><td>Stochastic Gradient Descent</td><td>随机梯度下降</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-25-10">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01814</td><td>Stochastic Gradient Descent With Warm Restarts</td><td>带热重启的随机梯度下降</td><td>SGDR</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01815</td><td>Stochastic Matrix</td><td>随机矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01816</td><td>Stochastic Maximum Likelihood</td><td>随机最大似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01817</td><td>Stochastic Neighbor Embedding</td><td>随机近邻嵌入</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01818</td><td>Stochastic Neural Network</td><td>随机神经网络</td><td>SNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01819</td><td>Stochastic Policy</td><td>随机性策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01820</td><td>Stochastic Process</td><td>随机过程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01821</td><td>Stop Words</td><td>停用词</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01822</td><td>Stratified Sampling</td><td>分层采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01823</td><td>Stream</td><td>流</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01824</td><td>Stride</td><td>步幅</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01825</td><td>String Kernel Function</td><td>字符串核函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01826</td><td>Strong Classifier</td><td>强分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01827</td><td>Strong Duality</td><td>强对偶性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01828</td><td>Strongly Connected Graph</td><td>强连通图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01829</td><td>Strongly Learnable</td><td>强可学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01830</td><td>Structural Risk</td><td>结构风险</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01831</td><td>Structural Risk Minimization</td><td>结构风险最小化</td><td>SRM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01832</td><td>Structure Learning</td><td>结构学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01833</td><td>Structured Learning</td><td>结构化学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01834</td><td>Structured Probabilistic Model</td><td>结构化概率模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01835</td><td>Structured Variational Inference</td><td>结构化变分推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01836</td><td>Student Network</td><td>学生网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01837</td><td>Sub-Optimal</td><td>次最优</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01838</td><td>Subatomic</td><td>亚原子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01839</td><td>Subsample</td><td>子采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01840</td><td>Subsampling</td><td>下采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01841</td><td>Subsampling Layer</td><td>子采样层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01842</td><td>Subset Evaluation</td><td>子集评价</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01843</td><td>Subset Search</td><td>子集搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01844</td><td>Subspace</td><td>子空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01845</td><td>Substitution</td><td>置换</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01846</td><td>Successive Halving</td><td>逐次减半</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01847</td><td>Sum Rule</td><td>求和法则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01848</td><td>Sum-Product</td><td>和积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01849</td><td>Sum-Product Network</td><td>和-积网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01850</td><td>Super-Parent</td><td>超父</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01851</td><td>Supervised</td><td>监督</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01852</td><td>Supervised Learning</td><td>监督学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01853</td><td>Supervised Learning Algorithm</td><td>监督学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01854</td><td>Supervised Model</td><td>监督模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01855</td><td>Supervised Pretraining</td><td>监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01856</td><td>Support Vector</td><td>支持向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01857</td><td>Support Vector Expansion</td><td>支持向量展式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01858</td><td>Support Vector Machine</td><td>支持向量机</td><td>SVM</td><td><a href="https://www.jiqizhixin.com/articles/2017-10-08">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[4]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01859</td><td>Support Vector Regression</td><td>支持向量回归</td><td>SVR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[3]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01860</td><td>Surrogat Loss</td><td>替代损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01861</td><td>Surrogate Function</td><td>替代函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01862</td><td>Surrogate Loss Function</td><td>代理损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01863</td><td>Symbol</td><td>符号</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01864</td><td>Symbolic Differentiation</td><td>符号微分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01865</td><td>Symbolic Learning</td><td>符号学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01866</td><td>Symbolic Representation</td><td>符号表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01867</td><td>Symbolism</td><td>符号主义</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01868</td><td>Symmetric</td><td>对称</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01869</td><td>Symmetric Matrix</td><td>对称矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01870</td><td>Synonymy</td><td>多词一义性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01871</td><td>Synset</td><td>同义词集</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01872</td><td>Synthetic Feature</td><td>合成特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01873</td><td>T-Distribution Stochastic Neighbour Embedding</td><td>T分布随机近邻嵌入</td><td>T-SNE</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01874</td><td>Tabular Value Function</td><td>表格值函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01875</td><td>Tagging</td><td>标注</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01876</td><td>Tangent Distance</td><td>切面距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01877</td><td>Tangent Plane</td><td>切平面</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01878</td><td>Tangent Propagation</td><td>正切传播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01879</td><td>Target</td><td>目标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01880</td><td>Target Domain</td><td>目标领域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01881</td><td>Taylor</td><td>泰勒</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01882</td><td>Taylor's Formula</td><td>泰勒公式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01883</td><td>Teacher Forcing</td><td>强制教学</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01884</td><td>Teacher Network</td><td>教师网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01885</td><td>Temperature</td><td>温度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01886</td><td>Tempered Transition</td><td>回火转移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01887</td><td>Tempering</td><td>回火</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01888</td><td>Temporal-Difference Learning</td><td>时序差分学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01889</td><td>Tensor</td><td>张量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01890</td><td>Tensor Processing Units</td><td>张量处理单元</td><td>TPU</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-05-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01891</td><td>Term Frequency-Inverse Document Frequency</td><td>单词频率-逆文本频率</td><td>TF-IDF</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01892</td><td>Terminal State</td><td>终止状态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01893</td><td>Test Data</td><td>测试数据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01894</td><td>Test Error</td><td>测试误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01895</td><td>Test Sample</td><td>测试样本</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01896</td><td>Test Set</td><td>测试集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01897</td><td>The Collider Case</td><td>碰撞情况</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01898</td><td>Threshold</td><td>阈值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-01899</td><td>Threshold Logic Unit</td><td>阈值逻辑单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01900</td><td>Threshold-Moving</td><td>阈值移动</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01901</td><td>Tied Weight</td><td>捆绑权重</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01902</td><td>Tikhonov Regularization</td><td>Tikhonov正则化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01903</td><td>Tiled Convolution</td><td>平铺卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01904</td><td>Time Delay Neural Network</td><td>时延神经网络</td><td>TDNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01905</td><td>Time Homogenous Markov Chain</td><td>时间齐次马尔可夫链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01906</td><td>Time Step</td><td>时间步</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01907</td><td>Toeplitz Matrix</td><td>Toeplitz矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01908</td><td>Token</td><td>词元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01909</td><td>Tokenize</td><td>词元化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01910</td><td>Tokenization</td><td>词元化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01911</td><td>Tokenizer</td><td>词元分析器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01912</td><td>Tolerance</td><td>容差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01913</td><td>Top-Down</td><td>自顶向下</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01914</td><td>Topic</td><td>话题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01915</td><td>Topic Model</td><td>话题模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01916</td><td>Topic Modeling</td><td>话题分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01917</td><td>Topic Vector Space</td><td>话题向量空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01918</td><td>Topic Vector Space Model</td><td>话题向量空间模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01919</td><td>Topic-Document Matrix</td><td>话题-文本矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01920</td><td>Topographic ICA</td><td>地质ICA</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01921</td><td>Total Cost</td><td>总体代价</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01922</td><td>Trace</td><td>迹</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01923</td><td>Tractable</td><td>易处理的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01924</td><td>Training</td><td>训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01925</td><td>Training Data</td><td>训练数据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01926</td><td>Training Error</td><td>训练误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01927</td><td>Training Instance</td><td>训练实例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01928</td><td>Training Sample</td><td>训练样本</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01929</td><td>Training Set</td><td>训练集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01930</td><td>Trajectory</td><td>轨迹</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01931</td><td>Transcribe</td><td>转录</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01932</td><td>Transcription System</td><td>转录系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01933</td><td>Transductive Learning</td><td>直推学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01934</td><td>Transductive Transfer Learning</td><td>直推迁移学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01935</td><td>Transfer Learning</td><td>迁移学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-04-7">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[4]</a></td><td></td></tr><tr class="even"><td>AITD-01936</td><td>Transform</td><td>变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01937</td><td>Transformer</td><td>Transformer</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01938</td><td>Transformer Model</td><td>Transformer模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01939</td><td>Transition</td><td>转移</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01940</td><td>Transition Kernel</td><td>转移核</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01941</td><td>Transition Matrix</td><td>状态转移矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01942</td><td>Transition Probability</td><td>转移概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01943</td><td>Transpose</td><td>转置</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01944</td><td>Transposed Convolution</td><td>转置卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01945</td><td>Tree-Structured LSTM</td><td>树结构的长短期记忆模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01946</td><td>Treebank</td><td>树库</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01947</td><td>Trial</td><td>试验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01948</td><td>Trial And Error</td><td>试错</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01949</td><td>Triangle Inequality</td><td>三角不等式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01950</td><td>Triangular Cyclic Learning Rate</td><td>三角循环学习率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01951</td><td>Triangulate</td><td>三角形化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01952</td><td>Triangulated Graph</td><td>三角形化图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01953</td><td>Trigram</td><td>三元语法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01954</td><td>True Negative</td><td>真负例</td><td>TN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-01955</td><td>True Positive</td><td>真正例</td><td>TP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01956</td><td>True Positive Rate</td><td>真正例率</td><td>TPR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-01957</td><td>Truncated Singular Value Decomposition</td><td>截断奇异值分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01958</td><td>Truncation Error</td><td>截断误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01959</td><td>Turing Completeness</td><td>图灵完备</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01960</td><td>Turing Machine</td><td>图灵机</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-11-7">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01961</td><td>Twice-Learning</td><td>二次学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01962</td><td>Two-Dimensional Array</td><td>二维数组</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01963</td><td>Ugly Duckling Theorem</td><td>丑小鸭定理</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01964</td><td>Unbiased</td><td>无偏</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01965</td><td>Unbiased Estimate</td><td>无偏估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01966</td><td>Unbiased Sample Variance</td><td>无偏样本方差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01967</td><td>Unconstrained Optimization</td><td>无约束优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01968</td><td>Undercomplete</td><td>欠完备</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01969</td><td>Underdetermined</td><td>欠定的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01970</td><td>Underestimation</td><td>欠估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01971</td><td>Underfitting</td><td>欠拟合</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01972</td><td>Underfitting Regime</td><td>欠拟合机制</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01973</td><td>Underflow</td><td>下溢</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01974</td><td>Underlying</td><td>潜在</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01975</td><td>Underlying Cause</td><td>潜在成因</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01976</td><td>Undersampling</td><td>欠采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01977</td><td>Understandability</td><td>可理解性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01978</td><td>Undirected</td><td>无向</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01979</td><td>Undirected Graph</td><td>无向图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01980</td><td>Undirected Graphical Model</td><td>无向图模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01981</td><td>Undirected Model</td><td>无向模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01982</td><td>Unequal Cost</td><td>非均等代价</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01983</td><td>Unfolded Graph</td><td>展开图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01984</td><td>Unfolding</td><td>展开</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01985</td><td>Unidirectional Language Model</td><td>单向语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01986</td><td>Unification</td><td>合一</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01987</td><td>Uniform Distribution</td><td>均匀分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01988</td><td>Uniform Sampling</td><td>均匀采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01989</td><td>Uniform Stability</td><td>均匀稳定性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01990</td><td>Unigram</td><td>一元语法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01991</td><td>Unimodal</td><td>单峰值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01992</td><td>Unit</td><td>单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01993</td><td>Unit Norm</td><td>单位范数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01994</td><td>Unit Test</td><td>单元测试</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01995</td><td>Unit Variance</td><td>单位方差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01996</td><td>Unit Vector</td><td>单位向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01997</td><td>Unit-Step Function</td><td>单位阶跃函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01998</td><td>Unitary Matrix</td><td>酉矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01999</td><td>Univariate Decision Tree</td><td>单变量决策树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02000</td><td>Universal Approximation Theorem</td><td>通用近似定理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02001</td><td>Universal Approximator</td><td>通用近似器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02002</td><td>Universal Function Approximator</td><td>通用函数近似器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02003</td><td>Unknown Token</td><td>未知词元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02004</td><td>Unlabeled</td><td>未标记</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02005</td><td>Unnormalized Probability Function</td><td>未规范化概率函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02006</td><td>Unprojection</td><td>反投影</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02007</td><td>Unshared Convolution</td><td>非共享卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02008</td><td>Unsupervised</td><td>无监督</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02009</td><td>Unsupervised Feature Learning</td><td>无监督特征学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02010</td><td>Unsupervised Layer-Wise Training</td><td>无监督逐层训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02011</td><td>Unsupervised Learning Algorithm</td><td>无监督学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02012</td><td>Unsupervised Learning</td><td>无监督学习</td><td>UL</td><td><a href="https://www.jiqizhixin.com/articles/2017-11-17-5">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-02013</td><td>Unsupervised Pretraining</td><td>无监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02014</td><td>Update Gate</td><td>更新门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02015</td><td>Update Model Parameter</td><td>迭代模型参数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02016</td><td>Upper Confidence Bounds</td><td>上置信界限</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02017</td><td>Upsampling</td><td>上采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02018</td><td>V-Structure</td><td>V型结构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02019</td><td>Valid</td><td>有效</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02020</td><td>Validation Set</td><td>验证集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02021</td><td>Validity Index</td><td>有效性指标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02022</td><td>Value Function</td><td>价值函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02023</td><td>Value Function Approximation</td><td>值函数近似</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02024</td><td>Value Iteration</td><td>值迭代</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02025</td><td>Vanishing And Exploding Gradient Problem</td><td>梯度消失与爆炸问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02026</td><td>Vanishing Gradient</td><td>梯度消失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02027</td><td>Vanishing Gradient Problem</td><td>梯度消失问题</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-07-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02028</td><td>Vapnik-Chervonenkis Dimension</td><td>VC维</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02029</td><td>Variable Elimination</td><td>变量消去</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02030</td><td>Variance</td><td>方差</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02031</td><td>Variance Reduction</td><td>方差减小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02032</td><td>Variance Scaling</td><td>方差缩放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02033</td><td>Variational Autoencoder</td><td>变分自编码器</td><td>VAE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02034</td><td>Variational Bayesian</td><td>变分贝叶斯</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02035</td><td>Variational Derivative</td><td>变分导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02036</td><td>Variational Distribution</td><td>变分分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02037</td><td>Variational Dropout</td><td>变分暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02038</td><td>Variational EM Algorithm</td><td>变分EM算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02039</td><td>Variational Free Energy</td><td>变分自由能</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02040</td><td>Variational Inference</td><td>变分推断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02041</td><td>Vector</td><td>向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02042</td><td>Vector Space</td><td>向量空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02043</td><td>Vector Space Model</td><td>向量空间模型</td><td>VSM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02044</td><td>Vectorization</td><td>向量化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02045</td><td>Version Space</td><td>版本空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02046</td><td>Virtual Adversarial Example</td><td>虚拟对抗样本</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02047</td><td>Virtual Adversarial Training</td><td>虚拟对抗训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02048</td><td>Visible Layer</td><td>可见层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02049</td><td>Visible Variable</td><td>可见变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02050</td><td>Viterbi Algorithm</td><td>维特比算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02051</td><td>Vocabulary</td><td>词表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02052</td><td>Von Neumann Architecture</td><td>冯 · 诺伊曼架构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02053</td><td>Voted Perceptron</td><td>投票感知器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02054</td><td>Wake Sleep</td><td>醒眠</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02055</td><td>Warp</td><td>线程束</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02056</td><td>Wasserstein Distance</td><td>Wasserstein距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02057</td><td>Wasserstein GAN</td><td>Wasserstein生成对抗网络</td><td>WGAN</td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-05">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02058</td><td>Weak Classifier</td><td>弱分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02059</td><td>Weak Duality</td><td>弱对偶性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02060</td><td>Weak Learner</td><td>弱学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02061</td><td>Weakly Learnable</td><td>弱可学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02062</td><td>Weakly Supervised Learning</td><td>弱监督学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02063</td><td>Weight</td><td>权重</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-08-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02064</td><td>Weight Decay</td><td>权重衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02065</td><td>Weight Normalization</td><td>权重规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02066</td><td>Weight Scaling Inference Rule</td><td>权重比例推断规则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02067</td><td>Weight Sharing</td><td>权共享</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02068</td><td>Weight Space Symmetry</td><td>权重空间对称性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02069</td><td>Weight Vector</td><td>权值向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02070</td><td>Weighted Distance</td><td>加权距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02071</td><td>Weighted Voting</td><td>加权投票</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02072</td><td>Whitening</td><td>白化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02073</td><td>Wide Convolution</td><td>宽卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02074</td><td>Width</td><td>宽度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02075</td><td>Winner-Take-All</td><td>胜者通吃</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02076</td><td>Within-Class Scatter Matrix</td><td>类内散度矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02077</td><td>Word Embedding</td><td>词嵌入</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-11-20-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02078</td><td>Word Sense Disambiguation</td><td>词义消歧</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02079</td><td>Word Vector</td><td>词向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02080</td><td>Word Vector Space Model</td><td>单词向量空间模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02081</td><td>Word-Document Matrix</td><td>单词-文本矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02082</td><td>Word-Topic Matrix</td><td>单词-话题矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02083</td><td>Working Memory</td><td>工作记忆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02084</td><td>Wrapper Method</td><td>包裹式方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02085</td><td>Z-Score Normalization</td><td>Z值规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02086</td><td>Zero Mean</td><td>零均值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02087</td><td>Zero Padding</td><td>零填充</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02088</td><td>Zero Tensor</td><td>零张量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02089</td><td>Zero-Centered</td><td>零中心化的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02090</td><td>Zero-Data Learning</td><td>零数据学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02091</td><td>Zero-Shot Learning</td><td>零试学习</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-03-31-6">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02092</td><td>Zipf's Law</td><td>齐普夫定律</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02093</td><td>ε-Greedy Method</td><td>ε-贪心法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02094</td><td>2D Qsar Models</td><td>二维定量构效关系模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="odd"><td>AITD-02095</td><td>3D Cartesian</td><td>三维笛卡尔（坐标）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="even"><td>AITD-02096</td><td>3D Conformation</td><td>三维构象</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>化学、生化</td></tr><tr class="odd"><td>AITD-02097</td><td>3D Grids</td><td>三维（坐标）网格</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02098</td><td>3D Qsar Models</td><td>三维定量构效关系模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="odd"><td>AITD-02099</td><td>Aberration-Corrected</td><td>像差矫正</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-02100</td><td>Active Machine Learning</td><td>主动机器学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02101</td><td>Adaptive Fuzzy Neural Network</td><td>自适应模糊神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02102</td><td>Adaptive Sampling</td><td>自适应采样</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02103</td><td>Admet Evaluation</td><td>毒性评估</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="even"><td>AITD-02104</td><td>Alexnet</td><td>AlexNet</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02105</td><td>Alphago</td><td>阿尔法狗</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02106</td><td>Adaptive Neuro Fuzzy Inference System</td><td>自适应神经模糊推理系统</td><td>ANFIS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02107</td><td>Approximate Probabilistic Models</td><td>近似概率模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02108</td><td>Artificial Neurons</td><td>人工神经元</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02109</td><td>Artificial Synapses</td><td>人工突触</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02110</td><td>Attention-Based</td><td>基于注意力（机制）的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02111</td><td>Automating Synthetic Planning</td><td>自动化综合规划</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02112</td><td>Automation</td><td>自动化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02113</td><td>Autonomous Decision-Making</td><td>自主决策</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02114</td><td>B-Clustering Algorithms</td><td>B树聚类算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02115</td><td>Balanced Accuracy</td><td>平衡精度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02116</td><td>Bandgap Energy</td><td>带隙能量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="odd"><td>AITD-02117</td><td>Baseline Test</td><td>基准测试</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02118</td><td>Basin Hopping</td><td>盆地跳跃（算法）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02119</td><td>Bayesian Approach</td><td>贝叶斯方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-02120</td><td>Bayesian Induction</td><td>贝叶斯归纳</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02121</td><td>Bayesian Mcmc Methods</td><td>贝叶斯马尔可夫链蒙特卡洛方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-02122</td><td>Bayesian Methods</td><td>贝叶斯方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02123</td><td>Bayesian Molecular</td><td>贝叶斯分子（设计方法）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td>统计，机器学习，化学</td></tr><tr class="even"><td>AITD-02124</td><td>Bayesian Prior</td><td>贝叶斯先验</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02125</td><td>Bayesian Program Learning</td><td>贝叶斯程序学习</td><td>BPL</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-02126</td><td>Bayesian Regularized Neural Network</td><td>贝叶斯正则化神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02127</td><td>Beam-Scanning</td><td>波束扫描</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-02128</td><td>Best Separates</td><td>最优分离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02129</td><td>Biased Dataset</td><td>有偏数据集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02130</td><td>Bit Collisions</td><td>字节碰撞/冲突</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>数据库</td></tr><tr class="odd"><td>AITD-02131</td><td>Black Box</td><td>黑盒子</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02132</td><td>Black-Box Attack</td><td>黑盒攻击</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02133</td><td>Bonding Environments</td><td>成键环境</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02134</td><td>Bonferroni Correction</td><td>邦弗朗尼校正</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02135</td><td>Bootstrap Aggregation</td><td>引导聚合</td><td>bagging</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02136</td><td>Broyden–Fletcher–Goldfarb–Shanno</td><td>BFGS（算法）</td><td>BFGS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>一种拟牛顿法，数学计算</td></tr><tr class="odd"><td>AITD-02137</td><td>Buchwald−Hartwig Cross-Coupling</td><td>Buchwald–Hartwig 偶联（反应）</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>化学</td></tr><tr class="even"><td>AITD-02138</td><td>C4.5 Algorithm</td><td>C4.5 算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>一种决策树算法，数据挖掘</td></tr><tr class="odd"><td>AITD-02139</td><td>Calculation Uncertainties</td><td>计算不确定性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02140</td><td>Canonical Ml Methods</td><td>经典机器学习方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02141</td><td>Cartesian Distance Vector</td><td>笛卡尔距离向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02142</td><td>CASP</td><td>国际蛋白质结构预测竞赛</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>生物</td></tr><tr class="odd"><td>AITD-02143</td><td>Categorical Data</td><td>分类数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02144</td><td>Categorization Algorithms</td><td>分类算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02145</td><td>ChemDataExtractor</td><td>化学数据提取器</td><td>CDE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02146</td><td>Chi-Squared</td><td>卡方（分布）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02147</td><td>Classification Model</td><td>分类模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02148</td><td>Cluster Resolution Feature Selection</td><td>聚类分辨率特征选择</td><td>CR-FS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02149</td><td>Cluster-Based Splitting</td><td>基于聚类的分离方法</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02150</td><td>Clustering Methods</td><td>聚类方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02151</td><td>Code Pipeline</td><td>代码流水线</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02152</td><td>Coefficient of Determination</td><td>决定系数</td><td>r^2 or R^2</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02153</td><td>Combined Gradient</td><td>组合梯度（算法）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02154</td><td>Complex Data</td><td>复合数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02155</td><td>Computational Cost</td><td>计算成本</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02156</td><td>Computational Optimisation</td><td>计算优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02157</td><td>Computational Science</td><td>计算科学</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02158</td><td>Computational Toxicology</td><td>计算毒理学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02159</td><td>Computer Science</td><td>计算机科学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02160</td><td>Computer Simulations</td><td>计算机模拟</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02161</td><td>Computer-Aided</td><td>计算机辅助</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02162</td><td>Constraint</td><td>约束</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02163</td><td>Core-Loss Spectrum</td><td>（电子能量损失谱中的）高能区域</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02164</td><td>Coulomb Matrix</td><td>库仑矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02165</td><td>Coupled-Cluster Predictions</td><td>耦合簇预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02166</td><td>Cross-Validated Coefficient of Determination</td><td>交叉验证的决定系数</td><td>q^2 or Q^2</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02167</td><td>Cross-Validation</td><td>交叉验证</td><td>CV</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02168</td><td>Crowd-Sourcing</td><td>众包</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>商业模式</td></tr><tr class="odd"><td>AITD-02169</td><td>Cut-Points</td><td>切点</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02170</td><td>Cutoff Radial Function</td><td>截断径向函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02171</td><td>Data Availability</td><td>数据可用性</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02172</td><td>Data Cleaning</td><td>数据清洗</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02173</td><td>Data Collection</td><td>数据采集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02174</td><td>Data Considerations</td><td>数据注意事项</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02175</td><td>Data Curation</td><td>数据监管</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02176</td><td>Data Disparity</td><td>数据差异</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02177</td><td>Data Dredging</td><td>数据挖掘</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02178</td><td>Data Imputation</td><td>数据填补</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02179</td><td>Data Labels</td><td>数据标签</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02180</td><td>Data Leakage</td><td>数据泄露</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02181</td><td>Data Pre-Processing</td><td>数据预处理</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02182</td><td>Data Processing</td><td>数据处理</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02183</td><td>Data Quality</td><td>数据质量</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02184</td><td>Data Reduction</td><td>数据缩减</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02185</td><td>Data Representation</td><td>数据表示</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02186</td><td>Data Selection</td><td>数据选择</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02187</td><td>Data Sources</td><td>数据源</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02188</td><td>Data Splitting</td><td>数据拆分</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02189</td><td>Data Transformation</td><td>数据转换</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02190</td><td>Data-Driven</td><td>数据驱动</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02191</td><td>Data-Driven Decision-Making</td><td>数据驱动的决策</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02192</td><td>Data-Driven Methods</td><td>数据驱动的方法</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02193</td><td>Data-Driven Spectral Analysis</td><td>数据驱动的光谱分析</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02194</td><td>Data-Mining</td><td>数据挖掘</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02195</td><td>Database</td><td>数据库</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02196</td><td>DE Algorithm</td><td>差分进化算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02197</td><td>Deeplift</td><td>DeepLift模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02198</td><td>Dendrogram</td><td>树状图</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02199</td><td>Density Functional Theory</td><td>密度泛函理论</td><td>DFT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[5]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[6]</a></td><td></td></tr><tr class="even"><td>AITD-02200</td><td>Density-Based Spatial Clustering Of Applications With Noise</td><td>DBSCAN密度聚类</td><td>DBSCAN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02201</td><td>Descriptor</td><td>描述符</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02202</td><td>DFT Calculations</td><td>DFT计算</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02203</td><td>Dice Similarity</td><td>戴斯相似度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02204</td><td>Differential Evolution</td><td>差分进化</td><td>DE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02205</td><td>Dimensionality Reduction</td><td>降维</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02206</td><td>Direct Neural Network Modeling</td><td>正向神经网络建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02207</td><td>Discrete Manner</td><td>离散方式</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02208</td><td>Discrete Quanta</td><td>离散量子</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02209</td><td>Discretization</td><td>离散化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02210</td><td>Distillation</td><td>蒸馏</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02211</td><td>Dynamic Datasets</td><td>动态数据集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02212</td><td>Dynamic Filter Networks</td><td>动态过滤网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02213</td><td>Dynamic Sampling</td><td>动态采样</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02214</td><td>Dynamics Simulations</td><td>动力学模拟</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02215</td><td>Eigenfunction</td><td>特征函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02216</td><td>Electronegativity</td><td>电负性</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02217</td><td>Elman</td><td>埃尔曼</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02218</td><td>Empirical Models</td><td>经验模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02219</td><td>Energy Derivatives</td><td>能源衍生品</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>在DP模型中：能量的导数</td></tr><tr class="even"><td>AITD-02220</td><td>Energy Potentials</td><td>能量潜力</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02221</td><td>Ensemble Methods</td><td>集成方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02222</td><td>Entity Normalisation</td><td>实体规范化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02223</td><td>Ethical Considerations</td><td>道德考虑</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02224</td><td>Euclidean Distances</td><td>欧几里得距离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02225</td><td>Evolutionary Algorithms</td><td>进化算法</td><td>EA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02226</td><td>Evolutionary Method</td><td>进化方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02227</td><td>Exchange–Correlation</td><td>交换关联（的能量/泛函）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02228</td><td>Excited-State Potentials</td><td>激发态能量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02229</td><td>Expected Reduction In Distortion</td><td>符合预期的失真减少</td><td>ERD</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02230</td><td>Experimental Validation Data</td><td>实验验证数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02231</td><td>Expert Systems</td><td>专家系统</td><td>ESS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02232</td><td>Extended-Connectivity Circular Fingerprint</td><td>扩展连接环形指纹</td><td>ECFP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02233</td><td>Extraction Techniques</td><td>提取技术</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02234</td><td>Faber-Christensen-Huang-Lilienfeld</td><td>Faber-Christensen-Huang-Lilienfeld</td><td>FCHL</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>四个人提出的化学结构量子机器学习方法</td></tr><tr class="odd"><td>AITD-02235</td><td>Facial Recognition</td><td>面部识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02236</td><td>FAIR Data Principles</td><td>FAIR数据原则</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>Findability可找寻 Accessibility可访问 Interoperability可交互Reuse可再用</td></tr><tr class="odd"><td>AITD-02237</td><td>False Negatives</td><td>假阴性</td><td>FNs</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02238</td><td>False Positives</td><td>假阳性</td><td>FPs</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02239</td><td>Fchl Representation</td><td>Fchl 表示</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02240</td><td>Feature Binarization</td><td>特征二值化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02241</td><td>Feature Transform</td><td>特征变换</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02242</td><td>Feature Vectors</td><td>特征向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02243</td><td>Features</td><td>特征</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02244</td><td>Feed Back</td><td>反馈</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02245</td><td>Feed-Forward Neural Networks</td><td>前馈神经网络</td><td>FFNN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02246</td><td>Feedback Structure</td><td>反馈结构</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02247</td><td>Final Evaluation</td><td>最终评估</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02248</td><td>Findable, Accessible, Interoperable, Reusable</td><td>可查找、可访问、可互操作、可重用</td><td>FAIR</td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02249</td><td>First-Principles</td><td>第一性原理</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02250</td><td>Flow Rate</td><td>流速</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02251</td><td>Forward Cross-Validation</td><td>前向交叉验证</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02252</td><td>Forward Prediction</td><td>前向预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02253</td><td>Forward Reaction Prediction</td><td>前向反应预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02254</td><td>Fuzzy Logic</td><td>模糊逻辑</td><td>FL</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02255</td><td>Fuzzy Neural Networks</td><td>模糊神经网络</td><td>FNN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02256</td><td>Ga-Based Approaches</td><td>基于遗传算法的方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02257</td><td>Garbage In, Garbage Out</td><td>无用数据入、无用数据出</td><td>GIGO</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02258</td><td>Gas-Phase Networks</td><td>气相网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02259</td><td>Gaussian Kernels</td><td>高斯核</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02260</td><td>Gaussian-Type Structure Descriptors</td><td>高斯型结构描述符</td><td>GTSD</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02261</td><td>General Intelligence</td><td>通用智能</td><td>GI</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02262</td><td>Generalized Gradient Approximation</td><td>广义梯度近似</td><td>GGA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02263</td><td>Generative Adversarial Networks</td><td>生成对抗网络</td><td>GAN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02264</td><td>Gradient Boosting Decision Tree</td><td>梯度提升决策树</td><td>GBDT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02265</td><td>Gradient-Based</td><td>基于梯度的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02266</td><td>Grain-Surface Networks</td><td>粒面网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02267</td><td>Graph Convolutional</td><td>图卷积</td><td>GC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02268</td><td>Graph Models</td><td>图模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02269</td><td>Graph Neural Networks</td><td>图神经网络</td><td>GNNS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02270</td><td>Graph-Based</td><td>基于图形</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02271</td><td>Graph-Based Models</td><td>基于图的模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02272</td><td>Graph-Based Neural Networks</td><td>基于图的神经网络</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02273</td><td>Graph-Based Representation</td><td>基于图的表示</td><td>GB-GA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02274</td><td>Graph-Convolutional Neural Network</td><td>图卷积神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02275</td><td>Graphics Processing Units</td><td>图形处理器</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02276</td><td>Gravimetric Polymerization Degree</td><td>比重聚合度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02277</td><td>Hamiltonian Matrix</td><td>哈密顿矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-02278</td><td>Hamiltonian Operator</td><td>哈密顿算符</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="odd"><td>AITD-02279</td><td>Heterogeneous Data</td><td>异构数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02280</td><td>Hidden Layers</td><td>隐藏层</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02281</td><td>High Data Throughput</td><td>高数据吞吐量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02282</td><td>High Throughput</td><td>高通量</td><td>HT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02283</td><td>High Throughput Screening</td><td>高通量筛选</td><td>HTS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02284</td><td>High Variance Models</td><td>高方差模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02285</td><td>High-Dimensional Data</td><td>高维数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02286</td><td>High-Dimensional NN</td><td>高维神经网络</td><td>HDNN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02287</td><td>High-Dimensional Objects</td><td>高维对象</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02288</td><td>High-Throughput</td><td>高通量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02289</td><td>Higher-Dimensional Space</td><td>高维空间</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="even"><td>AITD-02290</td><td>Higher-Dimensional Spectral Space</td><td>高维光谱空间</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02291</td><td>Homogenization</td><td>同质化</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02292</td><td>Homomorphic Encryption</td><td>同态加密</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02293</td><td>Human Face Recognition</td><td>人脸识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02294</td><td>Human-Encoded</td><td>人工编码的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02295</td><td>Hybrid Model</td><td>混合模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02296</td><td>Hybrid Technique</td><td>混合技术</td><td>HM</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02297</td><td>Hybrid-Neural Model</td><td>混合神经模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02298</td><td>Hyperparameter Opimization</td><td>超参数优化</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02299</td><td>Hyperparameters</td><td>超参数</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02300</td><td>Hyperplanes Separate</td><td>超平面分离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02301</td><td>Id3 Algorithm</td><td>Id3 算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02302</td><td>Image And Speech Recognition</td><td>图像和语音识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02303</td><td>Image Classification</td><td>图像分类</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02304</td><td>Image Classifier</td><td>图像分类器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02305</td><td>Image Recognition</td><td>图像识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02306</td><td>Informative Priors</td><td>信息先验</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02307</td><td>Input-Output Pairs</td><td>输入输出对</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02308</td><td>Instance-Based</td><td>基于实例的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02309</td><td>Intelligent Machine</td><td>智能机器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02310</td><td>Intermediate Neurons</td><td>中间神经元</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02311</td><td>Internet Of Things</td><td>物联网</td><td>IoT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02312</td><td>Interpolation Coordinate</td><td>插值坐标</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02313</td><td>Interpretability</td><td>可解释性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02314</td><td>Inverse Neural Modeling</td><td>逆神经建模</td><td>INN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02315</td><td>Inverse Neural Network Modeling</td><td>逆神经网络建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02316</td><td>Iterative Learning</td><td>迭代学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02317</td><td>Joint Distribution</td><td>联合分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02318</td><td>Jordan-Elman Neural Networks</td><td>Jordan-Elman 神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02319</td><td>K Clusters</td><td>K聚类</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02320</td><td>K Nearest Points</td><td>K 最近点</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02321</td><td>K-1 Folds</td><td>K-1 折</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02322</td><td>K-Edge (O-K Edge)</td><td>K-边缘（O-K 边缘）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02323</td><td>K-Means</td><td>K-均值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02324</td><td>Kendall’S Tau</td><td>肯德尔等级相关系数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02325</td><td>Kernel Ridge Regression</td><td>核岭回归</td><td>KRR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02326</td><td>Kernels</td><td>内核</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02327</td><td>Kinetic Curve</td><td>动力学曲线</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02328</td><td>KNN Model</td><td>K 近邻模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02329</td><td>Knowledge Extraction</td><td>知识提取</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02330</td><td>Knowledge Gradient</td><td>知识梯度</td><td>KG</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02331</td><td>L1 And L2 Regularization</td><td>L1与L2正则化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02332</td><td>Laboratory Level</td><td>实验室级别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02333</td><td>Language Processing</td><td>语言处理</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02334</td><td>Laplacian Prior</td><td>拉普拉斯先验</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02335</td><td>Large-Scale Data Storage</td><td>大规模数据存储</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02336</td><td>Lasers</td><td>激光器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02337</td><td>Lasso Regression</td><td>拉索回归</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02338</td><td>LBP</td><td>局部二值模式</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02339</td><td>Least Absolute Shrinkage And Selection Operator</td><td>Lasso回归</td><td>LASSO</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02340</td><td>Least Square Support Vector Machine</td><td>最小二乘支持向量机</td><td>LSSVM</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02341</td><td>Ligand-Field</td><td>配位场</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02342</td><td>Linear</td><td>线性的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-02343</td><td>Linear Dimension Reduction Methods</td><td>线性降维方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02344</td><td>Linear Vibronic Coupling Model</td><td>线性振子耦合模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02345</td><td>Local Recurrent</td><td>本地卷积</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02346</td><td>Logic And Heuristics Applied To Synthetic Analysis</td><td>LHASA 程序</td><td>LHASA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02347</td><td>Long-Range Prediction</td><td>长期预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02348</td><td>Long-Range Prediction Models</td><td>长期预测模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02349</td><td>Long-Term Planning</td><td>长期规划</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02350</td><td>Long-Term Reward</td><td>长期回报</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02351</td><td>Machine-Readable Data</td><td>机器可读的数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02352</td><td>Mae</td><td>平均绝对误差</td><td>MAE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02353</td><td>Mahalanobis Distances</td><td>马氏距离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02354</td><td>Matrices</td><td>矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-02355</td><td>Matthews Correlation Coefficient</td><td>马修斯相关系数</td><td>MCC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02356</td><td>Maximum Likelihood Methods</td><td>最大似然法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02357</td><td>Maximum Likelihood Procedures</td><td>最大似然估计法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02358</td><td>MCTS Method</td><td>蒙特卡洛树搜索方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02359</td><td>Mean-Squared Error</td><td>均方误差</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-02360</td><td>Mechanical Sympathy</td><td>机械同感，软硬件协同编程</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02361</td><td>Merging</td><td>合并</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02362</td><td>Message Passing Neural Networks</td><td>消息传递神经网络</td><td>MPNNS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02363</td><td>Microarray Data</td><td>微阵列数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02364</td><td>Mini Batch</td><td>小批次</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02365</td><td>Mining</td><td>挖掘</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02366</td><td>Mining Out</td><td>挖掘</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02367</td><td>Missing Values</td><td>缺失值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02368</td><td>ML Algorithm</td><td>机器学习算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02369</td><td>ML Modelling</td><td>机器学习建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02370</td><td>ML Potentials</td><td>机器学习势能</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02371</td><td>ML-Driven</td><td>机器学习驱动的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02372</td><td>ML-Driven Optimization</td><td>机器学习驱动的最优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02373</td><td>MLP Neural Model</td><td>多层感知机神经模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02374</td><td>Model Construction</td><td>模型构建</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02375</td><td>Model Evaluation</td><td>模型评估</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02376</td><td>Model Performance</td><td>模型性能</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02377</td><td>Model Statistics</td><td>模型统计</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02378</td><td>Model Training</td><td>模型训练</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02379</td><td>Model Validation</td><td>模型验证</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02380</td><td>Model-Based Iterative Reconstruction</td><td>基于模型的迭代重建</td><td>MBIR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02381</td><td>Model-Construction</td><td>模型构建</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02382</td><td>Modelling Scenario</td><td>建模场景</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02383</td><td>Molecular Graph Theory</td><td>分子图论</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02384</td><td>Molecular Modelling</td><td>分子建模</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02385</td><td>Monte Carlo Tree Search</td><td>蒙特卡洛树搜索</td><td>MCTS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[3]</a></td><td>数学</td></tr><tr class="even"><td>AITD-02386</td><td>Moore’S Law</td><td>摩尔定律</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a></td><td>计算机</td></tr><tr class="odd"><td>AITD-02387</td><td>ms-QSBER-EL Model</td><td>基于人工神经网络组合的结构生物学效应定量关系多尺度模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02388</td><td>Multi-Agent Control System</td><td>多智能体控制系统</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02389</td><td>Multi-Core Desktop Computer</td><td>多核台式计算机</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>计算机</td></tr><tr class="even"><td>AITD-02390</td><td>Multi-Dimensional Big Data Analysis</td><td>多维度大数据分析</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02391</td><td>Multi-Layer Feed-Forward</td><td>多层前馈</td><td>MLFF</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02392</td><td>Multi-Objective Genetic Algorithm</td><td>多目标遗传算法</td><td>MOGA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02393</td><td>Multi-Objective Optimization</td><td>多目标优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02394</td><td>Multi-Reaction Synthesis</td><td>多反应合成</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02395</td><td>Multilayer Perceptron</td><td>多层感知机</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02396</td><td>Multivariate Regression</td><td>多变量回归</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02397</td><td>N-Dimensional Space</td><td>N维空间</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02398</td><td>Naive Bayesian</td><td>朴素贝叶斯</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02399</td><td>Naive Bayesian Methods</td><td>朴素贝叶斯方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02400</td><td>Named Entity Recognition，NER</td><td>命名实体识别</td><td>NER</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02401</td><td>Nearest Neighbors</td><td>近邻</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02402</td><td>Nearest Neighbour Model</td><td>近邻模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02403</td><td>Negative Predictive Value</td><td>阴性预测值</td><td>NPV</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02404</td><td>Network Architecture</td><td>网络结构</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02405</td><td>Network Geometry</td><td>网络几何</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02406</td><td>Neural Turing Machines</td><td>神经图灵机</td><td>NTM</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02407</td><td>Neural-Network-Based Function</td><td>基于神经网络的函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02408</td><td>Neurons</td><td>神经元</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02409</td><td>Nuclear Magnetic Resonance</td><td>核磁共振</td><td>NMR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02410</td><td>Noise Filters</td><td>噪声过滤器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02411</td><td>Noise-Free</td><td>无噪的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02412</td><td>Non-Linear</td><td>非线性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>数学、统计</td></tr><tr class="odd"><td>AITD-02413</td><td>Non-Linear Correlation</td><td>非线性相关</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02414</td><td>Non-Linearity</td><td>非线性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02415</td><td>Non-Parametric Algorithm</td><td>非参数化学习算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02416</td><td>Non-Safety-Critical Applications</td><td>非安全关键型应用</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02417</td><td>Non-Steady-State</td><td>非稳态</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02418</td><td>Non-Stochastic</td><td>非随机的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02419</td><td>Non-Template</td><td>非模板</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02420</td><td>Non-Template Methods</td><td>非模板方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02421</td><td>Non-Zero Weight</td><td>非零权重</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02422</td><td>On-The-Fly Optimization</td><td>运行中优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>计算机</td></tr><tr class="odd"><td>AITD-02423</td><td>One-Hot Vector</td><td>独热向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>整个矢量中之后一个数为1 其余为0</td></tr><tr class="even"><td>AITD-02424</td><td>Open-Source</td><td>开源</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>软件工程</td></tr><tr class="odd"><td>AITD-02425</td><td>Open-Source Dataset</td><td>开源数据集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02426</td><td>Predicted Label</td><td>预测值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02427</td><td>Prediction</td><td>预测</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02428</td><td>Prediction Accuracy</td><td>预测准确率</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02429</td><td>Predictor</td><td>预测器/决策函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02430</td><td>Protein Folding</td><td>蛋白折叠</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a></td><td>生物</td></tr><tr class="odd"><td>AITD-02431</td><td>Quantum Chemistry</td><td>量子化学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="even"><td>AITD-02432</td><td>Quantum Theory</td><td>量子理论</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="odd"><td>AITD-02433</td><td>Random Selection</td><td>随机选择</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02434</td><td>Raw Datasets</td><td>原始数据集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02435</td><td>Root Mean Square Errors</td><td>均方根</td><td>RMSE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02436</td><td>Scaling</td><td>缩放</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>图像处理</td></tr><tr class="odd"><td>AITD-02437</td><td>Simulation</td><td>仿真</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02438</td><td>The Global Minimum</td><td>全局最小值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02439</td><td>Turing Test</td><td>图灵测试</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>AI，CS</td></tr><tr class="even"><td>AITD-02440</td><td>Version Control</td><td>版本控制</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02441</td><td>Workflow</td><td>工作流</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02442</td><td>Sequence-Function</td><td>序列-功能</td><td></td><td>[1]</td><td></td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>人工智能</tag>
      
      <tag>专业名词</tag>
      
      <tag>基础概念</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>XGBoost参数调优笔记</title>
    <link href="/2022/09/01/%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/09/01/%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>在机器学习中，参数调优是一门玄学，因为模型的最优参数可能依赖于许多场景。因此，不可能为参数调优创建一个全面的指南。本文尝试为XGBoost中的参数提供一些指导。</p><span id="more"></span><h3id="理解偏差-方差权衡bias-variance-tradeoff">1.理解偏差-方差权衡(Bias-VarianceTradeoff)</h3><p>如果你参加过机器学习或统计学课程，这可能是最重要的概念之一。当我们允许模型变得更加复杂（例如，增加深度）时，模型具有更好的拟合训练数据的能力，从而获得偏差较小的模型。然而，这种复杂的模型需要更多的数据来进行拟合。</p><p>XGBoost中的大多数参数都涉及偏差和方差的权衡。最好的模型应该在模型复杂度和预测能力之间进行仔细权衡。参数文档会告诉你每个参数是否会使模型更保守。这可以帮助你在复杂模型和简单模型之间进行调整。</p><h3 id="控制过拟合">2.控制过拟合</h3><p>当你观察到训练准确率高但测试准确率低时，很可能遇到了过拟合问题。</p><p>在XGBoost中，一般有两种方法可以控制过拟合：</p><blockquote><ul><li><p>第一种方法是直接控制模型复杂度。</p><p><code>这包括max_depth、min_child_weight和gamma。</code></p></li><li><p>第二种方法是增加随机性，使训练对噪声具有鲁棒性。</p></li></ul><p>​ <code>这包括subsample和colsample_bytree。</code></p></blockquote><p>你还可以减少步长eta。记住在这样做时增加num_round。</p><h3 id="处理数据集不平衡">3.处理数据集不平衡</h3><p>对于诸如广告点击日志等常见情况，数据集极度不平衡。这会影响XGBoost模型的训练，有两种方法可以改进。</p><blockquote><p>如果你只关心预测的整体性能指标（AUC）</p><ul><li><p>通过scale_pos_weight平衡正负样本的权重</p></li><li><p>使用AUC作为评估标准</p></li></ul><p>如果你关心预测的正确概率</p><ul><li><p>在这种情况下，你不能重新平衡数据集</p></li><li><p>将参数max_delta_step设置为一个有限的数值（例如1）以帮助收敛</p></li></ul></blockquote><h3 id="减少内存使用">4.减少内存使用</h3><p>如果你使用类似sklearn.model_selection.GridSearchCV的HPO库，请控制它可以使用的线程数。最好让XGBoost并行运行，而不是让GridSearchCV同时运行多个实验。例如，为交叉验证创建一个数据折叠可以消耗大量内存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这会创建数据集的副本。X和X_train同时存在于内存中。</span><br><span class="hljs-comment"># 如果你在n_jobs大于1的情况下运行`GridSearchCV`，每个线程都会同时发生这种情况。</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y)<br><br>df = pd.DataFrame()<br><span class="hljs-comment"># 这会创建数据框的新副本，即使你指定了inplace参数</span><br>new_df = df.drop(...)<br><br>array = np.array(...)<br><span class="hljs-comment"># 这可能会也可能不会复制数据，具体取决于数据类型</span><br>array.astype(np.float32)<br><br><span class="hljs-comment"># np默认使用双精度，你真的需要吗？</span><br>array = np.array(...)<br></code></pre></td></tr></table></figure><p>你可以在文档中找到一些更具体的减少内存使用的实践。例如：与Dask的分布式XGBoost、XGBoostGPU支持。然而，在深入研究这些之前，要意识到数据副本的创建是一个好的起点。它通常消耗的内存比人们预期的要多得多</p>]]></content>
    
    
    <categories>
      
      <category>参数调优</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>回归模型</tag>
      
      <tag>参数调优</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【原创小诗】完整</title>
    <link href="/2022/07/22/%E3%80%90%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F%E3%80%91-%20%E5%AE%8C%E6%95%B4/"/>
    <url>/2022/07/22/%E3%80%90%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F%E3%80%91-%20%E5%AE%8C%E6%95%B4/</url>
    
    <content type="html"><![CDATA[<center><p>我们本是完整的一个，但是分开了<br></p><p>于是开始有了期待，有了思念 <br></p><p>笑声很轻，眼泪也无声 <br></p><p>对天空诉说着爱是永恒 <br></p><p>跌跌撞撞寻找属于我们的完整 <br></p><p>未来的那一天在心里很重 <br></p><p>两只手里藏着全世界的星星 <br></p><p>在彼此的眼睛里看到了 <br></p><p>海洋陆地，银河苍穹 <br></p><p>北极光在脚下划过 <br></p><p>猎户座的虹手中缤纷 <br></p><p>你说那斑驳的星尘好像我们 <br></p><p>后来所有的时间空间都消失了 <br></p><p>拥抱里我们变成宇宙的最初 <br></p><p>完整 <br></p></center>]]></content>
    
    
    <categories>
      
      <category>生活感悟</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活感悟</tag>
      
      <tag>原创诗歌</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL数据库基础知识</title>
    <link href="/2022/04/15/SQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <url>/2022/04/15/SQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<p>数据库操作在日常工作非常常见，以下知识点你都掌握了吗？<span id="more"></span></p><h2 id="内容大纲">内容大纲</h2><ul><li>SQL的相关概述</li><li>环境搭建</li><li>SQL语句分类<ul><li>DDL</li><li>DML</li><li>DCL</li><li>DQL</li></ul></li><li>DDL语句之操作数据库</li><li>DDL语句之操作数据表</li><li>DML语句之操作表数据(增删改)</li><li>DQL语句之操作表数据(查)</li></ul><hr /><h2 id="sql概念">1.SQL概念</h2><p>结构化查询语言(Structured QueryLanguage)简称SQL，是<strong>关系型数据库</strong>管理系统都需要遵循的规范，是数据库认识的语句。不同的数据库生产厂商都支持SQL语句，但都有自己特有内容.</p><h3 id="数据库概念">1.1数据库概念</h3><p>数据库就是存储数据的仓库，其本质是一个文件系统，按照特定的格式将数据存储起来，用户可以对数据库中的数据进行增加，修改，删除及查询(<strong>CURD)</strong>操作。</p><ul><li><p>C: Create, 增</p></li><li><p>U: Update, 改</p></li><li><p>R: Read, 查</p></li><li><p>D: Delete, 删</p></li></ul><h3id="关系型数据库与非关系型数据库">1.2关系型数据库与非关系型数据库</h3><h4 id="关系型数据">关系型数据</h4><p>指采用了<strong>关系模型</strong>来组织数据的数据库。关系模型指的就是<strong>二维表格</strong>模型，而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织。</p><h4 id="非关系型数据">非关系型数据</h4><p>又被称为NoSQL（Not Only SQL)，<strong>意为不仅仅是SQL</strong>，对NoSQL最普遍的定义是“非关联型的”，强调 <strong>Key-Value</strong>的方式存储数据。</p><h3 id="sql常用数据类型">1.3 SQL常用数据类型</h3><p>-- SQL 根据每列值的不同, 数据类型也不同, 常用的如下.</p><ul><li><p>整数: int</p></li><li><p>小数; decimal, float, double</p></li><li><p>字符串: varchar(长度), char(长度)</p></li><li><p>日期: date, datetime</p></li></ul><h2 id="mysql基础语法">2.MySql基础语法</h2><ul><li><p>建议先通过小皮安装MySql数据库,并将mysql.exe的路径添加到path</p></li><li><p>建议通过Pycharm专业版或DataGrip运行MySql相关命令及可视化</p></li></ul><h3 id="sql通用语法">2.1 SQL通用语法</h3><ul><li><ol type="1"><li>SQL语句可以写单行, 也可以写多行, 最后以 分号; 结尾.</li></ol></li><li><ol start="2" type="1"><li>为了阅读方便, 我们可以用 者 空格来隔开SQL语句.</li></ol></li><li><ol start="3" type="1"><li>SQL语句不区分大小写, 为了阅读方便, 建议: 关键字大写, 其它小写.</li></ol></li><li><ol start="4" type="1"><li>SQL的注释写法如下 -- 单行注释 '# 单行注释' /<em> 多行 注释</em>/</li></ol></li></ul><p>-- 5. 我们目前在PyCharm或者DataGrip中写SQL语句, 是选中执行的, 即:不要漏选, 防止出错.</p><h3 id="sql语句分类">2.2 SQL语句分类</h3><ul><li><p><strong>DDL</strong>语句, DataBase Definition Language,数据定义语言</p><blockquote><p>作用对象: <strong>数据库, 数据表, 列的</strong>, 进行: CURD.</p><p><strong>关键字: create, drop, alter, show</strong></p></blockquote></li><li><p><strong>DML</strong>语句, DataBase Manipulation Language,数据操作语言.</p><blockquote><p>作用对象: <strong>表数据的, 进行: 增删改操作</strong>, 统称为:<strong>更新语句</strong></p><p><strong>关键字: insert, delete, update</strong></p></blockquote></li><li><p><strong>DQL</strong>语句, DataBase Query Language,数据查询语言.</p><blockquote><p>作用对象: <strong>表数据的, 进行: 查询操作</strong>.</p><p><strong>关键字: select, from, where...</strong></p></blockquote></li><li><p><strong>DCL</strong>语句, DataBase Control Language,数据控制语言.</p><blockquote><p>作用对象: 设置权限, 访问级别(隔离级别), 创建用户等的...</p></blockquote></li></ul><h2 id="ddl语句">3.DDL语句</h2><h3 id="ddl操作数据库">3.1DDL操作数据库</h3><ul><li><ol type="1"><li><strong>查看</strong>所有的<strong>数据库</strong>.<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">show databases;         # ctrl + 回车, 执行该行代.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li><strong>创建</strong>数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">create database day01 character set &#x27;utf8&#x27;;                 # 创建day01数据库, 采用: utf8 码表.  库不存在就创建, 存在就: 报错.<br>create database if not exists day01 character set &#x27;utf8&#x27;;   # 创建day01数据库, 采用: utf8 码表.  库不存在就创建, 存在就: 啥也不做.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">上述格式, 语法糖1: character <span class="hljs-built_in">set</span> =&gt; 可以写成 charset</span><br>create database day02 charset &#x27;utf8&#x27;;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li><strong>查看</strong>对象数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">show create database day01;     # utf8<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li><strong>修改</strong>数据库码表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">alter database day03 charset =&#x27;gbk&#x27;;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="5" type="1"><li><strong>删除</strong>数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">drop database day01;            # 删除数据库, 如果数据库存在就删除, 不存在就: 报错.<br>drop database if exists day01;  # 删除数据库, 如果数据库存在就删除, 不存在就: 啥也不做.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="6" type="1"><li><strong>应用</strong>数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">use day01; #之后: 建表, 查表, 查数据等操作, 都是基于数据库完成的.<br></code></pre></td></tr></table></figure> ### 3.2DDL操作数据表</li></ol></li><li><ol type="1"><li>查看当前库中, 所有的数据表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">show tables;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>查看表结构. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">show create table student;      # 查看建表的详细过程.<br>describe student;               # 语法糖,  desc student;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li>创建数据表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">create table if not exists student(<br>    sid int primary key,        # 学生id, primary key: 主键约束, 特点为: 唯一, 非空.<br>    name varchar(20) not null,  # 学生姓名, 非空约束(即: 不能为空)<br>    gender varchar(2),          # 学生性别<br>    age int                     # 学生年龄, 整数.<br>);<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li>删除数据表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">drop table if exists student;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="5" type="1"><li>修改表(名字) <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式: rename table 旧表名 to 新表名;</span><br>rename table student to stu;<br></code></pre></td></tr></table></figure> ### 3.3 DDL操作列</li></ol></li><li><ol type="1"><li>查看表结构. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">desc stu;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>给表新增一列, desc varchar(200), 非空约束. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">alter table stu add `desc` varchar(200) not null;       # 如果列名和关键字重名, 记得用 反引号包裹.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li>修改表的字段(列), 只修改: 数据类型, 约束. 将desc列改为: int类型..<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">alter table stu modify `desc` int;      # 因为没有加非空约束, 所以本次会认为, 不要非空约束了, 即: 会删除它.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li>修改表的字段(列), 修改: 列名, 数据类型, 约束. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">-- 格式: alter table 表名 change 旧列名 新列名 数据类型 约束;<br>alter table stu change `desc` address varchar(10) not null;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="5" type="1"><li>删除表的字段 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式: alter table 表名 drop 旧列名;</span><br>alter table stu drop address;<br></code></pre></td></tr></table></figure> ## 4 DML语句 ### 4.1添加数据</li></ol></li><li><ol type="1"><li>查看表数据, 这个数据DQL语句, 先用一下, 稍后详解. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">select * from stu;<br><span class="hljs-meta prompt_"># </span><span class="language-bash">查看表结构.</span><br>desc stu;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>添加表数据. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">场景1: 添加单条数据, 格式为: insert into 表名(列名1, 列名2, 列名3...) values(值1, 值2, 值3...);</span><br>insert into stu(sid, name, gender, age) values (1, &#x27;乔峰&#x27;, null, 38);<br><br>insert into stu(sid, name, gender, age) values (2, null, null, 38);     # 报错, name列有非空约束, 不能为null<br>场景2:添加多条数据, 格式为: insert into 表名(列名1, 列名2, 列名3...) values(值1, 值2, 值3...), (...), (...);<br>insert into stu(sid, name, gender, age)<br>values<br>    (2, &#x27;虚竹&#x27;, null, 26),<br>    (3, &#x27;段誉&#x27;, &#x27;男&#x27;, 21),<br>    (4, &#x27;阿朱&#x27;, &#x27;女&#x27;, 35),<br>    (5, &#x27;梦姑&#x27;, &#x27;女&#x27;, 23),<br>    (6, &#x27;钟灵儿&#x27;, &#x27;女&#x27;, 19);<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li>上述格式的变形版. 不一定非得是全列名, 只要值的个数, 类型 和列名的个数, 类型保持一致即可. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">insert into stu(sid, name) values (7, &#x27;木婉清&#x27;);<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li>上述格式的语法糖, 掌握, 实际开发一般是用这个.. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">insert into stu values (8, &#x27;鸠摩智&#x27;, &#x27;男&#x27;, 49);      # 如果不写列名, 则默认是: 全列名, 需要给每一个列都要传入值.<br></code></pre></td></tr></table></figure> ###4.2DML修改数据</li></ol></li><li><ol type="1"><li>修改 sid为3的数据, 姓名为: 段氏小王子, 渣男 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">update stu set name=&#x27;段氏小王子&#x27;, gender=&#x27;渣男&#x27; where sid = 3;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>危险操作, 修改数据时, 没有写 where条件,则会一次性修改表中所有的数据. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">update stu set name=&#x27;段氏小王子&#x27;, gender=&#x27;渣男&#x27;;<br></code></pre></td></tr></table></figure> ### 4.3DML删除数据</li></ol></li><li><ol type="1"><li>正常删除数据, 删除id &gt; 3的数据.(主键ID不变) <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">delete from stu where sid &gt; 3;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>删除数据, 删除id &gt; 3的数据.(主键ID改变) <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">truncate table stu where sid &gt; 3;<br></code></pre></td></tr></table></figure> ##5.备份表数据</li></ol></li><li><ol type="1"><li>场景1: 备份表不存在. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">create table stu_tmp select * from stu;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>场景1: 备份表存在. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">insert into hero_tmp select * from hero;<br></code></pre></td></tr></table></figure></li></ol></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL之DQL详解</title>
    <link href="/2022/03/18/SQL%E4%B9%8BDQL%E8%AF%A6%E8%A7%A3/"/>
    <url>/2022/03/18/SQL%E4%B9%8BDQL%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>SQL数据库中的查询语句整理 <span id="more"></span></p><img src="/images/00194-3462269573.jpeg" /><table><thead><tr class="header"><th><h3 id="基础查询">基础查询</h3></th><th></th></tr></thead><tbody><tr class="odd"><td>Select * from 数据表;</td><td>查看所有数据</td></tr><tr class="even"><td>Select 字段1,字段2 from 数据表;</td><td>查看字段</td></tr><tr class="odd"><td>select 地段名 as 字段别名 from 数据表;</td><td>别名查询</td></tr><tr class="even"><td>select distinct 地段名 from 数据表;</td><td>去重查询</td></tr><tr class="odd"><td><h3 id="运算符筛选">运算符筛选</h3></td><td></td></tr><tr class="even"><td>select * from 表名 where 字段名 = '字段值';</td><td>筛选某字段值的行信息</td></tr><tr class="odd"><td>select * from 表名 where 字段名 != '字段值';</td><td>筛选不含某字段值的行信息</td></tr><tr class="even"><td>select * from 表名 where 字段名 &gt; 字段值;</td><td>筛选显示大于字段值的行</td></tr><tr class="odd"><td>select * from 表名 where 字段名 in (a,b);</td><td>筛选显示包含a,b值的行</td></tr><tr class="even"><td>select * from 表名 where 字段名 <strong>between</strong> 20<strong>and</strong> 80;</td><td>筛选显示介于20 - 80之间的行</td></tr><tr class="odd"><td>select * from 表名 where 字段名=a <strong>and</strong> 字段名&gt;b;</td><td>交集条件</td></tr><tr class="even"><td>select * from 表名 where 字段名=a <strong>or</strong> 字段名&gt;b;</td><td>并集条件</td></tr><tr class="odd"><td>select * from 表名 where 字段名 like '_值%';</td><td>近似查询,_占位符,%任意字符</td></tr><tr class="even"><td>select * from 表名 where 字段名 is null;</td><td>空字段查询</td></tr><tr class="odd"><td>select * from 表名 where 字段名 is not null;</td><td>非空字段查询</td></tr><tr class="even"><td><h3 id="排序">排序</h3></td><td></td></tr><tr class="odd"><td>select * from 表名 order by 字段名 asc | desc;</td><td>排序</td></tr><tr class="even"><td>select * from 表名 order by 字段名1 asc,字段名2 desc</td><td>多重排序</td></tr><tr class="odd"><td><h3 id="聚合与分组">聚合与分组</h3></td><td></td></tr><tr class="even"><td><p>select 聚合函数 from 表名 where 字段名xxx;</p><p>聚合函数为:count(),max(),min(),sum(),avg()</p><p>xxx为筛选条件</p></td><td>相关条件值下的统计值</td></tr><tr class="odd"><td><p></p><p>select</p><p><strong>分组字段</strong>, 聚合函数(<strong>count(*)</strong>)...</p><p>from</p><p>数据表名</p><p>where</p><p>组前筛选</p><p><strong>group by</strong></p><p><strong>分组字段</strong></p><p>having</p><p>组后筛选;</p></td><td><p>分组查询</p><p>一般结合聚合函数一起用, 否则: 无意义</p><p></p><p>where: 组前筛选, 后边不能跟: 聚合函数.</p><p>having: 组后筛选, 后边可以跟: 聚合函数.</p></td></tr><tr class="even"><td><h3 id="分页查询">分页查询</h3></td><td></td></tr><tr class="odd"><td><p>select * from 表名 limit 起始索引, 数据条数;</p><p>注:索引从0开始,从0开始则0可以省略不写</p><p>经验:总页数:=(总条数 + 每页的数据条数 - 1) // 每页的数据条数</p></td><td>分页查询较为常用,有效减少服务器/用户压力</td></tr><tr class="even"><td><h3 id="重分类查询">重分类查询</h3></td><td></td></tr><tr class="odd"><td><p>select case</p><p>when 条件1 then 重命名值</p><p>when 条件2 then 重命名值2</p><p>.....</p><p>else 重命名值3</p><p>end as 字段名,</p><p>from 数据表</p></td><td></td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>查询语句</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo-fluid数学公式显示问题处理</title>
    <link href="/2022/03/15/hexo_fluid%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/"/>
    <url>/2022/03/15/hexo_fluid%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>在此补充一下之前公式不显示的问题。</p><span id="more"></span><p>虽然<ahref="https://hexo.fluid-dev.com/docs/">Fluid</a>主题支持<strong>LaTeX数学公式</strong>，但是需要手动操作，而且我按照<ahref="https://hexo.fluid-dev.com/docs/guide/#latex-数学公式">教程</a>开启本功能<code>mathjax</code>没有成功，即公式在网页里并没有被渲染和转换。通过网上查找，发现解决这类问题的思路主要是换渲染引擎，例如<code>pandoc</code>、<code>mathjax</code>、<code>katex</code>。我目前使用<code>mathjax</code>，操作如下：</p><ul><li><p><strong>卸载</strong>默认引擎，并<strong>安装</strong>这个新的渲染引擎</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ada">$ npm uninstall hexo-renderer-marked <span class="hljs-comment">--save </span><br>$ npm install hexo-renderer-kramed <span class="hljs-comment">--saveCopy</span><br></code></pre></td></tr></table></figure></li><li><p>修改<code>/node_modules/hexo-renderer-kramed/lib/renderer.js</code></p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs arcade"><span class="hljs-comment">// Change inline math rule</span><br><span class="hljs-keyword">function</span> <span class="hljs-title function_">formatText</span>(<span class="hljs-params">text</span>) &#123;<br>  <span class="hljs-comment">// Fit kramed&#x27;s rule: $$ + \1 + $$</span><br>  <span class="hljs-comment">// 直接返回text</span><br>  <span class="hljs-comment">// return text.replace(/`\$(.*?)\$`/g, &#x27;$$$$$1$$$$&#x27;);</span><br>  <span class="hljs-keyword">return</span> <span class="hljs-built_in">text</span>;<br>&#125;Copy<br></code></pre></td></tr></table></figure></li><li><p>修改hexo的渲染源码<code>/node_modules/kramed/lib/rules/inline.js</code></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs r"><span class="hljs-operator">/</span><span class="hljs-operator">/</span> 去掉`\\`的额外转义，第<span class="hljs-number">11</span>行，将其修改为<br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> escape<span class="hljs-operator">:</span> <span class="hljs-operator">/</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">\</span>`*&#123;&#125;\[\]()# +\-.!_&gt;])/, <br>escape: /^\\([`<span class="hljs-operator">*</span><span class="hljs-punctuation">&#123;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">)</span><span class="hljs-comment"># +\-.!_&gt;])/,</span><br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> 将em标签对应的符号中，去掉`_`，第<span class="hljs-number">20</span>行，将其修改为<br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> em<span class="hljs-operator">:</span> <span class="hljs-operator">/</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span>b_<span class="hljs-punctuation">(</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">:</span>__<span class="hljs-operator">|</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span>s<span class="hljs-punctuation">\</span>S<span class="hljs-punctuation">]</span><span class="hljs-punctuation">)</span><span class="hljs-operator">+</span><span class="hljs-operator">?</span><span class="hljs-punctuation">)</span>_<span class="hljs-punctuation">\</span>b<span class="hljs-operator">|</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">:</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-operator">|</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span>s<span class="hljs-punctuation">\</span>S<span class="hljs-punctuation">]</span><span class="hljs-punctuation">)</span><span class="hljs-operator">+</span><span class="hljs-operator">?</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">!</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">)</span><span class="hljs-operator">/</span><span class="hljs-punctuation">,</span>    <br>em<span class="hljs-operator">:</span> <span class="hljs-operator">/</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">:</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-operator">|</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span>s<span class="hljs-punctuation">\</span>S<span class="hljs-punctuation">]</span><span class="hljs-punctuation">)</span><span class="hljs-operator">+</span><span class="hljs-operator">?</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">!</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">)</span><span class="hljs-operator">/</span><span class="hljs-punctuation">,</span>Copy<br></code></pre></td></tr></table></figure></li><li><p>停止使用 <code>hexo-math</code>，安装<code>hexo-renderer-mathjax</code></p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-symbol">$</span> npm uninstall hexo-math --save<br><span class="hljs-comment">// 不知道是不是必要的</span><br><span class="hljs-symbol">$</span> npm install hexo-renderer-mathjax --saveCopy<br></code></pre></td></tr></table></figure></li><li><p>更新 <code>Mathjax</code> 的 <code>CDN</code>链接，打开<code>/node_modules/hexo-renderer-mathjax/mathjax.html</code>，在最后一行添加js：</p><ul><li>网上推荐的上面这个，但我使用失败了</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml">// <span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span>Copy<br></code></pre></td></tr></table></figure><ul><li>推荐下面这个，亲测可行，不过偶尔出问题，需要多部署几次就ok</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span>Copy<br></code></pre></td></tr></table></figure><ul><li><strong>更新于2020年6月6日</strong>：如果有人看到这，可以注意下<code>MathJax.js</code>版本已经到3.0.5了，参照mathjax<ahref="https://www.npmjs.com/package/mathjax#installation-and-use">文档</a>，那么现在的上面的一步可以自行修改，如果控制台报错可以到<ahref="https://cdn.jsdelivr.net/npm/mathjax@3/es5/">mathjax CDNfiles</a>下找到合适的js代替</li></ul><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs applescript">&lt;<span class="hljs-keyword">script</span> <span class="hljs-built_in">id</span>=<span class="hljs-string">&quot;MathJax-script&quot;</span> async<br>  src=<span class="hljs-string">&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js&quot;</span>&gt;<br>&lt;/<span class="hljs-keyword">script</span>&gt;Copy<br></code></pre></td></tr></table></figure><ul><li>当然，如果博客的<strong>内部静态</strong>文件<strong>第三方库</strong>包含了mathjax，上面的<code>MathJax.js</code>不用导入都行，导的不对甚至有冲突，虽然不影响公式的显示，但会在控制台报错。</li></ul></li></ul><p>经过<strong><ahref="https://github.com/Ningsir">Ningsir</a></strong>提醒，删除掉hexo-renderer-mathjax就行了，简单省事。</p><ul><li><p>按照<a href="https://hexo.fluid-dev.com/docs/">Fluid</a>的<ahref="https://hexo.fluid-dev.com/docs/guide/#快速开始">快速开始</a>，需要修改<strong>主题配置</strong>，打开<code>/source/_data/fluid_config.yml</code>文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">post:</span><br>  <span class="hljs-attr">math:</span>  <br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span>  <br>    <span class="hljs-attr">specific:</span> <span class="hljs-literal">false</span>   <br>    <span class="hljs-attr">engine:</span> <span class="hljs-string">mathjaxCopy</span><br></code></pre></td></tr></table></figure></li><li><p>在根目录下修改<code>_config.yml</code>，添加</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">mathjax: <span class="hljs-literal">true</span>Copy<br></code></pre></td></tr></table></figure></li><li><p>在<code>Front-matter</code>中打开<code>MathJax</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br>  <span class="hljs-attr">layout:</span> <span class="hljs-string">post</span><br>  <span class="hljs-attr">title:</span> <span class="hljs-string">title</span><br>  <span class="hljs-attr">date:</span> <span class="hljs-string">date</span><br>  <span class="hljs-attr">categories:</span> <br>  <span class="hljs-bullet">-</span> <span class="hljs-string">categories</span><br>  <span class="hljs-attr">tags:</span> <br>  <span class="hljs-bullet">-</span> <span class="hljs-string">tags1</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">tags2</span><br>  <span class="hljs-attr">mathjax:</span> <span class="hljs-literal">true</span><br><span class="hljs-string">---Copy</span><br></code></pre></td></tr></table></figure></li><li><p>显示数学公式 <span class="math display">\[\Sigma({n} ;{p})=\left\{\left(\zeta_{1}, \ldots, \zeta_{r}\right) \in\mathbb{C}^{n_{1}} \times \cdots \times \mathbb{C}^{n_{r}}:\sum_{k=1}^{r}\left\|{\zeta}_{k}\right\|^{2 p_{k}} &lt;1\right\}\]</span></p></li></ul><p>最后如果公式还是乱码可以尝试重启电脑，然后先尝试部署一下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hexo clean&amp;&amp;hexo g&amp;&amp;hexo d&amp;&amp;hexo s<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客维护</tag>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux常用命令整理</title>
    <link href="/2022/03/03/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"/>
    <url>/2022/03/03/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>整理了一下linux常用的一些命令 <span id="more"></span> ## 基本格式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>命令名 [-选项] [参数]# 有些命令要选项和参数, 有些不需要. 这里的[]表示可选项. <br></code></pre></td></tr></table></figure><h2 id="文件目录操作">文件目录操作</h2><h4 id="ls命令">2.ls命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">ls</span>命令, 来源于: list(列表)  即: 查看指定目录下所有的子级(不包括子级的子级)</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>ls [-a -l -h] [Linux的路径]<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">参数解释</span><br>-a显示所有(包括隐藏的) all<br>-l以行的形式展示详细信息 line<br>-h以人性化的方式展示.   human<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">例如:</span> <br>ls# 查看当前目录的子级, 不包括隐藏.<br>ls /# 查看根目录(/)下的内容.<br>ls -a # 查看当前目录的子级, 包括隐藏.<br>ls -l# 以行的方式, 查看当前目录的子级. 简写形式: ll<br>ls -h# 以人性化的方式展示当前目录的内容, 但是: 无效果.<br>ls -lh# 行的方式, 人性化展示当前目录下的内容. 简写形式:  ll -h<br>ls -al# 以行的形式, 展示当前目录下所有子级(包括 隐藏)<br>ls -alh # 以行, 人性化的方式展示当前目录下所有子级(包括 隐藏)<br></code></pre></td></tr></table></figure></p><h4 id="cd命令">3.cd命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">cd</span>命令, 来源于: change directory, 改变目录</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>cd 要切换到的目录的路径<br></code></pre></td></tr></table></figure></p><h4 id="pwd命令">4.pwd命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">来源于 Print Work Directory</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>pwd # 查看当前所在的工作目录,  即: 当前在Linux的哪个路径下. <br></code></pre></td></tr></table></figure></p><h4 id="linux中的路径写法">5.Linux中的路径写法</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">路径介绍</span><br>就是用来描述文件 或者 文件夹(目录)的路径的, 有: 绝对路径 和 相对路径两种写法.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">绝对路径</span><br>以 / 根目录开头.   <br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">相对路径</span><br>默认是相对于当前路径来写的. <br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">例如: 当前目录是在 /aa/bb  如果相切换到 /aa/bb/cc目录, 有如下两种写法.</span><br>绝对路径:   cd /aa/bb/cc<br>相对路径:   cd cc<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">几个特殊的路径写法</span><br>./# 代表当前路径, 例如: 上述的 cd cc 还可以写成 cd ./cc<br>..# 代表上级路径<br>../..# 代表上上级路径<br>~# 代表: 回到家目录, root账号的家 /root,  其它账号的家 /home/账号名<br><span class="hljs-meta prompt_"># </span><span class="language-bash">语法糖, 可以直接写 <span class="hljs-built_in">cd</span> 也是回家命令.</span><br>-# 代表: 在最近的两个目录之间做切换.<br></code></pre></td></tr></table></figure></p><h4 id="mkdir命令">6.mkdir命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">来源于 make directory, 创建目录(文件夹)的.</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>mkdir [-p] 文件夹路径# -p表示parent, 即: 父目录不存在, 也会自动创建.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">简单理解, 假设: 目前只有 /root/aa 文件夹</span><br>mkdir /root/aa/bb/cc# 报错, 因为不写-p, 只能创建单级文件夹.<br>mkdir -p /root/aa/bb/cc# 不报错, 加上-p可以创建多级目录.<br></code></pre></td></tr></table></figure></p><h4 id="文件相关">7.文件相关</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">touch</span>创建文件的.</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>touch 文件路径1 文件路径2...# 可以同时创建多个文件.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">cat</span>查看文件内容的</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>cat文件路径# 一次性查看文件所有内容, 如果内容较多, 会翻页, 只留最后一页.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">more查看文件内容的, 可以分页查看.</span><br>more 文件路径# 以分页的形式查看文件内容.<br><span class="hljs-meta prompt_"># </span><span class="language-bash">空格向下翻一页</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">d  down的意思, 向下翻半页</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">enter 向下翻一行</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">b  back, 向上翻一页.</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">q     quit, 表示退出.  也可以按下 ctrl + 字母C</span><br></code></pre></td></tr></table></figure></p><h2 id="文件和文件夹相关命令">文件和文件夹相关命令</h2><h4 id="cp命令-来源于-copy单词-可以拷贝-文件-文件夹">8.cp命令, 来源于copy单词, 可以拷贝 文件, 文件夹</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"> # </span><span class="language-bash">格式</span><br>cp [-r] 数据源 目的地# -r表示recursive(递归), 即: 拷贝文件夹时, 要写. <br>cp -r /root/aa /root/test<br><br>[root@linxkon ~]# cd /root/<br>[root@linxkon ~]# ls<br>2.avi  3.jpg  4.mp3  aa  anaconda-ks.cfg  a.txt<br>[root@linxkon ~]# mkdir lk<br>[root@linxkon ~]# <br>[root@linxkon ~]# cp a.txt lk# 拷贝<br>[root@linxkon ~]# ls<br>2.avi  3.jpg  4.mp3  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# ls lk/<br>a.txt<br>[root@linxkon ~]# <br>[root@linxkon ~]# cp 2.avi lk/abc.avi# 拷贝, 并改名<br>[root@linxkon ~]# ls lk/<br>abc.avi  a.txt<br>[root@linxkon ~]# cp aa lk# 报错, 拷贝文件夹必须夹-r, 递归拷贝.<br>cp: 略过目录&quot;aa&quot;<br>[root@linxkon ~]# cp -r aa lk# 拷贝文件夹<br>[root@linxkon ~]# ls lk/<br>aa  abc.avi  a.txt<br></code></pre></td></tr></table></figure></p><h4 id="mvmove剪切移动重命名">9.mv（move）剪切移动/重命名</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>mv 数据源 目的地# 注意: 如果是同级路径, 就是改名.<br><br>[root@linxkon ~]# ls<br>2.avi  3.jpg  4.mp3  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# ls lk/<br>aa  abc.avi  a.txt<br>[root@linxkon ~]# <br>[root@linxkon ~]# mv 3.jpg lk/# 剪切文件<br>[root@linxkon ~]# ls lk/<br>3.jpg  aa  abc.avi  a.txt<br>[root@linxkon ~]# ls<br>2.avi  4.mp3  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# <br>[root@linxkon ~]# <br>[root@linxkon ~]# mv 4.mp3 lk/好日子.xyz# 剪切(文件)并改名<br>[root@linxkon ~]# ls<br>2.avi  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# ls lk/<br>3.jpg  aa  abc.avi  a.txt  好日子.xyz<br><br>[root@linxkon ~]# mkdir xyz<br>[root@linxkon ~]# ls<br>2.avi  aa  anaconda-ks.cfg  a.txt  lk  xyz<br>[root@linxkon ~]# mv aa xyz# 剪切文件夹, 无需加: -r<br>[root@linxkon ~]# ls<br>2.avi  anaconda-ks.cfg  a.txt  lk  xyz<br>[root@linxkon ~]# ls xyz/<br>aa<br><br><br>[root@linxkon ~]# ls<br>[root@linxkon ~]# touch 1.txt<br>[root@linxkon ~]# <br>[root@linxkon ~]# mv 1.txt abc.txt# 改名操作<br>[root@linxkon ~]# ls<br>abc.txt<br></code></pre></td></tr></table></figure><h4 id="rm命令-来源于-remove单词-可以删除-文件-文件夹">10.rm命令, 来源于remove单词, 可以删除 文件, 文件夹</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">rm [-r -f] 要删除的文件或者文件夹路径# -r:递归,  -f: force(强制)<br><br>[root@linxkon ~]# rm -rf lk# 强制删除 lk文件夹, 且不询问<br>[root@linxkon ~]# ls<br>anaconda-ks.cfg  xyz<br>[root@linxkon ~]# touch 1.txt 2.txt 3.avi 4.avi 5.jpg<br>[root@linxkon ~]# ls<br>1.txt  2.txt  3.avi  4.avi  5.jpg  anaconda-ks.cfg  xyz<br>[root@linxkon ~]# rm -rf *.txt<br>[root@linxkon ~]# ls<br>3.avi  4.avi  5.jpg  anaconda-ks.cfg  xyz<br>[root@linxkon ~]# rm -rf *# 清空当前文件夹<br>[root@linxkon ~]# ls<br>[root@linxkon ~]# rm -rf /*  ^C# 慎用<br></code></pre></td></tr></table></figure><h4 id="一个坐牢命令">11.一个坐牢命令</h4><figure class="highlight shell"><figcaption><span>rm -rf</span><a href="/*">link</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">rm -rf /* #删除系统<br></code></pre></td></tr></table></figure><h2 id="查找命令">查找命令</h2><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">### 12.which命令,  查找Linux可执行命令 的路径的.</span></span> <br>  which ls# /usr/bin/ls<br>  which pwd# /usr/bin/pwd<br>  <br>  which ifconfig# /usr/sbin/ifconfig<br>  <br><span class="hljs-meta prompt_">  </span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">### 13.find命令, 根据文件名, 或者 文件大小查找指定文件.</span></span><br><span class="hljs-meta prompt_">  # </span><span class="language-bash">格式</span><br>  find 要被检索的目录路径 -name &#x27;要检索的文件名&#x27;<br>  <br>  find / -name &#x27;abc*&#x27;# 查找Linux中, 以abc开头的内容.<br>  <br><span class="hljs-meta prompt_">  # </span><span class="language-bash">格式</span><br>  find 要被检索的目录路径 -size +100M# 超过100MB,  -10K, 小于10KB<br>  <br>  find / -size +100M# 查找Linux中, 文件大小超过100M的文件.<br></code></pre></td></tr></table></figure></p><p>—————————————————华丽的分割线—————————————————<img src="/images/2024年5月10日genk.jpg" title="毕加索" alt="dolor"></p>]]></content>
    
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>基础命令</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo博客搭建教程</title>
    <link href="/2022/02/21/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"/>
    <url>/2022/02/21/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<p>hexo + github 搭建你的静态博客 <span id="more"></span> #一，搭建前的软件准备（git，node）</p><blockquote><p>搭建之前需要准备的软件： Git：官网下载：https://git-scm.com/ Node.js官网下载：http://nodejs.cn/</p></blockquote><h1 id="二-安装hexo完成简单本地页面展示">二，安装hexo，完成简单本地页面展示</h1><p>1.进入cmd窗口输入指令：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm">npm install -g hexo-<span class="hljs-keyword">cli</span><br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/d6d9c791b8f449fea0b64b1a72bd32b2.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>2.你可以先创建一个文件夹myblog，然后cd到这个文件夹下（或者在这个文件夹下直接右键gitbash打开）。 <imgsrc="/images/hexo博客搭建教程/5b5554d54f20471098768040624585ba.png"alt="在这里插入图片描述" /></p><p>接下来初始化一下hexo</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs csharp">hexo <span class="hljs-keyword">init</span><br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/6010660448004f25871ce6b5a479f832.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>3.查看是否能启动成功</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clike">hexo s<br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/4df56b5dc3ce40a2a2c9dc99585fd8ed.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><blockquote><p>新建完成后，指定文件夹目录下有： node_modules: 依赖包public：存放生成的页面 scaffolds：生成文章的一些模板source：用来存放你的文章 themes：主题 **_config.yml:博客的配置文件**</p></blockquote><p>4.复制网址打开</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clike">http://localhost:4000/<br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/2bcb9a6a09024ce68d095901dc1ca203.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>这是初始界面，我们需要部署到github上。</p><p>ctrl+C可以停止；</p><h1 id="三将hexo部署到github">三，将Hexo部署到Github</h1><h2 id="github创建个人仓库">1.Github创建个人仓库</h2><blockquote><p>首先，需要有一个github账号。登上账号后建一个仓库：仓库名为你的用户名.github.io，举例如下： 创建一个和你用户名相同的仓库，后面加.github.io，只有这样，将来要部署到GitHub的时候，才会被识别，也就是xxxx.github.io，其中xxx就是你注册GitHub的用户名.</p></blockquote><figure><img src="/images/hexo博客搭建教程/a697d02a363e48e08d07854051642860.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h2 id="生成ssh添加到github">2.生成ssh添加到Github</h2><blockquote><p>在Github上创建仓库完成之后，需要设置ssh免密登录</p></blockquote><p>1.打开cmd窗口：执行如下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clike">git config --global user.name &quot;yourname&quot;<br>git config --global user.email &quot;youremail&quot;<br></code></pre></td></tr></table></figure><p>这里的yourname输入你的GitHub用户名，youremail输入你GitHub的邮箱。这样GitHub才能知道你是不是对应它的账户。用户名为仓库的名称，邮箱为注册github的邮箱，举例如下：</p><figure><img src="/images/hexo博客搭建教程/ef3ce50ba8ce4e39bb4dd1387a9da316.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>防止输错可以检查：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clike">git config user.name<br>git config user.email<br></code></pre></td></tr></table></figure><p>2.接着进入到家目录：C:，右击打开git bash 。 输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clike">ssh-keygen -t rsa -C 2412757158@qq.com<br></code></pre></td></tr></table></figure><p>后面是自己注册github的邮箱，然后敲三次回车，</p><figure><img src="/images/hexo博客搭建教程/b07cadba4a484a7eac9c19884ea6f3b5.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>3.接着就会发现C:.ssh目录，打开后有一个公钥，一个私钥。id_rsa.pub是公钥，我们需要打开它，复制里面的内容。</p><p>然后进入github：</p><p>点击setings <imgsrc="/images/hexo博客搭建教程/2f3217c541b94d59bc17c3d8119e8801.png"alt="在这里插入图片描述" /></p><p>进行以下操作</p><p><imgsrc="/images/hexo博客搭建教程/a1def242038c4c77b125d0c0b597f987.png"alt="在这里插入图片描述" />发现我们需要一个密钥，把我们刚刚复制的密钥粘进去，title随便起</p><p><imgsrc="/images/hexo博客搭建教程/821106b4621d4a1a91cfc2f1510abd99.png"alt="在这里插入图片描述" /> 点击 Add SSH Key</p><h2 id="进行部署">3.进行部署</h2><blockquote><p>这一步，我们就可以将hexo和GitHub关联起来，也就是将hexo生成的文章部署到GitHub上，打开站点配置文件_config.yml，翻到最后，修改为 YourgithubName就是你的GitHub账户</p></blockquote><p>1.修改配置文件 <imgsrc="/images/hexo博客搭建教程/c87432a9b49c4552b931c51e0e94e61d.png"alt="在这里插入图片描述" /></p><p>修改内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs clike">deploy:<br>  type: git<br>  repo: git@github.com:goubin18/goubin18.github.io.git<br>  branch: main<br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/7955e295748647388285871fcf65b511.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p><strong>注意：后面有空格</strong></p><p><strong>repo：获取步骤如下</strong></p><p><strong>点进自己刚刚创建的仓库，复制</strong></p><p><strong><imgsrc="/images/hexo博客搭建教程/a8b5f30ed44448b88f759faf8f104ecb.png"alt="在这里插入图片描述" /></strong></p><p><strong>2.找到自己的博客路径打开</strong></p><p><strong><imgsrc="/images/hexo博客搭建教程/fa09bd6a0d7448deb1ce16a424e0c987.png"alt="在这里插入图片描述" /></strong></p><p><strong>这个时候需要先安装deploy-git，也就是部署的命令,这样你才能用命令部署到GitHub。</strong></p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">npm install hexo-deployer-git <span class="hljs-comment">--save</span><br></code></pre></td></tr></table></figure><p><strong><imgsrc="/images/hexo博客搭建教程/a4ff5a3aed8443d6b98b366fa63e724d.png"alt="在这里插入图片描述" /></strong></p><p><strong>2.然后依次执行以下命令：</strong></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs verilog">hexo c   #清除缓存文件 db<span class="hljs-variable">.json</span> 和已生成的静态文件 public<br>hexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo <span class="hljs-keyword">generate</span> 的缩写)<br>hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)<br></code></pre></td></tr></table></figure><p><strong>注意deploy时会让输个yes</strong></p><p><strong><em>*最后回到github上查看自己的仓库：*</em></strong></p><p><strong><em>*<imgsrc="/images/hexo博客搭建教程/5a62c4630f164385831ad449065b5b03.png"alt="在这里插入图片描述" /> 这就表示上传成功。*</em></strong></p><p>**<em>*</em>*现在就可以使用xxx.github.io来访问你的博客啦例如：我的用户名是goubin18，那么我的博客地址就是<code>goubin18.github.io</code>******</p><p>**<em>*</em>*举例如下：******</p><p>**<em>*</em>*<imgsrc="/images/hexo博客搭建教程/9eefc08e36464040bc8fbe8d9716073b.png"alt="在这里插入图片描述" />******</p><h1 id="写在最后">写在最后：</h1><blockquote><p>**<em>*</em>*现在简单的博客已经搭建完成了 现在你的个人网站的地址是xxx.github.io，如果觉得这个网址配不上帅气多金的你，你就可以设置个人域名了。但是需要花钱。小提示：操作要细心，如果出现了问题可以私信留言，大家一起想办法！******</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客搭建</tag>
      
      <tag>知识管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>集成学习常见模型比对</title>
    <link href="/2021/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A6%81%E7%82%B9%E5%AF%B9%E6%AF%94/"/>
    <url>/2021/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A6%81%E7%82%B9%E5%AF%B9%E6%AF%94/</url>
    
    <content type="html"><![CDATA[<p>集成学习的基础思想是通过组合多个基学习器形成整体强学习器,使基学习器在预测准确性、降低过拟合风险、增强模型的鲁棒性等方面获得明显提升,集成学习主要包含Bagging和Boosting两大分类。本文对比总结了四种集成学习常见模型。</p><span id="more"></span><p>Bagging是一种并行式的集成学习方法，其特点包括通过有放回的抽样产生不同的训练集，从而训练多个不同的学习器，并通过平均投票或多数表决的方式决定预测结果。此外，Bagging允许弱学习器并行训练，代表算法包括随机森林算法。</p><p>Boosting是一种串行式的集成学习方法，其特点是随着学习的积累从弱到强，每加入一个弱学习器，整体能力会得到提升。Boosting对学习器进行加权投票，采用串行方式进行学习，具有明确的先后顺序。代表算法包括Adaboost、GBDT、XGBoost以及LightGBM。</p><p><img src="/images/集成学习常见模型对比/集成学习对比示意.png" /></p><table><thead><tr class="header"><th>模型</th><th>核心要点</th><th>模型优缺点</th><th>模型应用</th></tr></thead><tbody><tr class="odd"><td>Bagging随机森林</td><td>1. 随机有放回的抽样产生不同的训练集(boostrap)<br>2.基于不同抽样训练多个基学习器（如决策树）<br>3.通过投票或平均组合预测结果</td><td>优点:<br>- 泛化错误率低<br>-易于并行训练<br>缺点:<br>-性能上限低</td><td>1. 分类问题<br>2. 回归问题</td></tr><tr class="even"><td>Adaptive Boosting</td><td>1. 迭代构建弱学习器<br>2.聚焦错误样本,每轮根据分类结果调整样本及模型权重<br>3.组合加权弱学习器成强学习器</td><td>优点:<br>- 泛化能力强<br>- 易于处理多种数据<br>缺点:<br>-对离群点敏感<br>- 需要预处理高维或不平衡数据</td><td>1. 分类问题<br>2. 图像识别</td></tr><tr class="odd"><td>GBDT (梯度提升树)</td><td>1. 迭代构建决策树<br>2. 拟合损失函数的负梯度训练新树<br>3.累加方式构建最终模型</td><td>优点:<br>- 准确性高<br>- 可以适应多种损失函数<br>缺点:<br>-容易过拟合<br>- 计算量大</td><td>1. 回归问题<br>2. 排名问题<br>3. 分类问题</td></tr><tr class="even"><td>XGBoost</td><td>1. 基于GBDT的高效实现<br>2. 加入正则化项解决GBDT过拟合问题<br>3.损失函数泰勒二阶近似优化拟合函数<br/>4.支持并行化和缺失值处理</td><td>优点:<br>- 速度快<br>- 准确性高,防止过拟合<br>-支持多种目标函数和评估指标<br>缺点:<br>- 参数调整复杂<br>-可能需要更多的内存</td><td>1. 赢取竞赛的首选算法<br>2. 排名问题<br>3. 分类和回归问题</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记整理</tag>
      
      <tag>集成学习</tag>
      
      <tag>总结归纳</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型调优指南--过拟合</title>
    <link href="/2021/06/21/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97--%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <url>/2021/06/21/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97--%E8%BF%87%E6%8B%9F%E5%90%88/</url>
    
    <content type="html"><![CDATA[<p>过拟合是机器学习模型在训练数据上表现很好，但在新数据上表现不佳的一种现象。它的发生通常是由于模型过于复杂，以至于能够记住训练数据的噪声和细节，而不是学习到数据的普遍模式和特征。以下是导致过拟合的常见原因以及相应的解决方法：<span id="more"></span></p><h3 id="过拟合的原因">过拟合的原因</h3><ol type="1"><li><strong>模型复杂度过高</strong>：模型参数过多（如深层神经网络的层数和节点数过多），导致模型具有很强的表达能力，能够拟合训练数据中的噪声。</li><li><strong>训练数据不足</strong>：训练数据量太少，使得模型只能依赖于有限的数据，容易记住而不是泛化。</li><li><strong>数据噪声</strong>：训练数据中包含大量噪声或异常值，模型在训练时会把这些噪声也当作有效模式来学习。</li><li><strong>训练次数过多</strong>：模型在训练数据上迭代次数过多，导致模型对训练数据的拟合过于精细。</li></ol><h3 id="解决过拟合的方法">解决过拟合的方法</h3><ol type="1"><li><strong>增加训练数据量</strong>：通过收集更多的数据或使用数据增强技术来扩展训练集，可以帮助模型学习到更加普遍的特征。<ul><li><strong>数据增强</strong>：对于图像数据，可以使用翻转、旋转、缩放等技术来生成更多的训练样本。</li></ul></li><li><strong>简化模型</strong>：减少模型的参数数量，选择一个较为简单的模型结构。<ul><li><strong>正则化</strong>：在损失函数中加入正则化项，如L1正则化（Lasso）和L2正则化（Ridge），可以防止模型参数过大，减小模型的复杂度。</li></ul></li><li><strong>使用交叉验证</strong>：将数据集划分为多个子集，进行交叉验证，以确保模型在不同数据子集上的表现一致，帮助发现和防止过拟合。<ul><li><strong>K折交叉验证</strong>：将数据集分成K个子集，每次用K-1个子集训练模型，剩下的一个子集测试，循环K次，综合评估模型表现。</li></ul></li><li><strong>提前停止（EarlyStopping）</strong>：在训练过程中监控验证集的误差，当验证误差不再降低时，停止训练，避免模型在训练集上过度拟合。</li><li><strong>集成方法</strong>：使用多种模型的组合来降低单一模型过拟合的风险。<ul><li><strong>袋装（Bagging）</strong>：如随机森林，通过对数据进行多次采样并训练多个模型，最后进行投票或平均来得到最终结果。</li><li><strong>提升（Boosting）</strong>：如梯度提升决策树（GradientBoosting Decision Trees,GBDT），通过逐步训练多个弱模型，每次针对前一轮模型的错误进行改进。</li></ul></li><li><strong>正则化技术</strong>：<ul><li><strong>Dropout</strong>：在神经网络训练中随机将一部分神经元输出设置为0，以防止模型过于依赖某些特定的路径。</li><li><strong>数据标准化</strong>：对输入数据进行归一化处理，使其均值为0，标准差为1，帮助模型更快收敛，并减少过拟合的可能。</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>AI基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>模型训练</tag>
      
      <tag>笔记整理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性回归知识点梳理</title>
    <link href="/2021/03/11/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86/"/>
    <url>/2021/03/11/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>🏠线性回归可以称得上最经典的回归模型，从房子值多少钱，再到股票价格的涨跌🌈，疾病与各种因素的关联，广告投放的收益等，都使它擅长的领域💼接下来让我们走进线性回归💖</p><span id="more"></span><h3 id="线性回归的概念">1.线性回归的概念</h3><h5id="利用-回归方程函数-对-一个或多个自变量特征值和因变量目标值之间-关系进行建模的一种分析方式.">利用回归方程(函数) 对 一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式.</h5><h5 id="yw1x1w2x2....wnb-wtb">y=w1x1+w2x2+....+w(n)+b= wT+b</h5><ul><li>w=weight</li><li>b=bias wT x为将权重系数转置并与x相乘,矩阵的乘法</li></ul><h5 id="常用于分类与回归问题">常用于分类与回归问题</h5><h3 id="损失函数">2.损失函数</h3><h5 id="loss-functioncost-function目标函数成本函数">loss function/costfunction/目标函数/成本函数</h5><h5 id="最小二乘损失计算">最小二乘损失计算:</h5><p><span class="math display">\[J(w,b)=\sum_{i=0}^m\bigl(h\bigl(x^i\bigr)-y^i\bigr)^2\]</span></p><h5 id="均方误差mse">均方误差MSE</h5><p><span class="math display">\[J(w,b)=\frac1m\sum_{i=0}^m\left(h\left(x^i\right)-y^i\right)^2\]</span></p><h5 id="平均绝对误差mae">平均绝对误差MAE</h5><h5id="注-mse与mae既能在模型训练阶段作为损失函数求解拟合函数最优解又可作为模型评估阶段衡量已有模型误差大小">注:MSE与MAE既能在模型训练阶段作为损失函数求解拟合函数最优解,又可作为模型评估阶段,衡量已有模型误差大小</h5><h3 id="损失函数推导_正规方程法">3.损失函数推导_正规方程法</h3><p><span class="math display">\[J(w) =∥Xw−y∥_2^2\]</span></p><p><span class="math display">\[w=(X^TX)^{-1}*X^Ty\]</span></p><h5 id="优点可以精确求解">优点:可以精确求解</h5><h5 id="缺点计算量极大xt-x的逆不存在时无解">缺点:计算量极大,X^TX的逆不存在时无解</h5><h3 id="梯度与导数">4.梯度与导数</h3><h5id="导数表征了函数在某点处的变化速率.">导数表征了函数在某点处的变化速率.</h5><h5 id="梯度gradient">梯度（gradient）</h5><ul><li>多元函数中，导数不再是一个单一的数值，而是一个向量，因为它涉及到函数对多个自变量的变化率。这个向量被称为梯度（gradient），它表示了函数在某一点上沿着各个自变量方向的变化率。</li></ul><h5 id="梯度的性质">梯度的性质</h5><ul><li>梯度的方向是函数在给定点增长最快的方向。</li><li>梯度的模（长度）是函数在给定点处沿最大增长方向的增长率</li><li>梯度是垂直于等值面的</li><li>对函数求导</li></ul><p><span class="math display">\[f(\theta)=\theta_0x_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\cdots+\theta_nx_n\]</span></p><p><span class="math display">\[\nabla f=\begin{bmatrix}\frac{\partialf}{\partial\theta_0}\\\frac{\partialf}{\partial\theta_1}\\\vdots\\\frac{\partialf}{\partial\theta_n}\end{bmatrix}=\begin{bmatrix}x_0\\x_1\\\vdots\\x_n\end{bmatrix}\]</span></p><h3 id="梯度下降">5.梯度下降</h3><h5 id="梯度下降公式"><strong>梯度下降公式</strong></h5><ul><li>循环迭代求当前点的梯度，更新当前的权重参数</li><li></li></ul><p><span class="math display">\[\theta_{i+1}=\theta_i-\alpha\frac\partial{\partial\theta_i}J(\theta)\]</span> - θ_i:初始位置 α:学习率(步长),一般取值范围0.001 ~ 0.01 ∂/(∂θ_i) J(θ) :损失函数在i处的导数</p><h5 id="梯度下降优化过程"><strong>梯度下降优化过程 </strong></h5><ul><li><p>给定学习率,步长,初始位置</p></li><li><p>计算该点梯度方向并取反</p></li><li><p>向梯度反方向移动</p></li><li><p>重复以上步骤</p></li><li><p>达到收敛条件</p><ul><li>两次差距小于指定的阈值 •</li><li>达到指定的迭代次数</li></ul></li></ul><h5 id="学习率"><strong>学习率</strong></h5><ul><li>步长决定了在梯度下降迭代过程中</li><li>学习率太小，下降的速度会慢</li><li>学习率太大：容易造成错过最低点、产生下降过程中的震荡、甚至梯度爆炸</li></ul><h5 id="推导过程">推导过程</h5><ul><li>已知</li></ul><p><span class="math display">\[h_{(\theta)}=\theta_{1}x_{1}+\theta_{2}x_{2}+\cdots+\theta_{\mathrm{m}}x_{\mathrm{m}}+b\\=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\cdots+\theta_{\mathrm{m}}x_{\mathrm{m}}\]</span> - 损失函数</p><p><span class="math display">\[J_{(\theta)}=\frac1{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2\]</span> - 梯度下降公式</p><p><span class="math display">\[\theta_{i+1}=\theta_i-\alpha\frac\partial{\partial\theta_i}J(\theta)\]</span></p><ul><li>对损失函数求导</li></ul><p><span class="math display">\[J&#39;_{(\theta)}=\frac{\partial\mathrm{J}(\theta)}{\partial\theta}=\frac{2*1}{2m}\sum_{i=1}^{m}(h_{\theta}\big(x^{i}\big)-y^{i})^{2-1}*h_{\theta}\big(x^{i}\big)^{\prime}\]</span> - <strong>带入梯度下降公式</strong></p><p><span class="math display">\[\theta_j=\theta_j-\alpha\frac1m\sum_{i=1}^m(h_\theta(x^i)-y^i)*x_j^i\]</span> - 参数说明</p><p>θ_j:当前损失函数的梯度位置/原函数的特征权重<br />m,n:行数,列数<br />i,j:列索引,行索引<br />x,y:特征向量与目标向量</p><h3 id="常见梯度下降算法">6.常见梯度下降算法</h3><h5 id="全梯度下降算法-fgd">全梯度下降算法 FGD</h5><ul><li>每次迭代使用全样本梯度<br /></li><li>(硬件要求极高,数据量大时无法实现)</li></ul><h5 id="随机梯度下降-sgd">随机梯度下降 SGD</h5><ul><li>每次迭代随机选择并使用一个样本梯度<br /></li><li>(容易受异常值影响)</li></ul><h5id="小批量梯度下降算法-mini-bantch-最常用"><strong>小批量梯度下降算法mini-bantch 最常用√</strong></h5><ul><li>每次迭代随机选择并使用小批量的样本梯度<br /></li><li>(在硬件性能满足的情况下,每批的量应该尽可能大)</li></ul><h5 id="随机平均梯度下降-sag">随机平均梯度下降 SAG</h5><ul><li>每次迭代随机选择并使用一个样本梯度并和以往样本梯度值做平均</li><li>(解决异常值影响问题,但训练初期受异常值影响较大)</li></ul><h3 id="回归问题的评估方法">7.回归问题的评估方法</h3><h5 id="平方误差mse">平方误差MSE</h5><p><span class="math display">\[=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}\]</span></p><h5 id="平均绝对误差mae-1">平均绝对误差MAE</h5><p><span class="math display">\[=\frac{1}{n}\sum_{i=1}^{n}|y_{i}-\hat{y}_{i}|\]</span></p><h5 id="均方根误差">均方根误差</h5><p><span class="math display">\[RMSE=\sqrt{\frac1n\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2}\]</span></p><h3 id="模型拟合">8.模型拟合</h3><h5 id="过拟合"><strong>过拟合</strong></h5><ul><li><p>原因</p><ul><li>模型过于复杂,学习到了过多异常特征</li><li>数据噪声大</li></ul></li><li><p>解决方案</p><ul><li>数据清洗</li><li>正则化</li><li>精简特征维度</li><li>增加数据量</li></ul></li></ul><h5 id="欠拟合"><strong>欠拟合</strong></h5><ul><li><p>原因</p><ul><li>模型复杂度低</li><li>特征选择不当</li><li>数据量不足</li><li>正则化过度</li></ul></li><li><p>解决方案</p><ul><li>添加多项式特征项</li><li>添加其它特征</li><li>增加训练量</li></ul></li></ul><h3 id="正则化">9.正则化</h3><h5id="概念在模型训练时数据中有些特征影响模型复杂度或者某个特征的异常值较多-所以要尽量减少这个特征的影响甚至删除某个特征的影响这就是正则化正则化是添加在损失函数中的特殊项.">概念:在模型训练时，数据中有些特征影响模型复杂度、或者某个特征的异常值较多，所以要尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化。正则化是添加在损失函数中的特殊项.</h5><h5 id="l1正则化"><strong>L1正则化:</strong></h5><p><span class="math display">\[J(w)=\mathrm{MSE}(w)+\alpha\sum_{i=1}^{n}\mid w_{i}\mid\]</span> - • α叫做惩罚系数，该值越大则权重调整的幅度就越大，即：表示对特征权重惩罚力度就越大- L1 正则化会使得权重趋向于 0，甚至等于0，使得某些特征失效，达到特征筛选的目的 - from sklearn.linear_modelimport Lasso</p><h5 id="l2正则化"><strong>L2正则化</strong></h5><ul><li></li></ul><p><span class="math display">\[J(w)=\mathrm{MSE}(w)+\alpha\sum_{i=1}^nw_i^2\]</span> - L2 正则化会使得权重趋向于 0，一般不等于 0 - fromsklearn.linear_model import Ridge - L2的线性回归又称为岭回归</p>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>回归模型</tag>
      
      <tag>笔记整理</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
