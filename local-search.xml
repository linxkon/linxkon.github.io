<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ollama部署常见问题解答</title>
    <link href="/ollama%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97.html"/>
    <url>/ollama%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97.html</url>
    
    <content type="html"><![CDATA[<h2 id="入门篇">入门篇</h2><h3 id="ollama是什么">1 ollama是什么？</h3><p>Ollama 是一个支持在本地运行大语言模型的工具，兼容 Windows、Linux 和MacOS 操作系统。使用 Ollama，您仅需一行命令即可启动模型。</p><h3 id="如何安装">2 如何安装？</h3><ul><li>Windows和MacOS用户，从下面链接下载安装即可：下载地址：https://ollama.com/download</li></ul><p>运行前注意先修改模型下载存放目录（见问题6）</p><p>执行下面命令，会自动下载模型（阿里qwen），并开始运行：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">ollama run qwen<br></code></pre></td></tr></table></figure><h3 id="ollama支持哪些模型">4 ollama支持哪些模型？</h3><p>官方支持的模型，可以在 https://ollama.com/library 上面找到。</p><p>下面列出了部分模型</p><table><thead><tr class="header"><th>模型名称</th><th>概要</th><th>其他</th></tr></thead><tbody><tr class="odd"><td>llama3</td><td>Meta Llama 3: The most capable openly available LLM to date</td><td>8B, 70B, 1.9M Pulls, 68 Tags, Updated 12 days ago</td></tr><tr class="even"><td>codestral</td><td>Codestral is Mistral AI’s first-ever code model designed for codegeneration tasks</td><td>22B, 22K Pulls, 18 Tags, Updated yesterday</td></tr><tr class="odd"><td>phi3</td><td>Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium)state-of-the-art open models by Microsoft</td><td>3B, 14B, 513K Pulls, 70 Tags, Updated 5 days ago</td></tr><tr class="even"><td>aya</td><td>Aya 23, released by Cohere, is a new family of state-of-the-art,multilingual models that support 23 languages</td><td>8B, 35B, 31.5K Pulls, 35 Tags, Updated 10 days ago</td></tr><tr class="odd"><td>mistral</td><td>The 7B model released by Mistral AI, updated to version 0.3</td><td>7B, 1.1M Pulls, 84 Tags, Updated 11 days ago</td></tr><tr class="even"><td>gemma</td><td>Gemma is a family of lightweight, state-of-the-art open models builtby Google DeepMind. Updated to version 1.1</td><td>2B, 7B</td></tr></tbody></table><h2 id="进阶篇">进阶篇</h2><h3 id="模型下载后存放到哪里了">5 模型下载后存放到哪里了？</h3><p>默认情况下，模型下载后存放在下面的位置：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs text">macOS: ~/.ollama/models<br>Linux: /usr/share/ollama/.ollama/models #作为系统服务启动时<br>Linux: /home/&lt;username&gt;/.ollama/models #当前用户启动时<br>Windows: C:\Users\&lt;username&gt;\.ollama\models<br></code></pre></td></tr></table></figure><h3 id="如何修改下载模型的默认存放目录">6如何修改下载模型的默认存放目录？</h3><p><strong>Windows用户</strong></p><ol type="1"><li>设置 OLLAMA_MODELS</li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs text"># 只设置当前用户<br>setx OLLAMA_MODELS &quot;D:\ollama_model&quot; <br># 为所有用户设置<br>setx OLLAMA_MODELS &quot;D:\ollama_model&quot; /M<br></code></pre></td></tr></table></figure><ol type="1"><li>重启终端（setx命令在Windows中设置环境变量时，这个变量的更改只会在新打开的命令提示符窗口或终端会话中生效。）</li><li>重启ollama服务</li></ol><p><strong>Linux一般用户</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs text"># 打开下面文件<br>nano ~/.bashrc<br># 添加设置<br>export OLLAMA_MODELS=&quot;/path/to/ollama_model&quot;<br></code></pre></td></tr></table></figure><ol type="1"><li>重启终端</li><li>重启ollama服务： ollama serve</li></ol><p>或者直接使用： OLLAMA_MODELS="/path/to/ollama_model" ollama serve启动服务</p><p><strong>Liunx root 服务模式</strong>在服务文件中设置环境变量，并且要为新的目录设置ollama用户的读写权限</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs text"># 打开服务文件<br>sudo nano /etc/systemd/system/ollama.service<br># 在文件中Service字段后添加<br>[Service]<br>Environment=&quot;OLLAMA_MODELS=/home/xxx/ollama/models&quot;<br>Environment=&quot;http_proxy=xxxxxx&quot;<br><br># 设置目录访问权限 (根据反馈做了些调整)<br>sudo chown ollama:ollama /home/xxx/ollama<br>sudo chmod u+w /home/xxx/ollama<br>sudo chown ollama:ollama /home/xxx/ollama/models<br>sudo chmod u+w /home/xxx/ollama/models<br><br># 重启服务<br>sudo systemctl daemon-reload<br>sudo systemctl restart ollama.service<br><br># 确认状态<br>sudo systemctl status ollama.service<br></code></pre></td></tr></table></figure><p>注：有小伙伴反馈，修改模型存储目录时，如果只设置/home/xxx/ollama/models 目录的权限会出现 mkdir没有权限的错误。所以，建议对目录 /home/xxx/ollama/models 和/home/xxx/ollama 都为ollama用户赋予读写权限（2024.04.28）。</p><h3 id="ollama下载的模型与huggingface上的模型有什么区别">7ollama下载的模型与huggingface上的模型有什么区别？</h3><p>通常情况下，Qwen 模型的表示方法为 <code>Qwen1.5-4B-Chat</code>。在Ollama 中，<code>Qwen</code> 指代的是与 Hugging Face 上的<code>qwen1_5-4B-Chat-q4_0.gguf</code> 模型相对应的版本，这是一个经过 4位量化处理的模型。</p><p><strong>ollama提供的qwen模型：默认为4B，4bit量化模型，大小为2.3G</strong></p><h3 id="什么是modelfile-它的作用是什么">8 什么是Modelfile？它的作用是什么？</h3><p>在创建自定义模型时，需要一个配置文件来指定模型推理相关的设置。</p><p>这个文件仅在创建自定义模型过程中是必需的。若需修改模型推理的参数，必须重新创建模型，可以通过在<code>modelfile</code> 中调整参数来实现。</p><h3 id="自定义模型如何-create-自定义模型基于gguf格式">9 自定义模型：如何create 自定义模型（基于GGUF格式）？</h3><p>制作自定义模型的过程如下（GGUF格式），以qwen1.5 0.5B模型为例：</p><ol type="1"><li>下载模型 qwen1_5-0_5b-chat-q4_0.gguf</li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">wget https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q4_0.gguf<br></code></pre></td></tr></table></figure><ol type="1"><li>准备modelfile文件</li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs text">#FROM qwen1_5-0_5b-chat-q4_0.gguf<br>FROM ./qwen1_5-0_5b-chat-q4_0.gguf<br><br># set the temperature to 1 [higher is more creative, lower is more coherent]<br>PARAMETER temperature 0.7<br>PARAMETER top_p 0.8<br>PARAMETER repeat_penalty 1.05<br>PARAMETER top_k 20<br><br>TEMPLATE &quot;&quot;&quot;&#123;&#123; if and .First .System &#125;&#125;&lt;|im_start|&gt;system<br>&#123;&#123; .System &#125;&#125;&lt;|im_end|&gt;<br>&#123;&#123; end &#125;&#125;&lt;|im_start|&gt;user<br>&#123;&#123; .Prompt &#125;&#125;&lt;|im_end|&gt;<br>&lt;|im_start|&gt;assistant<br>&#123;&#123; .Response &#125;&#125;&quot;&quot;&quot;<br><br># set the system message<br>SYSTEM &quot;&quot;&quot;<br>You are a helpful assistant.<br>&quot;&quot;&quot;<br></code></pre></td></tr></table></figure><ol type="1"><li>创建模型</li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">ollama create qwen0_5b -f Modelfile<br></code></pre></td></tr></table></figure><h3 id="自定义模型模型创建后去了哪里">10自定义模型：模型创建后去了哪里？</h3><p>模型被创建后，会被存放在以下位置： 模型文本被存储在：<code>/home/&lt;username&gt;/.ollama/models/blobs</code>配置文件位于：<code>/home/&lt;username&gt;/.ollama/models/manifests/registry.ollama.ai/library/qwen0_5b/latest</code></p><h3 id="如何重新修改模型的-temperature-等参数">11 如何重新修改模型的temperature 等参数？</h3><p>模型被创建后，修改temperature等参数，需要重新create模型。</p><p>通过以下命令可以查看一个模型的 modelfile 设置</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs text">$ ollama show --modelfile qwen0_5b<br># Modelfile generated by &quot;ollama show&quot;<br># To build a new Modelfile based on this one, replace the FROM line with:<br># FROM qwen0_5b:latest<br><br>FROM /home/&lt;username&gt;/.ollama/models/blobs/sha256:46a9de8316739892e2721fdc49f8353155e4c1bcfa0b17867cb590d2dfdf1d99<br>TEMPLATE &quot;&quot;&quot;&#123;&#123; if and .First .System &#125;&#125;&lt;|im_start|&gt;system<br>&#123;&#123; .System &#125;&#125;&lt;|im_end|&gt;<br>&#123;&#123; end &#125;&#125;&lt;|im_start|&gt;user<br>&#123;&#123; .Prompt &#125;&#125;&lt;|im_end|&gt;<br>&lt;|im_start|&gt;assistant<br>&#123;&#123; .Response &#125;&#125;&quot;&quot;&quot;<br>SYSTEM &quot;&quot;&quot;<br>You are a helpful assistant.<br>&quot;&quot;&quot;<br>PARAMETER repeat_penalty 1.05<br>PARAMETER temperature 0.7<br>PARAMETER top_k 20<br>PARAMETER top_p 0.8<br></code></pre></td></tr></table></figure><p>可以看到这些数据都被存放到了/home//.ollama/models/blobs/sha256:46a9de8316739892e2721fdc49f8353155e4c1bcfa0b17867cb590d2dfdf1d99文件中（此文件是二进制文件）。通过重新create模型，可以修改里面的参数。</p><h3 id="如何导入-pytorchsafetensors-格式的模型">12 如何导入PyTorch，Safetensors 格式的模型？</h3><p>ollama只支持GGUF格式的模型进行导入。对于pytorch和safetensors的模型，需要转换为gguf格式之后再导入。具体步骤，请参考：https://github.com/ollama/ollama/blob/main/docs/import.md</p><h3 id="是否可以链接webui有什么webui推荐">13是否可以链接WebUI，有什么WebUI推荐？</h3><p>ollama github首页中推荐了多种WebUI和终端访问方法的相关项目。https://github.com/ollama/ollama （Community Integrations部分）</p><p><strong>安装教程：Windows上如何安装Open WebUI</strong> B站：</p><iframe width="560" height="315" src="https://player.bilibili.com/player.html?bvid=BV1Ex421Q723&amp;page=1&amp;autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="box-sizing: border-box; color: rgb(53, 55, 64); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Segoe UI&quot;, Arial, freesans, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"></iframe><p>油管：</p><iframe width="560" height="315" src="https://www.youtube.com/embed/VfeCri0yplc?si=UCoZpuOyyJ-ymjix" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen style="box-sizing: border-box; color: rgb(53, 55, 64); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Segoe UI&quot;, Arial, freesans, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"></iframe><p><strong>Linux安装教程请访问：</strong>https://techdiylife.github.io/blog/blog.html?category1=c02&amp;blogid=0036</p><h3 id="如何对ollama进行速度评价">14 如何对ollama进行速度评价？</h3><p>当以普通用户身份启动服务时，可以在终端界面查看推理速度。比如{"function":"print_timings","level":"INFO","line":257,"msg":"prompt evaltime = 25.55 ms / 12 tokens ( 2.13 ms per token, 469.70 tokens persecond)","n_prompt_tokens_processed":12,"n_tokens_second":469.70408642555196,"slot_id":0,"t_prompt_processing":25.548,"t_token":2.129,"task_id":111,"tid":"140543230871296","timestamp":1711096105}</p><p>可以看到有：2.13 ms per token, 469.70 tokens per second</p><p>在Windows或者Linux上以服务启动时，也可以在日志文件中找到这些数据。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs text"># Windows<br>C:\Users\&lt;username&gt;\AppData\Local\Ollama\server.log<br>#MacOS<br>cat ~/.ollama/logs/server.log<br># Linux<br>sudo systemctl status ollama.service &gt; xxx.log<br></code></pre></td></tr></table></figure><h3 id="ollama是否可以对模型精度评价">15Ollama是否可以对模型精度评价？</h3><p>在ollama工具中，没有直接的方式来评估模型性能。然而，在<code>llama.cpp</code>中，有提供测试数据集，以及使用Perplexity指标来进行模型评估的示例。</p><p>常见LLM大模型评估方法如下：</p><ul><li><strong>主观评价</strong>：通过人工审查模型的输出，评估其生成内容的质量和相关性。</li><li><strong>测试集评价</strong>：利用特定的测试数据集，对模型性能进行量化评估。这种方法的细节可以参考相关模型的技术报告。</li><li><strong>利用其他模型进行评价</strong>：例如，使用GPT-4对ollama模型的输出结果进行评估，以此来比较不同模型的性能。</li></ul><h3 id="linux系统中以服务模式运行ollama如何查看运行日志">16Linux系统中以服务模式运行ollama，如何查看运行日志？</h3><p>使用以下命令可以查看ollama的日志：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs text"># 查看服务日志<br>sudo systemctl status ollama.service<br><br># 查看用户日志<br>sudo journalctl -u ollama<br></code></pre></td></tr></table></figure><h3 id="linux系统中如何卸载ollama">17 Linux系统中如何卸载Ollama？</h3><p>在Windows和MacOS上，卸载ollama的过程与卸载其他软件相同，支持一键卸载。</p><p>而在Linux上，卸载ollama需要执行更多步骤，包括关闭运行中的服务以及删除相关文件。具体操作步骤，请参考官方文档中的“Uninstall”部分：https://github.com/ollama/ollama/blob/main/docs/linux.md</p><h3 id="上网需要使用代理时模型无法下载怎么办">18上网需要使用代理时，模型无法下载怎么办？</h3><p><strong>出现错误：</strong> Error: pull model manifest: Get"https://registry.ollama.ai/v2/library/qwen/manifests/0.5b": dial tcp34.120.132.20:443: i/o timeout</p><p><strong>原因分析：</strong> 代理设置不正确。</p><ol type="1"><li><strong>服务文件中设置环境变量（sudo安装时）</strong>：在以服务启动后，默认以<code>ollama</code>用户的身份运行。可以为ollama.service服务设置环境变量。</li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs text"># 打开服务文件<br>sudo nano /etc/systemd/system/ollama.service<br># 在文件中Service字段后添加<br>[Service]<br>Environment=&quot;http_proxy=xxxxxx&quot;<br><br># 重启服务<br>sudo systemctl daemon-reload<br>sudo systemctl restart ollama.service<br><br># 确认状态<br>sudo systemctl status ollama.service<br></code></pre></td></tr></table></figure><ol type="1"><li><strong>以当前用户身份启动服务(一般用户)</strong>：通过为当前用户设置代理，然后以当前用户的身份启动服务。这要求当前用户具有启动该服务所需的权限。</li></ol><ul><li><p>方法1：.bashrc中设置</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs text"># 在本地账户 .bashrc 文件中加入<br>export HTTP_PROXY=xxxxxxxxxxx<br>export HTTPS_PROXY=xxxxxxxxxxx<br><br># 启动服务<br>ollama serve 启动（不能使用 sudo systemctl start ollama.service）<br></code></pre></td></tr></table></figure></li><li><p>方法2：通过下面的方式启动服务</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">HTTPS_PROXY=xxxxxxxxxxx ollama serve<br></code></pre></td></tr></table></figure></li></ul><p>参考资料：https://github.com/ollama/ollama/issues/1859</p><h3 id="有多个gpu时如何指定使用单张gpu">19有多个GPU时，如何指定使用单张GPU？</h3><p>ollama默认会使用所有它可见的GPU，如果希望限制它只使用其中的某个GPU时，可以在环境变量中设置CUDA_VISIBLE_DEVICES，比如： export CUDA_VISIBLE_DEVICES=0</p><h3 id="ollama是否支持rag">20 ollama是否支持RAG？</h3><p>ollama本身不支持RAG，但是可以结合langchain等工具来实现，比如：https://github.com/ollama/ollama/tree/main/examples/langchain-python-rag-websummary</p><h3 id="补充ollama模型库中instructtext等tag是指什么模型">21补充：Ollama模型库中instruct，text等tag是指什么模型？</h3><p>如下图所示的llama3模型有70b， 8b， instruct， text都分别对应着什么模型呢？</p><figure><imgsrc="data:image/png;charset=utf-8;base64,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"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>可以在模型的说明中找到相关的解释。没有特殊标记的对应着chat版本，也就是经过指令微调的模型。而text版本则对应着base model，也就是预训练模型。</p><figure><imgsrc="data:image/png;charset=utf-8;base64,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"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h3 id="补充open-webui-与现有服务端口冲突如何修改端口">22 补充：OpenWebUI 与现有服务端口冲突，如何修改端口？</h3><ul><li>如果是使用docker安装运行，可以在docker命令中修改：</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs text">#默认端口为3000<br>docker run -d -p 3000:8080 -e xxxxxxxx<br>#修改为端口3001<br>docker run -d -p 3001:8080 -e xxxxxxxx<br></code></pre></td></tr></table></figure><ul><li>如果是通过代码安装，启动</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">cd ./backend<br>PORT=1234 start.sh #使用端口1234<br></code></pre></td></tr></table></figure><h3 id="补充ollama支持中文名用户名或者目录吗">23补充：Ollama支持中文名用户名，或者目录吗？</h3><p>从0.1.33版开始增加了对中文名目录的支持。https://github.com/ollama/ollama/issues/33670.1.33之前的版本不支持。如果你使用的老版本，请检查：</p><ul><li>你的windows用户名是否为中文名</li><li>ollama的模型存放目录是否包含中文</li></ul><p>如果存在以上情况，可以修改ollama模型默认存放目录。（修改方式参考，问题6）</p><h3 id="补充ollama使用一段时间后为什么会自动停止">24补充：Ollama使用一段时间后，为什么会自动停止？</h3><p>很大的原因是由于资源不足，比如GPU显存不足等。</p><ol type="1"><li>查看ollama的日志看看是否有记录</li><li>尝试较小的模型，比如qwen 0.5B的模型看看是否出现同样的问题。</li></ol><h3id="补充windows上安装完后在终端执行提示-ollama-不是内部或外部命令">25补充：windows上安装完后，在终端执行提示 ollama 不是内部或外部命令</h3><p>【根据群里小伙伴的讨论整理】</p><ol type="1"><li>设置环境变量 PATH： win+r打开运行，输入“controlsystem”-高级系统设置-高级-环境变量-系统变量-编辑Path-新建-确定。</li><li>设置PATH后重新启动ollama服务，在ollama图标上点击右键，以管理员身份启动。启动后，在右下角任务栏中确认是否有ollama的“小羊”图标</li><li>启动终端输入：ollama run qwen确认是否可以运行（如果出错，可以尝试以管理员身份）</li></ol><h3id="补充windows系统提示错误-errorcould-not-connect-to-ollama-app-is-it-running">26补充：Windows系统，提示错误 Error：Could not connect to ollama app， isit running</h3><p>运行 ollama run xxx 出现此错误，说明ollama没有正确启动。可以尝试：</p><ol type="1"><li>关闭ollama程序</li><li>以管理员身份重新启动ollama。</li></ol><h2 id="参考资料">参考资料</h2><ul><li>Docker官网：https://www.docker.com/products/docker-desktop/</li><li>Ollama官网：https://ollama.com/</li><li>Open WebUI Github地址：https://github.com/open-webui/open-webui</li><li>Ollama官方QA：https://github.com/ollama/ollama/blob/main/docs/faq.md</li></ul>]]></content>
    
    
    <categories>
      
      <category>大模型部署</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人工智能领域常见概念中英文汇编</title>
    <link href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%E6%A6%82%E5%BF%B5%E4%B8%AD%E8%8B%B1%E6%96%87%E6%B1%87%E7%BC%96.html"/>
    <url>/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%E6%A6%82%E5%BF%B5%E4%B8%AD%E8%8B%B1%E6%96%87%E6%B1%87%E7%BC%96.html</url>
    
    <content type="html"><![CDATA[<h3id="人工智能领域常见概念中英文汇编">人工智能领域常见概念中英文汇编</h3><p>本文动态收入总结AI领域的一些常见英文概念及其对应的中文释义</p><span id="more"></span><h2 id="大模型专栏">大模型专栏:</h2><ol type="1"><li><p><strong>Transformer</strong>：</p><ul><li>一种用于自然语言处理的神经网络架构，通过自注意力机制（self-attention）来处理输入序列。Transformer模型包括编码器和解码器部分，常用于各种NLP任务，如翻译、文本生成等。</li></ul></li><li><p><strong>BERT（Bidirectional Encoder Representations fromTransformers）</strong>：</p><ul><li>一种预训练的Transformer模型，通过双向（上下文）编码来理解文本的上下文关系，广泛应用于问答、分类等任务。</li></ul></li><li><p><strong>GPT（Generative Pre-trained Transformer）</strong>：</p><ul><li>由OpenAI开发的生成式预训练Transformer模型，擅长生成连贯的文本。GPT-3是其著名的版本之一，具有1750亿参数。</li></ul></li><li><p><strong>Fine-tuning（微调）</strong>：</p><ul><li>在预训练模型的基础上，通过在特定任务或数据集上进行进一步训练，使模型更适合特定任务。</li></ul></li><li><p><strong>Zero-shot Learning</strong>：</p><ul><li>模型在没有见过特定任务的情况下，利用其预训练知识直接进行预测。GPT-3展示了强大的零样本学习能力。</li></ul></li><li><p><strong>Few-shot Learning</strong>：</p><ul><li>模型在只见过少量示例的情况下进行学习和预测。通过提供少量示例作为提示，模型可以更好地理解和执行任务。</li></ul></li><li><p><strong>Attention Mechanism（注意力机制）</strong>：</p><ul><li>一种让模型在处理输入序列时关注重要部分的机制，极大地提升了模型的性能。自注意力机制是Transformer的核心组件。</li></ul></li><li><p><strong>Self-Attention（自注意力）</strong>：</p><ul><li>Transformer中的一种机制，通过计算序列中每个元素与其他元素的相关性来生成新的表示。</li></ul></li><li><p><strong>Language Model（语言模型）</strong>：</p><ul><li>预测给定上下文下下一个词的概率模型。语言模型是生成文本和理解文本的基础。</li></ul></li><li><p><strong>Tokenization（分词）</strong>：</p><ul><li>将文本拆分成更小的单元（如单词、子词、字符等），是文本处理的第一步。常用的分词方法包括BPE（Byte-PairEncoding）和WordPiece。</li></ul></li><li><p><strong>Embedding（嵌入）</strong>：</p><ul><li>将离散的文本表示为连续的向量，以捕捉词语之间的语义关系。嵌入向量可以通过模型学习得到，如Word2Vec、GloVe等。</li></ul></li><li><p><strong>Sequence-to-Sequence（序列到序列，Seq2Seq）</strong>：</p><ul><li>一种用于将输入序列转换为输出序列的模型架构，广泛应用于机器翻译、文本摘要等任务。通常由编码器和解码器组成。</li></ul></li><li><p><strong>Beam Search（束搜索）</strong>：</p><ul><li>一种在生成过程中用于寻找最优序列的方法，通过保留多个最优候选序列以提高生成质量。</li></ul></li><li><p><strong>Perplexity（困惑度）</strong>：</p><ul><li>评价语言模型性能的指标，反映模型对测试集的预测能力。困惑度越低，模型性能越好。</li></ul><hr /><h2 id="机器学习领域">机器学习领域</h2></li></ol><h3 id="a">A</h3><p><strong>A/B Testing</strong>（A/B 测试）一种受控的真实实验，用于比较系统或模型的两个变体A和B。</p><p><strong>Activation Function</strong>（激活函数）在人工神经网络的情境中，接受来自上一层的所有输入的加权和并生成输出值来激活下一层的函数。</p><p>Active Learning (Active Learning Strategy)（主动学习/主动学习策略）半监督机器学习的一种特殊情况，在这种情况下，学习代理能够以交互的方式查询数据库（通常是人工标注员），以获取新数据点的标签。</p><p><strong>Algorithm</strong>（算法）一种关于如何解决某一类问题的过程的明确规范，它能够执行计算、处理数据并进行自动推理。</p><p><strong>Annotation</strong>（标注）附加到一条数据之上的元数据，通常由人工标注员提供。</p><p>Area Under the Curve (<strong>AUC</strong>)（曲线下面积）机器学习中用于确定在多个使用的模型中哪个模型具有最高性能的一种方法。</p><p><strong>Artificial Intelligence</strong>（人工智能）机器模拟人类智力和行为做出决策、执行任务的能力。</p><p><strong>Artificial Neural Networks</strong>（人工神经网络）由简单互联单元（称作神经元）的连续层所构成的一种架构，这些单元与非线性激活函数交织在一起，会让人模糊地联想到动物大脑中的神经元。</p><p><strong>Association Rule Learning</strong>（关联规则学习）一种基于规则的机器学习方法，用于发现大型数据集中变量之间的关系。</p><p>Autoencoder（自动解码器）一种人工神经网络，用于以无监督、非线性的方式生成高效的数据表示，通常用于降低维度。</p><p><strong>Automated Speech Recognition</strong>（自动语音识别）计算语言学的一个子领域，主要是关于通过计算机识别和翻译口语的方法。</p><h3 id="b">B</h3><p><strong>Backpropagation</strong> (Backpropagation ThroughTime)（反向传播/基于时间的反向传播）用于训练人工神经网络，进而计算网络权重计算所需梯度的一种方法。</p><p><strong>Batch</strong>（批量）在模型训练的单个梯度更新中使用的示例集。</p><p><strong>Bayes’s Theorem</strong>（贝叶斯定理）统计学家根据可能与某个存在相关的先验条件知识描述某个事件的概率时所用的一个著名定理。</p><p><strong>BERT（Bidirectional Encoder Representations fromTransformers）</strong>：</p><p>一种预训练的Transformer模型，通过双向（上下文）编码来理解文本的上下文关系，广泛应用于问答、分类等任务。</p><p><strong>Bias</strong> (Inductive Bias, ConfirmationBias)（偏差-归纳偏差、确认偏差）归纳偏差：学习者在给定输入条件下预测尚未遇到的输出时所用的假设事项集。确认偏差：以确认自己的信念或假设的方式搜索、解释、赞成和回想信息，而较少关注与之相矛盾的信息的趋势。</p><p><strong>Bias-Variance Tradeoff</strong>（偏差与方差权衡）当数据科学家尝试同时最大程度地减小偏差和方差时所产生的冲突，该冲突不利于监督算法推广到他们的训练集范围之外。</p><p><strong>Boosting</strong>（提升）主要用于减少监督学习中的偏差和方差的一种机器学习集成元算法，以及将弱学习者转化为强学习者的一系列机器学习算法。常见的boosting包括Baggingboosting(随机森林),GBDT(梯度提升树),XGBoost , LightGbm</p><p><strong>Bounding Box</strong>（边界框）完全包含一组点或一个对象的最小（矩形）框。</p><h3 id="c">C</h3><p><strong>Chatbot</strong>（聊天机器人）一种旨在通过对话与人类用户进行交互的计算机程序或 AI。</p><p><strong>Classification</strong>（分类）对映射函数进行从输入变量到离散输出变量的近似处理的任务，或者从广义上来说，是指用于确定特定实例所属的类的某一类机器学习算法。</p><p><strong>Clustering</strong>（聚类）在机器学习中，是指对一组对象进行分组，使得同一组（即集群）中的对象彼此之间的“相似性”高于与其他组中的对象“相似性”的无监督任务。</p><p>Cold-Start（冷启动）由于系统无法针对尚未收集到足够信息的用户或项目推断出任何信息而引起的潜在问题。</p><p>Collaborative Filtering（协作过滤）在推荐系统中使用的一种方法，用于通过收集来自较大用户组的偏好来预测用户的兴趣。</p><p>Computer Vision（计算机视觉）机器学习的领域之一，主要研究如何获得对图像或视频的高级理解。</p><p><strong>Confidence Interval</strong>（置信区间）一种区间估计，可能包含未知总体参数的真实值。该区间与置信水平相关，而置信水平用于量化参数在区间中的置信度。</p><p>Contributor（贡献者） 提供标注服务的人工标注员。</p><p>Convolutional Neural Network (<strong>CNN</strong>)（卷积神经网络）一种深层、前馈人工神经网络类别，通常用于计算机视觉。</p><p>Central Processing Unit (CPU)（中央处理单元）计算机中通过执行指令指定的基本算术、逻辑、控制和输入/输出操作来执行计算机程序的指令的电子电路。</p><p><strong>Cross-Validation</strong> (k-fold Cross-Validation,Leave-p-out Cross-Validation)（交叉验证-k 折交叉验证、留 p 法交叉验证）旨在评估如何将预测模型的结果推广到新数据集的一组流程，包括k折交叉验证及留p法交叉验证。</p><h3 id="d">D</h3><p>Data (Structured Data, Unstructured Data, Dataaugmentation)（数据-结构化数据、非结构化数据、数据增强）所有机器学习和人工智能项目的最基本要素。</p><p>非结构化数据：未经处理的原始数据。文本数据是非结构化数据的完美示例，因为它没有格式化为特定功能。</p><p>结构化数据：以机器学习算法可摄取的方式处理的数据；如果是监督机器学习，则为已标记的、经处理后的数据。</p><p>数据增强：将内外部来源衍生的新信息添加到数据集的过程（一般通过标注来实现）。</p><p><strong>Decision Tree</strong>（决策树）监督机器学习算法的一个类别，在此类算法中，数据会根据给定参数或条件进行迭代拆分。</p><p>Deep Blue（深蓝） 由 IBM开发的国际象棋游戏计算机，作为全球首个在常规时限内同时战胜了国际象棋游戏和国际象棋比赛卫冕世界冠军的计算机国际象棋游戏系统而闻名。</p><p><strong>Deep Learning</strong> (Deep ReinforcementLearning)（深度学习/深度强化学习）与特定任务的算法相反，基于学习数据表示的更广泛的机器学习方法系列。深度学习包括监督学习、半监督学习或无监督学习。</p><p><strong>Dimension</strong>维度（降维、维度灾难） 降维 DimensionalityReduction：通过获取一组主变量来减少所考虑的随机变量数量的过程。另请参见特征选择。</p><p>维度灾难 Curse ofDimensionality：由于维数越多，可用数据量越稀疏这一事实，在高维空间中分析和组织数据时出现的一种现象。</p><h3 id="e">E</h3><p><strong>Embedding</strong> (Word Embedding)（嵌入/词嵌入）某个实例中所含的某个数学结构的另一个实例，例如作为另一个组的子组的组。</p><p>Ensemble Methods（集成方法）在统计和机器学习中，集成方法使用多种学习算法来获得更好的预测性能，而这种性能可以单独从任何组合式学习算法中获得。与统计力学中通常是无限的统计集成不同，机器学习集成仅由一组有限的替代模型组成，但通常允许在这些替代模型之间存在更灵活的结构。</p><p><strong>Entropy</strong>（熵） 随机数据源传达的平均信息量。</p><p><strong>Epoch</strong>（时期）在深度学习模型训练场景中，完整训练数据集的一次训练循环。</p><h3 id="f">F</h3><p><strong>Feature</strong> (Feature Selection, FeatureLearning)（特征-特征选择、特征学习） 用作模型输入的变量。</p><p>Feature Learning（特征学习）旨在自动从原始数据中发现特征检测或分类所需的表示的一组技术。</p><p><strong>False Positive</strong>（误报）由于结果在虚无假设原本不应该存在的情况下拒绝虚无假设而导致的误差。</p><p><strong>False Negative</strong>（漏报）由于结果在虚无假设应该存在的情况下未拒绝虚无假设而导致的误差。</p><p><strong>Feed-Forward (Neural) Networks</strong>（前馈神经网络）一种人工神经网络，其中神经元之间的连接不会向后移动或形成循环。</p><p><strong>F-Score</strong>（F 得分）衡量模型准确性的一个指标，它会考量准确率和召回率来计算得分。更具体地说，F得分是准确率和召回率的调和平均值，该平均值的最大值为1（完美的准确率和召回率），最小值为 0。</p><h3 id="g">G</h3><p><strong>Garbage In, Garbage Out</strong>（垃圾进垃圾出）一项原则，具体说的是：只要输入数据存在缺陷，就会导致误导性的结果并产生无意义的输出，也就是“垃圾”。</p><p>General Data Protection Regulation (GDPR)（通用数据保护条例）欧盟颁布的一部针对欧盟内所有个体的数据保护和隐私法规，旨在控制公民和居民对其个人数据的控制。</p><p><strong>Genetic Algorithm</strong>（遗传算法）基于进化论的一种启发式搜索算法，进化论反映了自然选择的过程，在这个过程中，最能适应环境的个体会被选出生产下一代。</p><p><strong>Generative Adversarial Networks(GANs)</strong>（生成对抗网络）无监督机器学习中使用的一种人工智能算法类别，作为零和游戏框架中相互竞争的两个神经网络的组合予以实施。</p><p>Graphic Processing Unit (GPU)（图形处理单元）一种专用的电子电路，它采用并行处理架构，旨在快速操作和更改内存，以加速图像渲染，从而使其可以同时执行多个计算。</p><p><strong>Ground Truth</strong>（事实真相）通过直接观察（而非推论）获得的一条信息。</p><h3 id="h">H</h3><p><strong>Human-in-the-Loop</strong>（人机协同） 人机协同 (HITL)是人工智能的一个分支，它同时利用人类智能和机器智能来构建机器学习模型。在传统的“人机协同”方法中，人们会参与到一个良性循环，在其中训练、调整和测试特定算法。</p><p><strong>Hyperparameter</strong> (HyperparameterTuning)（超参数/超参数优化）模型外部的一种配置，其值无法从数据中估算出来，数据科学家会在模型训练过程中不断对其进行调整。-手动确定训练特定模型最佳配置的过程。</p><h3 id="i">I</h3><p><strong>ImageNet</strong>（ImageNet数据集）一个庞大的视觉数据集，由1400万个手工标注图像的URL组成，并以两万个不同类别进行组织，旨在用于视觉对象识别研究。</p><p>Image Recognition（图像识别）计算机视觉中用于确定图像是否包含某些特定对象、特征或活动的问题。</p><p><strong>Inference</strong>（推理）通过将经训练的模型运用到新的未标记实例来进行预测的过程。</p><p><strong>Information Retrieval</strong>（信息检索）计算机科学的一个领域，旨在研究在文档中搜索信息、搜索文档本身、搜索描述数据的元数据以及搜索文本、图像或声音数据库的过程。</p><h3 id="l">L</h3><p><strong>Layer (Hidden Layer)</strong>（层/隐藏层）人工神经网络中的一系列神经元，旨在处理一组输入特征，或者从广义上来说，处理这些神经元的输出。</p><p>隐藏层：神经元的一层，其输出连接到其他神经元的输入，因此不能作为网络输出直接实现可视化。</p><p><strong>Learning-to-Learn</strong>（元学习）机器学习领域的一个新方向，主要是研究算法如何通过分析自己的学习过程并对其加以改进来改变其归纳方式。</p><p><strong>Learning-to-Rank</strong>（排序学习）运用机器学习构建信息检索系统的排名模型。</p><p><strong>Learning Rate</strong>（学习率）梯度下降算法在人工神经网络训练阶段的每次迭代中所用的标量值，与梯度相乘得出结果。</p><p><strong>Logit Function</strong>（Logit 函数）在数学中（尤其是在统计学中）使用的 S 型“逻辑”函数的逆函数。</p><p><strong>Long Short-Term Memory Networks</strong>（长短期记忆网络）递归神经网络的一种变体，可用作梯度消失问题的一种解决方案。</p><h3 id="m">M</h3><p>Machine Learning（机器学习）人工智能的一个子领域，通常使用统计技术来赋予计算机“学习”能力，即借助数据来逐步提高特定任务的性能，而无需进行显式编程。</p><p>Machine Learning Lifecycle Management（机器学习生命周期管理）机器学习系统的 DevOps。</p><p>Machine Translation（机器翻译）计算语言学的一个子领域，主要是研究如何使用软件将文本或语音从一种语言翻译成另一种语言。</p><p>Model（模型）模型是机器学习系统通过训练过程从训练数据中所学到内容的抽象表示。</p><p><strong>Monte Carlo</strong>（蒙特卡洛方法）一种使用重复随机采样生成合成模拟数据的近似方法。</p><p><strong>Multi-Modal Learning</strong>（多模式学习）机器学习的一个子领域，旨在将多模式信号合并到一起进行解释，并构建模型来处理和关联来自多种数据类型的信息。</p><p>Multi-Task Learning（多任务学习）机器学习的一个子领域，同时利用多个任务之间的异同来解决多个任务。</p><h3 id="n">N</h3><p>Naive Bayes（朴素贝叶斯）基于贝叶斯定理并在特征之间具有很强的独立性假设的一系列简单概率分类器。</p><p>Named Entity Recognition（命名实体识别）信息提取的一个子任务，旨在将文本中的命名实体识别和分类为预定类别，例如名称、位置、词性等。</p><p>Natural Language Processing (NLP)（自然语言处理）人工智能领域之一，主要是研究计算机语言与人类语言之间的交互，尤其是如何处理和分析大量自然语言数据。</p><p>Neural Networks（神经网络） 参见人工神经网络。</p><p>Neuron（神经元）人工神经网络中的一个单元，用以处理多个输入值，以生成单个输出值。</p><p><strong>Node</strong>（节点） 参见神经元。</p><h3 id="o">O</h3><p>Optical Character Recognition（光学字符识别）将打印、手写或键入文本的图像转换为机器友好的文本格式。</p><p><strong>Optimization</strong>（优化）从可用替代方案中（基于某些标准）选择最佳方案。</p><p><strong>Overfitting</strong>（过度拟合）模型在不知情的情况下识别出噪声中的模式并假设这些模式代表了底层结构；模型的生成结果与特定数据集过于接近，因此无法很好地归纳到不可见的观察结果。</p><h3 id="p">P</h3><p>Pattern Recognition（模式识别）机器学习的领域之一，主要专注于数据模式的（监督或无监督）识别。</p><p>Pooling (Max Pooling)（轮询/最大轮询）将卷积层生成的矩阵缩减为较小矩阵的过程。</p><p>Personally Identifiable Information（个人可识别信息）可以单独使用或与某些其他信息结合使用，以识别特定个人的任何信息。</p><p><strong>Precision</strong>（准确率）正确的阳性结果数除以分类器返回的所有样阳性结果数。</p><p><strong>Prediction</strong>（预测）带有输入实例的训练模型的推断输出。</p><p><strong>Preprocessing</strong>（预处理）将原始数据转换为更易理解格式的过程。</p><p>Pre-trained Model（预训练模型）通常已使用另一个数据集进行了初步训练的模型或模型的组成部分。另请参见：转移学习。</p><p>Principal Component Analysis（主组件分析）使用正交变换将一组可能相关变量的观测值转换为一组线性不相关变量（称为主组件）的过程。</p><p>Prior（先前技术）在考虑新证据之前，代表特定数量的先前存在信念的概率分布。</p><h3 id="r">R</h3><p><strong>RAG（Retrieval-AugmentedGeneration，检索增强生成）</strong></p><p>RAG是一种结合信息检索和生成式模型的新方法。RAG的核心思想是通过将检索模块和生成模块结合起来，以提高生成的准确性和信息性。</p><p><strong>Random Forest</strong>（随机森林）一种集成学习方法，其工作原理是在训练时构造大量决策树并输出每个单独树的结果的组合版本（例如均值或众数）。</p><p><strong>Recall</strong>（召回率）所有相关样本中被正确分类为阳性的样本数所占百分比。</p><p>Rectified Linear Unit（整流线性单元）使用整流函数作为激活函数的单元。</p><p><strong>Recurrent Neural Networks</strong>（递归神经网络）人工神经网络的类别之一，其中神经元之间的连接沿着序列形成有向图，使其表现出时序动态时间行为并使用其内部状态（内存）来处理顺序信号。</p><p><strong>Regression</strong> (Linear Regression, LogisticRegression)（回归-线性回归、逻辑回归）一组用于估计变量间关系的统计过程。</p><p>线性回归：一种简单的回归类型，以特征的线性组合作为输入，并输出连续值。</p><p>逻辑回归：一种回归类型，通过将 S型函数运用到线性预测对分类问题中每个可能的离散标签值生成概率。</p><p><strong>Regressor</strong>（回归器）一种特征，即用作模型输入的解释性变量。</p><p><strong>Regularization</strong>（正则化）引入额外信息以防过度拟合的过程。</p><p>Reinforcement Learning（强化学习）机器学习的子领域之一，主要是受人类行为的启发，研究代理应如何在给定的环境中采取行动，以实现累积奖励概念的最大化。</p><p>Reproducibility (crisis of)（可再现性危机）科学领域的一种方法论危机，即学者们发现：许多科学研究的结果很难或不可能在独立研究人员或最初研究人员自己的后续研究中复制或再现。</p><p>Restricted Boltzmann Machines（受限玻尔兹曼机） 受限玻尔兹曼机 (RBM)是一种生成型随机人工神经网络，可以学习其输入集上的概率分布。</p><h3 id="s">S</h3><p><strong>Sora(State of the Art)</strong></p><p>指的是在某一领域内最新、最先进的技术或方法。无论是在科学研究、工程技术还是机器学习中，SOTA都代表了当前公认的最佳成果或最高水平的成就。SOTA方法和技术通常是通过同行评议的学术论文、行业报告或者标准评测中展示的，并被用作衡量其他研究或技术进展的基准。</p><p><strong>Semi-Supervised Learning</strong>（半监督学习）监督学习技术的一个类别，它还可以利用可用的未标记数据进行训练，通常结合使用少量的已标记实例与大量的未标记行。另请参见监督学习和无监督学习。</p><p>Sentiment Analysis 情绪分析使用自然语言处理、文本分析、计算语言学和生物特征识别等功能系统地识别、提取、量化和研究受影响的状态和主观信息。</p><p>Speech Recognition（语音识别） 参见自动语音识别。</p><p>Statistical Distribution（统计分布）在统计学中，经验分布函数是指与样本的经验指标相关的分布函数。该累积分布函数是一个阶跃函数，在n 个数据点中的每个数据点上都跳了 1/n次。它在测量变量的任何指定值处的值都是小于或等于对应指定值的测量变量观察值的分数。</p><p>Supervised Learning（监督学习）一种机器学习任务，主要是指基于示例输入/输出对学习将输入映射到输出的函数。</p><p><strong>Support Vector Machines</strong> (SVM)（支持向量机）由一个单独的超平面正式定义的一种判别分类器类别，对于每个提供的带标记训练数据点，算法都会输出一个对新示例进行分类的最佳超平面。</p><p>Synthetic Data（合成数据）当无法收集足够的实际数据或原始数据不满足特定要求时人工生成的数据。</p><h3 id="t">T</h3><p>TensorFlow（TensorFlow代码库）一种开源代码库，在机器学习社区中非常流行，用于跨一系列任务的数据流编程。它是一个符号数学库，还可用于神经网络等机器学习应用。</p><p><strong>Time Series</strong> (Time Series Data)（时序/时序数据）在特定时间记录并根据它们的出现顺序进行索引处理的一系列数据点。</p><p>Testing (Testing Data)（测试/测试数据）测试是指在监督机器学习情境中，使用保留数据评估模型最终性能的过程。</p><p>测试数据：数据科学家针对模型开发的测试阶段而选择的可用数据的子集。</p><p>Topic Modeling（主题建模）无监督机器学习算法的一种类别，它使用聚类功能在文本数据中查找隐藏的结构并作为一个主题对其进行解释。</p><p>Training Data（训练数据）在监督机器学习情境中，构建可从数据中学习并根据数据进行预测的算法。</p><p>训练数据：数据科学家针对模型开发的训练阶段而选择的可用数据的子集。</p><p>Transfer Learning（转移学习）机器学习的一个领域，其重点在于使用获得的知识来解决特定问题，并将此类知识运用到其他相关问题。</p><p><strong>Turing Test</strong>（图灵测试）由艾伦·图灵开发的一种测试，用于评估机器表现出与人类相同的智能行为的能力。该测试包括人机聊天。如果在测试房间之外见证对话的评估人员不能可靠地区分人类与受测机器，则可以认定该机器已经通过了图灵测试。</p><p>Type I Error（I 类误差） 参见误报。</p><p>Type II Error（II 类误差） 参见漏报。</p><h3 id="u">U</h3><p>Uncertainty（不确定性） 可能包含真实值的一系列值。</p><p><strong>Underfitting</strong>（拟合不足)机器学习算法无法正确捕获数据的底层结构，通常是因为模型不够高级或不适用于当前任务；与过度拟合的涵义相反。</p><p>Unsupervised Learning（无监督学习）机器学习的领域之一，包括对用于描述未标记数据结构的函数进行推断。</p><h3 id="v">V</h3><p><strong>Validation</strong>（验证）使用保留数据评估训练模型性能的过程；与模型性能最终评估的测试阶段相反，验证阶段旨在确定是否需要对模型进行任何迭代修改。</p><p><strong>Vanishing/Exploding Gradients</strong>（消失/爆炸梯度）数据科学家在采用基于梯度的学习方法和反向传播对人工神经网络进行训练时，由于神经网络中接收与误差函数偏导数成比例的更新的权重（考虑到每个训练迭代中的当前权重）而面临的可怕困难和主要障碍。</p><p><strong>Variance</strong>（方差）由于对训练集中小波动的敏感性而引起的误差，该误差按照针对随机变量与其平均值的平方偏差的期望值进行计算。</p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>基础知识</tag>
      
      <tag>中英文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常见大模型榜单整理</title>
    <link href="/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%92%E8%A1%8C%E6%A6%9C.html"/>
    <url>/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%92%E8%A1%8C%E6%A6%9C.html</url>
    
    <content type="html"><![CDATA[<p>大模型的发展日新月异,乱花渐欲迷人眼,那到底哪个模型更强呢,不同模型又有哪些各自擅长的领域呢?这里整理几个比较权威的LLM评测榜单和数据集供诸君参考.</p><span id="more"></span><h3 id="lmsys-chatbot-arena">LMSYS Chatbot Arena</h3><ul><li>LMSYS Chatbot Arena是一个众包的开放平台，用于大语言模型（LLM）的评估。收集了超过 1,000,000次人类成对比较，使用 Bradley-Terry 模型对 LLM 进行排名，并以 Elo评分展示模型的评级。</li></ul><p>https://chat.lmsys.org/?leaderboard</p><h3 id="opencompass">OpenCompass</h3><ul><li>OpenCompass是一个开源项目,提供丰富的算法和功能支持，能够帮助社区更便捷地对NLP模型的性能进行公平全面的评估。</li></ul><p>https://rank.opencompass.org.cn/home</p><h3 id="mmlu">MMLU</h3><ul><li><strong>MMLU</strong> ： 全称Massive Multitask LanguageUnderstanding，是一种针对大模型的<strong>语言理解能力的测评</strong>，是目前最著名的大模型语义理解测评之一，由UCBerkeley大学的研究人员在2020年9月推出。该测试涵盖57项任务，包括初等数学、美国历史、计算机科学、法律等。任务涵盖的知识很广泛，语言是英文，用以评测大模型基本的知识覆盖范围和理解能力。</li></ul><p>https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu</p><h3 id="c-eval">C-Eval</h3><ul><li><strong>C-Eval</strong> ： C-Eval是一个全面的中文基础模型评估套件。由上海交通大学、清华大学和匹兹堡大学研究人员在2023年5月份联合推出，它包含了13948个多项选择题，涵盖了52个不同的学科和四个难度级别。用以<strong>评测大模型中文理解能力</strong>。</li></ul><p>https://cevalbenchmark.com/static/leaderboard.html</p><h3 id="gsm8k">GSM8K</h3><ul><li><strong>GSM8K</strong> ：OpenAI发布的大模型数学推理能力评测基准，涵盖了8500个中学水平的高质量数学题数据集。数据集比之前的数学文字题数据集规模更大，语言更具多样性，题目也更具挑战性。该项测试在2021年10月份发布，至今仍然是非常困难的一种测试基准.</li></ul><p>https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k</p><h3 id="agi-eval">AGI Eval</h3><ul><li><strong>AGI Eval</strong> ：微软发布的大模型基础能力评测基准，在2023年4月推出，主要评测大模型在人类认知和解决问题的一般能力，涵盖全球20种面向普通人类考生的官方、公共和高标准录取和资格考试，包含中英文数据。因此，该测试更加倾向于<strong>人类考试结果</strong>，涵盖了中英文，论文地址：https://arxiv.org/abs/2304.06364</li></ul><h3 id="lmsys榜单">LMSYS榜单</h3><iframe aria-hidden="true" tabindex="-1" src="about:blank" style="box-sizing: border-box; border: 0px; display: block; vertical-align: middle; top: 0px; left: 0px; width: 1146.4px; height: 62.2px; overflow: hidden; opacity: 0; pointer-events: none; z-index: -1;"></iframe><table><thead><tr class="header"><th>🤖 Model</th><th>⭐ Arena Elo</th><th>📈 MT-bench</th><th>📚 MMLU</th><th>Organization</th><th>License</th></tr></thead><tbody><tr class="odd"><td><ahref="https://openai.com/index/hello-gpt-4o/">GPT-4o-2024-05-13</a></td><td>1287</td><td></td><td>88.7</td><td>OpenAI</td><td>Proprietary</td></tr><tr class="even"><td><ahref="https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4">GPT-4-Turbo-2024-04-09</a></td><td>1252</td><td></td><td></td><td>OpenAI</td><td>Proprietary</td></tr><tr class="odd"><td><ahref="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">GPT-4-1106-preview</a></td><td>1250</td><td>9.32</td><td></td><td>OpenAI</td><td>Proprietary</td></tr><tr class="even"><td><ahref="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">Gemini1.5 Pro API-0409-Preview</a></td><td>1248</td><td></td><td>81.9</td><td>Google</td><td>Proprietary</td></tr><tr class="odd"><td><a href="https://www.anthropic.com/news/claude-3-family">Claude 3Opus</a></td><td>1246</td><td></td><td>86.8</td><td>Anthropic</td><td>Proprietary</td></tr><tr class="even"><td><ahref="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">GPT-4-0125-preview</a></td><td>1244</td><td></td><td></td><td>OpenAI</td><td>Proprietary</td></tr><tr class="odd"><td><a href="https://www.01.ai/">Yi-Large-preview</a></td><td>1236</td><td></td><td></td><td>01 AI</td><td>Proprietary</td></tr><tr class="even"><td><a href="https://bard.google.com/">Bard (Gemini Pro)</a></td><td>1208</td><td></td><td></td><td>Google</td><td>Proprietary</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>大模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>排行榜</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从头开始实现llama3</title>
    <link href="/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama3.html"/>
    <url>/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama3.html</url>
    
    <content type="html"><![CDATA[<p>都说大模型是黑箱玄学，这次让我们打开黑箱，一起来探索它内部的世界。</p><span id="more"></span><p>在这个文件中，我从头开始实现了llama3，一次一个张量和矩阵乘法。另外，我将直接从Meta为 llama3提供的模型文件加载张量，您需要在运行此文件之前下载权重。这是下载权重的官方链接：<ahref="https://llama.meta.com/llama-downloads/">https://llama.meta.com/llama-downloads/</a></p><figure><imgsrc="/images/llama3实现/v2-513855262cb2170c7aa8d1db7e5260ed_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h1 id="分词器tokenizer">1.分词器（tokenizer）</h1><p>我不会实现 bpe tokenizer（但 andrej karpathy 有一个非常干净的实现）他的实现链接：[https://github.com/karpathy/minbpe)</p><figure><img src="/images/llama3实现/karpathyminbpe.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs text">from pathlib import Path<br>import tiktoken<br>from tiktoken.load import load_tiktoken_bpe<br>import torch<br>import json<br>import matplotlib.pyplot as plt<br><br>tokenizer_path = &quot;Meta-Llama-3-8B/tokenizer.model&quot;<br>special_tokens = [<br>            &quot;&lt;|begin_of_text|&gt;&quot;,<br>            &quot;&lt;|end_of_text|&gt;&quot;,<br>            &quot;&lt;|reserved_special_token_0|&gt;&quot;,<br>            &quot;&lt;|reserved_special_token_1|&gt;&quot;,<br>            &quot;&lt;|reserved_special_token_2|&gt;&quot;,<br>            &quot;&lt;|reserved_special_token_3|&gt;&quot;,<br>            &quot;&lt;|start_header_id|&gt;&quot;,<br>            &quot;&lt;|end_header_id|&gt;&quot;,<br>            &quot;&lt;|reserved_special_token_4|&gt;&quot;,<br>            &quot;&lt;|eot_id|&gt;&quot;,  # end of turn<br>        ] + [f&quot;&lt;|reserved_special_token_&#123;i&#125;|&gt;&quot; for i in range(5, 256 - 5)]<br>mergeable_ranks = load_tiktoken_bpe(tokenizer_path)<br>tokenizer = tiktoken.Encoding(<br>    name=Path(tokenizer_path).name,<br>    pat_str=r&quot;(?i:&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d)|[^\r\n\p&#123;L&#125;\p&#123;N&#125;]?\p&#123;L&#125;+|\p&#123;N&#125;&#123;1,3&#125;| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;,<br>    mergeable_ranks=mergeable_ranks,<br>    special_tokens=&#123;token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)&#125;,<br>)<br><br>tokenizer.decode(tokenizer.encode(&quot;hello world!&quot;))<br>&#x27;hello world!&#x27;<br></code></pre></td></tr></table></figure><h1 id="读取模型文件">2.读取模型文件</h1><p>通常，阅读本文取决于模型类的编写方式以及其中的变量名称。但由于我们是从头开始实现 llama3，因此我们将一次读取一个张量文件。</p><figure><imgsrc="/images/llama3实现/v2-c237a044abe6bbdc2556cbe7acf044b3_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs text">model = torch.load(&quot;Meta-Llama-3-8B/consolidated.00.pth&quot;)<br>print(json.dumps(list(model.keys())[:20], indent=4))<br>[<br>    &quot;tok_embeddings.weight&quot;,<br>    &quot;layers.0.attention.wq.weight&quot;,<br>    &quot;layers.0.attention.wk.weight&quot;,<br>    &quot;layers.0.attention.wv.weight&quot;,<br>    &quot;layers.0.attention.wo.weight&quot;,<br>    &quot;layers.0.feed_forward.w1.weight&quot;,<br>    &quot;layers.0.feed_forward.w3.weight&quot;,<br>    &quot;layers.0.feed_forward.w2.weight&quot;,<br>    &quot;layers.0.attention_norm.weight&quot;,<br>    &quot;layers.0.ffn_norm.weight&quot;,<br>    &quot;layers.1.attention.wq.weight&quot;,<br>    &quot;layers.1.attention.wk.weight&quot;,<br>    &quot;layers.1.attention.wv.weight&quot;,<br>    &quot;layers.1.attention.wo.weight&quot;,<br>    &quot;layers.1.feed_forward.w1.weight&quot;,<br>    &quot;layers.1.feed_forward.w3.weight&quot;,<br>    &quot;layers.1.feed_forward.w2.weight&quot;,<br>    &quot;layers.1.attention_norm.weight&quot;,<br>    &quot;layers.1.ffn_norm.weight&quot;,<br>    &quot;layers.2.attention.wq.weight&quot;<br>]<br>with open(&quot;Meta-Llama-3-8B/params.json&quot;, &quot;r&quot;) as f:<br>    config = json.load(f)<br>config<br>&#123;&#x27;dim&#x27;: 4096,<br> &#x27;n_layers&#x27;: 32,<br> &#x27;n_heads&#x27;: 32,<br> &#x27;n_kv_heads&#x27;: 8,<br> &#x27;vocab_size&#x27;: 128256,<br> &#x27;multiple_of&#x27;: 1024,<br> &#x27;ffn_dim_multiplier&#x27;: 1.3,<br> &#x27;norm_eps&#x27;: 1e-05,<br> &#x27;rope_theta&#x27;: 500000.0&#125;<br></code></pre></td></tr></table></figure><h2id="我们使用此配置来推断有关模型的详细信息例如">我们使用此配置来推断有关模型的详细信息，例如</h2><ol type="1"><li>该模型有 32 个transformer layers</li><li>每个多头注意力块有 32 个头</li><li>词汇大小等等</li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs text">dim = config[&quot;dim&quot;]<br>n_layers = config[&quot;n_layers&quot;]<br>n_heads = config[&quot;n_heads&quot;]<br>n_kv_heads = config[&quot;n_kv_heads&quot;]<br>vocab_size = config[&quot;vocab_size&quot;]<br>multiple_of = config[&quot;multiple_of&quot;]<br>ffn_dim_multiplier = config[&quot;ffn_dim_multiplier&quot;]<br>norm_eps = config[&quot;norm_eps&quot;]<br>rope_theta = torch.tensor(config[&quot;rope_theta&quot;])<br></code></pre></td></tr></table></figure><h2 id="将文本转换为标记tokens">将文本转换为标记（tokens）</h2><p>这里我们使用 tiktoken （我认为是一个 openai 库）作为 tokenizer</p><figure><imgsrc="/images/llama3实现/v2-1acdee68e45fc5503592b79e34c18258_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs text">prompt = &quot;the answer to the ultimate question of life, the universe, and everything is &quot;<br>tokens = [128000] + tokenizer.encode(prompt)<br>print(tokens)<br>tokens = torch.tensor(tokens)<br>prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens]<br>print(prompt_split_as_tokens)<br>[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]<br>[&#x27;&lt;|begin_of_text|&gt;&#x27;, &#x27;the&#x27;, &#x27; answer&#x27;, &#x27; to&#x27;, &#x27; the&#x27;, &#x27; ultimate&#x27;, &#x27; question&#x27;, &#x27; of&#x27;, &#x27; life&#x27;, &#x27;,&#x27;, &#x27; the&#x27;, &#x27; universe&#x27;, &#x27;,&#x27;, &#x27; and&#x27;, &#x27; everything&#x27;, &#x27; is&#x27;, &#x27; &#x27;]<br></code></pre></td></tr></table></figure><h2 id="将标记转换为嵌入embedding">将标记转换为嵌入（embedding）</h2><p>抱歉，但无论如何，这是代码库中我使用内置神经网络模块的唯一部分，因此我们的[17x1] 标记现在是 [17x4096]，即长度为 4096 的 17 个嵌入（每个标记一个）注意：跟踪形状，它让你更容易理解一切</p><figure><imgsrc="/images/llama3实现/v2-a4436330430517444590607a5af4bfcf_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs text">embedding_layer = torch.nn.Embedding(vocab_size, dim)<br>embedding_layer.weight.data.copy_(model[&quot;tok_embeddings.weight&quot;])<br>token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)<br>token_embeddings_unnormalized.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2 id="然后我们使用-rms-归一化对嵌入进行归一化">然后我们使用 rms归一化对嵌入进行归一化</h2><p>请注意，在这一步之后，形状不会改变，这些值只是需要记住的标准化内容，我们需要一个norm_eps（来自配置），因为我们不想意外地将rms设置为0并除以0，这里是公式：</p><figure><imgsrc="/images/llama3实现/v2-645012127903f431f8f9f5f9dd506e66_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs text"># def rms_norm(tensor, norm_weights):<br>#     rms = (tensor.pow(2).mean(-1, keepdim=True) + norm_eps)**0.5<br>#     return tensor * (norm_weights / rms)<br>def rms_norm(tensor, norm_weights):<br>    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights<br></code></pre></td></tr></table></figure><h1 id="构建transformer的第一层">3.构建transformer的第一层</h1><h2 id="正常化">正常化</h2><p>无论如何，你会看到我从模型字典访问layer.0（这是第一层），所以在标准化之后我们的形状仍然[17x4096]与嵌入相同但标准化</p><figure><imgsrc="/images/llama3实现/v2-979811529314dc783cb499cf2ca93ece_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">token_embeddings = rms_norm(token_embeddings_unnormalized, model[&quot;layers.0.attention_norm.weight&quot;])<br>token_embeddings.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2 id="从头开始实施注意力">从头开始实施注意力</h2><p>让我们加载transformer第一层的注意力头</p><figure><imgsrc="/images/llama3实现/v2-3a86ff1392e4ff420bdd9e65b3ce4d6d_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>&gt; 当我们从模型加载查询、键、值和输出向量时，我们注意到形状为[4096x4096]、[1024x4096]、[1024x4096]、[4096x4096] &gt;乍一看这很奇怪，因为理想情况下我们想要每个 q ,k,v 和 o 分别代表每个头&gt;代码的作者将它们捆绑在一起，因为它很容易，有助于并行化注意力头乘法。&gt; 我要打开所有东西...</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs text">print(<br>    model[&quot;layers.0.attention.wq.weight&quot;].shape,<br>    model[&quot;layers.0.attention.wk.weight&quot;].shape,<br>    model[&quot;layers.0.attention.wv.weight&quot;].shape,<br>    model[&quot;layers.0.attention.wo.weight&quot;].shape<br>)<br>torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])<br></code></pre></td></tr></table></figure><h2 id="展开查询">展开查询</h2><p>在下一节中，我们将从多个注意力头中解开查询，生成的形状为[32x128x4096]，其中 32 是 llama3 中注意力头的数量，128是查询向量的大小，4096 是令牌嵌入的大小</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs text">q_layer0 = model[&quot;layers.0.attention.wq.weight&quot;]<br>head_dim = q_layer0.shape[0] // n_heads<br>q_layer0 = q_layer0.view(n_heads, head_dim, dim)<br>q_layer0.shape<br>torch.Size([32, 128, 4096])<br></code></pre></td></tr></table></figure><h2 id="我要实现第一层的第一个头">我要实现第一层的第一个头</h2><p>这里我访问第一层的查询权重矩阵第一个头，这个查询权重矩阵的大小是[128x4096]</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">q_layer0_head0 = q_layer0[0]<br>q_layer0_head0.shape<br>torch.Size([128, 4096])<br></code></pre></td></tr></table></figure><h2id="我们现在将查询权重与令牌嵌入相乘以接收令牌的查询">我们现在将查询权重与令牌嵌入相乘，以接收令牌的查询</h2><p>在这里你可以看到结果的形状是 [17x128]，这是因为我们有 17个标记，每个标记都有一个 128 长度的查询。</p><figure><imgsrc="/images/llama3实现/v2-b946f4191582804b9e03e0b3c2f0003d_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)<br>q_per_token.shape<br>torch.Size([17, 128])<br></code></pre></td></tr></table></figure><h2 id="定位编码">定位编码</h2><p>我们现在处于这样一个阶段：提示中的每个标记都有一个查询向量，但如果你仔细想想——单独的查询向量不知道提示中的位置。查询：“生命、宇宙和一切的终极问题的答案是”在我们的提示中我们已经使用了“the”三次，我们需要所有 3个“the”标记的查询向量具有不同的查询向量（每个大小[1x128]）基于它们在查询中的位置。我们使用RoPE（旋转位置嵌入）执行这些旋转。</p><h2 id="rope">RoPE</h2><p>观看此视频（这就是我观看的）以理解数学。 <ahref="https://www.youtube.com/watch%3Fv%3Do29P0Kpobz0%26t%3D530s">https://www.youtube.com/watch?v=o29P0Kpobz0&amp;t=530s</a></p><figure><imgsrc="/images/llama3实现/v2-f49af1e8f64b0ce2eb5961232523607b_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)<br>q_per_token_split_into_pairs.shape<br>torch.Size([17, 64, 2])<br></code></pre></td></tr></table></figure><p>在上面的步骤中，我们将查询向量分成对，我们对每对应用旋转角度偏移！我们现在有一个大小为 [17x64x2] 的向量，这是针对提示中的每个标记将 128个长度的查询分为 64 对！这 64 对中的每一对都将旋转 m*(theta)，其中 m是我们旋转查询的标记的位置！</p><figure><imgsrc="/images/llama3实现/v2-79322d3dcc6c412772c7389e8c4ed8b9_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="使用复数的点积来旋转向量">使用复数的点积来旋转向量</h2><figure><imgsrc="/images/llama3实现/v2-58f0207e09f6c337fd2177a8e109a02c_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs text">zero_to_one_split_into_64_parts = torch.tensor(range(64))/64<br>zero_to_one_split_into_64_parts<br>tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,<br>        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,<br>        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,<br>        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,<br>        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,<br>        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,<br>        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,<br>        0.9844])<br>freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)<br>freqs<br>tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,<br>        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,<br>        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,<br>        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,<br>        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,<br>        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,<br>        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,<br>        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,<br>        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,<br>        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,<br>        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])<br>freqs_for_each_token = torch.outer(torch.arange(17), freqs)<br>freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)<br>freqs_cis.shape<br><br># viewing tjhe third row of freqs_cis<br>value = freqs_cis[3]<br>plt.figure()<br>for i, element in enumerate(value[:17]):<br>    plt.plot([0, element.real], [0, element.imag], color=&#x27;blue&#x27;, linewidth=1, label=f&quot;Index: &#123;i&#125;&quot;)<br>    plt.annotate(f&quot;&#123;i&#125;&quot;, xy=(element.real, element.imag), color=&#x27;red&#x27;)<br>plt.xlabel(&#x27;Real&#x27;)<br>plt.ylabel(&#x27;Imaginary&#x27;)<br>plt.title(&#x27;Plot of one row of freqs_cis&#x27;)<br>plt.show()<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-3b27398a4bea72f1f64f3115faf516ae_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2id="现在我们对每个标记token的查询元素都有一个复数角度变化向量">现在我们对每个标记（token）的查询元素都有一个复数（角度变化向量）</h2><p>我们可以将查询（我们分成对的查询）转换为复数，然后进行点积以根据位置诚实旋转查询，这想想就很美好:)</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs text">q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)<br>q_per_token_as_complex_numbers.shape<br>torch.Size([17, 64])<br>q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis<br>q_per_token_as_complex_numbers_rotated.shape<br>torch.Size([17, 64])<br></code></pre></td></tr></table></figure><h2 id="得到旋转向量后">得到旋转向量后</h2><p>我们可以通过再次将复数视为实数来返回成对的查询</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)<br>q_per_token_split_into_pairs_rotated.shape<br>torch.Size([17, 64, 2])<br></code></pre></td></tr></table></figure><p>旋转的对现在被合并，我们现在有一个新的查询向量（旋转查询向量），其形状为[17x128]，其中 17 是标记的数量，128 是查询向量的暗度</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)<br>q_per_token_rotated.shape<br>torch.Size([17, 128])<br></code></pre></td></tr></table></figure><h2 id="键几乎与查询相同">键（几乎与查询相同）</h2><figure><imgsrc="/images/llama3实现/v2-c997910e27f5cebae65b38a6b46d5b85_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>我太懒了，所以我不会对键进行数学计算，你需要记住的唯一事情是： &gt;键也生成维度 128 的键向量 &gt; 键的权重数量只有1/4查询，这是因为键的权重一次在 4 个头之间共享，为了减少需要的计算数量&gt; 键也会旋转以添加位置信息，就像查询一样，原因相同</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs text">k_layer0 = model[&quot;layers.0.attention.wk.weight&quot;]<br>k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim)<br>k_layer0.shape<br>torch.Size([8, 128, 4096])<br>k_layer0_head0 = k_layer0[0]<br>k_layer0_head0.shape<br>torch.Size([128, 4096])<br>k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)<br>k_per_token.shape<br>torch.Size([17, 128])<br>k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)<br>k_per_token_split_into_pairs.shape<br>torch.Size([17, 64, 2])<br>k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)<br>k_per_token_as_complex_numbers.shape<br>torch.Size([17, 64])<br>k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)<br>k_per_token_split_into_pairs_rotated.shape<br>torch.Size([17, 64, 2])<br>k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)<br>k_per_token_rotated.shape<br>torch.Size([17, 128])<br></code></pre></td></tr></table></figure><h2id="在此阶段现在每个标记都有查询和键的旋转值">在此阶段，现在每个标记都有查询和键的旋转值。</h2><figure><imgsrc="/images/llama3实现/v2-b3bacaeb87eba8b665968d1c4e06ad28_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>现在每个查询和键的形状都是 [17x128]。</p><h1id="在下一步中我们将乘以查询和关键矩阵">4.在下一步中，我们将乘以查询和关键矩阵</h1><p>这样做将为我们提供一个将每个标记相互映射的分数，该分数描述了每个标记的查询与每个标记的密钥的相关程度。这是自我注意力:) 注意力分数矩阵 (qk_per_token) 的形状是 [17x17]，其中 17是提示中的标记数量</p><figure><imgsrc="/images/llama3实现/v2-e4b38700f8f38d052ddc34bb770077d3_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5<br>qk_per_token.shape<br>torch.Size([17, 17])<br></code></pre></td></tr></table></figure><h2 id="我们现在必须屏蔽查询关键分数">我们现在必须屏蔽查询关键分数</h2><p>在 llama3 的训练过程中，未来的 token qk分数被屏蔽。为什么？因为在训练期间我们只学习使用过去的标记来预测标记。因此，在推理过程中，我们将未来的标记设置为零。</p><figure><imgsrc="/images/llama3实现/v2-32b947acb627e83f1be3cfaf7bcea213_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs text">def display_qk_heatmap(qk_per_token):<br>    _, ax = plt.subplots()<br>    im = ax.imshow(qk_per_token.to(float).detach(), cmap=&#x27;viridis&#x27;)<br>    ax.set_xticks(range(len(prompt_split_as_tokens)))<br>    ax.set_yticks(range(len(prompt_split_as_tokens)))<br>    ax.set_xticklabels(prompt_split_as_tokens)<br>    ax.set_yticklabels(prompt_split_as_tokens)<br>    ax.figure.colorbar(im, ax=ax)<br>    <br>display_qk_heatmap(qk_per_token)<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-7200e3d7069abc5a550654c2e2c0635a_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs text">mask = torch.full((len(tokens), len(tokens)), float(&quot;-inf&quot;), device=tokens.device)<br>mask = torch.triu(mask, diagonal=1)<br>mask<br>tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],<br>        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])<br>qk_per_token_after_masking = qk_per_token + mask<br>display_qk_heatmap(qk_per_token_after_masking)<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-9d128697d492aac646b8458d83c59a86_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure><imgsrc="/images/llama3实现/v2-f023e1085ebbe8e6b882d54ca7a4e147_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)<br>display_qk_heatmap(qk_per_token_after_masking_after_softmax)<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-af163b0bde4ab6ddc644d5d30a3dea53_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="值values注意力几乎结束">值（values）（注意力几乎结束）</h2><figure><imgsrc="/images/llama3实现/v2-f636fe202c7a600487771fb278566cc9_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>这些分数（0-1）用于确定每个标记使用了多少值矩阵&gt;就像键一样，值权重也每4个注意力头共享（以节省计算）&gt;因此，值的形状下面的权重矩阵是[8x128x4096]</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs text">v_layer0 = model[&quot;layers.0.attention.wv.weight&quot;]<br>v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[0] // n_kv_heads, dim)<br>v_layer0.shape<br>torch.Size([8, 128, 4096])<br></code></pre></td></tr></table></figure><p>下面给出第一层第一头值权重矩阵</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">v_layer0_head0 = v_layer0[0]<br>v_layer0_head0.shape<br>torch.Size([128, 4096])<br></code></pre></td></tr></table></figure><h2 id="值向量value-vectors">值向量（value vectors）</h2><figure><imgsrc="/images/llama3实现/v2-4f9bf9fa8cb5ec2f7f338049e98df276_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>我们现在使用值权重来获取每个标记的注意力值，其大小为 [17x128]，其中17 是提示中标记的数量，128 是每个标记的值向量的暗度</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T)<br>v_per_token.shape<br>torch.Size([17, 128])<br></code></pre></td></tr></table></figure><h1 id="注意力">5.注意力</h1><figure><imgsrc="/images/llama3实现/v2-c022102b0ca593356099825ddc0cb312_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>与每个标记的值相乘后得到的注意力向量的形状为 [17*128]</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>qkv_attention.shape<br>torch.Size([17, 128])<br></code></pre></td></tr></table></figure><h2 id="多头注意力">多头注意力</h2><p>我们现在有了第一层和第一个头的注意力值，现在我将运行一个循环并执行与上面的单元完全相同的数学运算，但对于第一层中的每个头</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs text">qkv_attention_store = []<br><br>for head in range(n_heads):<br>    q_layer0_head = q_layer0[head]<br>    k_layer0_head = k_layer0[head//4] # key weights are shared across 4 heads<br>    v_layer0_head = v_layer0[head//4] # value weights are shared across 4 heads<br>    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T)<br>    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T)<br>    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T)<br><br>    q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)<br>    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)<br>    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis[:len(tokens)])<br>    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)<br><br>    k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)<br>    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)<br>    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis[:len(tokens)])<br>    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)<br><br>    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5<br>    mask = torch.full((len(tokens), len(tokens)), float(&quot;-inf&quot;), device=tokens.device)<br>    mask = torch.triu(mask, diagonal=1)<br>    qk_per_token_after_masking = qk_per_token + mask<br>    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)<br>    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>    qkv_attention_store.append(qkv_attention)<br><br>len(qkv_attention_store)<br>32<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-8031063609f976093e7b47ba068f359d_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>我们现在有了第一层所有 32 个头的 qkv_attention矩阵，接下来我将把所有注意力分数合并到一个大小为 [17x4096] 的大矩阵中，我们即将结束:)</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)<br>stacked_qkv_attention.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h1 id="权重矩阵最后步骤之一">6.权重矩阵，最后步骤之一</h1><figure><imgsrc="/images/llama3实现/v2-076def42e5c789cd9e63170716ed7fea_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>对于第 0 层注意力要做的最后一件事是乘以</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">w_layer0 = model[&quot;layers.0.attention.wo.weight&quot;]<br>w_layer0.shape<br>torch.Size([4096, 4096])<br></code></pre></td></tr></table></figure><h2id="这是一个简单的线性层所以我们只需-matmul">这是一个简单的线性层，所以我们只需matmul</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">embedding_delta = torch.matmul(stacked_qkv_attention, w_layer0.T)<br>embedding_delta.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><figure><imgsrc="/images/llama3实现/v2-e3f7584e47bc523b3f1711d74d64534e_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>我们现在在注意力之后嵌入值发生了变化，这应该添加到原始令牌嵌入中</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">embedding_after_edit = token_embeddings_unnormalized + embedding_delta<br>embedding_after_edit.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h1id="我们进行标准化然后通过嵌入增量运行前馈神经网络">7.我们进行标准化，然后通过嵌入增量运行前馈神经网络</h1><figure><imgsrc="/images/llama3实现/v2-bab8f207f6096fe5f7f0b4908e20a202_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[&quot;layers.0.ffn_norm.weight&quot;])<br>embedding_after_edit_normalized.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2 id="加载-ff-权重并实现前馈网络">加载 ff 权重并实现前馈网络</h2><figure><imgsrc="/images/llama3实现/v2-c632ca45a493952c3ba94901629df5e5_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>在 llama3 中，他们使用了 SwiGLU前馈网络，这种网络架构非常擅长在模型需要时添加非线性。 如今在 llms中使用这种前馈网络架构是相当标准的</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs text">w1 = model[&quot;layers.0.feed_forward.w1.weight&quot;]<br>w2 = model[&quot;layers.0.feed_forward.w2.weight&quot;]<br>w3 = model[&quot;layers.0.feed_forward.w3.weight&quot;]<br>output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)<br>output_after_feedforward.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2id="我们终于在第一层之后为每个令牌有了新编辑的嵌入">我们终于在第一层之后为每个令牌有了新编辑的嵌入</h2><p>在我们完成之前还需要 31 层（一个 for 循环），您可以想象这个编辑后的嵌入包含有关第一层上提出的所有查询的信息，现在每一层都会对所提出的问题编码越来越复杂的查询，直到我们有一个嵌入知道我们需要的下一个令牌的所有信息。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">layer_0_embedding = embedding_after_edit+output_after_feedforward<br>layer_0_embedding.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2 id="天哪一切都同时发生">天哪，一切都同时发生</h2><figure><imgsrc="/images/llama3实现/v2-9abf6da09122332cf71f63526ec24dd9_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>是的，就是这样。我们之前为每一层所做的一切都是一次性完成的。</p><p>祝阅读愉快:)</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs text">final_embedding = token_embeddings_unnormalized<br>for layer in range(n_layers):<br>    qkv_attention_store = []<br>    layer_embedding_norm = rms_norm(final_embedding, model[f&quot;layers.&#123;layer&#125;.attention_norm.weight&quot;])<br>    q_layer = model[f&quot;layers.&#123;layer&#125;.attention.wq.weight&quot;]<br>    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)<br>    k_layer = model[f&quot;layers.&#123;layer&#125;.attention.wk.weight&quot;]<br>    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)<br>    v_layer = model[f&quot;layers.&#123;layer&#125;.attention.wv.weight&quot;]<br>    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)<br>    w_layer = model[f&quot;layers.&#123;layer&#125;.attention.wo.weight&quot;]<br>    for head in range(n_heads):<br>        q_layer_head = q_layer[head]<br>        k_layer_head = k_layer[head//4]<br>        v_layer_head = v_layer[head//4]<br>        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)<br>        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)<br>        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)<br>        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)<br>        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)<br>        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis)<br>        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)<br>        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)<br>        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)<br>        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)<br>        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)<br>        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5<br>        mask = torch.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float(&quot;-inf&quot;))<br>        mask = torch.triu(mask, diagonal=1)<br>        qk_per_token_after_masking = qk_per_token + mask<br>        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)<br>        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>        qkv_attention_store.append(qkv_attention)<br><br>    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)<br>    w_layer = model[f&quot;layers.&#123;layer&#125;.attention.wo.weight&quot;]<br>    embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T)<br>    embedding_after_edit = final_embedding + embedding_delta<br>    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f&quot;layers.&#123;layer&#125;.ffn_norm.weight&quot;])<br>    w1 = model[f&quot;layers.&#123;layer&#125;.feed_forward.w1.weight&quot;]<br>    w2 = model[f&quot;layers.&#123;layer&#125;.feed_forward.w2.weight&quot;]<br>    w3 = model[f&quot;layers.&#123;layer&#125;.feed_forward.w3.weight&quot;]<br>    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)<br>    final_embedding = embedding_after_edit+output_after_feedforward<br></code></pre></td></tr></table></figure><h1id="我们现在有了最终的嵌入模型可以对下一个标记做出的最佳猜测">8.我们现在有了最终的嵌入，模型可以对下一个标记做出的最佳猜测</h1><p>嵌入的形状与常规令牌嵌入 [17x4096] 相同，其中 17 是令牌数量，4096是嵌入暗淡</p><figure><imgsrc="/images/llama3实现/v2-5b3c042a93c2a044a82e2e64f84d3065_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">final_embedding = rms_norm(final_embedding, model[&quot;norm.weight&quot;])<br>final_embedding.shape<br>torch.Size([17, 4096])<br></code></pre></td></tr></table></figure><h2id="最后让我们将嵌入解码到令牌值中">最后，让我们将嵌入解码到令牌值中</h2><figure><imgsrc="/images/llama3实现/v2-6e7276c4975ec431052164c8c49b946a_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>我们将使用输出解码器将最终的嵌入转换为令牌</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">model[&quot;output.weight&quot;].shape<br>torch.Size([128256, 4096])<br></code></pre></td></tr></table></figure><h2id="我们使用最后一个标记的嵌入来预测下一个值">我们使用最后一个标记的嵌入来预测下一个值</h2><p>希望在我们的例子中，42 :) 注意：42是“生命、宇宙和一切的终极问题的答案”的答案，根据《银河系漫游指南》一书，大多数现代llms 都会回答这里有 42，这应该验证我们的整个代码！祝我好运 ：）</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">logits = torch.matmul(final_embedding[-1], model[&quot;output.weight&quot;].T)<br>logits.shape<br>torch.Size([128256])<br></code></pre></td></tr></table></figure><h2id="模型预测令牌编号-2983-作为下一个令牌这是-42-的令牌编号吗">模型预测令牌编号2983 作为下一个令牌，这是 42 的令牌编号吗？</h2><p>我正在向您宣传，这是代码的最后一个单元格，希望您玩得开心:)</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">next_token = torch.argmax(logits, dim=-1)<br>next_token<br>tensor(2983)<br></code></pre></td></tr></table></figure><h2 id="lets-go">Let's go</h2><figure><imgsrc="/images/llama3实现/v2-e436800962c747e6de871c364891ae90_720w.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">tokenizer.decode([next_token.item()])<br>#输出&#x27;42&#x27;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>nlp</tag>
      
      <tag>原理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型训练Guidelines</title>
    <link href="/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83Guidelines.html"/>
    <url>/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83Guidelines.html</url>
    
    <content type="html"><![CDATA[<p>根据scaling law，模型越大，高质量数据越多，效果越好。</p><p>但还有一个很直观的情况，随着预训练样本的质量不断提升，训练手段的优化。新的模型，往往效果能轻松反超参数量两倍于它的模型。</p><span id="more"></span><h2 id="背景">1 背景</h2><p>根据scaling law，模型越大，高质量数据越多，效果越好。</p><p>但还有一个很直观的情况，随着预训练样本的质量不断提升，训练手段的优化。新的模型，往往效果能轻松反超参数量两倍于它的模型。</p><p>例如，最新出的minicpm，微信内部评测效果也是非常棒的。跟规模相对接近的2b、7b模型比，得分比qwen2b高，和qwen7b比有的高有的低。</p><p>这个是minicpm的详细技术文档。</p><p>[https://shengdinghu.notion.site/MiniCPM-c805a17c5c8046398914e47f0542095a]</p><p>这说明，现有参数量情况下，哪怕是2B尺度，也并没有得到充分训练。</p><h2 id="样本">2 样本</h2><h3 id="样本构成">2.1 样本构成</h3><p>大家已经达成一些基础的共识。</p><p>如中英混合比例大家都大差不差。</p><p>逻辑推理比较强的样本，像代码，数学。这种就是模型越大，混合的比例反而可以越高。</p><p>跟SFT是类似的，越大的模型，越聪明的模型，需要的SFT数据就越少。同理，越大的模型，越聪明，复杂样本混合比例就可以越高。</p><h3 id="样本质量">2.2 样本质量</h3><h3 id="基本清洗">2.1.1 基本清洗</h3><p>导致ppl崩掉的，都要清洗掉，政治敏感数据清洗，去重等，肯定是一个很长的pipeline。</p><p>大家比较一致的结论是，天工开源的那份预训练数据，是一个比较好的满足基础清洗要求的数据。</p><h3 id="进阶清洗">2.1.2 进阶清洗</h3><p>大家都不太方便展开，但可以透露的信息。</p><p>跟SFT一样，产出各种各样的label来刻画数据，有的公司实习生就优化几个label。</p><p>不过随着优化的往后拓展，这些label的投入产出比越来越难以评估。</p><h3 id="phi式的生成synthetic数据">2.1.3 PHI式的生成(synthetic)数据</h3><p>预训练清洗的pipeline搭建，对于开源团队，小公司来讲，成本其实还是蛮高的。</p><p>所以，基于开源数据，做一些聚类的topic。然后基于这些topic，丢到更大的模型，来构建一批高质量的数据，是一个反而比较低成本的方案。</p><h3 id="买数据">2.1.4 买数据</h3><p>嗯，这次大模型，除了李一舟。卖数据的公司，也是真的赚到钱了。</p><h3 id="不同训练阶段的训练样本">2.3 不同训练阶段的训练样本</h3><p>经过讨论，发现有三种方案。</p><h3 id="末期高质量样本minicpm">2.3.1 末期高质量样本（minicpm)</h3><p>快速收敛阶段和平稳阶段，都采用普通样本。</p><p>退火阶段，混入高质量样本来做教科书式的学习。</p><h3 id="初期高质量样本">2.3.2 初期高质量样本</h3><p>快速收敛阶段，以高质量样本为主，让模型快速收敛。</p><p>平稳阶段，逐步调整比例，增加更多的普通样本。</p><p>退火阶段，跟平稳阶段一致</p><h3 id="全程高质量样本phil方式">2.3.3全程高质量样本（PHIL方式）</h3><p>全程都是高质量样本</p><p><strong>这里大家讨论的蛮激烈的，有这么几点。</strong></p><p>第一，初期就加入高质量样本，初期收敛的更快。但高质量样本少，不断的重复学习高质量样本，会不会导致过拟合？但反方认为，人类的本质上就是复读机，特别对于小模型，不断的重复学习，好像也没太大问题。</p><p>第二，初期学习高质量样本，会不会导致初期模型的初始化，局限在特定的区域，后面的普通样本学了之后，也不一定能很好的跳出来，会不会影响泛化？但反方认为，末期加入高质量样本，反而会不会因为最后加入高质量样本，导致泛化能力受损，集中在高质量样本的领域？</p><p>第三，PHIL方式，大家很快达成一致，PHIL就是探索小模型能不能在特定领域达到SOTA。好处，特定榜单/领域效果会特别好。坏处，模型泛化能力会很差（但PHIL从来没说要做世界模型。</p><h3 id="小模型样本的极限在哪">2.4 小模型样本的极限在哪？</h3><p>到底喂了多少tokens，小模型参数才算是充分得到训练？</p><p>当天讨论，并没有一个很好的结论。</p><p>最近YI-9B的公开技术文档，做了一个有趣的尝试。把每层的输入和输出算cos，来评估模型是否训练的非常充分。</p><p>但内部讨论后，发现这种尝试有一个巨大的遗漏点。</p><p>前段时间，我们做longcontext调研，也是把每层也都单独做了一个分析。结论是，如果模型深度足够的话，有些层其实区分度是在降低的，相当于几层做了一层做的事情。</p><p>以及，另外一个可能，小模型每一层cos都小，有可能每一层在干不同的事，或者每一层都会注意到新的东西。大模型某些层cos大，有可能是因为句子太简单，大模型对结果更加肯定，靠后的层的功能没有被激活。</p><p>感觉这种评估方式，仍旧有一定的优化空间，也期待业内能公开更多好用的评估方式。</p><h2 id="训练">3 训练</h2><h3 id="tokenizer">3.1 tokenizer</h3><p>小模型过大的tokenizer的确是一种浪费。很多小模型有大tokenizer，一个潜在的可能性，作者人力不足，直接是把大模型的tokenizer拿来复用了。</p><h3 id="阶段">3.2 阶段</h3><p>现在大家预训练分成三个阶段。</p><p>快速收敛阶段，稳定阶段，<strong>退火阶段(minicpm比较显式的提出这个阶段）</strong></p><h3 id="为什么要分阶段">3.2.1 为什么要分阶段</h3><p>这个阶段来自于大家对loss曲线的观察，发现loss曲线的收敛就是这么一个特点。</p><p>然后，大家发现不同的loss曲线阶段，做一些针对性样本和参数的调整，能带来更好的效果，于是就这么分了。</p><h3 id="不同阶段学的是什么东西">3.2.2 不同阶段学的是什么东西？</h3><p>首先，我们现在的评估手段还是比较粗糙的，假如有了更细的评估手段，可能能观测到更多有趣的东西。</p><p>例如之前俊林做过关于涌现的分享，涌现从指标观测来看，就是突然出现的。但当把指标细化后，可以发现这个涌现好像也没那么突然，这个可以参考<ahref="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2310.03262">https://arxiv.org/abs/2310.03262</a>把原本离散的准确率在1e-5级别的时候的值估计出来了。</p><p>但反方这里又有不同的观点，我们用物理学的一个理论来解释涌现。</p><p>我们可以把涌现替换成相变来聊一聊它和指标突变的辩证关系：当我们谈论相变时，我们指的是物质从一种状态转变为另一种状态的过程，比如水从液态变成固态的过程（冰冻）或者从液态变成气态的过程（蒸发）。而指标突变通常指的是某种性质或者物理量在某个条件下突然发生明显的变化，比如在某个温度或者压力下，某种物质的导电性、磁性或者其他性质突然发生变化。</p><p>相变与指标突变之间存在着密切的关系，因为相变往往伴随着物质性质的突变。当物质经历相变时，它的一些性质会突然改变，这些性质就是我们所说的指标。举个例子，当水温降到0摄氏度以下时，水会由液态变成固态，这就是相变，同时水的密度也会突然增加，导致它的体积变小，这就是指标突变。</p><p>虽然相变往往伴随着物质性质的指标突破，但是不意味着不突变就不是相变，指标的突变不是相变的重点，相变的重点在于从一个状态/性质，变成另外一个状态/性质，这两种状态有着很不一样的差别。</p><p>尽管可以使用一些技巧方法来构造一些看起来特别平滑的指标来反对大模型涌现这个词汇，但是不可否认的事实是，在不同的尺寸变化或者数据量、计算量变化之后，人们可以非常明显地感知到大模型表现的巨大差异，这就是一个相变的结果，就像是炼制一门18连环刃的法器，从第一把的炼制到第18把，从个数的指标上来说是非常平滑的，但是从威力上来说，18把可以构建一个法阵，极大地增加了武器的威力，与之前不可同日而语。</p><h3 id="batch-size">3.3 batch size</h3><p>老调重弹的问题。</p><p>2020年，transformer出来后，当时大家就碰到这么一个问题。模型太大了，用尽可能能塞进内存的batchsize去train模型，来提升速度。</p><p>很快，大家发现batch size有个trade off。</p><p>batchsize过小，波动会比较大，不太容易收敛。但这种波峰，也有助于跳出局部最优，模型更容易有更好的泛化能力。</p><p>batchsize变大，步数整体变少，训练的步数更少，本来就波动就小，步数也少，同样本的情况下，你收敛的会更慢。</p><p>2020年其实有人就研究，如何用大batchsize，更快的训练的同时，也能收敛的更好。一个解决思路是优化了优化器，例如谷歌当年出的LAMB，就把batchsize从512扩充到了6w，收敛效果也不错。</p><h3 id="lr-scheduler">3.4 LR scheduler</h3><p>机器学习的目标，都是为了收敛loss，让学习的target和我们预测的target的loss尽可能低。</p><p>学习的过程，就是基于样本，分批（batchsize）丢进去。根据过去，现在学习的效果，来决定参数更新的方向和大小。</p><p>batch size这里是很清晰的。比较纠结的点是，优化器和LRscheduler这俩好像边界不是很清晰。</p><h3 id="lr-scheduler是什么">3.4.1 LR scheduler是什么</h3><p>假如我们要下山，山脚就是我们的目标，learningrate就是我们每一步要走多远。如果速度太快，可能开到山脚后，发现刹不住车，还会往山上多开一会，于是这样反复在目标处来回震荡。如果太小的话，到山脚的速度又太慢了。</p><p>现在主流的就是cosine，初期warmup learning rate线性增长，然后learningrate就是以余弦函数的周期方式周期性变换。</p><h3 id="优化器做什么">3.4.2 优化器做什么？</h3><p>优化器核心要解决的问题，初期怎么更好的学，那些地方要加速学，那些地方容易陷入局部最优，要如何跳出来。</p><p>现在的主流做法都是基于历史的反馈。</p><p>类似于爬山，某个地方你发现爬的很慢，那么就加下油门。有的地方你发现是下坡路，跑的贼快，那就就要松下油门，免得油门太快，直接从主路跑出去了。</p><p>从momentum，到adagrad，再到adam，这两年还有人在各种折腾。</p><h3 id="优化器和lr-scheduler如何协同工作">3.4.3 优化器和LRscheduler如何协同工作？</h3><p>问题就来了，LR scheduler决定了learningrate的大小。优化器也会根据历史情况来自动调整。</p><p>这俩会不会冲突？</p><p>优化器的优点刚刚说了，但它的缺点就是无论优化器怎么说的高大上，它本质上还规则，是大家基于调参经验，或者一些假设，定的规则。</p><p>规则就很难完美适配所有任务，事实上2020年左右，大家就发现不同的任务上不同的优化器效果是不同的。例如当年的炼丹经验，计算机视觉优先使用SGD(withMomentum)，NLP（特别是用Transformer）优先使用Adam，现在CV都上transformer了，那么就又切到了AdamW。</p><p>除此之外，还有一个learning ratedecay的问题，但这个偏经验，并不一定完全solid！</p><p>用CIFAR或者ImageNet跑一跑常见的模型，就会发现，最后如果不把learningrate降低下去，loss很难收敛到一个很小的数字。</p><p>SGD和Adam的收敛性证明也都是要求learningrate最后会降到足够低的。但自适应优化器的学习率不会在训练中自动降到很低。</p><p>现在大模型预训练，大家其实最关注的就是这个loss的收敛效果。</p><p>这个时候，LRschedule的出现就是一个比较好的补充，能够补足优化器的一些问题。</p><p>所以，你可以理解为，现在我们没有一个完美的油门，所以搞了俩油门，互相辅助。</p><p>优化器是个老司机的油门，好用，但人类的经验是有局限性的，很容易陷入局部最优跑不出来。</p><p>LR schedule像是一个全局的油门，定期更新，帮助老司机跳出局部最优。</p><h3 id="w-s-d的讨论和优化方案">3.4.4 W-S-D的讨论和优化方案</h3><p>minicpm提出了W-S-D LR scheduler，但stable阶段高learningrate，相当于把调节油门的压力全给到优化器了。</p><p>但S-D的确也有很多好处，例如我想train到什么时候就train到什么时候。</p><p>这里提出了一个解决思路，W-S-D是不是可以改成，warm-cosine-stable-decay，cosine占据训练阶段大头，甚至多个余弦波段，余弦波段多了，如上文所说，是不是能更好的跳出局部最优？</p><p>快要结束训练的时候，把cosine的learningrate给升上去，走一段stable+decay。</p><h3 id="退火加sft-和面">3.5 退火加sft &amp;“和面”</h3><p>前段时间，业界流行一个说法，你发现某块效果差，在预训练最后阶段补充一些类似的数据，效果就会蹭蹭的往上涨。</p><p>简称，和面——水多了加面，面多了加水。</p><p>刚开始，大家都很鄙视这种行为，觉得这种行为不就是刷榜么？</p><p>但现在我们来探讨这块的合理性，minicpm可以算是更进一步的“作弊”了，如果按照之前的观点。他都直接把sft数据混入了预训练数据里面，更加明目张胆的“刷榜”。</p><p>我个人觉得这里可以用两个角度去理解:</p><p>角度一，模型学习的训练动态角度，在退火的时候loss下降较stable和正常的cosine都要快，证明这里的学习效率在提升(最聪明的时候?)，而此时在这一刻使用高质量数据来喂给模型,可以尽可能发挥高质量数据的作用;</p><p>角度二， SFT数据较正常的文本数据，我猜测会更偏向于benchmark一些，因为SFT很多都是"QA型"结构的数据,对于模型认识bechmark有一定的改善。</p><p>之前预训练完毕后，直接上SFT数据，语料分布差距很大，其实天然是不太好的。这种作弊的行为，只要控制样本比例得当，反而能保证后面通用对其的一致性。</p><h2 id="再看scaling-law"><strong>4 再看scaling law</strong></h2><p>随着一些common sense的建立，scalinglaw的指导意义的确是在不断下降的。</p><p>举个例子，假如我有300张卡，我要train多大的模型？</p><p>计算方式，往往就变成，我大致知道训练1T-2Ttokens效果往往就不错了，这个模型两个月后我想赶一个发布会。那么就来反推，1T的tokens训练2个月，300张卡能train多大的。</p><p>但我们回到2020年，当大部分人都在基于bert做各种魔改的时候。</p><p>OpenAI发现了这么一个规律。数据，训练，参数一直增长下去，好像loss的确是在不断的下降哎？</p><p>于是，他们拿着这个paper去问微软的CTO，你想不想看看这个loss下降到一定程度会发生什么？</p><p>会发生什么？</p><p>chatgpt就出来了</p><blockquote><p>转载自:<ahref="https://zhuanlan.zhihu.com/p/686664720">如何从零开始训练大模型（minicpm分享&amp;讨论）- 知乎 (zhihu.com)</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>大模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>模型训练</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI行业技能点含金量统计分析</title>
    <link href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E7%9A%84%E4%B8%8D%E5%90%8C%E6%8A%80%E8%83%BD%E6%A0%91%E5%90%AB%E9%87%91%E9%87%8F%E5%88%86%E6%9E%90.html"/>
    <url>/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E7%9A%84%E4%B8%8D%E5%90%8C%E6%8A%80%E8%83%BD%E6%A0%91%E5%90%AB%E9%87%91%E9%87%8F%E5%88%86%E6%9E%90.html</url>
    
    <content type="html"><![CDATA[<p>在人工智能领域，有很多热议的技能选择，如CV还是NLP，TensorFlow还是PyTorch，python还是C++……<br />话题有争议，但是数据非常客观，让我们拭目以待吧~ <span id="more"></span></p><h3 id="数据说明">1.数据说明</h3><p><strong>数据来源：</strong>为BOSS直聘北京地区算法工程师相关岗位,数据时间是2024年五月,有效样本总量为2200个.<br /><strong>数据内容：</strong>招聘标题,薪资,福利待遇,关键词,岗位描述,公司,地址,招聘网址<br /><strong>统计方法：</strong>对样本数据通过相关的岗位关键词进行筛选,并统计筛选结果的薪资均值,岗位数量等信息,汇总为下表。</p><h3 id="ai技能点含金量排名">2.AI技能点含金量排名</h3><div id="container0" style="height: 500px;"></div><script type="text/javascript" src="https://registry.npmmirror.com/echarts/5.5.0/files/dist/echarts.min.js"></script><script type="text/javascript">var dom = document.getElementById('container0');var myChart = echarts.init(dom, null, {  renderer: 'canvas',  useDirtyRect: false});var app = {};var option;option = {  xAxis: {  type: 'category',  data: [    "视觉|cv|视频|图像",    "NLP|自然语言|LLM",    "多模态|大模型",    "机器学习",    "深度学习",    "软件开发",    "控制算法",    "推荐算法",    "数据挖掘|数据分析",    "地图|路径算法"  ],  axisLabel: {    rotate: 45, // 旋转标签以适应显示    interval: 0 // 设置为0表示显示全部标签  }  },  yAxis: {  type: 'value'  },  series: [  {    data: [378.43, 414.87, 439.42, 396.63, 395.89, 359.30, 379.71, 435.72, 358.50, 415.83],    type: 'bar',    showBackground: true,    backgroundStyle: {      color: 'rgba(180, 180, 180, 0.2)'    },  label: {      show: true, // 开启数据标签显示      position: 'top', // 数据标签的位置，这里是顶部      formatter: '{c}' // 格式化函数，这里使用默认的'{c}'表示显示数值    }}  ]  };if (option && typeof option === 'object') {  myChart.setOption(option);}window.addEventListener('resize', myChart.resize);</script><p><strong>多模态大模型</strong>以均值439k的平均年薪拔得头筹，<strong>推荐算法</strong>以436k紧随其后，<strong>NLP自然语言处理与地图路径算法工程师</strong>以415K的薪资并列第三。说起薪资较低的，分别是<strong>AI软件开发岗、数据挖掘分析岗</strong>，将近360K，也是难能可贵了。</p><h3id="ai技能点的需求量与含金量分布">3.AI技能点的需求量与含金量分布</h3><div id="container" style="height: 500px;"></div><script type="text/javascript" src="https://registry.npmmirror.com/echarts/5.5.0/files/dist/echarts.min.js"></script><script type="text/javascript">  var dom = document.getElementById('container');  var myChart = echarts.init(dom, null, {    renderer: 'canvas',    useDirtyRect: false  });  var app = {};  var option;  option = {    dataset: {      source: [        ['平均年薪(k)', '岗位数量', '职业技能'],        [378.43, 708, "视觉|cv|视频|图像"],        [414.87, 350, "NLP|自然语言|LLM"],        [439.42, 545, "多模态|大模型"],        [396.63, 691, "机器学习"],        [395.89, 785, "深度学习"],        [359.3, 154, "软件开发"],        [379.71, 98, "控制算法"],        [435.72, 171, "推荐算法"],        [358.5, 100, "数据挖掘|数据分析"],        [415.83, 36, "地图|路径算法"],        [384.27, 1032, "python"],        [396.68, 1225, "C++"],        [384.61, 206, "TensorFlow"],        [390.4, 193, "PyTorch"],      ]    },    grid: { containLabel: true },    xAxis: { name: '岗位数量' },    yAxis: { type: 'category' },    visualMap: {      orient: 'horizontal',      left: 'center',      min: 350,      max: 450,      text: ['High 平均年薪(k)', 'Low 平均年薪(k)'],      // Map the 平均年薪(k) column to color      dimension: 0,      inRange: {        color: ['#65B581', '#FFCE34', '#FD665F']      }    },    series: [      {        type: 'bar',        encode: {          // Map the "岗位数量" column to X axis.          x: '岗位数量',          // Map the "职业技能" column to Y axis          y: '职业技能'        }      }    ]  };  if (option && typeof option === 'object') {    myChart.setOption(option);  }  window.addEventListener('resize', myChart.resize);</script><p><strong>计算机视觉CV与自然语言NLP</strong>：视觉（CV）与图像、视频打交道，年薪约378k，岗位多；而自然语言处理（NLP）年薪诱人达415k，但岗位少。想多赚钱选NLP，想稳就业选CV。</p><p><strong>多模态与大模型</strong>：新兴的多模态与大模型领域，年薪高达439k，岗位也不少。想站风口就选它！</p><p><strong>机器学习与深度学习</strong>：机器学习年薪约397k，岗位稳定；深度学习略低但需求多。两者薪资相近，看需求选。</p><p><strong>软件开发与控制算法</strong>：软件开发年薪359k但岗位少；控制算法稍好，年薪380k。两者传统但重要。</p><p><strong>推荐算法与数据挖掘</strong>：推荐算法年薪高达436k但岗位少；数据挖掘年薪359k。喜欢数据处理就选它们。</p><p><strong>地图与路径算法</strong>：小众但高薪的地图与路径算法，年薪416k但竞争大。适合专长者挑战。</p><p><strong>编程语言：Python与C++</strong>Python年薪384k岗位多，C++年薪略高且岗位更多。两者都是AI开发利器。</p><p><strong>框架选择：TensorFlow与PyTorch</strong>TensorFlow年薪385k，PyTorch年薪390k。两者差距小，选谁看心情和项目需求。</p><h3id="技能点与教育程度工作经验及薪资的关系">4.技能点与教育程度，工作经验及薪资的关系</h3><h4id="技能点和教育程度对薪资的影响单位k">4.1技能点和教育程度对薪资的影响（单位：K）</h4><table><thead><tr class="header"><th></th><th>专科</th><th>本科</th><th>985/211</th><th>硕士</th><th>博士</th></tr></thead><tbody><tr class="odd"><td>地图/路径</td><td>-</td><td>481</td><td>330</td><td>470</td><td>1260</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>-</td><td>394</td><td>383</td><td>358</td><td>277</td></tr><tr class="odd"><td>推荐算法</td><td>-</td><td>378</td><td>456</td><td>397</td><td>397</td></tr><tr class="even"><td>PyTorch</td><td>360</td><td>370</td><td>427</td><td>383</td><td>365</td></tr><tr class="odd"><td>控制算法</td><td>-</td><td>364</td><td>390</td><td>400</td><td>-</td></tr><tr class="even"><td>c++/C++</td><td>315</td><td>359</td><td>395</td><td>418</td><td>420</td></tr><tr class="odd"><td>多模态/大模型</td><td>-</td><td>352</td><td>367</td><td>465</td><td>459</td></tr><tr class="even"><td>TensorFlow</td><td>360</td><td>343</td><td>367</td><td>383</td><td>365</td></tr><tr class="odd"><td>机器学习</td><td>360</td><td>343</td><td>395</td><td>434</td><td>433</td></tr><tr class="even"><td>深度学习</td><td>315</td><td>339</td><td>408</td><td>423</td><td>429</td></tr><tr class="odd"><td>python</td><td>315</td><td>330</td><td>361</td><td>411</td><td>394</td></tr><tr class="even"><td>NLP/自然语言/LLM</td><td>-</td><td>328</td><td>373</td><td>480</td><td>556</td></tr><tr class="odd"><td>软件/开发</td><td>360</td><td>327</td><td>252</td><td>394</td><td>393</td></tr><tr class="even"><td>视觉/cv/视频/图像</td><td>360</td><td>320</td><td>434</td><td>411</td><td>514</td></tr></tbody></table><h4id="技能点和教育程度对工作岗位数量的影响单位个">4.2技能点和教育程度对工作岗位数量的影响（单位：个）</h4><table><thead><tr class="header"><th></th><th>专科</th><th>本科</th><th>985/211</th><th>硕士</th><th>博士</th></tr></thead><tbody><tr class="odd"><td>c++/C++</td><td>2</td><td>248</td><td>23</td><td>255</td><td>42</td></tr><tr class="even"><td>python</td><td>2</td><td>223</td><td>22</td><td>212</td><td>39</td></tr><tr class="odd"><td>深度学习</td><td>2</td><td>157</td><td>25</td><td>166</td><td>25</td></tr><tr class="even"><td>机器学习</td><td>1</td><td>144</td><td>23</td><td>117</td><td>29</td></tr><tr class="odd"><td>视觉/cv/视频/图像</td><td>1</td><td>126</td><td>16</td><td>159</td><td>29</td></tr><tr class="even"><td>多模态/大模型</td><td>-</td><td>115</td><td>21</td><td>111</td><td>26</td></tr><tr class="odd"><td>NLP/自然语言/LLM</td><td>-</td><td>91</td><td>21</td><td>82</td><td>11</td></tr><tr class="even"><td>推荐算法</td><td>-</td><td>50</td><td>11</td><td>31</td><td>5</td></tr><tr class="odd"><td>TensorFlow</td><td>1</td><td>41</td><td>7</td><td>40</td><td>9</td></tr><tr class="even"><td>PyTorch</td><td>1</td><td>39</td><td>7</td><td>38</td><td>9</td></tr><tr class="odd"><td>软件/开发</td><td>1</td><td>38</td><td>3</td><td>20</td><td>3</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>-</td><td>21</td><td>5</td><td>17</td><td>4</td></tr><tr class="odd"><td>地图/路径</td><td>-</td><td>17</td><td>1</td><td>11</td><td>1</td></tr><tr class="even"><td>控制算法</td><td>-</td><td>13</td><td>3</td><td>19</td><td>-</td></tr></tbody></table><h4id="技能点和工作经验对平均年薪的影响单位k">4.3技能点和工作经验对平均年薪的影响（单位：K）</h4><table><thead><tr class="header"><th></th><th>应届</th><th>一年</th><th>两年</th><th>三年</th><th>四年</th><th>五年及以上</th></tr></thead><tbody><tr class="odd"><td>推荐算法</td><td>351</td><td>261</td><td>433</td><td>433</td><td>-</td><td>462</td></tr><tr class="even"><td>TensorFlow</td><td>272</td><td>343</td><td>324</td><td>415</td><td>360</td><td>427</td></tr><tr class="odd"><td>PyTorch</td><td>285</td><td>350</td><td>318</td><td>409</td><td>360</td><td>517</td></tr><tr class="even"><td>c++/C++</td><td>373</td><td>356</td><td>389</td><td>407</td><td>383</td><td>438</td></tr><tr class="odd"><td>多模态/大模型</td><td>409</td><td>355</td><td>427</td><td>397</td><td>423</td><td>428</td></tr><tr class="even"><td>控制算法</td><td>360</td><td>-</td><td>402</td><td>397</td><td>-</td><td>338</td></tr><tr class="odd"><td>NLP/自然语言/LLM</td><td>544</td><td>359</td><td>392</td><td>392</td><td>442</td><td>416</td></tr><tr class="even"><td>软件/开发</td><td>-</td><td>339</td><td>325</td><td>386</td><td>216</td><td>344</td></tr><tr class="odd"><td>机器学习</td><td>330</td><td>392</td><td>383</td><td>383</td><td>454</td><td>472</td></tr><tr class="even"><td>python</td><td>304</td><td>356</td><td>396</td><td>382</td><td>425</td><td>366</td></tr><tr class="odd"><td>深度学习</td><td>357</td><td>368</td><td>399</td><td>381</td><td>442</td><td>420</td></tr><tr class="even"><td>视觉/cv/视频/图像</td><td>406</td><td>360</td><td>454</td><td>379</td><td>397</td><td>505</td></tr><tr class="odd"><td>地图/路径</td><td>-</td><td>406</td><td>375</td><td>350</td><td>927</td><td>-</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>242</td><td>415</td><td>294</td><td>308</td><td>-</td><td>530</td></tr></tbody></table><h4id="技能点和工作经验对工作岗位数量的影响单位个">4.4技能点和工作经验对工作岗位数量的影响（单位：个）</h4><table><thead><tr class="header"><th></th><th>应届</th><th>一年</th><th>两年</th><th>三年</th><th>四年</th><th>五年及以上</th></tr></thead><tbody><tr class="odd"><td>c++/C++</td><td>38</td><td>70</td><td>142</td><td>211</td><td>17</td><td>92</td></tr><tr class="even"><td>python</td><td>35</td><td>49</td><td>143</td><td>187</td><td>14</td><td>70</td></tr><tr class="odd"><td>深度学习</td><td>19</td><td>44</td><td>99</td><td>146</td><td>9</td><td>58</td></tr><tr class="even"><td>视觉/cv/视频/图像</td><td>18</td><td>32</td><td>89</td><td>125</td><td>8</td><td>59</td></tr><tr class="odd"><td>机器学习</td><td>26</td><td>32</td><td>85</td><td>120</td><td>10</td><td>41</td></tr><tr class="even"><td>多模态/大模型</td><td>13</td><td>25</td><td>71</td><td>116</td><td>9</td><td>39</td></tr><tr class="odd"><td>NLP/自然语言/LLM</td><td>7</td><td>16</td><td>58</td><td>78</td><td>4</td><td>42</td></tr><tr class="even"><td>推荐算法</td><td>6</td><td>4</td><td>40</td><td>33</td><td>-</td><td>14</td></tr><tr class="odd"><td>PyTorch</td><td>5</td><td>16</td><td>19</td><td>32</td><td>1</td><td>21</td></tr><tr class="even"><td>TensorFlow</td><td>7</td><td>18</td><td>21</td><td>32</td><td>1</td><td>19</td></tr><tr class="odd"><td>软件/开发</td><td>-</td><td>6</td><td>22</td><td>25</td><td>1</td><td>11</td></tr><tr class="even"><td>数据挖掘/数据分析</td><td>3</td><td>4</td><td>18</td><td>16</td><td>-</td><td>6</td></tr><tr class="odd"><td>控制算法</td><td>1</td><td>-</td><td>13</td><td>13</td><td>-</td><td>8</td></tr><tr class="even"><td>地图/路径</td><td>-</td><td>9</td><td>5</td><td>9</td><td>7</td><td>-</td></tr></tbody></table><h3 id="高性价比工作岗位排名">5高性价比工作岗位排名</h3><h4id="加权工作经验工作岗位数量与薪资计算技能点的得分及排名情况">加权工作经验,工作岗位数量与薪资,计算技能点的得分及排名情况</h4><ul><li>基本原则：学历要求越低，工作经验要求越低，平均年薪越高的技能点越好，其评分会越高</li><li>学历，工作经验数据划分五个等级，然后将三个特征数据归一化处理，彼此相乘，再乘上百分系数，得到最终评分</li></ul><div id="container2" style="height: 400px;"></div><script type="text/javascript" src="https://registry.npmmirror.com/echarts/5.5.0/files/dist/echarts.min.js"></script><script type="text/javascript">  var dom = document.getElementById('container2');  var myChart = echarts.init(dom, null, {    renderer: 'canvas',    useDirtyRect: false  });  var app = {};  var option;  option = {    dataset: [      {        dimensions: ['name','score'],        source: [          ["NLP/自然语言/LLM",30.16],          ["PyTorch",14.78],          ["TensorFlow",15.56],          ["c++/C++",83.54],          ["python",74.84],          ["地图/路径",15.25],          ["多模态/大模型",45.56],          ["控制算法",10.59],          ["推荐算法",29.53],          ["数据挖掘/数据分析",5.26],          ["机器学习",46.19],          ["深度学习",52.13],          ["视觉/cv/视频/图像",42.48],          ["软件/开发",13.56]        ]      },      {        transform: {          type: 'sort',          config: { dimension: 'score', order: 'desc' }        }      }    ],    xAxis: {      type: 'category',      axisLabel: { interval: 0, rotate: 30 }    },    yAxis: {},    series: {      type: 'bar',      encode: { x: 'name', y: 'score' },            datasetIndex: 1    }  };  if (option && typeof option === 'object') {    myChart.setOption(option);  }  window.addEventListener('resize', myChart.resize);</script>]]></content>
    
    
    <categories>
      
      <category>Data Visualization</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ECharts</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【B站】大模型之路-从分治法至端到端,再到存算训一体</title>
    <link href="/B%E7%AB%99_%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E4%B9%8B%E8%B7%AF--%E4%BB%8E%E5%88%86%E6%B2%BB%E6%B3%95%E8%87%B3%E7%AB%AF%E5%88%B0%E7%AB%AF,%E5%86%8D%E5%88%B0%E5%AD%98%E7%AE%97%E8%AE%AD%E4%B8%80%E4%BD%93.html"/>
    <url>/B%E7%AB%99_%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E4%B9%8B%E8%B7%AF--%E4%BB%8E%E5%88%86%E6%B2%BB%E6%B3%95%E8%87%B3%E7%AB%AF%E5%88%B0%E7%AB%AF,%E5%86%8D%E5%88%B0%E5%AD%98%E7%AE%97%E8%AE%AD%E4%B8%80%E4%BD%93.html</url>
    
    <content type="html"><![CDATA[<h1id="大模型发展之路--从分治法至端到端再到存算训一体">大模型发展之路--从分治法至端到端,再到存算训一体</h1><p>安克创新CEO阳萌对人工智能过去、现在和未来的思考。他认为大模型和transformer只是阶段性的算法实现,未来一定是仿生算法的大趋势。他还谈到了分治法作为经典的范式有其明显的局限,而端到端的方案是人类理性解决问题的必经之路。他指出,人工智能领域的范式每五到十年就会出现一个全新的范式,存算一体已经成为业界新宠。</p><span id="more"></span><iframe src="//player.bilibili.com/player.html?isOutside=true&amp;aid=1954475860&amp;bvid=BV1gC41177xR&amp;cid=1538517104&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe><h3id="人工智能领域的发展历程和未来趋势以及斯蒂文杨萌对于人工智能的看法和建议">人工智能领域的发展历程和未来趋势,以及斯蒂文杨萌对于人工智能的看法和建议。</h3><p>00:01大模型解决不了英伟达的难题</p><p>00:45人工智能的发展历程</p><p>03:16人工智能领域的范式转变</p><h3id="分治法在人工智能领域的应用并探讨了端到端算法和分支法的优缺点">分治法在人工智能领域的应用,并探讨了端到端算法和分支法的优缺点。</h3><p>04:38分治法和端到端学习：分治法是一种解决问题的方法，而端到端学习则是一种新兴的方法。</p><p>05:53分治法可以帮助解决自然语言处理和自动驾驶等领域的问题。</p><p>08:29柔性连接和人类智能：人类的智能具有柔性连接的特点，而机器智能需要更多的研究来实现这样的特性。</p><h3id="搜索算法工程师使用分治法进行搜索并探讨了算法硬件和数据在人工智能中的重要性">搜索算法工程师使用分治法进行搜索,并探讨了算法、硬件和数据在人工智能中的重要性。</h3><p>09:04分治法在搜索引擎中的应用</p><p>12:05GPU和transformer算法的关系</p><p>13:10特斯拉和英伟达在自动驾驶领域的竞争</p><h3id="gpu芯片的结构和工作原理以及现代大模型在训练和推理端的不同应用">GPU芯片的结构和工作原理,以及现代大模型在训练和推理端的不同应用。</h3><p>13:28GPU是封装的芯片，其中包括运算核心和内存。在运算过程中，参数存在两边的内存中。</p><p>14:12大模型的训练需要多卡参与，而推理是将参数从内存中搬运到计算中心进行计算。</p><p>16:38Transformer模型不是一段一段地解决问题的，而是通过整体的参数进行端到端的解决问题。</p><h3id="gpu的发展趋势和优势并提出了人类应该借鉴大脑的运行方式来设计未来的芯片">GPU的发展趋势和优势,并提出了人类应该借鉴大脑的运行方式来设计未来的芯片。</h3><p>17:53计算性能与模型发展问题</p><p>21:35GPU的不足与大脑的差异</p><p>22:04适合大脑的计算芯片与内存位置</p><h3id="存算一体的概念以及如何实现存算一体的芯片并探讨了其在未来ai发展中的潜力">存算一体的概念,以及如何实现存算一体的芯片,并探讨了其在未来AI发展中的潜力。</h3><p>22:10存算一体芯片可以实现大模型的算法，节省能耗，是未来AI硬件的发展趋势。</p><p>25:32存算一体芯片可以应用在智能家居、智能音箱等场景中。</p><p>26:31安克创新正在研发存算一体芯片，并已经取得了一些成果。</p><h3id="算法和硬件之间的关系以及未来可能的发展趋势同时探讨了人工智能可能带来的影响">算法和硬件之间的关系，以及未来可能的发展趋势，同时探讨了人工智能可能带来的影响。</h3><p>26:44下一代算法：下一代算法可能会是一种一边学习一边进化的算法。</p>]]></content>
    
    
    <categories>
      
      <category>B站</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
      <tag>视频分享</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一首歌的时间部署本地Llama3大模型</title>
    <link href="/%E4%B8%80%E9%A6%96%E6%AD%8C%E7%9A%84%E6%97%B6%E9%97%B4-%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E4%B8%93%E5%B1%9Ellama3%E5%A4%A7%E6%A8%A1%E5%9E%8B.html"/>
    <url>/%E4%B8%80%E9%A6%96%E6%AD%8C%E7%9A%84%E6%97%B6%E9%97%B4-%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E4%B8%93%E5%B1%9Ellama3%E5%A4%A7%E6%A8%A1%E5%9E%8B.html</url>
    
    <content type="html"><![CDATA[<p>LLaMA3真的是相当相当炸裂啊！远超过去的体验！看数据Llama3-8B超过Mistra-7BMMLU10分；70B超过Claude3Sonet3分。这是一个惊人的成绩，一个开源模型超过闭源模型这样多。我只能说Meta是真正的OpenAI。自从它从Meta这个邪路上转正后，在OpenAI的路上一骑绝尘了！不废话，动手来给自己的电脑部署下吧。 <span id="more"></span></p><h2 id="有什么硬件要求"><strong>有什么硬件要求</strong></h2><p>N卡独占，起步4G显存，建议8G＋。纯CPU也能跑，如果你不嫌慢的话。</p><h2 id="安装lm-studio"><strong>1. 安装LM studio</strong></h2><p>就这个软件(<a href="https://lmstudio.ai/">LM Studio - Discover,download, and run local LLMs</a>)</p><figure><imgsrc="/images/本地部署Llama3大模型/v2-3a61b06246c57b88fcd83f17062c10df_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>安装成功，打开后应该出现如下界面</p><figure><img src="/images/本地部署Llama3大模型/image-20240514080559847.png"alt="image-20240514080559847" /><figcaption aria-hidden="true">image-20240514080559847</figcaption></figure><h2 id="选择llama3-8b模型"><strong>2. 选择llama3-8B模型</strong></h2><p>我们直接搜索llama 3-8B，找到该模型</p><figure><img src="/images/本地部署Llama3大模型/image-20240514081038674.png"alt="image-20240514081038674" /><figcaption aria-hidden="true">image-20240514081038674</figcaption></figure><p>当然我们也可以选择其他模型，模型选择的重要因素是大小，也就是参数量。模型参数量一般写在名字上，比如Dolphin 2.6 Mistral 7b – DPO Laser就是7B大小，也就是70亿参数。根据自己的电脑内存和显存容量选（CPU运行就看内存，GPU运行就看显存，混合运行就两个加起来），我电脑是8G显存，用的7B模型。</p><p>然后就是模型指标，现在huggingface上有成百上千个LLM，可以根据benchmark的成绩选，排名网页在此：<ahref="https://link.zhihu.com/?target=https%3A//huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">OpenLLM Leaderboard - a Hugging Face Space by HuggingFaceH4</a> 。</p><p>还有就是模型特性，比如是否经过审查，适合于什么类型的工作等。</p><h2 id="下载gguf文件"><strong>3. 下载gguf文件</strong></h2><h3 id="在lm-studio内部下载需要配置网络"><strong>1. 在LMStudio内部下载，需要配置网络</strong></h3><p>如果有国际互联网连接就可以直接下载。如果没有见下一步。</p><h3 id="在huggingface下载并转移到lm-studio中"><strong>2.在huggingface下载并转移到LM Studio中</strong></h3><h3 id="下载"><strong>1. 下载</strong></h3><p>手动将网址复制到浏览器下载。</p><figure><img src="/images/本地部署Llama3大模型/image-20240514081632465.png"alt="image-20240514081632465" /><figcaption aria-hidden="true">image-20240514081632465</figcaption></figure><h3 id="移动下载的gguf文件到lm-studio识别的位置"><strong>2.移动下载的gguf文件到LM studio识别的位置</strong></h3><figure><img src="/images/本地部署Llama3大模型/image-20240514081756888.png"alt="image-20240514081756888" /><figcaption aria-hidden="true">image-20240514081756888</figcaption></figure><p>打开My models,找到gguf文件位置，然后在系统文件管理器中管理好你下载的gguf文件路径，格式为models/A/B/xxx.gguf。再重启LMstudio就能看到它。</p><h2 id="运行"><strong>4. 运行</strong></h2><h3 id="cpu运行"><strong>1.CPU运行</strong></h3><p>同GPU运行，但不用改settings 中的 GPU 参数。</p><h3 id="gpu运行"><strong>2.GPU运行</strong></h3><figure><img src="/images/本地部署Llama3大模型/image-20240514082144052.png"alt="image-20240514082144052" /><figcaption aria-hidden="true">image-20240514082144052</figcaption></figure><p>然后点击窗口上方的Select a model toload，加载上一步下载的模型就可以了。任务管理器中可以监视显存占用。</p><p>如果成功加载到显卡，就可以在下方与其对话了。</p><figure><img src="/images/本地部署Llama3大模型/image-20240514082345176.png"alt="image-20240514082345176" /><figcaption aria-hidden="true">image-20240514082345176</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>项目部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>北京人工智能行业薪资大揭秘</title>
    <link href="/%E5%8C%97%E4%BA%AC%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E8%96%AA%E8%B5%84%E5%A4%A7%E6%8F%AD%E7%A7%98.html"/>
    <url>/%E5%8C%97%E4%BA%AC%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E8%96%AA%E8%B5%84%E5%A4%A7%E6%8F%AD%E7%A7%98.html</url>
    
    <content type="html"><![CDATA[<p><img src="/images/ai_salary/1714717993428.png" />BOSS直聘数据，含北京市各区[<code>算法工程师|人工智能</code>]岗位数据7534条<span id="more"></span> # <strong>北京2024人工智能行业薪资大揭秘</strong></p><h3 id="数据说明">1.数据说明</h3><p>数据时间:<code>2024年5月</code></p><p>数据来源:**BOSS直聘,爬取北京市各区[<code>算法工程师|人工智能</code>]岗位数据7534条(多次爬取结果)</p><p><strong>数据清洗:</strong>提取岗位内容中包含<code>[人工智能|算法|nlp|cv]</code>的内容,并执行去重操作,得到岗位数据2208个.</p><p><img src="/images/ai_salary/1714721994550.png" /></p><h3 id="薪资分析">2.薪资分析</h3><p><img src="/images/ai_salary/1714716697686.png" /></p><p><strong>薪资上限，星辰大海般的梦想</strong>：首先，让人眼前一亮的是薪资上限——竟然高达<strong>1800k（年薪）</strong>！这意味着在这个行业，如果你拥有出色的才华和丰富的经验，那么年薪百万的梦想并非遥不可及。当然，这样的高薪也对应着极高的工作要求和挑战。</p><p><strong>薪资下限，无良公司真没下限</strong>：而对于那些刚刚步入人工智能行业的新人或者初入这个领域的小伙伴们来说，薪资下限为24k（年薪）,在北京还不够房租的,试问这些无良公司,你们的良心不会痛吗。</p><p><strong>平均年薪，舒适圈的魅力</strong>：说到最吸引人的部分，莫过于平均年薪了。北京人工智能行业的平均年薪高达<strong>400k</strong>左右，这真是一个赏心悦目的数字,另外年薪的众数和中位数都是<strong>360k</strong>,不知道屏幕前的你达到平均水平没有。</p><p><strong>人工智能岗位平均年薪与下限年薪对比:</strong></p><p><img src="/images/ai_salary/1714717993428.png" /></p><h3 id="岗位要求">3.岗位要求</h3><p>人工智能行业这么卷,是不是得<code>985\211起步,研究生占半数</code>呢?我们用数据来说话:</p><p><img src="/images/ai_salary/1714721621417.png" /></p><p>根据统计的2208个岗位数据来看,研究生占比30.4%,反而是<strong>本科生占据了大多数</strong>,占比达<strong>64.9%</strong>,本科生才是人工智能产业的中坚力量.不过AI行业的起步门槛是真高,大专和学历不限的岗位占比仅<strong>2.4%.</strong></p><p>在岗位经验来看,<strong>人工智能行业的包容性还是比较大的</strong>,经验不限的占到了17.91%(越缺人才的行业,这个指标越高),3-5年的岗位占比超过一半(鲜明的新兴行业).现在来看,又是招兵买马又是百模大战,<strong>人工智能的时代才刚开始.</strong></p><h3 id="薪资与学历和经验的关系">4.薪资与学历和经验的关系</h3><h4 id="学历vs薪资">4.1 学历VS薪资</h4><p><strong>大专小鲜肉</strong>：虽然起步稍低，但凭借着一股不服输的劲头，也能拿到251k的薪资，证明了在人工智能领域，实力非常的重要。</p><p><strong>本科高手</strong>：他们像是中流砥柱，稳稳地占据了薪资的中上游，379k的薪资，是对他们扎实基础和广泛知识的认可。</p><p><strong>硕士精英</strong>：他们在学历上更上一层楼，薪资也随之水涨船高，416k的薪资，是他们辛勤付出的回报。</p><p><strong>博士大佬</strong>：一出场就自带光环，稳稳地站在了薪资的金字塔尖，462k的薪资，仿佛在告诉大家：“知识就是力量，学历就是金钱！”</p><p><strong>学历不限</strong>：这个神秘的角色，似乎不受学历的束缚，凭借着自己的独特技能和经验，也能轻松拿到385k的薪资，可谓是“英雄不问出处”。</p><p><img src="/images/ai_salary/1714719207889.png" /></p><h3 id="经验vs薪资">4.2 经验VS薪资</h3><p><strong>1-3年新鲜人儿</strong>：初出茅庐的你，薪资322k，够你喝不少星巴克了！但别停步，未来更精彩！</p><p><strong>1年以内小鲜肉</strong>：应届生们，你们薪资333k，起点不错！不过这只是起点，挑战还在后头哦！</p><p><strong>3-5年小有成就</strong>：404k的薪资，帝都<strong>租房</strong>没问题！继续加油，成为公司顶梁柱！</p><p><strong>5-10年资深玩家</strong>：资深大佬，458k薪资，生活舒适还能追梦！多年打拼，果然值得！</p><p><strong>10年以上大佬级人物</strong>：传奇大佬，467k薪资，人生赢家！人脉经验都丰富，这钱你应得！</p><p><strong>在校/应届小白</strong>：小白们，150k只是开始，努力学习，未来可期！</p><p><strong>经验不限的小伙伴</strong>：无门槛岗位，378k薪资，虽有挑战，但你有实力，定能闯出一片天！</p><p><img src="/images/ai_salary/1714719746033.png" /></p><hr /><p>在北京,人工智能行业以平均年薪<strong>400k</strong>的高薪,<strong>经验不限</strong>的要求,让无数人心生向往。尽管如此,本科学历只是<strong>入行地板砖</strong>,稳妥些确实得硕士学历.但长远来看,AI行业是一个不断发展的增量市场,它注定要成为推动社会变革的新质生产力,你<strong>准备好迎接这个崭新的时代了吗</strong>?</p>]]></content>
    
    
    
    <tags>
      
      <tag>可视化</tag>
      
      <tag>数据分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>院士讲人工智能与智能计算的发展</title>
    <link href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%9A%84%E5%8F%91%E5%B1%95-%E5%A5%BD%E6%96%87%E5%88%86%E4%BA%AB.html"/>
    <url>/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%9A%84%E5%8F%91%E5%B1%95-%E5%A5%BD%E6%96%87%E5%88%86%E4%BA%AB.html</url>
    
    <content type="html"><![CDATA[<h1 id="人工智能与智能计算的发展">人工智能与智能计算的发展</h1><p>这是孙凝晖院士给正国级、副国级讲课的万字长稿,全面深入地梳理了人工智能行业的发展情况,行文高屋建瓴,见微知著,读完收获良多,在此与诸位分享。</p><span id="more"></span><p>人工智能领域近年来正在迎来一场由生成式人工智能大模型引领的爆发式发展。2022年11月30日，OpenAI公司推出一款人工智能对话聊天机器人ChatGPT，其出色的自然语言生成能力引起了全世界范围的广泛关注，2个月突破1亿用户，国内外随即掀起了一场大模型浪潮，Gemini、文心一言、Copilot、LLaMA、SAM、SORA等各种大模型如雨后春笋般涌现，2022年也被誉为大模型元年。</p><p>当前信息时代正加快进入智能计算的发展阶段，人工智能技术上的突破层出不穷，逐渐深入地赋能千行百业，推动人工智能与数据要素成为新质生产力的典型代表。习近平总书记指出，把新一代人工智能作为推动科技跨越发展、产业优化升级、生产力整体跃升的驱动力量，努力实现高质量发展。党的十八大以来，以习近平同志为核心的党中央高度重视智能经济发展，促进人工智能和实体经济深度融合，为高质量发展注入强劲动力。</p><h3 id="一计算技术发展简介"><strong>一、计算技术发展简介</strong></h3><p>计算技术的发展历史大致可分为四个阶段，算盘的出现标志着人类进入第一代——机械计算时代，第二代——电子计算的标志是出现电子器件与电子计算机，互联网的出现使我们进入第三代——网络计算，当前人类社会正在进入第四阶段——智能计算。</p><p>早期的计算装置是手动辅助计算装置和半自动计算装置，人类计算工具的历史是从公元1200年的中国算盘开始，随后出现了纳皮尔筹（1612年）和滚轮式加法器（1642年），到1672年第一台自动完成四则运算的计算装置——步进计算器诞生了。</p><p>机械计算时期已经出现了现代计算机的一些基本概念。查尔斯∙巴贝奇（CharlesBabbage）提出了差分机（1822年）与分析机（1834年）的设计构想，支持自动机械计算。这一时期，编程与程序的概念基本形成，编程的概念起源于雅卡尔提花机，通过打孔卡片控制印花图案，最终演变为通过计算指令的形式来存储所有数学计算步骤；人类历史的第一个程序员是诗人拜伦之女艾达（Ada），她为巴贝奇差分机编写了一组求解伯努利数列的计算指令，这套指令也是人类历史上第一套计算机算法程序，它将硬件和软件分离，第一次出现程序的概念。</p><p>直到在二十世纪上半叶，出现了布尔代数(数学)、图灵机(计算模型)、冯诺依曼体系结构(架构)、晶体管(器件)这四个现代计算技术的科学基础。其中，布尔代数用来描述程序和硬件如CPU的底层逻辑；图灵机是一种通用的计算模型，将复杂任务转化为自动计算、不需人工干预的自动化过程；冯诺依曼体系结构提出了构造计算机的三个基本原则：采用二进制逻辑、程序存储执行、以及计算机由运算器、控制器、存储器、输入设备、输出设备这五个基本单元组成；晶体管是构成基本的逻辑电路和存储电路的半导体器件，是建造现代计算机之塔的“砖块”。基于以上科学基础，计算技术得以高速发展，形成规模庞大的产业。</p><p>从1946年世界上第一台电子计算机ENIAC诞生到二十一世纪的今天，已经形成了五类成功的平台型计算系统。当前各领域各种类型的应用，都可以由这五类平台型计算装置支撑。<strong>第一类</strong>是高性能计算平台，解决了国家核心部门的科学与工程计算问题；<strong>第二类</strong>是企业计算平台，又称服务器，用于企业级的数据管理、事务处理，当前像百度、阿里和腾讯这些互联网公司的计算平台都属于这一类；<strong>第三类</strong>是个人电脑平台，以桌面应用的形式出现，人们通过桌面应用与个人电脑交互；<strong>第四类</strong>是智能手机，主要特点是移动便携，手机通过网络连接数据中心，以互联网应用为主，它们分布式地部署在数据中心和手机终端；<strong>第五类</strong>是嵌入式计算机，嵌入到工业装备和军事设备，通过实时的控制，保障在确定时间内完成特定任务。这五类装置几乎覆盖了我们信息社会的方方面面，长期以来人们追求的以智能计算应用为中心的第六类平台型计算系统尚未形成。</p><h4id="现代计算技术的发展大致可以划分为三个时代"><strong>现代计算技术的发展大致可以划分为三个时代。</strong></h4><h4id="it1.0又称电子计算时代1950-1970"><strong>IT1.0又称电子计算时代（1950-1970）</strong></h4><p>基本特征是以“机”为中心。计算技术的基本架构形成，随着集成电路工艺的进步，基本计算单元的尺度快速微缩，晶体管密度、计算性能和可靠性不断提升，计算机在科学工程计算、企业数据处理中得到了广泛应用。</p><h4id="it2.0又称网络计算时代1980-2020"><strong>IT2.0又称网络计算时代（1980-2020）</strong></h4><p>以“人”为中心。互联网将人使用的终端与后台的数据中心连接，互联网应用通过智能终端与人进行交互。以亚马逊等为代表的互联网公司提出了云计算的思想，将后台的算力封装成一个公共服务租借给第三方用户，形成了云计算与大数据产业。</p><h4id="it3.0又称智能计算时代"><strong>IT3.0又称智能计算时代</strong></h4><p>始于2020年，与IT2.0相比增加了“物”的概念，即物理世界的各种端侧设备，被数字化、网络化和智能化，实现“人-机-物”三元融合。智能计算时代，除了互联网以外，还有数据基础设施，支撑各类终端通过端边云实现万物互联，终端、物端、边缘、云都嵌入AI，提供与ChatGPT类似的大模型智能服务，最终实现有计算的地方就有AI智能。智能计算带来了巨量的数据、人工智能算法的突破和对算力的爆发性需求。</p><h3 id="二智能计算发展简介"><strong>二、智能计算发展简介</strong></h3><p>智能计算包括人工智能技术与它的计算载体，大致历经了四个阶段，分别为通用计算装置、逻辑推理专家系统、深度学习计算系统、大模型计算系统。</p><p><strong>智能计算的起点是通用自动计算装置（1946年）。</strong></p><p>艾伦·图灵（Alan Turing）和冯·诺依曼（John vonNeumann）等科学家，一开始都希望能够模拟人脑处理知识的过程，发明像人脑一样思考的机器，虽未能实现，但却解决了计算的自动化问题。通用自动计算装置的出现，也推动了1956年人工智能（AI）概念的诞生，此后所有人工智能技术的发展都是建立在新一代计算设备与更强的计算能力之上的。</p><p><strong>智能计算发展的第二阶段是逻辑推理专家系统（1990年）。</strong></p><p>E.A.费根鲍姆（Edward AlbertFeigenbaum）等符号智能学派的科学家以逻辑和推理能力自动化为主要目标，提出了能够将知识符号进行逻辑推理的专家系统。人的先验知识以知识符号的形式进入计算机，使计算机能够在特定领域辅助人类进行一定的逻辑判断和决策，但专家系统严重依赖于手工生成的知识库或规则库。这类专家系统的典型代表是日本的五代机和我国863计划支持的306智能计算机主题，日本在逻辑专家系统中采取专用计算平台和Prolog这样的知识推理语言完成应用级推理任务；我国采取了与日本不同的技术路线，以通用计算平台为基础，将智能任务变成人工智能算法，将硬件和系统软件都接入通用计算平台，并催生了曙光、汉王、科大讯飞等一批骨干企业。</p><p>符号计算系统的局限性在于其爆炸的计算时空复杂度，即符号计算系统只能解决线性增长问题，对于高维复杂空间问题是无法求解的，从而限制了能够处理问题的大小。同时因为符号计算系统是基于知识规则建立的，我们又无法对所有的常识用穷举法来进行枚举，它的应用范围就受到了很大的限制。随着第二次AI寒冬的到来，第一代智能计算机逐渐退出历史舞台。</p><p><strong>直到2014年左右，智能计算进阶到第三阶段——深度学习计算系统。</strong></p><p>以杰弗里·辛顿（GeoffreyHinton）等为代表的连接智能学派，以学习能力自动化为目标，发明了深度学习等新AI算法。通过深度神经元网络的自动学习，大幅提升了模型统计归纳的能力，在模式识别①等应用效果上取得了巨大突破，某些场景的识别精度甚至超越了人类。以人脸识别为例，整个神经网络的训练过程相当于一个网络参数调整的过程，将大量的经过标注的人脸图片数据输入神经网络，然后进行网络间参数调整，让神经网络输出的结果的概率无限逼近真实结果。神经网络输出真实情况的概率越大，参数就越大，从而将知识和规则编码到网络参数中，这样只要数据足够多，就可以对各种大量的常识进行学习，通用性得到极大的提升。连接智能的应用更加广泛，包括语音识别、人脸识别、自动驾驶等。在计算载体方面，中国科学院计算技术研究所2013年提出了国际首个深度学习处理器架构，国际知名的硬件厂商英伟达（NVIDIA）持续发布了多款性能领先的通用GPU芯片，都是深度学习计算系统的典型代表。</p><p><strong>智能计算发展的第四阶段是大模型计算系统（2020年）。</strong></p><p>在人工智能大模型技术的推动下，智能计算迈向新的高度。2020年，AI从“小模型+判别式”转向“大模型+生成式”，从传统的人脸识别、目标检测、文本分类，升级到如今的文本生成、3D数字人生成、图像生成、语音生成、视频生成。大语言模型在对话系统领域的一个典型应用是OpenAI公司的ChatGPT，它采用预训练基座大语言模型GPT-3，引入3000亿单词的训练语料，相当于互联网上所有英语文字的总和。其基本原理是：通过给它一个输入，让它预测下一个单词来训练模型，通过大量训练提升预测精确度，最终达到向它询问一个问题，大模型产生一个答案，与人即时对话。在基座大模型的基础上，再给它一些提示词进行有监督的指令微调，通过人类的&lt;指令，回复&gt;对逐渐让模型学会如何与人进行多轮对话；最后，通过人为设计和自动生成的奖励函数来进行强化学习迭代，逐步实现大模型与人类价值观的对齐。</p><blockquote><p>大模型的特点是以“大”取胜，其中有三层含义</p><p><strong>（1）参数大</strong>，GPT-3就有1700亿个参数；</p><p><strong>（2）训练数据大</strong>，ChatGPT大约用了3000亿个单词，570GB训练数据；</p><p><strong>（3）算力需求大</strong>，GPT-3大约用了上万块V100GPU进行训练。为满足大模型对智能算力爆炸式增加的需求，国内外都在大规模建设耗资巨大的新型智算中心，英伟达公司也推出了采用256个H100芯片，150TB海量GPU内存等构成的大模型智能计算系统。</p></blockquote><h3id="三大模型的出现带来了三个变革">三、<strong>大模型的出现带来了三个变革</strong></h3><p><strong>一是技术上的规模定律</strong>（ScalingLaw），即很多AI模型的精度在参数规模超过某个阈值后模型能力快速提升，其原因在科学界还不是非常清楚，有很大的争议。AI模型的性能与模型参数规模、数据集大小、算力总量三个变量成“对数线性关系”，因此可以通过增大模型的规模来不断提高模型的性能。目前最前沿的大模型GPT-4参数量已经达到了万亿到十万亿量级，并且仍在不断增长中；</p><p><strong>二是产业上算力需求爆炸式增长</strong>，千亿参数规模大模型的训练通常需要在数千乃至数万GPU卡上训练2-3个月时间，急剧增加的算力需求带动相关算力企业超高速发展，英伟达的市值接近两万亿美元，对于芯片企业以前从来没有发生过；</p><p><strong>三是社会上冲击劳动力市场</strong>，北京大学国家发展研究院与智联招聘联合发布的《AI大模型对我国劳动力市场潜在影响研究》报告指出，受影响最大的20个职业中财会、销售、文书位于前列，需要与人打交道并提供服务的体力劳动型工作，如人力资源、行政、后勤等反而相对更安全。</p><h4id="人工智能的技术前沿将朝着以下四个方向发展"><strong>人工智能的技术前沿将朝着以下四个方向发展。</strong></h4><p><strong>第一个前沿方向为多模态大模型。</strong>从人类视角出发，人类智能是天然多模态的，人拥有眼、耳、鼻、舌、身、嘴(语言)，从AI视角出发，视觉，听觉等也都可以建模为token②的序列，可采取与大语言模型相同的方法进行学习，并进一步与语言中的语义进行对齐，实现多模态对齐的智能能力。</p><p><strong>第二个前沿方向为视频生成大模型。</strong>OpenAI于2024年2月15日发布文生视频模型SORA，将视频生成时长从几秒钟大幅提升到一分钟，且在分辨率、画面真实度、时序一致性等方面都有显著提升。SORA的最大意义是它具备了世界模型的基本特征，即人类观察世界并进一步预测世界的能力。世界模型是建立在理解世界的基本物理常识（如，水往低处流等）之上，然后观察并预测下一秒将要发生什么事件。虽然SORA要成为世界模型仍然存在很多问题，但可以认为SORA学会了画面想象力和分钟级未来预测能力，这是世界模型的基础特征。</p><p><strong>第三个前沿方向为具身智能。</strong>具身智能指有身体并支持与物理世界进行交互的智能体，如机器人、无人车等，通过多模态大模型处理多种传感数据输入，由大模型生成运动指令对智能体进行驱动，替代传统基于规则或者数学公式的运动驱动方式，实现虚拟和现实的深度融合。因此，具有具身智能的机器人，可以聚集人工智能的三大流派：以神经网络为代表的连接主义，以知识工程为代表的符号主义和控制论相关的行为主义，三大流派可以同时作用在一个智能体，这预期会带来新的技术突破。</p><p><strong>第四个前沿方向是AI4R(AI forResearch)成为科学发现与技术发明的主要范式。</strong>当前科学发现主要依赖于实验和人脑智慧，由人类进行大胆猜想、小心求证，信息技术无论是计算和数据，都只是起到一些辅助和验证的作用。相较于人类，人工智能在记忆力、高维复杂、全视野、推理深度、猜想等方面具有较大优势，是否能以AI为主进行一些科学发现和技术发明，大幅提升人类科学发现的效率，比如主动发现物理学规律、预测蛋白质结构、设计高性能芯片、高效合成新药等。因为人工智能大模型具有全量数据，具备上帝视角，通过深度学习的能力，可以比人向前看更多步数，如能实现从推断(inference)到推理(reasoning)的跃升，人工智能模型就有潜力具备爱因斯坦一样的想象力和科学猜想能力，极大提升人类科学发现的效率，打破人类的认知边界。这才是真正的颠覆所在。</p><p><strong>最后，通用人工智能③（Artificial GeneralIntelligence，简称AGI）是一个极具挑战的话题，极具争论性。</strong>曾经有一个哲学家和一个神经科学家打赌：25年后（即2023年）科研人员是否能够揭示大脑如何实现意识？当时关于意识有两个流派，一个叫集成信息理论，一个叫全局网络工作空间理论，前者认为意识是由大脑中特定类型神经元连接形成的“结构”，后者指出意识是当信息通过互连网络传播到大脑区域时产生的。2023年，人们通过六个独立实验室进行了对抗性实验，结果与两种理论均不完全匹配，哲学家赢了，神经科学家输了。通过这一场赌约，可以看出人们总是希望人工智能能够了解人类的认知和大脑的奥秘。从物理学的视角看，物理学是对宏观世界有了透彻理解后，从量子物理起步开启了对微观世界的理解。智能世界与物理世界一样，都是具有巨大复杂度的研究对象，AI大模型仍然是通过数据驱动等研究宏观世界的方法，提高机器的智能水平，对智能宏观世界理解并不够，直接到神经系统微观世界寻找答案是困难的。人工智能自诞生以来，一直承载着人类关于智能与意识的种种梦想与幻想，也激励着人们不断探索。</p><h3id="四人工智能的安全风险">四、<strong>人工智能的安全风险</strong></h3><p>人工智能的发展促进了当今世界科技进步的同时，也带来了很多安全风险，要从技术与法规两方面加以应对。</p><h4id="首先是互联网虚假信息泛滥这里列举若干场景"><strong>首先是互联网虚假信息泛滥。</strong>这里列举若干场景：</h4><p><strong>一是数字分身。</strong>AI Yoon是首个使用 DeepFake技术合成的官方“候选人”，这个数字人以韩国国民力量党候选人尹锡悦（YoonSuk-yeol）为原型，借助尹锡悦 20小时的音频和视频片段、以及其专门为研究人员录制的 3000多个句子，由当地一家 DeepFake 技术公司创建了虚拟形象 AIYoon，并在网络上迅速走红。实际上 AI Yoon表达的内容是由竞选团队撰写的，而不是候选人本人。</p><p><strong>二是伪造视频，</strong>尤其是伪造领导人视频引起国际争端，扰乱选举秩序，或引起突发舆情事件，如伪造尼克松宣布第一次登月失败，伪造乌克兰总统泽连斯基宣布“投降”的信息，这些行为导致新闻媒体行业的社会信任衰退。</p><p><strong>三是伪造新闻</strong>，主要通过虚假新闻自动生成牟取非法利益，使用ChatGPT生成热点新闻，赚取流量，截至2023年6月30日全球生成伪造新闻网站已达277个，严重扰乱社会秩序。</p><p><strong>四是换脸变声，用于诈骗。</strong>如由于AI语音模仿了企业高管的声音，一家香港国际企业因此被骗3500万美元。</p><p><strong>五是生成不雅图片，特别是针对公众人物。</strong>如影视明星的色情视频制作，造成不良社会影响。因此，迫切需要发展互联网虚假信息的伪造检测技术。</p><h4id="其次ai大模型面临严重可信问题"><strong>其次，AI大模型面临严重可信问题。</strong></h4><p>这些问题包括：（1）“一本正经胡说八道”的事实性错误；（2）以西方价值观叙事，输出政治偏见和错误言论；（3）易被诱导，输出错误知识和有害内容；（4）数据安全问题加重，大模型成为重要敏感数据的诱捕器，ChatGPT将用户输入纳入训练数据库，用于改善ChatGPT，美方能够利用大模型获得公开渠道覆盖不到的中文语料，掌握我们自己都可能不掌握的“中国知识”。因此，迫切需要发展大模型安全监管技术与自己的可信大模型。</p><h4id="除了技术手段外人工智能安全保障需要相关立法工作"><strong>除了技术手段外，人工智能安全保障需要相关立法工作。</strong></h4><p>2021年科技部发布《新一代人工智能伦理规范》，2022年8月，全国信息安全标准化技术委员会发布《信息安全技术机器学习算法安全评估规范》，2022-2023年，中央网信办先后发布《互联网信息服务算法推荐管理规定》《互联网信息服务深度合成管理规定》《生成式人工智能服务管理办法》等。欧美国家也先后出台法规，2018年5月25日，欧盟出台《通用数据保护条例》，2022年10月4日，美国发布《人工智能权利法案蓝图》，2024年3月13日，欧洲议会通过了欧盟《人工智能法案》。</p><p>我国应加快推进《人工智能法》出台，构建人工智能治理体系，确保人工智能的发展和应用遵循人类共同价值观，促进人机和谐友好；创造有利于人工智能技术研究、开发、应用的政策环境；建立合理披露机制和审计评估机制，理解人工智能机制原理和决策过程；明确人工智能系统的安全责任和问责机制，可追溯责任主体并补救；推动形成公平合理、开放包容的国际人工智能治理规则。</p><h3id="五中国智能计算发展困境">五、<strong>中国智能计算发展困境</strong></h3><p>人工智能技术与智能计算产业处于中美科技竞争的焦点，我国在过去几年虽然取得了很大的成绩，但依然面临诸多发展困境，特别是由美国的科技打压政策带来的困难。</p><h4id="困境一为美国在ai核心能力上长期处于领先地位中国处于跟踪模式"><strong>困境一</strong>为美国在AI核心能力上长期处于领先地位，中国处于跟踪模式。</h4><p>中国在AI高端人才数量、AI基础算法创新、AI底座大模型能力（大语言模型、文生图模型、文生视频模型）、底座大模型训练数据、底座大模型训练算力等，都与美国存在一定的差距，并且这种差距还将持续很长一段时间。</p><h4id="困境二为高端算力产品禁售高端芯片工艺长期被卡"><strong>困境二</strong>为高端算力产品禁售，高端芯片工艺长期被卡。</h4><p>A100，H100，B200等高端智算芯片对华禁售。华为、龙芯、寒武纪、曙光、海光等企业都进入实体清单，它们芯片制造的先进工艺④受限，国内可满足规模量产的工艺节点落后国际先进水平2-3代，核心算力芯片的性能落后国际先进水平2-3代。</p><h4id="困境三为国内智能计算生态孱弱ai开发框架渗透率不足"><strong>困境三</strong>为国内智能计算生态孱弱，AI开发框架渗透率不足。</h4><p>英伟达CUDA⑤(Compute Unified Device Architecture,通用计算设备架构)生态完备，已形成了事实上的垄断。国内生态孱弱，具体表现在：一是研发人员不足，英伟达CUDA生态有近2万人开发，是国内所有智能芯片公司人员总和的20倍；二是开发工具不足，CUDA有550个SDK(SoftwareDevelopment Kit,软件开发工具包)，是国内相关企业的上百倍；三是资金投入不足，英伟达每年投入50亿美元，是国内相关公司的几十倍；四是AI开发框架TensorFlow占据工业类市场，PyTorch占据研究类市场，百度飞桨等国产AI开发框架的开发人员只有国外框架的1/10。更为严重的是国内企业之间山头林立，无法形成合力，从智能应用、开发框架、系统软件、智能芯片，虽然每层都有相关产品，但各层之间没有深度适配，无法形成一个有竞争力的技术体系。</p><h4id="困境四为ai应用于行业时成本门槛居高不下"><strong>困境四</strong>为AI应用于行业时成本、门槛居高不下。</h4><p>当前我国AI应用主要集中在互联网行业和一些国防领域。AI技术推广应用于各行各业时，特别是从互联网行业迁移到非互联网行业，需要进行大量的定制工作，迁移难度大，单次使用成本高。最后，我国在AI领域的人才数量与实际需求相比也明显不足。</p><h3id="六中国如何发展智能计算的道路选择">六、<strong>中国如何发展智能计算的道路选择</strong></h3><p>人工智能发展的道路选择对我国至关重要，关系到发展的可持续性与最终的国际竞争格局。当前人工智能的使用成本十分高昂，微软Copilot套件要支付每月10美元的使用费用，ChatGPT每天消耗50万千瓦时的电力，英伟达B200芯片价格高达3万美元以上。总体来说，我国应发展用得起、安全可信的人工智能技术，消除我国信息贫困人口、并造福“一带一路”国家；低门槛地赋能各行各业，让我国的优势产业保持竞争力，让相对落后的产业能够大幅地缩小差距。</p><h4id="选择一统一技术体系走闭源封闭还是开源开放的道路"><strong>选择一：统一技术体系走闭源封闭，还是开源开放的道路？</strong></h4><p>支撑智能计算产业的是一个相互紧耦合的技术体系，即由一系列技术标准和知识产权将材料、器件、工艺、芯片、整机、系统软件、应用软件等密切联系在一起的技术整体。我国发展智能计算技术体系存在三条道路：</p><p><strong>一是追赶兼容美国主导的A体系</strong>。我国大多数互联网企业走的是GPGPU/CUDA兼容道路，很多芯片领域的创业企业在生态构建上也是尽量与CUDA兼容，这条道路较为现实。由于在算力方面美国对我国工艺和芯片带宽的限制，在算法方面国内生态林立很难形成统一，生态成熟度严重受限，在数据方面中文高质量数据匮乏，这些因素会使得追赶者与领先者的差距很难缩小，一些时候还会进一步拉大。　　</p><p><strong>二是构建专用封闭的B体系。</strong>在军事、气象、司法等专用领域构建企业封闭生态，基于国产成熟工艺生产芯片，相对于底座大模型更加关注特定领域垂直类大模型，训练大模型更多采用领域专有高质量数据等。这条道路易于形成完整可控的技术体系与生态，我国一些大型骨干企业走的是这条道路，它的缺点是封闭，无法凝聚国内大多数力量，也很难实现全球化。　　</p><p><strong>三是全球共建开源开放的C体系。</strong>用开源打破生态垄断，降低企业拥有核心技术的门槛，让每个企业都能低成本地做自己的芯片，形成智能芯片的汪洋大海，满足无处不在的智能需求。用开放形成统一的技术体系，我国企业与全球化力量联合起来共建基于国际标准的统一智能计算软件栈。形成企业竞争前共享机制，共享高质量数据库，共享开源通用底座大模型。对于全球开源生态，我国企业在互联网时代收益良多，我国更多的是使用者，是参与者，在智能时代我国企业在RISC-V⑥+AI开源技术体系上应更多地成为主力贡献者，成为全球化开放共享的主导力量。</p><h4id="选择二拼算法模型还是拼新型基础设施"><strong>选择二：拼算法模型，还是拼新型基础设施？</strong>　　</h4><p>人工智能技术要赋能各行各业，具有典型的长尾效应⑦。我国80%的中小微企业，需要的是低门槛、低价格的智能服务。因此，我国智能计算产业必须建立在新的数据空间基础设施之上，其中关键是我国应率先实现智能要素即数据、算力、算法的全面基础设施化。这项工作可比肩二十世纪初美国信息高速公路计划（即信息基础设施建设）对互联网产业的历史作用。　　</p><p>信息社会最核心的生产力是网络空间(Cyberspace)。网络空间的演进过程是：从机器一元连接构成的计算空间，演进到人机信息二元连接构成的信息空间，再演进到人机物数据三元连接构成的数据空间。从数据空间看，人工智能的本质是数据的百炼成钢，大模型就是对互联网全量数据进行深度加工后的产物。在数字化时代，在互联网上传输的是信息流，是算力对数据进行粗加工后的结构化抽象；在智能时代，在互联网上传输的是智能流，是算力对数据进行深度加工与精炼后的模型化抽象。智能计算的一个核心特征就是用数值计算、数据分析、人工智能等算法，在算力池中加工海量数据件，得到智能模型，再嵌入到信息世界、物理世界的各个过程中。　　</p><p>我国政府已经前瞻性地提前布局了新型基础设施，在世界各国竞争中抢占了先机。</p><p><strong>首先，数据已成为国家战略信息资源。</strong>数据具有资源要素与价值加工两重属性，数据的资源要素属性包括生产、获取、传输、汇聚、流通、交易、权属、资产、安全等各个环节，我国应继续加大力度建设国家数据枢纽与数据流通基础设施。　　</p><p><strong>其次，AI大模型就是数据空间的一类算法基础设施。</strong>以通用大模型为基座，构建大模型研发与应用的基础设施，支撑广大企业研发领域专用大模型，服务于机器人、无人驾驶、可穿戴设备、智能家居、智能安防等行业，覆盖长尾应用。　　</p><p><strong>最后，全国一体化算力网建设在推动算力的基础设施化上发挥了先导作用。</strong>算力基础设施化的中国方案，应在大幅度降低算力使用成本和使用门槛的同时，为最广范围覆盖人群提供高通量、高品质的智能服务。算力基础设施的中国方案需要具备“两低一高”，即在供给侧，大幅度降低算力器件、算力设备、网络连接、数据获取、算法模型调用、电力消耗、运营维护、开发部署的总成本，让广大中小企业都消费得起高品质的算力服务，有积极性开发算力网应用；在消费侧，大幅度降低广大用户的算力使用门槛，面向大众的公共服务必须做到易获取、易使用，像水电一样即开即用，像编写网页一样轻松定制算力服务，开发算力网应用。在服务效率侧，中国的算力服务要实现低熵高通量，其中高通量是指在实现高并发⑧度服务的同时，端到端服务的响应时间可满足率高；低熵是指在高并发负载中出现资源无序竞争的情况下，保障系统通量不急剧下降。保障“算得多”对中国尤其重要。　　</p><h4id="选择三ai着重赋能虚拟经济还是发力实体经济"><strong>选择三：AI+着重赋能虚拟经济，还是发力实体经济？</strong>　　</h4><p>“AI+”的成效是人工智能价值的试金石。次贷危机后，美国制造业增加值占GDP的比重从1950年的28%降低为2021年的11%，美国制造业在全行业就业人数占比从1979年的35%降低为2022年的8%，可见美国更倾向于回报率更高的虚拟经济，轻视投资成本高且经济回报率低的实体经济。中国倾向于实体经济与虚拟经济同步发展，更加重视发展装备制造、新能源汽车、光伏发电、锂电池、高铁、5G等实体经济。　　</p><p>相应地美国AI主要应用于虚拟经济和IT基础工具，AI技术也是“脱实向虚”，自2007年以来硅谷不断炒作虚拟现实（VirtualReality，VR）、元宇宙、区块链、Web3.0、深度学习、AI大模型等，是这个趋势的反映。　　</p><p>我国的优势在实体经济，制造业全球产业门类最齐全，体系最完整，特点是场景多、私有数据多。我国应精选若干行业加大投入，形成可低门槛全行业推广的范式，如选择装备制造业作为延续优势代表性行业，选择医药业作为快速缩短差距的代表性行业。赋能实体经济的技术难点是AI算法与物理机理的融合。</p><p>人工智能技术成功的关键是能否让一个行业或一个产品的成本大幅下降，从而将用户数与产业规模扩大10倍，产生类似于蒸汽机对于纺织业，智能手机对于互联网业的变革效果。</p><p>我国应走出适合自己的人工智能赋能实体经济的高质量发展道路。</p><p><strong>注释：</strong>　　</p><p>①模式识别是指用计算的方法根据样本的特征将样本划分到一定的类别中去，是通过计算机用数学方法来研究模式的自动处理和判读，把环境与客体统称为“模式”。以图像处理与计算机视觉、语音语言信息处理、脑网络组、类脑智能等为主要研究方向。　　</p><p>②Token可翻译为词元，指自然语言处理过程中用来表示单词或短语的符号。token可以是单个字符,也可以是多个字符组成的序列。　　</p><p>③通用人工智能是指拥有与人类相当甚至超过人类智能的人工智能类型。通用人工智能不仅能像人类一样进行感知、理解、学习和推理等基础思维能力，还能在不同领域灵活应用、快速学习和创造性思考。通用人工智能的研究目标是寻求统一的理论框架来解释各种智能现象。　　</p><p>④芯片制造工艺指制造CPU或GPU的制程，即晶体管门电路的尺寸，单位为纳米，目前国际上实现量产的最先进工艺以台积电的3nm为代表。更先进的制造工艺可以使CPU与GPU内部集成更多的晶体管，使处理器具有更多的功能以及更高的性能，面积更小，成本更低等。　　</p><p>⑤CUDA是英伟达公司设计研发一种并行计算平台和编程模型，包含了CUDA指令集架构以及GPU内部的并行计算引擎。开发人员可以使用C语言来为CUDA架构编写程序，所编写出的程序可以在支持CUDA的处理器上以超高性能运行。　　</p><p>⑥RISC-V（发音为“risk-five”）是一个由美国加州大学伯克利分校发起的开放通用指令集架构，相比于其他付费指令集，RISC-V允许任何人免费地使用RISC-V指令集设计、制造和销售芯片和软件。　　</p><p>⑦长尾效应是指那些原来不受到重视的销量小但种类多的产品或服务由于总量巨大，累积起来的总收益超过主流产品的现象。在互联网领域，长尾效应尤为显著。　　</p><p>⑧高并发通常指通过设计保证系统能够同时并行处理很多请求。</p>]]></content>
    
    
    <categories>
      
      <category>阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>行业发展</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习框架中的动态图与静态图</title>
    <link href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E4%B8%AD%E7%9A%84%E5%8A%A8%E6%80%81%E5%9B%BE%E4%B8%8E%E9%9D%99%E6%80%81%E5%9B%BE.html"/>
    <url>/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E4%B8%AD%E7%9A%84%E5%8A%A8%E6%80%81%E5%9B%BE%E4%B8%8E%E9%9D%99%E6%80%81%E5%9B%BE.html</url>
    
    <content type="html"><![CDATA[<p>深度学习框架中，动态图（Dynamic Computation Graph）和静态图（StaticComputationGraph）是两种构建和执行计算图的方式。他们一个面向开发,一个面向部署,各有优势。</p><span id="more"></span><h3 id="一.-动态图dynamic-computation-graph">一. 动态图（DynamicComputation Graph）</h3><p>动态图，也称为即时执行模式（EagerExecution），是指在代码运行时即时构建和执行计算图。这种方式的特点是：</p><ol type="1"><li><strong>即时性</strong>：每一行代码在运行时都会立即执行相应的计算操作。</li><li><strong>灵活性</strong>：因为计算图是在运行时动态构建的，修改和调试都非常方便。可以轻松地使用Python的控制流（如条件语句和循环）构建复杂的模型。</li><li><strong>直观性</strong>：代码更加直观和易于理解，便于调试和开发。</li></ol><p>代表性的深度学习框架有： - PyTorch - TensorFlow 2.x 的EagerExecution模式</p><p>示例（PyTorch）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 动态构建和执行计算图</span><br>x = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], requires_grad=<span class="hljs-literal">True</span>)<br>y = x * <span class="hljs-number">2</span><br>z = y.mean()<br>z.backward()<br><br><span class="hljs-built_in">print</span>(x.grad)  <span class="hljs-comment"># 输出: tensor([0.6667, 0.6667, 0.6667])</span><br></code></pre></td></tr></table></figure><h3 id="二.-静态图static-computation-graph">二. 静态图（StaticComputation Graph）</h3><p>静态图，也称为定义-运行模式（Define-and-Run），是指在代码运行之前，先定义好计算图，然后再执行。这种方式的特点是：</p><ol type="1"><li><strong>高效性</strong>：由于计算图在运行前已经完全定义好，框架可以进行各种优化，提升执行效率和性能。</li><li><strong>可移植性</strong>：静态图可以保存为文件，便于在不同环境中加载和运行。</li><li><strong>可调度性</strong>：在执行前可以进行图的优化和分布式调度，提高资源利用率。</li></ol><p>代表性的深度学习框架有： - TensorFlow 1.x - TensorFlow 2.x 的GraphExecution模式</p><p>示例（TensorFlow 1.x）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br><span class="hljs-comment"># 定义计算图</span><br>x = tf.placeholder(tf.float32, shape=(<span class="hljs-literal">None</span>, <span class="hljs-number">3</span>))<br>y = x * <span class="hljs-number">2</span><br>z = tf.reduce_mean(y)<br><br><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:<br>    <span class="hljs-comment"># 执行计算图</span><br>    result = sess.run(z, feed_dict=&#123;x: [[<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>]]&#125;)<br>    <span class="hljs-built_in">print</span>(result)  <span class="hljs-comment"># 输出: 4.0</span><br></code></pre></td></tr></table></figure><h3 id="三.-对比总结">三. 对比总结</h3><ol type="1"><li><strong>开发体验</strong>：<ul><li><strong>动态图</strong>：开发体验更好，调试和代码修改更加方便，适合研究和快速原型开发。</li><li><strong>静态图</strong>：需要先定义完整的计算图，修改和调试相对复杂，但更适合大规模训练和部署。</li></ul></li><li><strong>执行性能</strong>：<ul><li><strong>动态图</strong>：灵活性高，但在大规模训练中，性能可能不如静态图。</li><li><strong>静态图</strong>：由于可以进行多种优化，执行性能通常更高，适合在生产环境中部署。</li></ul></li><li><strong>灵活性</strong>：<ul><li><strong>动态图</strong>：可以动态调整模型结构，支持复杂的控制流。</li><li><strong>静态图</strong>：在定义时就确定了模型结构，灵活性相对较低。</li></ul></li><li><strong>适用场景</strong>：<ul><li><strong>动态图</strong>：适用于研究、开发和模型调试。</li><li><strong>静态图</strong>：适用于模型训练和部署，尤其是在资源受限的环境下。</li></ul></li></ol><p>综合来看，动态图和静态图各有优劣，选择使用哪种方式取决于具体的应用需求和开发环境。在实际项目中，常常会根据不同的阶段和任务需求，灵活选择使用动态图或静态图。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>笔记整理</tag>
      
      <tag>概念</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【B站】从零开始学习大语言模型-Lyi</title>
    <link href="/B%E7%AB%99_%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E4%B9%A0%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-Lyi.html"/>
    <url>/B%E7%AB%99_%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E4%B9%A0%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-Lyi.html</url>
    
    <content type="html"><![CDATA[<h1 id="从零开始学习大语言模型-lyi">从零开始学习大语言模型-Lyi</h1><p>林亦是我比较喜欢的一个UP，视频讲述了他对深度学习基本范式的回顾和梳理。主要介绍了神经网络模型的结构和训练过程，以及当前流行的大语言模型——基于神经网络的技术。视频指出，构建一个能力强、学习效率高的模型是影响学习效果的关键，也是深度学习研究的核心问题。整个视频围绕着数据和模型展开，梳理了深度学习的核心概念和基本流程。</p><span id="more"></span><iframe src="//player.bilibili.com/player.html?isOutside=true&amp;aid=1750586968&amp;bvid=BV1v4421w7pU&amp;cid=1441154247&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe><h3id="机器学习的概念和分类着重讲解了监督学习和无监督学习以及模型的重要性">机器学习的概念和分类,着重讲解了监督学习和无监督学习,以及模型的重要性。</h3><p>00:01机器学习概念与语言任务</p><p>02:01监督学习与无监督学习</p><p>04:00机器学习模型的构建和选择</p><h3id="模型的概念以及神经网络模型的运作原理包括感知机单元和多层神经网络模型">模型的概念,以及神经网络模型的运作原理,包括感知机单元和多层神经网络模型。</h3><p>04:26模型训练与神经网络结构</p><p>06:40感知机的原理和激活函数</p><p>07:45多层神经网络模型和信息归纳能力</p><h3id="神经网络中的函数导数偏导数损失函数和梯度下降等概念以及如何用反向传播算法优化权重值">神经网络中的函数、导数、偏导数、损失函数和梯度下降等概念,以及如何用反向传播算法优化权重值。</h3><p>08:15每个单元之间的连接是一个权重数值，这些数值可以通过反向传播算法进行优化。</p><p>09:27训练神经网络的流程：训练程序会为每个输入变量随机分配一个权重值，然后通过前向传播和梯度下降算法不断优化权重值，直到损失函数最小化。</p><p>11:08梯度下降算法：通过不断更新权重值，沿着梯度下降的方向最快达到最低损失函数值。</p><h3id="如何使用链式法则解决复杂函数的导数计算问题以及模型训练中的收敛和超参数设置">如何使用链式法则解决复杂函数的导数计算问题,以及模型训练中的收敛和超参数设置。</h3><p>12:22链式法则与模型更新</p><p>15:26深度学习中的残差网络</p><p>15:50梯度消失与跳过连接</p><h3id="残差网络的作用和意义以及机器学习中泛化能力的评估和重要性">残差网络的作用和意义,以及机器学习中泛化能力的评估和重要性。</h3><p>17:05模型遇到从未见过的数据时，能不能整明白。</p><p>19:16大模型的转变：从小型专用模型到大型通用模型的转变。</p><h3id="机器如何理解人类语言的基础知识并提到了接下来将进一步探讨这一话题">机器如何理解人类语言的基础知识,并提到了接下来将进一步探讨这一话题。</h3><p>19:45机器理解人类语言的基础知识</p><p>19:52视频结束及作者告别</p>]]></content>
    
    
    <categories>
      
      <category>B站</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
      <tag>视频分享</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ROC曲线原理及绘制</title>
    <link href="/ROC%E6%9B%B2%E7%BA%BF%E7%BB%98%E5%88%B6.html"/>
    <url>/ROC%E6%9B%B2%E7%BA%BF%E7%BB%98%E5%88%B6.html</url>
    
    <content type="html"><![CDATA[<p>ROC曲线（Receiver Operating CharacteristicCurve）是一种用于评价二分类模型性能的图形工具。它展示了模型在不同阈值下的分类性能，通过绘制假阳性率（FPR）和真阳性率（TPR）之间的关系来表现。</p><span id="more"></span><h3 id="混淆矩阵">1.混淆矩阵</h3><p><strong>混淆矩阵</strong>是对<strong>预测正例</strong>样本的进一步分析,而<strong>准确率</strong>是综合了正例与反例的比例.</p><table><thead><tr class="header"><th></th><th>预测值</th><th>预测值</th><th></th></tr></thead><tbody><tr class="odd"><td></td><td><strong>正例</strong> (positive)</td><td><strong>假例</strong> (negtive)</td><td></td></tr><tr class="even"><td><strong>真实正例</strong></td><td>真正例TP</td><td><strong>伪反例</strong>FN</td><td>TPR=TP/真实正例</td></tr><tr class="odd"><td><strong>真实假例</strong></td><td><strong>伪正例</strong> FP</td><td>真反例TN</td><td>FPR=FP/真实假例</td></tr></tbody></table><h3 id="roc曲线的含义">2.ROC曲线的含义</h3><p>ROC曲线（Receiver Operating CharacteristicCurve）是一种用于评价二分类模型性能的图形工具。它展示了模型在不同阈值下的分类性能，通过绘制假阳性率（FPR）和真阳性率（TPR）之间的关系来表现。</p><ul><li><p><strong>真阳性率（TPR, True PositiveRate）</strong>：也称为灵敏度（sensitivity）或召回率（recall），表示真正被分类为正类的比例。公式为：<span class="math display">\[\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}\]</span></p></li><li><p><strong>假阳性率（FPR, False PositiveRate）</strong>：表示被错误分类为正类的负类样本比例。公式为： <spanclass="math display">\[\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}\]</span></p></li></ul><h3 id="roc曲线的绘制步骤">3.ROC曲线的绘制步骤</h3><ol type="1"><li><strong>计算预测概率</strong>：使用二分类模型对数据进行预测，得到每个样本属于正类的概率。</li><li><strong>确定阈值</strong>：从0到1选择一系列阈值，对每个阈值进行如下操作：<ul><li>将预测概率与当前阈值比较，得到分类结果（大于等于阈值为正类，小于阈值为负类）。</li><li>计算对应的TPR和FPR。</li></ul></li><li><strong>绘制曲线</strong>：以FPR为横坐标，TPR为纵坐标，绘制曲线。</li></ol><h3 id="实例说明">4.实例说明</h3><p>假设有一个简单的二分类问题，以下是一些预测结果及对应的实际标签：</p><table><thead><tr class="header"><th>实际标签</th><th>预测概率</th></tr></thead><tbody><tr class="odd"><td>1</td><td>0.9</td></tr><tr class="even"><td>0</td><td>0.8</td></tr><tr class="odd"><td>1</td><td>0.7</td></tr><tr class="even"><td>1</td><td>0.6</td></tr><tr class="odd"><td>0</td><td>0.4</td></tr><tr class="even"><td>1</td><td>0.3</td></tr><tr class="odd"><td>0</td><td>0.2</td></tr><tr class="even"><td>0</td><td>0.1</td></tr></tbody></table><p>我们使用这些数据来绘制ROC曲线。</p><h4 id="步骤1计算tpr和fpr">步骤1：计算TPR和FPR</h4><p>我们选择几个阈值来计算TPR和FPR：</p><ul><li>阈值 = 0.9</li><li>阈值 = 0.7</li><li>阈值 = 0.5</li><li>阈值 = 0.3</li><li>阈值 = 0.1</li></ul><p>对于每个阈值，计算TPR和FPR：</p><ol type="1"><li><p><strong>阈值 = 0.9</strong>:</p><ul><li>预测结果：1 0 0 0 0 0 0 0</li><li>TPR = 1/4 = 0.25</li><li>FPR = 0/4 = 0</li></ul></li><li><p><strong>阈值 = 0.7</strong>:</p><ul><li>预测结果：1 1 1 0 0 0 0 0</li><li>TPR = 2/4 = 0.5</li><li>FPR = 1/4 = 0.25</li></ul></li><li><p><strong>阈值 = 0.4</strong>:</p><ul><li>预测结果：1 1 1 1 1 0 0 0</li><li>TPR = 3/4 = 0.75</li><li>FPR = 2/4 = 0.5</li></ul></li><li><p><strong>阈值 = 0.3</strong>:</p><ul><li>预测结果：1 1 1 1 1 1 0 0</li><li>TPR = 4/4 = 1</li><li>FPR = 2/4 = 0.5</li></ul></li><li><p><strong>阈值 = 0.2</strong>:</p><ul><li>预测结果：1 1 1 1 1 1 1 0</li><li>TPR = 4/4 = 1</li><li>FPR = 3/4 = 0.75</li></ul><p><img src="/images/roc/image-20240526220530303.png" /></p></li></ol><h4 id="python绘制曲线">5.python绘制曲线</h4><p>我们将这些TPR和FPR值在图上绘制出来，得到ROC曲线。</p><p>下面用Python代码实现绘制ROC曲线：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_curve<br><br><span class="hljs-comment"># 实际标签</span><br>y_true = [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># 预测概率</span><br>y_scores = [<span class="hljs-number">0.9</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.1</span>]<br><br><span class="hljs-comment"># 计算FPR和TPR</span><br>fpr, tpr, thresholds = roc_curve(y_true, y_scores)<br><br><span class="hljs-comment"># 绘制ROC曲线</span><br>plt.figure()<br>plt.plot(fpr, tpr, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;False Positive Rate&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;True Positive Rate&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;ROC Curve&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>这段代码将绘制出对应的数据的ROC曲线。ROC曲线越靠近左上角，表示模型性能越好。曲线下面积（AUC,Area Under theCurve）可以用来量化模型的整体性能，AUC值越大表示模型性能越好。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ROC曲线</tag>
      
      <tag>模型评估</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何知道一个大模型是否可以在自己的显卡上运行呢？</title>
    <link href="/%E5%A6%82%E4%BD%95%E7%9F%A5%E9%81%93%E4%B8%80%E4%B8%AA%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E5%9C%A8%E8%87%AA%E5%B7%B1%E7%9A%84%E6%98%BE%E5%8D%A1%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%91%A2%EF%BC%9F.html"/>
    <url>/%E5%A6%82%E4%BD%95%E7%9F%A5%E9%81%93%E4%B8%80%E4%B8%AA%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E5%9C%A8%E8%87%AA%E5%B7%B1%E7%9A%84%E6%98%BE%E5%8D%A1%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%91%A2%EF%BC%9F.html</url>
    
    <content type="html"><![CDATA[<h4 id="经验评估">1.经验评估</h4><ul><li>推理显存估算：7B-float 是 28 GB，7B-BF16 是 14GB，7B-int8 是7GB；其他版本以此类推即可。</li><li>训练的参数类型，只能是 float / BF16</li><li>训练 所需显存 保守估算 是 同参数同类型llm 推理 的 4倍。<ul><li>例子：7B-float 训练 显存：28 * 4 = 112 GB</li></ul></li></ul><table><thead><tr class="header"><th>方法</th><th>bits</th><th>7B</th><th>13B</th><th>30B</th><th>65B</th><th>8*7B</th></tr></thead><tbody><tr class="odd"><td>全参数微调</td><td>16</td><td>160GB</td><td>320GB</td><td>600GB</td><td>1200GB</td><td>900GB</td></tr><tr class="even"><td>Freeze</td><td>16</td><td>20GB</td><td>40GB</td><td>120GB</td><td>240GB</td><td>200GB</td></tr><tr class="odd"><td>LoRA</td><td>16</td><td>16GB</td><td>32GB</td><td>80GB</td><td>160GB</td><td>120GB</td></tr><tr class="even"><td>QLoRA</td><td>8</td><td>10GB</td><td>16GB</td><td>40GB</td><td>80GB</td><td>80GB</td></tr><tr class="odd"><td>QLoRA</td><td>4</td><td>6GB</td><td>12GB</td><td>24GB</td><td>48GB</td><td>32GB</td></tr></tbody></table><h4 id="精确评估">2.精确评估</h4><h5 id="在线评估">2.1 在线评估</h5><p>accelerate estimate-memory 是 huggingface 的 accelerate开发库中提供的一个工具。可网页在线访问</p><p>https://huggingface.co/spaces/hf-accelerate/model-memory-usage选择相应模型进行评估</p><h5 id="本地评估">2.2 本地评估</h5><ul><li>安装 accelerate, transformers</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">pip install accelerate<br>pip install transformers<br></code></pre></td></tr></table></figure><ul><li>使用方法举例</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs text"># 基本使用方法<br>accelerate estimate-memory mistralai/Mistral-7B-v0.1<br><br># 只显示指定的数据类型<br>accelerate estimate-memory mistralai/Mistral-7B-v0.1 --dtypes float16<br><br># 指定开发库(针对本地模型，Hub上存储的模型不需要指定)<br>accelerate estimate-memory mistralai/Mistral-7B-v0.1 --dtypes float32 float16 --library_name transformers<br><br># 设置 trust_remote_code=True<br>accelerate estimate-memory Qwen/Qwen1.5-7B #正常<br>accelerate estimate-memory Qwen/Qwen-7B #报错<br>accelerate estimate-memory Qwen/Qwen-7B --trust_remote_code #可以运行<br><br># 其他模型<br>accelerate estimate-memory google/gemma-7b<br>accelerate estimate-memory baichuan-inc/Baichuan2-7B-Base --trust_remote_code<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>大模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>部署</tag>
      
      <tag>模型训练</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客上云 纵享丝滑</title>
    <link href="/%E5%8D%9A%E5%AE%A2%E4%B8%8A%E4%BA%91%E7%BA%B5%E4%BA%AB%E4%B8%9D%E6%BB%91.html"/>
    <url>/%E5%8D%9A%E5%AE%A2%E4%B8%8A%E4%BA%91%E7%BA%B5%E4%BA%AB%E4%B8%9D%E6%BB%91.html</url>
    
    <content type="html"><![CDATA[<p>让你的博客上云，体验丝般顺滑~</p><span id="more"></span><h1 id="一工作原理">一、工作原理</h1><p>使用Hexo搭建个人博客并自动部署到阿里云ECS服务器的原理如下图所示：</p><p><a href="/images/hexo_aliyun/Hexo_ALiYun.jpg"><imgsrc="/images/hexo_aliyun/Hexo_ALiYun.jpg"alt="基于Hexo的博客搭建和阿里云部署原理" /></a></p><p>简单来说就是在本地计算机搭建Hexo环境，Hexo通过generate命令将*.md文件渲染成静态的html页面，然后Hexo通过deploy命令触发git用户通过公钥免密登陆服务器，进而将静态页面推送到服务器的git仓库（repository）中。然后，服务器再通过钩子（git-hooks）将静态页面checkout到网站的根目录下，进而实现博客的自动部署。具体过程如图中实线箭头所示。</p><h1 id="二搭建步骤">二、搭建步骤</h1><h2 id="在本地计算机安装hexo环境">1、在本地计算机安装Hexo环境</h2><p>首先需要说明的是：我本地使用的是Win10（64位）操作系统。更权威的安装过程可以参照<ahref="https://hexo.io/zh-cn/">Hexo官方主页</a>。</p><h3 id="安装node.js">1.1 安装Node.js</h3><p>去<a href="https://nodejs.org/en/">Node.js官网</a>下载Windows(x64)长期支持版 Long Term Support (LTS)schedule。按提示逐步安装即可，安装完成后打开cmd查看版本号验证是否安装成功。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">C:\Users\tangcl&gt; <span class="hljs-keyword">node</span> <span class="hljs-title">-v</span><br>v12.<span class="hljs-number">13.1</span><br></code></pre></td></tr></table></figure><p>Node.js中自带了npm包管理工具，在cmd中查看npm版本。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">C</span>:\Users\tangcl&gt; npm -v<br><span class="hljs-attribute">6</span>.<span class="hljs-number">12</span>.<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h3 id="安装git">1.2 安装Git</h3><p>git是一个版本控制工具，国外镜像下载巨慢，建议前往<ahref="https://npm.taobao.org/mirrors/git-for-windows/">淘宝 Git forWindows 镜像</a>下载 git安装包。按提示逐步安装即可，安装完成后右键菜单中出现Git Bash和GitGUI菜单表明安装成功，如下图所示。 <ahref="/images/hexo_aliyun/git_menu.png"><imgsrc="/images/hexo_aliyun/git_menu.png" alt="git右键菜单" /></a></p><p>注：git和github是两个东西。github是基于git二次开发的，git是github的核心，git负责与github相关的所有本地工作。</p><h3 id="安装hexo">1.3 安装Hexo</h3><p>在D盘新建MyHexoBlogs文件夹用来存放个人博客，进入该文件夹，右键打开GitBash，使用 npm 安装 Hexo。</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm">npm install -g hexo-<span class="hljs-keyword">cli</span><br></code></pre></td></tr></table></figure><p>运行结果如下： <a href="/images/hexo_aliyun/installHexo.png"><imgsrc="/images/hexo_aliyun/installHexo.png" alt="安装Hexo" /></a>Hexo安装完成后，在MyHexoBlogs文件夹下新建myblogs项目，并对其进行初始化。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo init myblogs<br><span class="hljs-built_in">cd</span> myblogs<br>npm install<br></code></pre></td></tr></table></figure><p>此时，会在MyHexoBlogs文件夹下新建myblogs文件夹，并在其内部生成相应的项目文件。如下图所示：<a href="/images/hexo_aliyun/files.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="文件" /></a> 在myblogs文件夹下启动hexo服务。</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs axapta">hexo <span class="hljs-keyword">server</span><br></code></pre></td></tr></table></figure><p>此时在本地打开浏览器，通过 http://localhost:4000/便可访问基于Hexo的个人博客主页了。如下图所示： <ahref="/images/hexo_aliyun/hexo.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="个人博客本地主页" /></a></p><h2 id="服务端准备工作">2、服务端准备工作</h2><h3 id="域名注册">2.1 域名注册</h3><p>网站搭建之前我们需要注册自己的域名，因为我们不可能让用户通过“公网IP+端口”的方式访问我们的服务器，这样太不方便记忆了。</p><p>因为万网已被阿里收购，所以对于阿里云用户，我们可以直接在<ahref="https://wanwang.aliyun.com/?spm=5176.12825654.eofdhaal5.9.3dbd2c4anS0SLJ&amp;aly_as=SIqz0Gsr">阿里云域名注册官网</a>上直接注册购买。<a href="/images/hexo_aliyun/yuming.png"><imgsrc="/images/hexo_aliyun/yuming.png" alt="域名注册" /></a>由于域名可以交易，所以域名注册应当有点战略性眼光，应简单直观、方便记忆。域名格式参考是<ahref="http://www.xxxxxx.com/">www.xxxxxx.com</a>。</p><h3 id="域名实名认证">2.2 域名实名认证</h3><p>域名注册过程中，必须进行邮箱和身份证实名认证才可以继续购买，我们只需按提示进行操作即可。</p><h3 id="购买阿里云ecs服务器">2.3 购买阿里云ECS服务器</h3><p>阿里的云服务产品有很多种，如阿里云主机、ECS服务器等。我这里购买的是阿里云ECS服务器。所谓ECS，即弹性计算服务。</p><p>进入阿里云官网的ECS专区购买即可。 <ahref="/images/hexo_aliyun/aliyunECS.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="阿里云ECS" /></a>如下图所示，我这里购买的是双十二入门级活动套餐：实例1核1G（预装CentOS7.4） + 40G高效云盘 + 1M带宽，小白用户选择此配置足以。</p><p><a href="/images/hexo_aliyun/myECS.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="我的ECS配置" /></a>付款成功后，你就拥有一个属于自己的ECS服务器实例了。所谓实例，就是一台装了CentOS的电脑。接下来就是对该实例进行设置，并在它上面搭建相应的部署环境了。</p><h3 id="ecs服务器备案">2.4 ECS服务器备案</h3><p>备案需要有服务器和域名。</p><p>国家法律规定，使用中国大陆境内服务器托管你的网站时，你必须对你的网站进行备案申请。当你使用阿里云中国大陆境内节点的服务器时，你可以直接在<ahref="https://beian.aliyun.com/">阿里云备案管理系统</a>中提交ICP备案申请。<a href="/images/hexo_aliyun/beian.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="备案" /></a>ICP备案申请审核通过后，管局（工信部）会给我们一个ICP备案号，我们需要将备案号在网站底部标明。网站在工信部备案成功后，还需要在网站开通之日起30日内登录<ahref="http://beian.gov.cn/portal/index">全国公安机关互联网站安全管理服务平台</a>提交公安联网备案申请。</p><h3 id="阿里云服务器设置">2.5 阿里云服务器设置</h3><h4 id="重置实例密码">（1）重置实例密码</h4><p>点击阿里云首页的控制台按钮，登录到云服务器管理控制台，便可以查看自己购买的实例了。<a href="/images/hexo_aliyun/control.png"><imgsrc="/images/hexo_aliyun/control.png" alt="登录控制台" /></a>新买的ECS服务器实例对root用户是没有设置初始密码的,ECS服务器的root密码需要重置才能用。重置步骤如下：选中ECS服务器实例，点击下面的重置密码按钮即可重置root用户的密码，密码在实例重启后生效。（该密码必须是字母、数字和其它字符组成的8位以上字符串。）</p><p><a href="/images/hexo_aliyun/shili.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="实例" /></a></p><h4 id="远程连接linux实例">（2）远程连接Linux实例</h4><p>远程连接服务器的方法都很多。我们既可以通过阿里云自带的VNC（VirtualNetworkConsole，虚拟网络控制台）远程连接Linux实例，也可以通过远程连接软件（例如PuTTY、Xshell、SecureCRT等）连接Linux实例。</p><p>我这里用到是VNC方法。需要说明的是：使用阿里云自带的VNC远程连接Linux实例，登录VNC窗口时还要输入一个6位数的远程连接密码，用于连接ECS管理控制台的管理终端，注意不要与root密码混淆。</p><p>注：</p><ul><li>远程连接密码用于连接ECS管理控制台的管理终端，而实例登录密码（root密码）用于登录实例。</li><li>远程连接密码仅在第一次连接管理终端时显示一次，建议启用后立即修改远程连接密码。</li></ul><p>具体连接步骤如下： a.在实例列表中选中当前实例，点击右侧按钮：远程连-&gt;VNC。 b.输入远程连接密码。 <a href="/images/hexo_aliyun/connect.png"><imgsrc="/images/hexo_aliyun/connect.png" alt="远程连接密码" /></a> c.在控制台中输入用户名：root，及其root密码（实例密码）。回车即可进入阿里云ECS服务器的后台，如下图所示。<a href="/images/hexo_aliyun/aliyunServer.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="ECS服务器后台" /></a></p><p>后面，我们主要就是利用此终端在ECS上部署网站运行环境了。</p><h4 id="配置安全组">（3）★ 配置安全组</h4><p>由于我们要通过80端口访问nginx服务，而阿里云默认是禁止80端口访问权限的，所以我们要为实例手动添加安全组，让阿里云给相应的端口和IP放行。该步骤非常重要，若不手动配置，我们将无法通过“公网IP+端口”的方式访问我们的ECS服务器。</p><p>具体操作步骤如下： a.打开阿里云服务管理控制台，点击左侧菜单中的“安全组”按钮，查看安全组列表。b. 点击右上角的“创建安全组”按钮，创建一个新的安全组。 c.立即为新建的安全组添加安全组规则，在入方向解除端口和IP限制，具体参数设置如下图所示。<a href="/images/hexo_aliyun/safe.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="添加安全规则" /></a> d. 在实例列表中为实例添加安全组。</p><p><a href="/images/hexo_aliyun/add.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="为实例添加安全组" /></a></p><p>这样就完成了安全组的配置。<em>注：安全组出方向默认允许所有访问，即从安全组内ECS访问外部都是放行的。</em></p><h2 id="hexo博客的阿里云部署">3、Hexo博客的阿里云部署</h2><p><em>该步骤是整个博客搭建过程中最重要的一步，实现过程中一定要注意是在服务端操作还是在本地计算机上操作。若在服务器上操作，还要注意是使用root用户进行操作还是使用git用户进行操作。</em></p><h3 id="安装nginx">3.1 ★ 安装nginx</h3><p>因为我们用nginx作Web服务器，所以我们需要先安装nginx服务。具体步骤如下：</p><p>使用root用户远程登录阿里云服务器，使用yum命令进行安装。</p><ol type="a"><li>安装nginx依赖环境，安装期间有提示一律选yes。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#yum install gcc-c++</span><br><span class="hljs-meta">#yum install -y pcre pcre-devel</span><br><span class="hljs-meta">#yum install -y zlib zlib-devel</span><br><span class="hljs-meta">#yum install -y openssl openssl-devel</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>下载nginx安装包。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#wget -c https:<span class="hljs-comment">//nginx.org/download/nginx-1.10.1.tar.gz</span></span><br></code></pre></td></tr></table></figure><ol start="3" type="a"><li>将安装包解压到/usr/local目录下。</li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-id">#tar</span> -xvf nginx-<span class="hljs-number">1.10</span>.<span class="hljs-number">1</span><span class="hljs-selector-class">.tar</span><span class="hljs-selector-class">.gz</span> -C /usr/local<br></code></pre></td></tr></table></figure><ol start="4" type="a"><li>进入/usr/local目录，确认nginx解压到该目录下。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> /usr/local</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">ls</span></span><br></code></pre></td></tr></table></figure><ol start="5" type="a"><li>进入nginx-1.10.1目录，会发现该目录下有一个configure文件，执行该配置文件。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> nginx-1.10.1/</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">ls</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">./configure</span><br></code></pre></td></tr></table></figure><ol start="6" type="a"><li>编译并安装nginx。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#make</span><br><span class="hljs-meta">#make install</span><br></code></pre></td></tr></table></figure><ol start="7" type="a"><li>查找nginx安装目录。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#whereis nginx</span><br></code></pre></td></tr></table></figure><p>h.进入安装目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> /usr/local/nginx</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">ls</span></span><br></code></pre></td></tr></table></figure><ol type="i"><li>由于nginx默认通过80端口访问，而Linux默认情况下不会开发该端口号，因此需要开放linux的80端口供外部访问。</li></ol><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">#/sbin/iptables -<span class="hljs-selector-tag">I</span> <span class="hljs-selector-tag">INPUT</span> -<span class="hljs-selector-tag">p</span> tcp <span class="hljs-attr">--dport</span> <span class="hljs-number">80</span> -j ACCEPT<br></code></pre></td></tr></table></figure><ol start="10" type="a"><li>进入/usr/local/nginx/sbin目录，启动nginx。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> sbin</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">./nginx</span><br></code></pre></td></tr></table></figure><p>没有任何消息，代表启动成功。此时，便可以通过“公网IP+端口”的方式访问<a href="http://xx.xx.xxx.xxx/">http://xx.xx.xxx.xxx:80/</a>进入nginx欢迎页面了。 <strong>注：</strong> <strong>（1）可以使用./nginx-s stop命令停止服务；</strong><strong>（2）网站搭建成功后，若出现宕机现象，很有可能是nginx服务器挂了，此时应检查nginx服务器状态，并进行重启操作。</strong></p><h3 id="配置nginx服务器路由">3.2 配置nginx服务器路由</h3><ol type="a"><li>专门为hexo创建一个部署目录/home/www/hexo。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">mkdir</span> -p /home/www/hexo</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>进入/usr/local/nginx/conf目录，打开该文件夹下的nginx.conf配置文件。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> /usr/local/nginx/conf</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">ls</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">vim nginx.conf</span><br></code></pre></td></tr></table></figure><p>进入后按i键由命令模式切换到编辑模式。</p><ul><li>将其中的部署根目录（root）修改为/home/www/hexo；</li><li>将域名（server_name）<ahref="http://xn--www-c88dx1fq77c.xxxxxx.com/">修改为www.xxxxxx.com</a>，如果暂时没有域名就填阿里云实例的公网ip，以后有了再改回来；</li><li>查看监听端口（listen）的系统默认值是否为80（不用修改）。</li></ul><p>完成以上修改后，先按Esc由编辑模式切换到命令模式，再输入:wq命令保存并退出编辑器。</p><h3 id="安装node.js-1">3.3 安装node.js</h3><ol type="a"><li>退回根目录，安装node.js。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">cd</span> ~</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">curl -sL https://rpm.nodesource.com/setup_10.x | bash -</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">yum install -y nodejs</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>查看安装结果，打印版本号即为安装成功。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#node -v</span><br><span class="hljs-meta">#npm -v</span><br></code></pre></td></tr></table></figure><h3 id="安装git-1">3.4 安装Git</h3><ol type="a"><li>使用yum命令安装Git，安装期间有提示一律选yes。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#yum install git</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>安装成功后，查看版本号。</li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-id">#git</span> <span class="hljs-attr">--version</span><br></code></pre></td></tr></table></figure><h3 id="创建git用户">3.5 创建git用户</h3><p>为了实现博客的自动部署，我们后面要使用公钥免密登录服务器。为了安全起见，最好不要使用root用户免密登录。因此，我们要创建一个新的git用户，用于远程公钥免密登录服务器。</p><ol type="a"><li>创建git用户。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#adduser git</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>修改git用户的权限。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">chmod</span> 740 /etc/sudoers</span><br></code></pre></td></tr></table></figure><ol start="3" type="a"><li>打开文件。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">vim /etc/sudoers</span><br></code></pre></td></tr></table></figure><p>进入后按i键由命令模式切换到编辑模式。找到 root ALL=(ALL)ALL，在下面添加一行 <strong>git ALL=(ALL)ALL</strong>。修改完成后，先按Esc由编辑模式切换到命令模式，再输入:wq命令保存并退出编辑器。</p><ol start="4" type="a"><li>保存退出后改回权限。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">chmod</span> 400 /etc/sudoers</span><br></code></pre></td></tr></table></figure><ol start="5" type="a"><li>设置git用户的密码。</li></ol><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-meta">#sudo passwd git</span><br></code></pre></td></tr></table></figure><p>设置密码：************，这样我们就可以使用git用户远程登录阿里云服务器了。</p><h3 id="给git用户配置ssh免密公钥登录">3.6 ★给git用户配置ssh免密公钥登录</h3><p>该步骤是基于Hexo搭建个人博客的核心步骤，也是坑我时间最长的地方。它既需要在本地计算机上操作，也需要在服务器上进行操作，新手一定要搞清原理才不会弄错。</p><p>使用git用户免密公钥登录阿里云服务器的原理是：在本地计算机生成一个公钥文件和一个秘钥文件（类似于一个钥匙配一把锁)，然后使用FTP工具将公钥文件上传到阿里云服务器，并公钥安装到authorized_keys列表中去（即：将公钥文件的内容拷贝到authorized_keys文件中去）。这样本地计算机便可以通过ssh方式免密连接我们的阿里云服务器了。</p><p>具体操作步骤如下：</p><ol type="a"><li>在服务器端将登陆用户切换到git用户，然后在~目录(根目录)下创建.ssh文件夹，用来存放公钥。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">su git</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">cd</span> ~</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">mkdir</span> .ssh</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>在本地计算机桌面右键打开GitBash，在本地生成公钥/私钥对。</li></ol><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-meta"><span class="hljs-keyword">$cd</span> ~</span><br><span class="hljs-meta"><span class="hljs-keyword">$cd</span> .ssh</span><br><span class="hljs-meta"><span class="hljs-keyword">$ssh</span>-keygen</span><br></code></pre></td></tr></table></figure><p>接下来，碰见系统询问就直接按回车键。此时便会在本地计算机的用户根目录（C:）下自动生成.ssh（隐藏）文件夹，并在其中创建两个文件，分别为：id_rsa（私钥）和id_rsa.pub（公钥）。</p><ol start="3" type="a"><li>在本地计算机上给私钥设置权限。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">chmod</span> 700 ~/.ssh<br><span class="hljs-built_in">chmod</span> 600 ~/.ssh/id_rsa <br></code></pre></td></tr></table></figure><ol start="4" type="a"><li><p>下载并安装FTP工具，我这里用的是阿里云官方提供的<ahref="https://help.aliyun.com/knowledge_detail/36243.html">FileZilla（Windows版本）</a>。</p></li><li><p>打开FileZilla，使用git用户通过22端口远程连接到阿里云服务器，将客服端生成的公钥上传到服务器的~/.ssh目录下。</p></li></ol><p><a href="/images/hexo_aliyun/ftp.png"><imgsrc="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"alt="FTP" /></a></p><ol start="6" type="a"><li>上传完成后切回服务器端，继续以git用户的身份进入服务器~/.ssh目录，新建一个authorized_keys文件，并将id_rsa.pub文件中公钥的内容拷贝到该文件中。<em>（注：该步骤既可以用命令行操作，也可使用FTP工具操作。）</em></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">cd</span> ~/.ssh</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">cp</span> id_rsa.pub authorized_keys</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">cat</span> id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></code></pre></td></tr></table></figure><ol start="7" type="a"><li>在服务器上设置文件权限：</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">chmod</span> 600 ~/.ssh/authorized_keys</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">chmod</span> 700 ~/.ssh</span><br></code></pre></td></tr></table></figure><ol start="8" type="a"><li>确保设置了正确的SELinux上下文。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">restorecon -Rv ~/.ssh <br></code></pre></td></tr></table></figure><p>现在，当您使用ssh远程登录服务器时，将不会提示您输入密码（除非您在创建密钥对时输入了密码）。i. 接下来在本地计算机上使用ssh方式连接我们的云服务器。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-variable">$ssh</span> -v git@xxx<span class="hljs-selector-class">.xxx</span><span class="hljs-selector-class">.xxx</span>.xxx（阿里云公网IP）<br></code></pre></td></tr></table></figure><p>或</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-variable">$ssh</span> git@xxx<span class="hljs-selector-class">.xxx</span><span class="hljs-selector-class">.xxx</span>.xxx（阿里云公网IP）<br></code></pre></td></tr></table></figure><p>使用git用户ssh免密公钥登录成功界面如下图所示。 <ahref="/images/hexo_aliyun/ssh.png"><imgsrc="/images/hexo_aliyun/ssh.png" alt="ssh" /></a></p><h3 id="配置git仓库">3.7 配置Git仓库</h3><ol type="a"><li>在服务器上使用git用户创建一个Git仓库，并且在该仓库中新建一个post-receive钩子文件。</li></ol><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-meta"><span class="hljs-keyword">$cd</span> ~</span><br><span class="hljs-meta"><span class="hljs-keyword">$git</span> init --bare hexo.git</span><br><span class="hljs-meta"><span class="hljs-keyword">$vi</span> ~/hexo.git/hooks/post-receive</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li>进入后按i键由命令模式切换到编辑模式。输入： <strong>git--work-tree=/home/www/hexo --git-dir=/home/git/hexo.git checkout-f</strong></li></ol><p>即：让钩子文件删除/home/www/hexo目录下原有的文件，然后从blog.git仓库clone 新的博客静态文件到/home/www/hexo目录下。</p><p>完成以上修改后，先按Esc由编辑模式切换到命令模式，再输入:wq命令保存并退出编辑器。</p><ol start="3" type="a"><li>授予钩子文件可执行权限。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">chmod</span> +x ~/hexo.git/hooks/post-receive</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash"><span class="hljs-built_in">cd</span> ~</span><br><span class="hljs-meta prompt_">$</span><span class="language-bash">sudo <span class="hljs-built_in">chmod</span> -R 777 /home/www/hexo</span><br></code></pre></td></tr></table></figure><ol start="4" type="a"><li>重启ECS服务器实例。</li></ol><p>至此我们就完成了所有关于服务器端的配置。</p><h2 id="其它配置">4、其它配置</h2><h3 id="客服端hexo配置">4.1 客服端hexo配置</h3><ol type="a"><li><p>在本地计算机hexo的工程目录下，找到_config.yml，对deploy参数进行修改，如下图所示。<a href="/images/hexo_aliyun/deploy.png"><imgsrc="/images/hexo_aliyun/deploy.png" alt="deploy" /></a></p></li><li><p>在本地计算机安装插件: hexo-deployer-git 和hexo-server。在myblogs文件夹下右键打开GitBash，输入以下命令：</p></li></ol><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-meta"><span class="hljs-keyword">$npm</span> install hexo-deployer-git --save</span><br><span class="hljs-meta"><span class="hljs-keyword">$npm</span> install hexo-server</span><br></code></pre></td></tr></table></figure><p><em>这俩插件的作用分别是使用Git自动部署，和hexo本地简单的服务器。</em></p><ol start="3" type="a"><li>在本地计算机配置Git全局变量。 输入以下命令：</li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.email</span> <span class="hljs-string">&quot;xxxxxxxxxx@qq.com&quot;</span><br>git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.name</span> “tangcl”<br></code></pre></td></tr></table></figure><ol start="4" type="a"><li>使用Hexo生成、发布个人博客。</li></ol><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs verilog">hexo clean<br>hexo <span class="hljs-keyword">generate</span><br>hexo deploy<br></code></pre></td></tr></table></figure><p>此时，便可以通过浏览器访问<ahref="http://xxx.xxx.xxx.xxx/">http://xxx.xxx.xxx.xxx:80/</a>进入hexo我的博客主页了。</p><h3 id="域名绑定">4.2 域名绑定</h3><p>待ECS服务器备案审核通过，在阿里云后台对域名解析进行设置，将域名的解析值修改为ECS实例的公网IP。进而完成域名与ECS服务器实例的公网IP进行绑定。</p><p><a href="/images/hexo_aliyun/jiexi.png"><imgsrc="/images/hexo_aliyun/jiexi.png" alt="解析" /></a></p><p>十分钟后，我们便可以通过浏览器访问http://www.xxxxx.com/进入hexo的博客主页了。</p><p><a href="/images/hexo_aliyun/success.png"><imgsrc="/images/hexo_aliyun/success.png" alt="success" /></a></p><h1 id="结束语">结束语</h1><p>新手搭建个人博客过程中难免会出一些小问题，千万不要害怕遇到问题。解决它，你就进步了</p>]]></content>
    
    
    <categories>
      
      <category>博客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>服务器</tag>
      
      <tag>博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GPU版pytorch安装指南</title>
    <link href="/pytorch%20%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B.html"/>
    <url>/pytorch%20%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B.html</url>
    
    <content type="html"><![CDATA[<h4 id="安装或更新nvida显卡驱动">1. 安装或更新NVIDA显卡驱动</h4><p>官方驱动下载地址：https://www.nvidia.cn/Download/index.aspx?lang=cn</p><figure><imgsrc="/images/gpu版torch安装/v2-530262e6bd5b221121ebef91fe86b351_1440w.jpg"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h4 id="安装cuda-toolkit-cudnn">2.安装<code>CUDA Toolkit + cudnn</code>：</h4><h5 id="cuda安装">1）CUDA安装</h5><p>在<code>CUDA Toolkit</code>安装前用以下命令查询机器上显卡最高支持的CUDA 版本：</p><p>终端输入：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">nvidia-smi</span><br></code></pre></td></tr></table></figure><p>下图中CUDA Version是12.2。</p><blockquote><p>如果你没有安装<code>cuda toolkit</code>或者需要升级，可以去官网下载：https://developer.nvidia.com/cuda-toolkit-archive</p></blockquote><figure><imgsrc="/images/gpu版torch安装/v2-976094b4950297cb63db4d8938179a01_1440w.jpg"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h5 id="cudnn安装">2）CUDNN安装</h5><p>NVIDIA CUDA深度神经网络库 (cuDNN) 是一个 <strong>GPU加速</strong>的<strong>深度神经网络基元库</strong>，能够以高度优化的方式实现标准例程（如前向和反向卷积、池化层、归一化和激活层）。</p><p>全球的深度学习研究人员和框架开发者都依赖 cuDNN 来实现高性能 GPU加速。借助cuDNN，研究人员和开发者可以专注于训练神经网络及开发软件应用，而不必花时间进行低层级的GPU 性能调整。cuDNN 可加速广泛应用的深度学习框架，包括Caffe2、Keras、MATLAB、MxNet、PaddlePaddle、PyTorch和 TensorFlow。</p><p><strong>下载地址：</strong><ahref="https://developer.nvidia.com/rdp/cudnn-archive">cuDNN Archive |NVIDIA Developer</a></p><p><strong>（1）下载并解压文件</strong></p><figure><img src="/images/gpu版torch安装/7429c11c55ca4e268719b103fbe72547.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p><strong>（2）复制内容到CUDA安装路径</strong></p><p>CUDA安装默认路径：</p><ul><li>Windows：<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA</code></li><li>Linux：<code>/usr/local/cuda</code></li></ul><figure><img src="/images/gpu版torch安装/e7b791a3d7bc454a9fc6a2373e33dddb.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h4 id="安装pytorch">3. 安装Pytorch</h4><p><strong>（1）在线安装</strong></p><p>打开<ahref="https://pytorch.org/get-started/locally/">pytorch安装指导网站</a>，选择合适的系统平台，关键是在<code>compute platform</code>选择一个不高于你电脑上的<code>CUDA Version</code>，复制命令安装。</p><ul><li>pip install torch==版本号</li><li>conda install torch==版本号</li></ul><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># 使用conda安装</span><br>conda install python pytorch torchvision torchaudio pytorch-cuda=<span class="hljs-number">11.7</span> -c pytorch -c nvidia<br><span class="hljs-comment"># 使用pip安装</span><br>pip install torch torchvision torchaudio --index-url https:<span class="hljs-regexp">//</span>download.pytorch.org<span class="hljs-regexp">/whl/</span>cu117<br>或者<br>pip install torch==<span class="hljs-number">2.0</span>.<span class="hljs-number">0</span>+cu118 torchvision==<span class="hljs-number">0.15</span>.<span class="hljs-number">0</span>+cu118 torchaudio==<span class="hljs-number">2.0</span>.<span class="hljs-number">1</span>+cu118 -f https:<span class="hljs-regexp">//</span>download.pytorch.org<span class="hljs-regexp">/whl/</span>torch_stable.html<br></code></pre></td></tr></table></figure><figure><img src="/images/gpu版torch安装/image-20240522173540087.png"alt="image-20240522173540087" /><figcaption aria-hidden="true">image-20240522173540087</figcaption></figure><p><strong>（2）离线安装</strong></p><ul><li>离线包下载地址：<ahref="https://download.pytorch.org/whl/torch_stable.html">download.pytorch.org/whl/torch_stable.html</a></li><li>安装方式</li></ul><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">pip</span> install torch-<span class="hljs-number">2</span>.<span class="hljs-number">0</span>.<span class="hljs-number">1</span>+cu118-cp310-cp310-win_amd64.whl<br></code></pre></td></tr></table></figure><p>注意：</p><h5id="pytorch与torchvision版本对应问题">1）PYTORCH与TORCHVISION版本对应问题</h5><p><ahref="https://gitcode.com/pytorch/vision/overview?utm_source=csdn_blog_hover">Pytorch与torchvision版本配套</a></p><figure><img src="/images/gpu版torch安装/3eb20f49e5f547b0b87853b2ed5430c9.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><blockquote><p>如果你的conda解决环境很慢，可以试一试pip安装。</p></blockquote><h5 id="使用镜像源">2）使用镜像源</h5><ul><li>使用镜像源：</li><li>pip install torch -i [镜像源]</li><li>conda install torch -c [镜像源]</li><li>常用镜像源</li><li>清华源：https://pypi.tuna.tsinghua.edu.cn/simple</li><li>豆瓣源：https://pypi.doubanio.com/simple/</li></ul><h5 id="安装验证">3）安装验证</h5><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch<br><span class="hljs-comment"># 打印出正在使用的PyTorch和CUDA版本。</span><br><span class="hljs-built_in">print</span>(torch.__version__)<br><span class="hljs-built_in">print</span>(torch.version.cuda)<br><br><span class="hljs-comment"># 测试GPU是否生效</span><br><span class="hljs-built_in">print</span>(torch.cuda.is_available())<br></code></pre></td></tr></table></figure><h3 id="导入pytoch">（3）导入PyToch</h3><p>导入 PyTorch 并检查正在使用的版本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>torch.__version__<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-string">&#x27;2.0.1&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="pycharm配置anaconda环境">（4）Pycharm配置Anaconda环境</h3><ol type="1"><li>打开Pycharm，点击File--&gt;NewProject，例如新建项目工程名字为：Pycharm_conda</li></ol><figure><img src="/images/gpu版torch安装/image-20240530094628328.png"alt="image-20240530094628328" /><figcaption aria-hidden="true">image-20240530094628328</figcaption></figure><ol type="1"><li>选择新建工程所在文件位置，并命名，点击create。选择New Window。</li></ol><figure><img src="/images/gpu版torch安装/image-20240530094449673.png"alt="image-20240530094449673" /><figcaption aria-hidden="true">image-20240530094449673</figcaption></figure><ol type="1"><li>在创建好的新工程窗口下，点击File--&gt;Settings</li></ol><figure><img src="/images/gpu版torch安装/image-20240530094835776.png"alt="image-20240530094835776" /><figcaption aria-hidden="true">image-20240530094835776</figcaption></figure><ol type="1"><li>Settings--&gt;Project :Pycharm_conda--&gt;Python Interpreter,然后点击右边齿轮状图标或者"Add Interpreter"，点击Add ，添加解释器。</li></ol><figure><img src="/images/gpu版torch安装/image-20240530095248878.png"alt="image-20240530095248878" /><figcaption aria-hidden="true">image-20240530095248878</figcaption></figure><ol type="1"><li>打开后选择Conda Environment，然后选中Existingenvironment，选择自己创建的环境，点击OK，低版本可勾选Make available toall projects。</li></ol><figure><img src="/images/gpu版torch安装/image-20240530095810448.png"alt="image-20240530095810448" /><figcaption aria-hidden="true">image-20240530095810448</figcaption></figure><ol type="1"><li>等待加载完毕后，会看到在PythonInterpreter页面多了许多包。表示在该环境下创建的工程就可以使用anaconda中已有的库了。</li></ol><figure><img src="/images/gpu版torch安装/image-20240530100118494.png"alt="image-20240530100118494" /><figcaption aria-hidden="true">image-20240530100118494</figcaption></figure><ol type="1"><li>点击OK，配置完成，在主界面的右下角会显示当前环境处于刚配置好的环境中，等待加载完毕后即可正常使用</li></ol><figure><img src="/images/gpu版torch安装/image-20240530100154229.png"alt="image-20240530100154229" /><figcaption aria-hidden="true">image-20240530100154229</figcaption></figure><ol type="1"><li>使用terminal安装依赖</li></ol><figure><img src="/images/gpu版torch安装/image-20240530100803248.png"alt="image-20240530100803248" /><figcaption aria-hidden="true">image-20240530100803248</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>环境搭建</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>半小时速通正则表达式</title>
    <link href="/%E5%8D%8A%E5%B0%8F%E6%97%B6%E9%80%9F%E9%80%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.html"/>
    <url>/%E5%8D%8A%E5%B0%8F%E6%97%B6%E9%80%9F%E9%80%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.html</url>
    
    <content type="html"><![CDATA[<p>正则表达式在文本范式处理时有者非常重要的应用，快来一起巩固一下正则的相关知识吧！<span id="more"></span> ### 正则表达式应用场景(Regular Expression)</p><ul><li>数据验证（表单验证、如手机、邮箱、IP地址）</li><li>爬虫功能</li><li>数据检索（数据检索、数据抓取）</li><li>数据隐藏（135****6235王先生）</li><li>数据过滤（论坛敏感关键词过滤）</li></ul><h3 id="正则--match">正则--match</h3><p><strong>re.match(pattern, string, flags=0)</strong></p><table><thead><tr class="header"><th><strong>参数</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr class="odd"><td>pattern</td><td>匹配的正则表达式</td></tr><tr class="even"><td>string</td><td>要匹配的字符串。</td></tr><tr class="odd"><td>flags</td><td>标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：<ahref="https://www.runoob.com/python/python-reg-expressions.html#flags">正则表达式修饰符- 可选标志</a></td></tr></tbody></table><p>re.match 尝试从字符串的<strong>起始位置匹配</strong>一个模式，匹配成功 re.match 方法返回一个匹配的对象，否则返回 None。</p><h3 id="正则--search">正则--search</h3><p><strong>re.search(pattern, string, flags=0)</strong></p><table><thead><tr class="header"><th><strong>参数</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr class="odd"><td>pattern</td><td>匹配的正则表达式</td></tr><tr class="even"><td>string</td><td>要匹配的字符串。</td></tr><tr class="odd"><td>flags</td><td>标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：<ahref="https://www.runoob.com/python/python-reg-expressions.html#flags">正则表达式修饰符- 可选标志</a></td></tr></tbody></table><p>re.match尝试从字符串的<strong>任意位置匹配</strong>一个模式，(常用于全词匹配)匹配成功 re.search 方法返回一个匹配的对象，否则返回 None。</p><p>re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。</p><h3id="re.compile正则表达式.sub用来替换的内容要被替换的内容">re.compile（正则表达式）.sub（用来替换的内容，要被替换的内容）</h3><p>compile 函数用于编译正则表达式，生成一个正则表达式（ Pattern）对象，供 match() 和 search() 这两个函数使用。 ### ###re.sub(正则,替换字符,被替换的内容)</p><h3 id="正则常用符号释义">正则常用符号释义</h3><table><thead><tr class="header"><th><strong>符号</strong></th><th><strong>解释</strong></th><th><strong>示例</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr class="odd"><td>.</td><td><strong>匹配任意字符</strong></td><td>b.t</td><td>可以匹配bat / but / b#t / b1t等</td></tr><tr class="even"><td><a href="file://w"></a></td><td><strong>匹配字母/数字/下划线/汉字</strong></td><td>b</td><td>可以匹配bat / b1t / b_t等&lt;br&gt;但不能匹配b#t</td></tr><tr class="odd"><td><a href="file://s"></a></td><td>**匹配空白字符（包括、*）</td><td>love</td><td>可以匹配love you</td></tr><tr class="even"><td>[(file://d)</td><td><strong>匹配数字</strong></td><td>[(file://d/d)</td><td>可以匹配01 / 23 / 99等</td></tr><tr class="odd"><td>[(file://b)</td><td>匹配单词的边界</td><td>[(file://bThe/b)</td><td></td></tr><tr class="even"><td>^</td><td><strong>匹配字符串的开始</strong></td><td>^The</td><td>可以匹配The开头的字符串</td></tr><tr class="odd"><td>$</td><td><strong>匹配字符串的结束</strong></td><td>.exe$</td><td>可以匹配.exe结尾的字符串</td></tr><tr class="even"><td><a href="file://W"></a></td><td>匹配非字母/数字/下划线</td><td>b</td><td>可以匹配b#t / b@t等&lt;br&gt;但不能匹配but / b1t / b_t等</td></tr><tr class="odd"><td><a href="file://S"></a></td><td>匹配非空白字符</td><td>love</td><td>可以匹配love#you等&lt;br&gt;但不能匹配love you</td></tr><tr class="even"><td><a href="file://D"></a></td><td>匹配非数字</td><td><a href="file://d/D"></a></td><td>可以匹配9a / 3# / 0F等</td></tr><tr class="odd"><td><a href="file://B"></a></td><td>匹配非单词边界</td><td><a href="file://Bio/B"></a></td><td></td></tr><tr class="even"><td>[]</td><td>匹配来自字符集的任意单一字符</td><td>[aeiou]</td><td>可以匹配任一元音字母字符</td></tr><tr class="odd"><td><strong>[^]</strong></td><td>匹配不在字符集中的任意单一字符</td><td>[^aeiou]</td><td>可以匹配任一非元音字母字符</td></tr><tr class="even"><td><strong>*</strong></td><td><strong>匹配0次或多次</strong></td><td><a href="file://w*">*</a></td><td></td></tr><tr class="odd"><td><strong>+</strong></td><td><strong>匹配1次或多次</strong></td><td><a href="file://w+">+</a></td><td></td></tr><tr class="even"><td><strong>?</strong></td><td><strong>匹配0次或1次</strong></td><td><a href="file://w">?</a></td><td></td></tr><tr class="odd"><td><strong>{N}</strong></td><td><strong>匹配N次</strong></td><td></td><td></td></tr><tr class="even"><td><strong>{M,}</strong></td><td><strong>匹配至少M次</strong></td><td></td><td></td></tr><tr class="odd"><td><strong>{M,N}</strong></td><td><strong>匹配至少M次至多N次</strong></td><td></td><td></td></tr><tr class="even"><td><strong>\</strong></td><td>分支</td><td>foo\bar</td><td>可以匹配foo或者bar</td></tr><tr class="odd"><td><strong>(?#)</strong></td><td>注释</td><td></td><td></td></tr><tr class="even"><td><strong>(exp)</strong></td><td>匹配exp并捕获到自动命名的组中</td><td></td><td></td></tr><tr class="odd"><td><strong>(?&lt;name&gt;exp)</strong></td><td>匹配exp并捕获到名为name的组中</td><td></td><td></td></tr><tr class="even"><td><strong>(?:exp)</strong></td><td>匹配exp但是不捕获匹配的文本</td><td></td><td></td></tr><tr class="odd"><td><strong>(?=exp)</strong></td><td>匹配exp前面的位置</td><td><a href="file://b/w+(%3f=ing)">+(?=ing)</a></td><td>可以匹配I'm dancing中的danc</td></tr><tr class="even"><td><strong>(?&lt;=exp)</strong></td><td>匹配exp后面的位置</td><td>[(?&lt;=)+(file://bdanc)/w+/b)</td><td>可以匹配I love dancing and reading中的第一个ing</td></tr><tr class="odd"><td><strong>(?!exp)</strong></td><td>匹配后面不是exp的位置</td><td></td><td></td></tr><tr class="even"><td><strong>(?&lt;!exp)</strong></td><td>匹配前面不是exp的位置</td><td></td><td></td></tr><tr class="odd"><td><strong>*?</strong></td><td>重复任意次，但尽可能少重复</td><td>a.\b&lt;br&gt;a.\?b</td><td>将正则表达式应用于aabab，前者会匹配整个字符串aabab，后者会匹配aab和ab两个字符串</td></tr><tr class="even"><td><strong>+?</strong></td><td>重复1次或多次，但尽可能少重复</td><td></td><td></td></tr><tr class="odd"><td><strong>??</strong></td><td>重复0次或1次，但尽可能少重复</td><td></td><td></td></tr><tr class="even"><td><strong>{M,N}?</strong></td><td>重复M到N次，但尽可能少重复</td><td></td><td></td></tr><tr class="odd"><td><strong>{M,}?</strong></td><td>重复M次以上，但尽可能少重复</td><td></td><td></td></tr></tbody></table><p>说明：如果需要匹配的字符是正则表达式中的特殊字符，那么可以使用\进行转义处理，例如想匹配小数点可以写成\.就可以了，因为直接写.会匹配任意字符；同理，想匹配圆括号必须写成\(和\)，否则圆括号被视为正则表达式中的分组。</p>]]></content>
    
    
    
    <tags>
      
      <tag>正则表达式</tag>
      
      <tag>基础语法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>用wordcoloud生成超炫酷的词云_内含python源码</title>
    <link href="/%E7%94%A8wordcoloud%E7%94%9F%E6%88%90%E8%B6%85%E7%82%AB%E9%85%B7%E7%9A%84%E8%AF%8D%E4%BA%91-%E5%86%85%E5%90%ABpython%E6%BA%90%E7%A0%81.html"/>
    <url>/%E7%94%A8wordcoloud%E7%94%9F%E6%88%90%E8%B6%85%E7%82%AB%E9%85%B7%E7%9A%84%E8%AF%8D%E4%BA%91-%E5%86%85%E5%90%ABpython%E6%BA%90%E7%A0%81.html</url>
    
    <content type="html"><![CDATA[<p>使用jieba分词，wordcoloud词云可视化</p><span id="more"></span><p><img src="/images/word_cloud/2024年5月10日ai.jpg" /><strong>环境准备</strong> pip 安装jieba库,wordcloud库与scipy库<strong>资料准备</strong></p><ul class="task-list"><li><label><inputtype="checkbox" />用于分词的文本:词频统计_AIjob.csv</label></li><li><label><inputtype="checkbox" />禁止统计词库:stopwords.txt</label></li><li><label><inputtype="checkbox" />自定义分词库:人工智能词汇.txt</label></li><li><label><inputtype="checkbox" />掩膜用的形状图片:mask.jpg</label></li></ul><p>不废话,直接上码 ### 1.用结巴分词,生成字典对象 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">import</span> wordcloud<br><span class="hljs-comment"># 读取文件</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;词频统计_AIjob.csv&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    desc = f.read()<br><br><span class="hljs-comment"># 加载停用词列表</span><br>stop_words = []<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;stopwords.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:<br>        stop_words.append(line.strip())<br><br>jieba.load_userdict(<span class="hljs-string">&quot;人工智能词汇.txt&quot;</span>)<br><span class="hljs-comment"># 分词</span><br>words = jieba.cut(desc, cut_all=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 过滤停用词</span><br>filtered_words = []<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(word) &gt; <span class="hljs-number">1</span>:<br>        filtered_words.append(word)<br><br><span class="hljs-comment"># 统计词频</span><br>word_counts = Counter(filtered_words)<br><br>w100=word_counts.most_common(<span class="hljs-number">500</span>)<br><span class="hljs-comment"># 使用字典推导将列表转换为字典  </span><br>dict_result = &#123;key: value <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> w100&#125; <br></code></pre></td></tr></table></figure> ###2.用wordcloud 生成词云 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> scipy.ndimage <span class="hljs-keyword">import</span> gaussian_gradient_magnitude<br><span class="hljs-keyword">from</span> wordcloud <span class="hljs-keyword">import</span> WordCloud, ImageColorGenerator<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pic_wordcloud</span>(<span class="hljs-params">dict_result,img_path,out_path</span>):<br><br>    <br>    <span class="hljs-comment"># img_path=r&quot;E:\jupyter\spyder\bosszhipin\词频统计\pic\T1.jpg&quot;</span><br>    <br>    parrot_color = np.array(Image.<span class="hljs-built_in">open</span>(img_path))<br>    <br>    parrot_color = parrot_color[::<span class="hljs-number">3</span>, ::<span class="hljs-number">3</span>]<br>    <br>    <span class="hljs-comment"># create mask  white is &quot;masked out&quot;</span><br>    parrot_mask = parrot_color.copy()<br>    parrot_mask[parrot_mask.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">2</span>) == <span class="hljs-number">0</span>] = <span class="hljs-number">255</span><br>    <br>    <br>    edges = np.mean([gaussian_gradient_magnitude(parrot_color[:, :, i] / <span class="hljs-number">255.</span>, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)], axis=<span class="hljs-number">0</span>)<br>    parrot_mask[edges &gt; <span class="hljs-number">.08</span>] = <span class="hljs-number">255</span><br>    <br>    <br>    <span class="hljs-comment"># acurately but it makes a better picture</span><br>    wc = WordCloud(max_words=<span class="hljs-number">1000</span>, mask=parrot_mask, max_font_size=<span class="hljs-number">40</span>, random_state=<span class="hljs-number">42</span>, font_path=<span class="hljs-string">r&quot;C:\Users\10921\AppData\Local\Microsoft\Windows\Fonts\方正正准黑简体.ttf&quot;</span>,relative_scaling=<span class="hljs-number">0</span>,<br>                   <span class="hljs-comment">#width=1920, height=1080</span><br>                   )<br>    <br>    <span class="hljs-comment"># generate word cloud</span><br>    wc.generate_from_frequencies(dict_result)<br>    <span class="hljs-comment"># plt.imshow(wc)</span><br>    <br>    <span class="hljs-comment"># create coloring from image</span><br>    image_colors = ImageColorGenerator(parrot_color)<br>    wc.recolor(color_func=image_colors)<br>    <span class="hljs-comment"># plt.figure(figsize=(10, 10))</span><br>    <span class="hljs-comment"># plt.imshow(wc, interpolation=&quot;bilinear&quot;)</span><br>    <span class="hljs-comment"># wc.to_file(&quot;parrot_new.png&quot;)</span><br>    wc.to_file(out_path)<br>img_path=<span class="hljs-string">&quot;mask.jpg&quot;</span><br>out_path=<span class="hljs-string">&#x27;output.png&#x27;</span><br>pic_wordcloud(dict_result,img_path,out_path)<br></code></pre></td></tr></table></figure> <imgsrc="/images/word_cloud/2024年5月10日color112.png" /> <imgsrc="/images/word_cloud/2024年5月10日parrot_new.png" /></p>]]></content>
    
    
    
    <tags>
      
      <tag>可视化</tag>
      
      <tag>数据分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch VS TensorFlow</title>
    <link href="/TensorFlow%20VS%20Pytorch.html"/>
    <url>/TensorFlow%20VS%20Pytorch.html</url>
    
    <content type="html"><![CDATA[<p>TensorFlow和PyTorch是两种流行的深度学习框架，它们各自具有独特的优缺点和优势领域。以下是对它们的比较：<span id="more"></span></p><h3 id="tensorflow">TensorFlow</h3><h4 id="优点">优点</h4><ol type="1"><li><strong>生态系统完善</strong>：TensorFlow提供了丰富的工具和库，如TensorFlowExtended (TFX)用于生产部署，TensorFlowLite用于移动设备，TensorFlow.js用于JavaScript开发等。</li><li><strong>高性能</strong>：通过TensorFlowServing，可以实现高效的模型部署和推理。此外，TensorFlow有XLA（加速线性代数）编译器，可以进一步优化性能。</li><li><strong>跨平台支持</strong>：TensorFlow可以在多种平台上运行，包括CPU、GPU、TPU，且支持分布式计算。</li><li><strong>社区和企业支持</strong>：TensorFlow由Google开发并维护，拥有广泛的社区支持和大量的企业用户。</li></ol><h4 id="缺点">缺点</h4><ol type="1"><li><strong>学习曲线陡峭</strong>：TensorFlow的API较为复杂，新手入门相对困难。</li><li><strong>调试困难</strong>：虽然TensorFlow 2.x版本引入了EagerExecution模式，提升了可调试性，但总体上调试体验仍不如PyTorch。</li></ol><h3 id="优势领域">优势领域</h3><ul><li><strong>生产环境</strong>：TensorFlow的工具链和生态系统使其非常适合于从研究到生产的全流程。</li><li><strong>大规模分布式训练</strong>：TensorFlow在大规模分布式训练方面具有明显优势。</li><li><strong>移动和嵌入式设备</strong>：TensorFlowLite专门优化了在移动和嵌入式设备上的性能。</li></ul><h3 id="pytorch">PyTorch</h3><h4 id="优点-1">优点</h4><ol type="1"><li><strong>易用性</strong>：PyTorch的API设计直观且友好，非常适合研究和实验。动态图计算模式使得代码更加灵活和易于调试。</li><li><strong>调试便捷</strong>：由于PyTorch使用动态图计算模式，开发者可以使用标准的Python调试工具来调试模型。</li><li><strong>动态计算图</strong>：PyTorch的动态计算图机制使得开发者可以在运行时改变模型结构，非常适合于研究和实验。</li><li><strong>社区支持</strong>：PyTorch由Facebook（现Meta）开发，得到广泛的学术界支持，许多研究论文和前沿技术都首先在PyTorch上实现。</li></ol><h4 id="缺点-1">缺点</h4><ol type="1"><li><strong>生态系统相对较小</strong>：相比TensorFlow，PyTorch的工具链和扩展库较少，虽然近年来有显著改进。</li><li><strong>生产部署工具较少</strong>：虽然PyTorch推出了TorchServe等部署工具，但整体上在生产部署方面的支持不如TensorFlow成熟。</li></ol><h3 id="优势领域-1">优势领域</h3><ul><li><strong>研究和实验</strong>：PyTorch因其灵活性和易用性，非常适合学术研究和快速原型开发。</li><li><strong>调试和开发</strong>：由于其动态计算图机制，开发和调试深度学习模型更加便捷。</li><li><strong>计算机视觉和自然语言处理</strong>：很多前沿的计算机视觉和自然语言处理研究和应用首先在PyTorch上实现。</li></ul><h3 id="总结">总结</h3><p>选择TensorFlow还是PyTorch主要取决于具体需求。如果你需要一个强大的生产环境，广泛的工具链支持，尤其是在大规模分布式训练和部署方面，TensorFlow可能是更好的选择。而如果你更关注研究、实验以及开发过程的灵活性和可调试性，PyTorch则可能更适合你。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>笔记整理</tag>
      
      <tag>学习框架</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>光影集-2022</title>
    <link href="/%E5%85%89%E5%BD%B1%E9%9B%862022.html"/>
    <url>/%E5%85%89%E5%BD%B1%E9%9B%862022.html</url>
    
    <content type="html"><![CDATA[<h1 id="难忘那年">难忘那年</h1><figure><img src="/images/光影集2022/image-20240513215734472.png"alt="月色苍茫" /><figcaption aria-hidden="true">月色苍茫</figcaption></figure><figure><img src="/images/光影集2022/image-20240513215908875.png"alt="鱼来鱼往" /><figcaption aria-hidden="true">鱼来鱼往</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220042681.png"alt="花开灿烂" /><figcaption aria-hidden="true">花开灿烂</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220159390.png"alt="绿意盎然" /><figcaption aria-hidden="true">绿意盎然</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220313485.png"alt="雨打蕉叶" /><figcaption aria-hidden="true">雨打蕉叶</figcaption></figure><figure><img src="/images/光影集2022/image-20240513220422759.png"alt="风华正茂" /><figcaption aria-hidden="true">风华正茂</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>生活记录</tag>
      
      <tag>摄影作品</tag>
      
      <tag>难忘那年</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LightGBM参数调优</title>
    <link href="/LightGBM%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98.html"/>
    <url>/LightGBM%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98.html</url>
    
    <content type="html"><![CDATA[<p>LightGBM模型在各领域运用广泛，但想获得更好的模型表现，调参这一过程必不可少，下面我们就来聊聊LightGBM在sklearn接口下调参数的方法，也会在文末给出调参的代码模板。</p><span id="more"></span><h2 id="概述">概述</h2><p><strong>按经验预先固定的参数</strong></p><ul><li>learning_rate</li><li>n_estimators</li><li>min_split_gain</li><li>min_child_sample</li><li>min_child_weight</li></ul><p><strong>需要算法调节的参数</strong></p><ul><li>max_depth</li><li>num_leaves</li><li>subsample</li><li>colsample_bytree</li><li>reg_alpha</li><li>reg_lambda</li></ul><h2 id="lightgbm参数详解">LightGBM参数详解</h2><p>LightGBM有众多参数，建议大家在使用LightGBM前，先仔细阅读参数介绍。</p><p>参数介绍传送门：</p><p>英文版：<ahref="https://lightgbm.readthedocs.io/en/latest/Parameters.html">https://lightgbm.readthedocs.io/en/latest/Parameters.html</a></p><p>中文版：<ahref="https://lightgbm.apachecn.org/%23/docs/6">https://lightgbm.apachecn.org/#/docs/6</a></p><p>其他注解：<ahref="https://medium.com/@gabrieltseng/gradient-boosting-and-xgboost-c306c1bcfaf5">https://medium.com/<spanclass="citation"data-cites="gabrieltseng/gradient-boosting-and-xgboost-c306c1bcfaf5">@gabrieltseng/gradient-boosting-and-xgboost-c306c1bcfaf5</span></a></p><h3id="我们提取对模型性能比较重要的参数来介绍下">我们提取对模型性能比较重要的参数来介绍下。</h3><p><span class="math display">\[w_j=\text{learning rate}\times\frac{\sum_{i\in I_j}\frac{\partialloss}{\partial(\hat{y}=0)}}{\sum_{i\inI_j}(\frac{\partial^2loss}{\partial(\hat{y}=0)^2})+\lambda}\]</span></p><ol type="1"><li>learning_rate:学习率。默认设置为0.1，一般设置在0.05-0.1之间。选择比较小的学习率能获得稳定较好的模型性能。</li><li>n_estimators:boosting的迭代次数。默认设置为100。一般根据数据集和特征数据选择100~1000之间。更保守的做法是设置一个较大的值配合early_stopping_round来让模型根据性能自动选择最好的迭代次数。选择比较大的迭代次数会在训练集获得比较好的性能但容易过拟合造成测试集的性能下降。</li><li>min_split_gain:执行节点分裂的最小增益。默认设置为0。不建议去调整。增大这个数值会得到相对浅的树深。可调整其他参数得到类似效果。</li><li>min_child_sample:一个叶子上的最小数据量。默认设置为20。根据数据量来确定，当数据量比较大时，应该提升这个数值，让叶子节点的数据分布相对稳定，提高模型的泛华能力。</li><li>min_child_weight:一个叶子上的最小hessian和。默认设置为0.001，一般设置为1。不建议调整，增大数值会得到较浅的树深。</li><li>max_depth:树模型的最大深度。防止过拟合的最重要的参数，一般限制为3~5之间。是需要调整的核心参数，对模型性能和泛化能力有决定性作用。</li><li>num_leaves:一棵树上的叶子节点个数。默认设置为31，和max_depth配合来空值树的形状，一般设置为(0,2^max_depth -1]的一个数值。是一个需要重点调节的参数，对模型性能影响很大。</li><li>subsample:若此参数小于1.0，LightGBM将会在每次迭代中在不进行重采样的情况下随机选择部分数据（row），可以用来加速训练及处理过拟合。默认设置为1，一般设置为0。8~1.0之间，防止过拟合。</li><li>colsample_bytree:若此参数小于1.0，LightGBM将会在每次迭代中随机选择部分特征(col)，可以用来加速训练及处理过拟合。默认设置为1，一般设置为0.8~1.0之间，防止过拟合。</li></ol><p><span class="math display">\[L=\sum_{i=0}^nloss(y_{res},h(x))+\frac12\lambda\sum_{j=1}^Tw_j^2+\alpha\sum_{j=1}^T|w_j|\]</span></p><ol type="1"><li>reg_alpha:L1正则化参数，别名：lambda_l1。默认设置为0。一般经过特征选择后这个参数不会有特别大的差异，如果发现这个参数数值大，则说明有一些没有太大作用的特征在模型内。需要调节来控制过拟合</li><li>reg_lambda:L2正则化参数，别名：lambda_l2。默认设置为0。较大的数值会让各个特征对模型的影响力趋于均匀，不会有单个特征把持整个模型的表现。需要调节来控制过拟合</li></ol><h2 id="调参建议">调参建议</h2><p>接下来介绍一下根据我们自己的经验和网上相关帖子总结出来的一点小经验供参考，不同参数在不同数据集上表现会有一定的差异性。</p><p><strong>建议根据经验确定的参数：</strong></p><h4 id="learning_rate">1.<strong>learning_rate:</strong></h4><p>通常来说，学习率越小模型表现的最终表现容易获得比较好的结果，但是过小的学习率往往会导致模型的过拟合以及影响模型训练的时间。一般来说，在调参的过程中会预设一个固定的值如0.1或者0.05，再其他参数确定后再在0.05-0.2之间搜索一个不错的值作为最终模型的参数。通常在学习率较小的时候，n_estimators的数值会大，而学习率大的时候,n_estimators会比较小，他们是一对此消彼长的参数对。</p><h4 id="n_estimators">2.<strong>n_estimators</strong>:</h4><ol type="1"><li>通常来说迭代次数越多模型表现越好，但是过大的迭代次往往会导致模型的过拟合以及影响模型训练的时间。一般我们选择的值在100~1000之间，训练时需要时刻关注过拟合的情况以便及时调整迭代次数。通常通过lgb.plot_metrics(model,metrics='auc)来观察学习曲线的变化，如果在测试集表现趋于下降的时候模型还没有停止训练就说明出现过拟合了。</li><li>通常为了防止过拟合，都会选一个比较大的n_estimators，然后设置early_stop_round为20，50，100来让模型停止在测试集效果还不错的地方，但如果模型过早的停止训练，比如只迭代了20次，那可能这样的结果是有问题的，需要再仔细研究下原因。</li><li>还有个通过交叉检验确定n_estimators的办法，但我们实验的结果表明没有加early-stop_round来的稳定，但也分享给大家，说不定在你的项目里有奇效。具体做法：跑3-5折的交叉检验，训练时加上early_stop_round，记录下每折模型停止时的n_estimators的数值，然后n_estimators取交叉检验模型停止的迭代次数的平均值的1.1倍。然后确定这个数值来调整其他参数，最终模型再通过early_stop_round得到最终的n_estimators的数值。</li></ol><h4 id="min_split_gain">3.<strong>min_split_gain</strong>:</h4><p>不建议去调整。增大这个数值会得到相对浅的树深。可调整其他参数得到类似效果。如果实在要调整，可以画出第一颗树和最后一颗树，把每次决策分叉的gain的数值画出来看一看大致范围，然后确定一个下限。但往往设置后模型性能会下降不少，所以如果不是过拟合很严重且没有其他办法缓解才建议调整这个参数。</p><h4 id="min_child_sample">4.<strong>min_child_sample</strong>:</h4><p>这个参数需要根据数据集来确定，一般小数据集用默认的20就够了，但大数据集还用这个20的话会使得生成的叶子节点上数据量过小，这会出现数据集没有代表性的问题，所以建议按树深为4共16个叶子时平均的训练数据个数的25%的数值来确定这个参数或者在这个范围稍微搜索下。这样模型的稳定性会有所保障。</p><h4 id="min_child_weight">5.<strong>min_child_weight</strong>:</h4><p>和min_child_sample的作用类似，但这个参数本身对模型的性能影响并不大，而且影响的方式不容易被人脑所理解，不建议过多的进行调整。</p><h3id="需要通过算法来搜索的参数"><strong>需要通过算法来搜索的参数：</strong></h3><ol type="1"><li><strong>max_depth:</strong>一般在3，4，5这三个数里挑一个就好了，设置过大的数值过拟合会比较严重。</li><li>num_leaves:在LightGBM里，叶子节点数设置要和max_depth来配合，要小于2<sup>max_depth-1。一般max_depth取3时，叶子数要&lt;=2</sup>3-1=7。如果比这个数值大的话,LightGBM可能会有奇怪的结果。在参数搜索时，需要用max_depth去限制num_leaves的取值范围。</li><li><strong>subsample:</strong>不建议过度的精细的调节，比如用搜索算法搜一个0.814325这样一个数值就不是很好。一般给出大致的搜索范围如[0.8,0.9, 1.0]这样几个比较整的数值就足够了。</li><li><strong>colsample_bytree:</strong> 和subsample同理，在[0.8, 0.9,1.0]这样几个比较整的数值搜索就足够了。不建议过度调节。</li><li><strong>reg_alpha:</strong>此参数服务于L1正则化，一般我们取0-1000的范围去进行调参。如果优化出来这个参数数值过大，则说明有一些不必要的特征可以剔除，可以先做特征筛选后再进行调参，然后调节出来模型效果好的时候reg_alpha是个相对小的数值，那我们对这个模型的信心会大很多。</li><li><strong>reg_lambda:</strong>此参数服务于L2正则化，一般也是在0-1000的范围去进行调参。如果有非常强势的特征，可以人为加大一些reg_lambda使得整体特征效果平均一些，一般会比reg_alpha的数值略大一些，但如果这个参数大的夸张也需要再查看一遍特征是否合理。</li></ol><p>总的来说，再开始时调参前应该做特征筛选，在确定特征后，根据数据规模和几个模型尝试的结果来初步敲定learning_rate,n_estimators, min_split_gain, min_child_sample,min_child_weight这几个参数，然后使用grid_search, Bayesianoptimization或random search来调整 max_depth, num_leaves, subsample,colsample_bytree, reg_alpha, reg_lambda。其中重点要调节max_depth,num_leaves，并且注意他们的约束关系num_leaves&lt;=2^max_depth-1，其次subsample, colsample_bytree在[0.8,0.9, 1.0]几个粗略的离散值上调整下即可，reg_alpha, reg_lambda在[0,1000]的范围调整，最后比较好的模型上这俩参数值不应该过大，尤其是reg_alpha，过大的话需要查看特征。</p><hr /><h2 id="贝叶斯优化调参实战">贝叶斯优化调参实战</h2><p>在参数调节中，常用的调参算法有</p><p>1 grid search</p><p>2 Bayesian optimization</p><p>3 random search</p><p>其中Bayesianoptimization是个性价比比较高的方法，可以在比较短的时间内找出还不错的参数组合。但实际操作中，如果时间等得起，我们会同时使用这三个方法去搜参数然后对比下结果，找出三个算法都认为好的参数组合作为最终模型的结果。接下来给出详细的代码实操。</p><p>参数空间示例：</p><p>使用到hyperopt包定义参数空间：</p><p><ahref="https://link.zhihu.com/?target=http%3A//hyperopt.github.io/hyperopt/getting-started/search_spaces/">http://hyperopt.github.io/hyperopt/getting-started/search_spaces/</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python3">space = &#123;<br>    &#x27;max_depth&#x27;: hp.choice(&#x27;max_depth&#x27;, [3, 4, 5]),<br>    &#x27;num_leaves&#x27;: hp.choice(&#x27;num_leaves&#x27;, [5, 6, 7, 12, 13, 14, 15, 28, 29, 30, 31]),<br>    &#x27;subsample&#x27;: hp.choice(&#x27;subsample&#x27;, [0.8, 0.9, 1.0]),<br>    &#x27;colsample_bytree&#x27;: hp.choice(&#x27;colsample_bytree&#x27;, [0.8, 0.9, 1.0]),<br>    &#x27;reg_alpha&#x27;: hp.loguniform(&#x27;reg_alpha&#x27;, np.log(0.01), np.log(1000)),<br>    &#x27;reg_lambda&#x27;: hp.loguniform(&#x27;reg_lambda&#x27;, np.log(0.01), np.log(1000))<br>&#125;<br></code></pre></td></tr></table></figure><p>具体调参的类如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LGBBO</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, fp_path, **kwargs</span>):<br>        self.fp_path = fp_path<br>        self.<span class="hljs-built_in">iter</span> = <span class="hljs-number">0</span><br>        self.train_set = <span class="hljs-literal">None</span><br><br>        self.kfold = kwargs.get(<span class="hljs-string">&#x27;kfold&#x27;</span>, <span class="hljs-number">3</span>)<br>        self.n_estimators = kwargs.get(<span class="hljs-string">&#x27;n_estimators&#x27;</span>, <span class="hljs-number">800</span>)<br><br>        csv_conn = <span class="hljs-built_in">open</span>(self.fp_path, <span class="hljs-string">&#x27;w&#x27;</span>)<br>        writer = csv.writer(csv_conn)<br>        writer.writerow([<span class="hljs-string">&#x27;loss&#x27;</span>, <span class="hljs-string">&#x27;train_auc&#x27;</span>, <span class="hljs-string">&#x27;valid_auc&#x27;</span>, <span class="hljs-string">&#x27;train_ks&#x27;</span>, <span class="hljs-string">&#x27;valid_ks&#x27;</span>,<br>                         <span class="hljs-string">&#x27;lst_train_auc&#x27;</span>, <span class="hljs-string">&#x27;lst_valid_auc&#x27;</span>, <span class="hljs-string">&#x27;lst_train_ks&#x27;</span>, <span class="hljs-string">&#x27;lst_valid_ks&#x27;</span>,<br>                         <span class="hljs-string">&#x27;params&#x27;</span>, <span class="hljs-string">&#x27;iteration&#x27;</span>, <span class="hljs-string">&#x27;train_time&#x27;</span>])<br>        csv_conn.close()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">self, df_data, feature_list, label</span>):<br>        self.df_data = df_data.reset_index(drop=<span class="hljs-literal">True</span>)<br>        self.feature_list = feature_list<br>        self.label = label<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">objective</span>(<span class="hljs-params">self, params</span>):<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">eval_ks</span>(<span class="hljs-params">ytrue, yprob</span>):<br>            fpr, tpr, thr = roc_curve(ytrue, yprob)<br>            ks = <span class="hljs-built_in">max</span>(tpr - fpr)<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;ks&quot;</span>, ks, <span class="hljs-literal">True</span><br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">eval_auc</span>(<span class="hljs-params">ytrue, yprob</span>):<br>            auc = roc_auc_score(ytrue, yprob)<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;auc&quot;</span>, auc, <span class="hljs-literal">True</span><br><br>        self.<span class="hljs-built_in">iter</span> += <span class="hljs-number">1</span><br>        start = timer()<br>        model = lgb.LGBMClassifier(**params,<br>                                   learning_rate=<span class="hljs-number">0.1</span>,<br>                                   min_child_samples=<span class="hljs-number">20000</span>,<br>                                   objective=<span class="hljs-string">&#x27;cross_entropy&#x27;</span>,<br>                                   importance_type=<span class="hljs-string">&#x27;gain&#x27;</span>,<br>                                   class_weight=<span class="hljs-string">&#x27;balanced&#x27;</span>,<br>                                   boosting_type=<span class="hljs-string">&#x27;gbdt&#x27;</span>, n_estimators=self.n_estimators,<br>                                   silent=<span class="hljs-literal">True</span>, n_jobs=<span class="hljs-number">1</span>, random_state=<span class="hljs-number">0</span><br>                                   )<br><br>        lst_train_auc, lst_valid_auc = <span class="hljs-built_in">list</span>(), <span class="hljs-built_in">list</span>()<br>        lst_train_ks, lst_valid_ks = <span class="hljs-built_in">list</span>(), <span class="hljs-built_in">list</span>()<br><br>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.kfold):<br>            df_cv_train = self.df_data[self.df_data[<span class="hljs-string">f&quot;cv<span class="hljs-subst">&#123;k&#125;</span>&quot;</span>] == <span class="hljs-string">&#x27;train&#x27;</span>]<br>            df_cv_valid = self.df_data[self.df_data[<span class="hljs-string">f&quot;cv<span class="hljs-subst">&#123;k&#125;</span>&quot;</span>] == <span class="hljs-string">&#x27;valid&#x27;</span>]<br>            <span class="hljs-built_in">print</span>(k, df_cv_train.shape, df_cv_valid.shape)<br>            <span class="hljs-built_in">print</span>(params)<br><br>            eval_set = [(df_cv_train[self.feature_list], df_cv_train[self.label]),<br>                        (df_cv_valid[self.feature_list], df_cv_valid[self.label])<br>                        ]<br><br>            model.fit(df_cv_train[self.feature_list], df_cv_train[self.label],<br>                      eval_set=eval_set,<br>                      eval_metric=<span class="hljs-keyword">lambda</span> ytrue, yprob: [eval_auc(ytrue, yprob), eval_ks(ytrue, yprob)],<br>                      early_stopping_rounds=<span class="hljs-number">50</span>,<br>                      verbose=<span class="hljs-number">20</span>)<br><br>            yprob = model.predict_proba(df_cv_train[self.feature_list])[:, <span class="hljs-number">1</span>]<br>            _, train_auc, _ = eval_auc(df_cv_train[self.label], yprob)<br>            _, train_ks, _ = eval_ks(df_cv_train[self.label], yprob)<br>            yprob = model.predict_proba(df_cv_valid[self.feature_list])[:, <span class="hljs-number">1</span>]<br>            _, valid_auc, _ = eval_auc(df_cv_valid[self.label], yprob)<br>            _, valid_ks, _ = eval_ks(df_cv_valid[self.label], yprob)<br><br>            lst_train_auc.append(train_auc)<br>            lst_valid_auc.append(valid_auc)<br>            lst_train_ks.append(train_ks)<br>            lst_valid_ks.append(valid_ks)<br><br>            <span class="hljs-built_in">print</span>(train_auc, valid_auc, train_ks, valid_ks)<br><br>        run_time = timer() - start<br><br>        train_auc_avg = np.mean(lst_train_auc)<br>        valid_auc_avg = np.mean(lst_valid_auc)<br>        train_ks_avg = np.mean(lst_train_ks)<br>        valid_ks_avg = np.mean(lst_valid_ks)<br><br>        loss = -valid_ks_avg<br><br>        csv_conn = <span class="hljs-built_in">open</span>(self.fp_path, <span class="hljs-string">&#x27;a&#x27;</span>)<br>        writer = csv.writer(csv_conn)<br><br>        writer.writerow([loss,<br>                         train_auc_avg, valid_auc_avg,<br>                         train_ks_avg, valid_ks_avg,<br>                         lst_train_auc, lst_valid_auc,<br>                         lst_train_ks, lst_valid_ks,<br>                         params, self.<span class="hljs-built_in">iter</span>, run_time])<br><br>        res = &#123;<span class="hljs-string">&#x27;loss&#x27;</span>: loss,<br>               <span class="hljs-string">&#x27;train_auc&#x27;</span>: train_auc_avg, <span class="hljs-string">&#x27;valid_auc&#x27;</span>: valid_auc_avg,<br>               <span class="hljs-string">&#x27;train_ks&#x27;</span>: train_ks_avg, <span class="hljs-string">&#x27;valid_ks&#x27;</span>: valid_ks_avg,<br>               <span class="hljs-string">&#x27;lst_train_auc&#x27;</span>: lst_train_auc, <span class="hljs-string">&#x27;lst_valid_auc&#x27;</span>: lst_valid_auc,<br>               <span class="hljs-string">&#x27;lst_train_ks&#x27;</span>: lst_train_ks, <span class="hljs-string">&#x27;lst_valid_ks&#x27;</span>: lst_valid_ks,<br>               <span class="hljs-string">&#x27;params&#x27;</span>: params, <span class="hljs-string">&#x27;iteration&#x27;</span>: self.<span class="hljs-built_in">iter</span>, <span class="hljs-string">&#x27;train_time&#x27;</span>: run_time,<br>               <span class="hljs-string">&#x27;status&#x27;</span>: STATUS_OK&#125;<br><br>        <span class="hljs-built_in">print</span>(self.<span class="hljs-built_in">iter</span>)<br>        <span class="hljs-built_in">print</span>(res)<br><br>        <span class="hljs-keyword">return</span> res<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">optimize</span>(<span class="hljs-params">self, max_evals</span>):<br>        self.<span class="hljs-built_in">iter</span> = <span class="hljs-number">0</span><br><br>        space = &#123;<br>            <span class="hljs-string">&#x27;max_depth&#x27;</span>: hp.choice(<span class="hljs-string">&#x27;max_depth&#x27;</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]),<br>            <span class="hljs-string">&#x27;num_leaves&#x27;</span>: hp.choice(<span class="hljs-string">&#x27;num_leaves&#x27;</span>, [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>, <span class="hljs-number">28</span>, <span class="hljs-number">29</span>, <span class="hljs-number">30</span>, <span class="hljs-number">31</span>]),<br>            <span class="hljs-string">&#x27;subsample&#x27;</span>: hp.choice(<span class="hljs-string">&#x27;subsample&#x27;</span>, [<span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">1.0</span>]),<br>            <span class="hljs-string">&#x27;colsample_bytree&#x27;</span>: hp.choice(<span class="hljs-string">&#x27;colsample_bytree&#x27;</span>, [<span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">1.0</span>]),<br>            <span class="hljs-string">&#x27;reg_alpha&#x27;</span>: hp.loguniform(<span class="hljs-string">&#x27;reg_alpha&#x27;</span>, np.log(<span class="hljs-number">0.01</span>), np.log(<span class="hljs-number">1000</span>)),<br>            <span class="hljs-string">&#x27;reg_lambda&#x27;</span>: hp.loguniform(<span class="hljs-string">&#x27;reg_lambda&#x27;</span>, np.log(<span class="hljs-number">0.01</span>), np.log(<span class="hljs-number">1000</span>))<br>        &#125;<br><br>        best = fmin(fn=self.objective, space=space, algo=tpe.suggest, max_evals=max_evals,<br>                    trials=Trials(), rstate=np.random.RandomState(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-built_in">print</span>(best)<br>        <span class="hljs-keyword">return</span> best<br></code></pre></td></tr></table></figure><p>调用样例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">kf = StratifiedKFold(n_splits=<span class="hljs-number">3</span>, random_state=<span class="hljs-number">0</span>, shuffle=<span class="hljs-literal">True</span>)<br>pdf_train = pdf_train.reset_index(drop=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> k, (itrain, ivalid) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(kf.split(pdf_train[selected_features], pdf_train[label])):<br>    pdf_train[<span class="hljs-string">f&quot;cv<span class="hljs-subst">&#123;k&#125;</span>&quot;</span>] = <span class="hljs-literal">None</span><br>    pdf_train.loc[itrain, <span class="hljs-string">f&#x27;cv<span class="hljs-subst">&#123;k&#125;</span>&#x27;</span>] = <span class="hljs-string">&#x27;train&#x27;</span><br>    pdf_train.loc[ivalid, <span class="hljs-string">f&#x27;cv<span class="hljs-subst">&#123;k&#125;</span>&#x27;</span>] = <span class="hljs-string">&#x27;valid&#x27;</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;start tuning...&#x27;</span>)<br>bo = LGBBO(<span class="hljs-string">f&#x27;param.csv&#x27;</span>)<br>bo.load_data(pdf_train, selected_features, label)<br>bo.optimize(<span class="hljs-number">10000</span>)<br></code></pre></td></tr></table></figure><p>最后我们想说下，一般在样本不均衡时会额外调节scale_<em>pos_</em>weight这个参数，但在我们实际项目中，如果样本不是特别的偏，class_weight='balanced'就足够能产生不错的效果了，所以在参数调节中没有强调。一般情况下，这一整套调参流程跑下来是足够得到一个还不错的模型效果的参数的。</p>]]></content>
    
    
    <categories>
      
      <category>调参</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>集成学习</tag>
      
      <tag>笔记整理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CatBoost简明教程</title>
    <link href="/CatBoost%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B.html"/>
    <url>/CatBoost%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B.html</url>
    
    <content type="html"><![CDATA[<h2 id="一-catboost简介"><strong>一 CatBoost简介</strong></h2><p>CatBoost和XGBoost、LightGBM并称为GBDT的三大主流神器，都是在GBDT算法框架下的一种改进实现。</p><p>正如其名字所说那样，CatBoost主要是在类别特征上的处理上做了很多的改进。</p><span id="more"></span><p>从用户使用角度来看，相比XGBoost和LightGBM，CatBoost具有如下特点。</p><ul><li>模型精度：XGBoost和LightGBM相当，CatBoost往往略好一些，无需调参即可获取很好的结果。</li><li>训练速度：LightGBM远快于XGBoost，CatBoost快于XGBoost但比LightGBM慢。</li><li>预测速度：LightGBM与XGBoost相当，CatBoost远快于LightGBM与XGBoost，是它们的几十分之一。</li><li>内存消耗：LightGBM远小于XGBoost，CatBoost小于XGBoost，但大于LightGBM。</li><li>类别特征：XGBoost不支持类别特征，需要OneHot编码预处理。LightGBM支持类别特征，需转换成整数编码。CatBoost提供更强大的对类别特征的支持，直接支持字符串类型的类别特征，无需预处理。</li><li>缺失值特征：XGBoost和LightGBM都可以自动处理特征缺失值，CatBoost不能自动处理缺失值(或者将缺失值视为最小值/最大值)。</li><li>GPU支持：LightGBM与CatBoost支持GPU训练，XGBoost不支持GPU训练。</li><li>可视化：CatBoost还自带一套可视化工具，可以在JupyterNotebook或者TensorBoard中实时看到指标变化。</li></ul><p>CatBoost主要创新点如下：</p><ul><li>类别特征的 Ordered Target Statistics 数值编码方法。</li><li>基于贪心策略的特征组合方法。</li><li>避免预测偏移的 Ordered Boosting 方法。</li><li>使用对称二叉树作为基模型，有正则作用且预测极快。</li></ul><figure><img src="/images/catboost/v2-98453423ce486634a73c57943fd8737f.jpg"alt="v2-98453423ce486634a73c57943fd8737f" /><figcaptionaria-hidden="true">v2-98453423ce486634a73c57943fd8737f</figcaption></figure><h2 id="二-原理说明"><strong>二 原理说明</strong></h2><h3 id="类别特征的ordered-target-statistics-数值编码方法"><strong>1,类别特征的Ordered Target Statistics 数值编码方法</strong></h3><p>对于类别特征，如果类别数目不多，可以使用onehot编码。</p><p>但如果类别数量成百上千，使用onehot编码会导致特征数量爆炸。</p><p>CatBoost设计了一种基于预测目标统计值的方法可以将类别特征转化为数值特征。</p><p>以风控领域的预测信贷用户是否会违约为例，假设有一个类别特征是根据身份证号码解析出来的用户所出生的城市。</p><p>全国有几百个城市，转化为onehot编码会造成特征维数爆炸。</p><p>一种非常make sense的方式是我们用某个城市用户的平均逾期率来作为该城市的数值特征编码。</p><p>简而言之,我们用如下方式将 city = "上海" 这一类别特征取值代替为如下值。</p><p>city_numeric("上海") = sample_count(city="上海" and label=1(逾期)) /sample_count(city="上海")</p><p>这就是所谓的 Target Statistics 编码方法。</p><p>但是考虑到有一些小城市，比如黑龙江鹤岗市，可能在训练样本中数量很少甚至没有，这时候用训练样本中鹤岗市的用户平均逾期率来估计会比较不靠谱。</p><p>例如鹤岗市只有1个样本，并且这个样本是逾期的，那么数值编码</p><p>city_numeric("鹤岗") = sample_count(city="鹤岗" and label=1(逾期)) /sample_count(city="鹤岗") = 1.0</p><p>我们可以考虑加入先验值来抑制这种小样本的波动。</p><p>假设不区分城市，全部训练样本中用户的 逾期率 为 P = 0.1,我们可以在分子分母上分别加入 a = 100个 逾期率为P 的先验样本。</p><p>city_numeric("鹤岗") = (sample_count(city="鹤岗" and label=1(逾期)) +a·P) / （sample_count(city="鹤岗")+ a） = 11/101</p><p>这样就合理多了。</p><p>这种数值编码方式虽然好，但是会造成训练集中label的泄露，因为对于某个样本来说，其数值编码计算过程中已经把这个样本的label值纳入了计算过程中。</p><p>未来要预测的验证集的数据分布未必与训练集相同，例如训练集中 上海市用户的平均逾期率为0.12，但是验证集中上海市用户的平均逾期率可能只有0.04，在训练集中这个city_numeric特征可能会特别好用，特别重要，但是在验证集中可能会变得没有那么好用，没有那么重要。</p><p>为了让模型正确地评估 city_numeric特征的真实有效性和重要程度，我们可以拿出一部分数据来计算这个特征编码，用另外一部分数据来训练。但是这样会造成可用数据的减少。</p><p>CatBoost巧妙地设计了如下trick，来缓解这个问题。先将样本随机打乱，然后每个样本只使用它排序在它前面的样本来计算其类别特征的数值编码。这样就防止了label的泄露，并且能够较为合理地评估这个特征的真实有效性。</p><p>不过这种方式会造成排在前面的样本的类别特征的数值编码估计不是很准，为了减少这个影响，CatBoost会设计多个样本随机排列(默认4个)，在每次建树前从中随机取一个排列。</p><p>以上就是所谓的 Ordered Target Statistics编码方法，也是CatBoost最重要的创新。</p><figure><img src="/images/catboost/v2-90cfdd4e26bbcdc3825abe0a84e727a5.jpg"alt="v2-90cfdd4e26bbcdc3825abe0a84e727a5" /><figcaptionaria-hidden="true">v2-90cfdd4e26bbcdc3825abe0a84e727a5</figcaption></figure><h3id="基于贪心策略的特征交叉方法"><strong>2，基于贪心策略的特征交叉方法</strong></h3><p>使用Ordered Target Statistics方法将类别特征转化成为数值特征以后，会影响到特征交叉，因为数值特征无法有效地进行交叉。</p><p>依然以风控领域的预测信贷用户是否会违约为例，假设 city="北京市" 且job="保安"的用户信用特别好，但不是北京市所有的用户都信用好，也不是所有的保安都信用特别好。只有北京市的保安这个群体才信用好。</p><p>如果我们将city转换为数值编码，也将保安转换为数值编码之后，我们得到两个数，这两个数相乘是没有意义的，我们无法表示北京市的保安这个群体。</p><p>为了有效地利用特征交叉，CatBoost在将类别特征转换为数值编码的同时，会自动生成 交叉特征。</p><p>如果让全部的类别特征之间都进行交叉，两两交叉，三三交叉，四四交叉，这个复杂度是指数级的，特征维度一定会爆炸。</p><p>CatBoost使用一种贪心的策略来进行特征交叉。生成tree的第一次分裂，CatBoost不使用任何交叉特征。在后面的分裂中，CatBoost会使用生成tree所用到的全部原始特征和交叉特征跟 数据集中的全部 类别特征进行交叉。</p><p>在定义CatBoost模型时，我们可以用'max_ctr_complexity'来控制允许的特征交叉的最大特征数量，如果设置为3，那么生成tree时所用到的交叉特征最多只会来自3个特征的交叉，也就是我们只能表示city='北京市' 且 job='保安' 且education='高中'这样的三阶交叉特征，而无法表示 city='北京市' 且job='保安' 且 education='高中' 且 hobby='抽烟' 这样的四阶交叉特征。</p><h3 id="避免预测偏移的-ordered-boosting-方法"><strong>3，避免预测偏移的Ordered Boosting 方法。</strong></h3><p>使用XGBoost或者LightGBM做模型时，我们可能经常会发现模型在训练集上拟合的很好，train_auc甚至达到了1.0,但是在验证集上却差了很多, va_auc可能只有0.7。这当然有可能是因为tree的数量太多了，或者是每棵tree的leaves太多了，总之模型太复杂了造成了过拟合。</p><p>但也有一些XGBoost和LightGBM自身算法的缺陷因素。我们知道LightGBM在训练下一棵tree的时候，需要计算前面这些tree构成的加法模型在所有样本上的一阶梯度和二阶梯度(Loss对模型预测结果的导数)，然后用这些梯度来决定下一棵树的结构和叶子节点取值。</p><p>但是我们计算的这些一阶梯度和二阶梯度值是问题的。前面的这些tree都是在这些样本上训练的，现在我们又在这些样本上估计模型预测结果的一阶和二阶梯度。我们应该换一些新的样本才更合理。但是我们从哪里找这些新的样本呢？</p><p>CatBoost的作者故伎重演。先将样本随机打乱，然后每个样本只使用排序在它前面的样本来训练模型。用这样的模型来估计这个样本预测结果的一阶和二阶梯度。然后用这些梯度构建一棵tree的结构，最终tree的每个叶子节点的取值，是使用全体样本进行计算的。</p><p>这就是OrderedBoosting的主要思想。可以有效地减少梯度估计的误差，缓解预测偏移。但是会增加较多的计算量，影响训练速度。</p><p>在定义CatBoost模型时，我们可以用'boosting_type'这个参数来设置是使用OrderedBoosting 还是 LightGBM那样的 PlainBoosting。如果不显式设置，CatBoost会根据样本和特征数量自己决定。</p><figure><img src="/images/catboost/v2-7dbcacde377b5172a666fb1d981efbbb.jpg"alt="v2-7dbcacde377b5172a666fb1d981efbbb" /><figcaptionaria-hidden="true">v2-7dbcacde377b5172a666fb1d981efbbb</figcaption></figure><h3id="使用对称二叉树作为基模型有正则作用且预测极快"><strong>4，使用对称二叉树作为基模型，有正则作用且预测极快</strong></h3><p>XGBoost和LightGBM采用的基模型是普通的二叉树，但是CatBoost采用的是对称的二叉树。</p><p>这种对树结构上的约束有一定的正则作用。更为重要的是，它可以让CatBoost模型的推断过程极快。</p><p>对于CatBoost的tree的预测过程来说，每个特征的分裂都是独立的，不分先后顺序，多个样本可以一起预测。</p><figure><img src="/images/catboost/v2-e99a44ac409aa702ed46dd07f1136cdb.jpg"alt="v2-e99a44ac409aa702ed46dd07f1136cdb" /><figcaptionaria-hidden="true">v2-e99a44ac409aa702ed46dd07f1136cdb</figcaption></figure><h2 id="三-使用范例"><strong>三 使用范例</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!pip install catboost </span><br><span class="hljs-keyword">import</span> catboost <span class="hljs-keyword">as</span> cb <br><span class="hljs-built_in">print</span>(cb.__version__)<br></code></pre></td></tr></table></figure><p>1.0.4</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> display <br><br><span class="hljs-keyword">import</span> datetime,json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> catboost <span class="hljs-keyword">as</span> cb <br><span class="hljs-keyword">from</span> catboost.datasets <span class="hljs-keyword">import</span> titanic<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> StratifiedKFold<br><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score,roc_auc_score,accuracy_score<br><span class="hljs-keyword">import</span> plotly.graph_objs <span class="hljs-keyword">as</span> go <br><span class="hljs-keyword">import</span> plotly.express <span class="hljs-keyword">as</span> px <br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">printlog</span>(<span class="hljs-params">info</span>):<br>    nowtime = datetime.datetime.now().strftime(<span class="hljs-string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n&quot;</span>+<span class="hljs-string">&quot;==========&quot;</span>*<span class="hljs-number">8</span> + <span class="hljs-string">&quot;%s&quot;</span>%nowtime)<br>    <span class="hljs-built_in">print</span>(info+<span class="hljs-string">&#x27;...\n\n&#x27;</span>)<br>     <br><span class="hljs-comment">#================================================================================</span><br><span class="hljs-comment"># 一，准备数据</span><br><span class="hljs-comment">#================================================================================</span><br>printlog(<span class="hljs-string">&quot;step1: preparing data...&quot;</span>)<br><br>dfdata,dftest = titanic()<br><br>display(dfdata.head()) <br><br>label_col = <span class="hljs-string">&quot;Survived&quot;</span><br><br><span class="hljs-comment"># 填充空值特征</span><br>dfnull = pd.DataFrame(dfdata.isnull().<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>),columns = [<span class="hljs-string">&quot;null_cnt&quot;</span>]).query(<span class="hljs-string">&quot;null_cnt&gt;0&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;null_features:&quot;</span>) <br><span class="hljs-built_in">print</span>(dfnull)<br><br>dfdata.fillna(-<span class="hljs-number">9999</span>, inplace=<span class="hljs-literal">True</span>)<br>dftest.fillna(-<span class="hljs-number">9999</span>, inplace=<span class="hljs-literal">True</span>)<br><br><br><span class="hljs-comment"># 刷选类别特征</span><br>cate_cols = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> dfdata.columns <br>             <span class="hljs-keyword">if</span> dfdata[x].dtype <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [np.float32,np.float64] <span class="hljs-keyword">and</span> x!=label_col]<br><span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> cate_cols:<br>    dfdata[col] = pd.Categorical(dfdata[col]) <br>    dftest[col] = pd.Categorical(dftest[col]) <br><br><span class="hljs-comment"># 分割数据集</span><br>dftrain,dfvalid = train_test_split(dfdata, train_size=<span class="hljs-number">0.75</span>, random_state=<span class="hljs-number">42</span>)<br>Xtrain,Ytrain = dftrain.drop(label_col,axis = <span class="hljs-number">1</span>),dftrain[label_col]<br>Xvalid,Yvalid = dfvalid.drop(label_col,axis = <span class="hljs-number">1</span>),dfvalid[label_col]<br>cate_cols_indexs = np.where(Xtrain.columns.isin(cate_cols))[<span class="hljs-number">0</span>]<br><br><br><span class="hljs-comment"># 整理成Pool</span><br>pool_train = cb.Pool(data = Xtrain, label = Ytrain, cat_features=cate_cols)<br>pool_valid = cb.Pool(data = Xvalid, label = Yvalid, cat_features=cate_cols)<br><br><br><span class="hljs-comment">#================================================================================</span><br><span class="hljs-comment"># 二，设置参数</span><br><span class="hljs-comment">#================================================================================</span><br>printlog(<span class="hljs-string">&quot;step2: setting parameters...&quot;</span>)<br>                               <br>iterations = <span class="hljs-number">1000</span><br>early_stopping_rounds = <span class="hljs-number">200</span><br><br>params = &#123;<br>    <span class="hljs-string">&#x27;learning_rate&#x27;</span>: <span class="hljs-number">0.05</span>,<br>    <span class="hljs-string">&#x27;loss_function&#x27;</span>: <span class="hljs-string">&quot;Logloss&quot;</span>,<br>    <span class="hljs-string">&#x27;eval_metric&#x27;</span>: <span class="hljs-string">&quot;Accuracy&quot;</span>,<br>    <span class="hljs-string">&#x27;depth&#x27;</span>: <span class="hljs-number">6</span>,<br>    <span class="hljs-string">&#x27;min_data_in_leaf&#x27;</span>: <span class="hljs-number">20</span>,<br>    <span class="hljs-string">&#x27;random_seed&#x27;</span>: <span class="hljs-number">42</span>,<br>    <span class="hljs-string">&#x27;logging_level&#x27;</span>: <span class="hljs-string">&#x27;Silent&#x27;</span>,<br>    <span class="hljs-string">&#x27;use_best_model&#x27;</span>: <span class="hljs-literal">True</span>,<br>    <span class="hljs-string">&#x27;one_hot_max_size&#x27;</span>: <span class="hljs-number">5</span>,   <span class="hljs-comment">#类别数量多于此数将使用ordered target statistics编码方法,默认值为2。</span><br>    <span class="hljs-string">&#x27;boosting_type&#x27;</span>:<span class="hljs-string">&quot;Ordered&quot;</span>, <span class="hljs-comment">#Ordered 或者Plain,数据量较少时建议使用Ordered,训练更慢但能够缓解梯度估计偏差。</span><br>    <span class="hljs-string">&#x27;max_ctr_complexity&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-comment">#特征组合的最大特征数量，设置为1取消特征组合，设置为2只做两个特征的组合,默认为4。</span><br>    <span class="hljs-string">&#x27;nan_mode&#x27;</span>: <span class="hljs-string">&#x27;Min&#x27;</span> <br>&#125;<br><br><br><span class="hljs-comment">#================================================================================</span><br><span class="hljs-comment"># 三，训练模型</span><br><span class="hljs-comment">#================================================================================</span><br>printlog(<span class="hljs-string">&quot;step3: training model...&quot;</span>)<br><br><br>model = cb.CatBoostClassifier(<br>    iterations = iterations,<br>    early_stopping_rounds = early_stopping_rounds,<br>    train_dir=<span class="hljs-string">&#x27;catboost_info/&#x27;</span>,<br>    **params<br>)<br><br><br><span class="hljs-comment">#直接训练</span><br>model.fit(<br>    pool_train,<br>    eval_set=pool_valid,<br>    plot=<span class="hljs-literal">True</span><br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;model.get_all_params():&quot;</span>)<br><span class="hljs-built_in">print</span>(model.get_all_params() )<br><br><br><span class="hljs-comment">#5折交叉验证</span><br>cv_data= cb.cv(<br>    cb.Pool(dfdata.drop(label_col,axis = <span class="hljs-number">1</span>), dfdata[label_col], cat_features=cate_cols_indexs),<br>    params,<br>    fold_count = <span class="hljs-number">3</span>,<br>    plot=<span class="hljs-literal">True</span><br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Best validation accuracy score: &#123;:.2f&#125;±&#123;:.2f&#125; on step &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>    np.<span class="hljs-built_in">max</span>(cv_data[<span class="hljs-string">&#x27;test-Accuracy-mean&#x27;</span>]),<br>    cv_data[<span class="hljs-string">&#x27;test-Accuracy-std&#x27;</span>][np.argmax(cv_data[<span class="hljs-string">&#x27;test-Accuracy-mean&#x27;</span>])],<br>    np.argmax(cv_data[<span class="hljs-string">&#x27;test-Accuracy-mean&#x27;</span>])<br>))<br><br><br><span class="hljs-comment">#================================================================================</span><br><span class="hljs-comment"># 四，评估模型</span><br><span class="hljs-comment">#================================================================================</span><br>printlog(<span class="hljs-string">&quot;step4: evaluating model ...&quot;</span>)<br><br><br>y_pred_train = model.predict(Xtrain)<br>y_pred_valid = model.predict(Xvalid)<br><br>train_score = f1_score(Ytrain,y_pred_train)<br>valid_score = f1_score(Yvalid,y_pred_valid)<br><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;train f1_score: &#123;:.5&#125; &#x27;</span>.<span class="hljs-built_in">format</span>(train_score))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;valid f1_score: &#123;:.5&#125; \n&#x27;</span>.<span class="hljs-built_in">format</span>(valid_score))   <br><br><br><br><span class="hljs-comment">#feature importance </span><br>dfimportance = model.get_feature_importance(prettified=<span class="hljs-literal">True</span>) <br>dfimportance = dfimportance.sort_values(by = <span class="hljs-string">&quot;Importances&quot;</span>).iloc[-<span class="hljs-number">20</span>:]<br>fig_importance = px.bar(dfimportance,x=<span class="hljs-string">&quot;Importances&quot;</span>,y=<span class="hljs-string">&quot;Feature Id&quot;</span>,title=<span class="hljs-string">&quot;Feature Importance&quot;</span>)<br><br>display(dfimportance)<br>display(fig_importance)<br><br><br><span class="hljs-comment">#score distribution</span><br>y_test_prob = model.predict_proba(dftest)[:,-<span class="hljs-number">1</span>]<br>trace1 = go.Histogram(x = y_test_prob,histnorm = <span class="hljs-string">&#x27;probability&#x27;</span>,nbinsx=<span class="hljs-number">50</span>)<br>layout = go.Layout(title = <span class="hljs-string">&quot;Score Distribution&quot;</span>,xaxis=&#123;<span class="hljs-string">&quot;title&quot;</span>:<span class="hljs-string">&quot;score&quot;</span>&#125;,yaxis = &#123;<span class="hljs-string">&quot;title&quot;</span>:<span class="hljs-string">&quot;frequecy&quot;</span>&#125;)<br>fig_distribution = go.Figure(data = [trace1])<br>fig_distribution.update_layout(layout)<br>display(fig_distribution)<br><br><br><span class="hljs-comment">#================================================================================</span><br><span class="hljs-comment"># 五，使用模型</span><br><span class="hljs-comment">#================================================================================</span><br>printlog(<span class="hljs-string">&quot;step5: using model ...&quot;</span>)<br><br>y_pred_test = model.predict(dftest)<br>y_pred_test_prob = model.predict_proba(dftest)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y_pred_test:\n&quot;</span>,y_pred_test[:<span class="hljs-number">10</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y_pred_test_prob:\n&quot;</span>,y_pred_test_prob[:<span class="hljs-number">10</span>])<br><br><br><br><span class="hljs-comment">#================================================================================</span><br><span class="hljs-comment"># 六，保存模型</span><br><span class="hljs-comment">#================================================================================</span><br>printlog(<span class="hljs-string">&quot;step6: saving model ...&quot;</span>)<br><br>model_dir = <span class="hljs-string">&#x27;catboost_model&#x27;</span><br>model.save_model(model_dir)<br>model_loaded = cb.CatBoostClassifier()<br>model.load_model(model_dir)<br></code></pre></td></tr></table></figure><figure><img src="/images/catboost/v2-c657c93122dd4eefaa9023a7ef49b495.jpg"alt="v2-c657c93122dd4eefaa9023a7ef49b495" /><figcaptionaria-hidden="true">v2-c657c93122dd4eefaa9023a7ef49b495</figcaption></figure><figure><img src="/images/catboost/v2-d451da5658789992a0a910062972049b.jpg"alt="v2-d451da5658789992a0a910062972049b" /><figcaptionaria-hidden="true">v2-d451da5658789992a0a910062972049b</figcaption></figure><figure><img src="/images/catboost/v2-86df3c443efed79653479c614bd73b4d.jpg"alt="v2-86df3c443efed79653479c614bd73b4d" /><figcaptionaria-hidden="true">v2-86df3c443efed79653479c614bd73b4d</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>集成学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>集成学习</tag>
      
      <tag>模型框架</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>XGBoost参数调优</title>
    <link href="/XGB%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98.html"/>
    <url>/XGB%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98.html</url>
    
    <content type="html"><![CDATA[<p>在机器学习中，参数调优是一门玄学，因为模型的最优参数可能依赖于许多场景。因此，不可能为参数调优创建一个全面的指南。本文尝试为XGBoost中的参数提供一些指导。</p><span id="more"></span><h2 id="一.-思路概述">一. 思路概述</h2><h3 id="理解偏差-方差权衡bias-variance-tradeoff">1.1理解偏差-方差权衡(Bias-Variance Tradeoff)</h3><p>如果你参加过机器学习或统计学课程，这可能是最重要的概念之一。当我们允许模型变得更加复杂（例如，增加深度）时，模型具有更好的拟合训练数据的能力，从而获得偏差较小的模型。然而，这种复杂的模型需要更多的数据来进行拟合。</p><p>XGBoost中的大多数参数都涉及偏差和方差的权衡。最好的模型应该在模型复杂度和预测能力之间进行仔细权衡。参数文档会告诉你每个参数是否会使模型更保守。这可以帮助你在复杂模型和简单模型之间进行调整。</p><h3 id="控制过拟合">1.2 控制过拟合</h3><p>当你观察到训练准确率高但测试准确率低时，很可能遇到了过拟合问题。</p><p>在XGBoost中，一般有两种方法可以控制过拟合：</p><blockquote><ul><li><p>第一种方法是直接控制模型复杂度。</p><p><code>这包括max_depth、min_child_weight和gamma。</code></p></li><li><p>第二种方法是增加随机性，使训练对噪声具有鲁棒性。</p></li></ul><p>​ <code>这包括subsample和colsample_bytree。</code></p></blockquote><p>你还可以减少步长eta。记住在这样做时增加num_round。</p><h3 id="处理数据集不平衡">1.3 处理数据集不平衡</h3><p>对于诸如广告点击日志等常见情况，数据集极度不平衡。这会影响XGBoost模型的训练，有两种方法可以改进。</p><blockquote><p>如果你只关心预测的整体性能指标（AUC）</p><ul><li><p>通过scale_pos_weight平衡正负样本的权重</p></li><li><p>使用AUC作为评估标准</p></li></ul><p>如果你关心预测的正确概率</p><ul><li><p>在这种情况下，你不能重新平衡数据集</p></li><li><p>将参数max_delta_step设置为一个有限的数值（例如1）以帮助收敛</p></li></ul></blockquote><h3 id="减少内存使用">1.4 减少内存使用</h3><p>如果你使用类似sklearn.model_selection.GridSearchCV的HPO库，请控制它可以使用的线程数。最好让XGBoost并行运行，而不是让GridSearchCV同时运行多个实验。例如，为交叉验证创建一个数据折叠可以消耗大量内存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这会创建数据集的副本。X和X_train同时存在于内存中。</span><br><span class="hljs-comment"># 如果你在n_jobs大于1的情况下运行`GridSearchCV`，每个线程都会同时发生这种情况。</span><br>X_train, X_test, y_train, y_test = train_test_split(X, y)<br><br>df = pd.DataFrame()<br><span class="hljs-comment"># 这会创建数据框的新副本，即使你指定了inplace参数</span><br>new_df = df.drop(...)<br><br>array = np.array(...)<br><span class="hljs-comment"># 这可能会也可能不会复制数据，具体取决于数据类型</span><br>array.astype(np.float32)<br><br><span class="hljs-comment"># np默认使用双精度，你真的需要吗？</span><br>array = np.array(...)<br></code></pre></td></tr></table></figure><p>你可以在文档中找到一些更具体的减少内存使用的实践。例如：与Dask的分布式XGBoost、XGBoostGPU支持。然而，在深入研究这些之前，要意识到数据副本的创建是一个好的起点。它通常消耗的内存比人们预期的要多得多</p><h2 id="二.-xgb参数详解">二. XGB参数详解</h2><p>在运行 XGBoost 之前, 我们必须设置三种类型的参数: 常规参数,提升器参数和任务参数.</p><ul><li>常规参数与我们用于提升的提升器有关，通常是树模型或线性模型</li><li>提升器参数取决于你所选择的提升器</li><li>学习任务的参数决定了学习场景,例如回归任务可以使用不同的参数进行排序相关的任务</li><li>命令行参数的行为与 xgboost 的 CLI 版本相关</li></ul><h3 id="常规参数">2.1 常规参数</h3><h4 id="booster-默认gbtree">booster [默认=gbtree]</h4><p>选择使用哪种booster，可以是gbtree、gblinear或dart。gbtree和dart使用基于树的模型，而gblinear使用线性函数。</p><h4 id="silent-默认0">silent [默认=0]</h4><p>0表示打印运行信息，1表示静默模式。</p><h4 id="nthread-默认值为可用的最大线程数如果未设置">nthread[默认值为可用的最大线程数，如果未设置]</h4><p>用于运行xgboost的并行线程数。</p><h4 id="num_pbuffer-由xgboost自动设置无需用户设置">num_pbuffer[由xgboost自动设置，无需用户设置]</h4><p>预测缓冲区的大小，通常设置为训练实例的数量。缓冲区用于保存上一次boosting步骤的预测结果。</p><h4 id="num_feature-由xgboost自动设置无需用户设置">num_feature[由xgboost自动设置，无需用户设置]</h4><p>boosting过程中使用的特征维度，设置为特征的最大维度。</p><h3 id="用于-tree-提升的参数">2.2 用于 Tree 提升的参数</h3><h4 id="eta-默认0.3">⭐eta [默认=0.3]</h4><p>用于更新的步长缩减，以防止过拟合。在每个提升步骤之后，我们可以直接获得新特征的权重。eta实际上是缩小了特征权重，使提升过程更加保守。 范围: [0,1]</p><h4 id="gamma-默认0">⭐gamma [默认=0]</h4><p>在树的叶节点上进行进一步分区所需的最小损失减少量。值越大，算法越保守。范围: [0,∞]</p><h4 id="max_depth-默认6">⭐max_depth [默认=6]</h4><p>树的最大深度，增加此值将使模型更加复杂/更容易过拟合。 范围: [1,∞]</p><h4 id="min_child_weight-默认1">⭐min_child_weight [默认=1]</h4><p>子节点所需的实例权重（赫西安矩阵）的最小和。如果树分区步骤导致叶节点的实例权重和小于min_child_weight，那么构建过程将放弃进一步分区。在线性回归模式下，这仅对应于每个节点所需的最小实例数。值越大，算法越保守。范围: [0,∞]</p><h4 id="max_delta_step-默认0">max_delta_step [默认=0]</h4><p>每棵树的权重估计允许的最大步长。如果设置为0，则表示没有约束。如果设置为正值，可以帮助使更新步骤更加保守。通常不需要此参数，但在类极不平衡的逻辑回归中可能会有所帮助。设置为1-10的值可能有助于控制更新。范围: [0,∞]</p><h4 id="subsample-默认1">subsample [默认=1]</h4><p>训练实例的子采样比例。设置为0.5表示XGBoost随机收集一半的数据实例来生长树，这将防止过拟合。范围: (0,1]</p><h4 id="colsample_bytree-默认1">colsample_bytree [默认=1]</h4><p>构建每棵树时的列的子采样比例。 范围: (0,1]</p><h4 id="colsample_bylevel-默认1">colsample_bylevel [默认=1]</h4><p>在每个层次上分割时的列的子采样比例。 范围: (0,1]</p><h4 id="lambda-默认1">⭐lambda [默认=1]</h4><p>权重上的L2正则化项，增加此值将使模型更加保守。</p><h4 id="alpha-默认0">alpha [默认=0]</h4><p>权重上的L1正则化项，增加此值将使模型更加保守。</p><h4 id="tree_method-string-默认auto">tree_method, string[默认=‘auto’]</h4><p>XGBoost中使用的树构建算法（参见参考文献中的描述）。分布式和外部存储版本仅支持近似算法。 选择：{‘auto’, ‘exact’,‘approx’}</p><blockquote><p>‘auto’: 使用启发式方法选择更快的算法。对于中小型数据集，将使用精确贪婪算法。对于非常大的数据集，将选择近似算法。由于旧行为始终在单机上使用精确贪婪算法，因此在选择近似算法时用户将收到通知。‘exact’: 精确贪婪算法。 ‘approx’: 使用素描和直方图的近似贪婪算法。</p></blockquote><h4 id="sketch_eps-默认0.03">sketch_eps, [默认=0.03]</h4><p>仅用于近似贪婪算法。 这大致转换为 O(1 / sketch_eps)个箱子的数量。相比于直接选择箱子的数量，这种方法带有理论上的素描准确性保证。通常用户不需要调整此参数，但可以考虑设置为较低的值以获得更准确的枚举。范围: (0, 1)</p><h4 id="scale_pos_weight-默认0">⭐scale_pos_weight, [默认=0]</h4><p>控制正负权重的平衡，对不平衡类别有用。一个典型的考虑值是：sum(负例) /sum(正例)。详见参数调优中的更多讨论。参见 Higgs Kaggle 比赛的示例：R,py1, py2, py3</p><h3 id="学习任务的参数">2.3 学习任务的参数</h3><p>指定学习任务及相应的学习目标。可选的目标如下：</p><h4 id="objective-默认值reglinear">objective [默认值=reg:linear]</h4><blockquote><ul><li><p>“reg:linear” – 线性回归</p></li><li><p>“reg:logistic” – 逻辑回归</p></li><li><p>“binary:logistic” – 用于二分类的逻辑回归，输出概率</p></li><li><p>“binary:logitraw” –用于二分类的逻辑回归，输出逻辑变换前的得分</p></li><li><p>“count:poisson” – 计数数据的泊松回归，输出泊松分布的均值泊松回归中max_delta_step默认设为0.7（用于保障优化过程）</p></li><li><p>“multi:softmax” –使用softmax目标设置XGBoost进行多类分类，还需要设置num_class（类别数）</p></li><li><p>“multi:softprob” – 与softmax相同，但输出一个ndata *nclass的向量，可以进一步重塑为ndata,nclass矩阵。结果包含每个数据点属于每个类别的预测概率。</p></li><li><p>“rank:pairwise” –设置XGBoost通过最小化成对损失来执行排序任务</p></li><li><p>“reg:gamma” –用于严重度数据的伽玛回归，输出伽玛分布的均值</p></li></ul></blockquote><h4 id="base_score-默认值0.5">base_score [默认值=0.5]</h4><p>所有实例的初始预测得分，全局偏差对于足够多的迭代次数，改变这个值不会有太大影响。</p><h4 id="eval_metric-默认值根据objective确定">⭐eval_metric[默认值根据objective确定]</h4><p>验证数据的评估指标，将根据objective分配一个默认指标（回归任务为rmse，分类任务为error，排序任务为平均精度）用户可以添加多个评估指标，对于Python用户，请记住将指标作为参数对的列表传递，而不是映射，这样后面的‘eval_metric’不会覆盖前面的。</p><p>可选的评估指标如下：</p><blockquote><ul><li><p>“rmse”：均方根误差</p></li><li><p>“mae”：平均绝对误差</p></li><li><p>“logloss”：负对数似然</p></li><li><p>“error”：二分类错误率。计算方法为#(错误案例)/#(所有案例)。对于预测，评估会将预测值大于0.5的实例视为正例，其余视为负例。</p></li><li><p>“merror”：多类分类错误率。计算方法为#(错误案例)/#(所有案例)。</p></li><li><p>“mlogloss”：多类对数损失</p></li><li><p>“auc”：用于排序评估的曲线下面积</p></li><li><p>“ndcg”：归一化折扣累积增益</p></li><li><p>“map”：平均精度</p></li><li><p>“ndcg@n”，“map@n”：n可以设为一个整数，用于截断评估列表中的前n个位置。</p></li><li><p>“ndcg-”，“map-”，“ndcg@n-”，“map@n-”：在XGBoost中，NDCG和MAP将评估没有任何正样本的列表的得分为1。通过在评估指标中添加“-”，XGBoost将在某些条件下将这些得分评估为0。</p></li><li><p>“gamma-deviance”： [伽玛回归的残差偏差]</p></li></ul></blockquote><h4 id="seed-默认值0">seed [默认值=0]</h4><p>随机数种子。</p><h3 id="用于-dart-booster-的其它参数">2.4 用于 Dart Booster的其它参数</h3><h4 id="sample_type-默认值uniform"><code>sample_type</code>[默认值=”uniform”]</h4><p>采样算法的类型。<br />“uniform”：均匀选择丢弃的树。<br />“weighted”：按权重比例选择丢弃的树。</p><h4 id="normalize_type-默认值tree"><code>normalize_type</code>[默认值=”tree”]</h4><p>归一化算法的类型。<br />“tree”：新树的权重与每棵丢弃的树相同。<br />新树的权重为 1 / (k + 学习率)<br />丢弃的树按 k / (k + 学习率) 进行缩放。<br />“forest”：新树的权重与丢弃树（森林）的权重总和相同。<br />新树的权重为 1 / (1 + 学习率)<br />丢弃的树按 1 / (1 + 学习率) 进行缩放。</p><h4 id="rate_drop-默认值0.0"><code>rate_drop</code> [默认值=0.0]</h4><p>丢弃率。<br />范围：[0.0, 1.0]</p><h3 id="用于-linear-booster-的参数">2.5 用于 Linear Booster 的参数</h3><h4 id="skip_drop-默认值0.0"><code>skip_drop</code> [默认值=0.0]</h4><p>跳过丢弃的概率。<br />如果跳过丢弃，新树将以与 gbtree 相同的方式添加。<br />范围：[0.0, 1.0]</p><h4 id="lambda-默认值0">lambda [默认值=0]</h4><p>权重的L2正则化项，增加这个值会使模型更加保守。</p><p>alpha [默认值=0] 权重的L1正则化项，增加这个值会使模型更加保守。</p><h4 id="lambda_bias">lambda_bias</h4><p>偏置的L2正则化项，默认值为0（没有偏置的L1正则化，因为它不重要）。</p><h2 id="三.-xgb代码示例">三. XGB代码示例</h2><p>此代码用于二分类问题,使用XGB建模,并输出评估报告,绘制AUC曲线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_auc_score,roc_curve<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report<br><br><span class="hljs-comment"># 参数设置</span><br>params=&#123;<br><span class="hljs-string">&#x27;booster&#x27;</span>:<span class="hljs-string">&#x27;gblinear&#x27;</span>,<br><span class="hljs-string">&#x27;objective&#x27;</span>:<span class="hljs-string">&#x27;binary:logistic&#x27;</span>,<br><span class="hljs-string">&#x27;metric&#x27;</span>:<span class="hljs-string">&#x27;auc&#x27;</span>,<br><span class="hljs-string">&#x27;eval_metric&#x27;</span>:<span class="hljs-string">&#x27;auc&#x27;</span>,<br><span class="hljs-string">&#x27;eta&#x27;</span>:<span class="hljs-number">0.0425</span>,<br><span class="hljs-string">&#x27;max_depth&#x27;</span>:<span class="hljs-number">15</span>,<br><span class="hljs-string">&#x27;min_child_weight&#x27;</span>:<span class="hljs-number">20</span>,<br><span class="hljs-string">&#x27;gamma&#x27;</span>:<span class="hljs-number">0</span>,<br><span class="hljs-string">&#x27;subsample&#x27;</span>:<span class="hljs-number">1</span>,<br><span class="hljs-string">&#x27;colsample_bytree&#x27;</span>:<span class="hljs-number">1</span>,<br><span class="hljs-string">&#x27;scale_pos_weight&#x27;</span>:<span class="hljs-number">1</span><br>&#125;<br><br>dtrain = xgb.DMatrix(x, y)<br><br>lr_class_weight = xgb.train(params=params,dtrain=dtrain,num_boost_round=<span class="hljs-number">165</span>)<br><span class="hljs-comment"># 加载测试数据</span><br>data_test = test_slct3<br><br>data_test_encoding = pd.get_dummies(data_test)<br>y_test = data_test_encoding[<span class="hljs-string">&#x27;Attrition&#x27;</span>]<br>x_test = transfer.transform(data_test_encoding.drop(<span class="hljs-string">&#x27;Attrition&#x27;</span>, axis=<span class="hljs-number">1</span>))<br><br><span class="hljs-comment"># 使用xgb.DMatrix进行预测</span><br>y_pre_prob = lr_class_weight.predict(xgb.DMatrix(x_test))<br>y_pre = (y_pre_prob &gt;= <span class="hljs-number">0.70</span>).astype(<span class="hljs-built_in">int</span>)  <span class="hljs-comment"># 将概率转换为0或1</span><br><br>train_y_pre_prob = lr_class_weight.predict(xgb.DMatrix(x))<br>train_y_pre = (train_y_pre_prob &gt;= <span class="hljs-number">0.70</span>).astype(<span class="hljs-built_in">int</span>)  <span class="hljs-comment"># 将概率转换为0或1</span><br><br><span class="hljs-comment"># 计算并打印ROC AUC得分</span><br>roc_auc = roc_auc_score(y_test, y_pre_prob)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;ROC AUC Score: <span class="hljs-subst">&#123;roc_auc&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 计算并打印ROC AUC得分</span><br>roc_auc = roc_auc_score(y_test, y_pre_prob)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;ROC AUC Score: <span class="hljs-subst">&#123;roc_auc&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 生成并打印classification_report</span><br>report = classification_report(y_test, y_pre)<br><span class="hljs-built_in">print</span>(report)<br><span class="hljs-comment"># 绘制ROC曲线</span><br>fpr, tpr, _ = roc_curve(y_test, y_pre_prob)<br>fpr_train, tpr_train,_=roc_curve(y, train_y_pre_prob)<br>plt.plot(fpr, tpr, label=<span class="hljs-string">&#x27;evl ROC&#x27;</span>)<br>plt.plot(fpr_train, tpr_train, label=<span class="hljs-string">&#x27;Train ROC&#x27;</span>)<br>plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;k--&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;False Positive Rate&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;True Positive Rate&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;ROC Curve&#x27;</span>)<br>plt.legend(loc=<span class="hljs-string">&#x27;best&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>集成学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>回归模型</tag>
      
      <tag>参数调优</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人工智能专业术语汇编</title>
    <link href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E6%B1%87%E7%BC%96.html"/>
    <url>/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E6%B1%87%E7%BC%96.html</url>
    
    <content type="html"><![CDATA[<blockquote><p>本术语库目前拥有专业术语约 2442 个、专项领域篇 2篇，主要为人工智能领域基础概念和术语。</p><p>本术语库前两版主要是将机器之心在编译技术文章和论文过程中所遇到的专业术语记录下来，希望有助于AI从业者的查阅和学习<span id="more"></span> # 人工智能--术语库 引自 github的 <ahref="https://github.com/jiqizhixin/Artificial-Intelligence-Terminology-Database">Artificial-Intelligence-Terminology-Database</a>项目</p></blockquote><table style="width:100%;"><thead><tr class="header"><th>索引编号</th><th>英文术语</th><th>中文翻译</th><th>常用缩写</th><th>来源&amp;扩展</th><th>备注</th></tr></thead><tbody><tr class="odd"><td>AITD-00000</td><td>0-1 Loss Function</td><td>0-1损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00001</td><td>Absolute Loss Function</td><td>绝对损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00002</td><td>Absolute Value Rectification</td><td>绝对值整流</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00003</td><td>Accept-Reject Sampling Method</td><td>接受-拒绝抽样法/接受-拒绝采样法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00004</td><td>Acceptance Distribution</td><td>接受分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00005</td><td>Access Parameters</td><td>访问参数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00006</td><td>Accumulated Error Backpropagation</td><td>累积误差反向传播</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00007</td><td>Accuracy</td><td>准确率</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00008</td><td>Acoustic</td><td>声学</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00009</td><td>Acoustic Modeling</td><td>声学建模</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00010</td><td>Acquisition Function</td><td>采集函数</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-18-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00011</td><td>Action</td><td>动作</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00012</td><td>Action Value Function</td><td>动作价值函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00013</td><td>Actionism</td><td>行为主义</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00014</td><td>Activation</td><td>活性值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00015</td><td>Activation Function</td><td>激活函数</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-06-11-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-18-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[4]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00016</td><td>Active Learning</td><td>主动学习</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00017</td><td>Actor</td><td>演员</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00018</td><td>Actor-Critic Algorithm</td><td>演员-评论员算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00019</td><td>Actor-Critic Method</td><td>演员-评论员法</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-14">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00020</td><td>Adaptive Bitrate Algorithm</td><td>自适应比特率算法</td><td>ABR</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00021</td><td>Adaptive Boosting</td><td>AdaBoost</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00022</td><td>Adaptive Gradient Algorithm</td><td>AdaGrad</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00023</td><td>Adaptive Moment Estimation Algorithm</td><td>Adam算法</td><td>Adam</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00024</td><td>Adaptive Resonance Theory</td><td>自适应谐振理论</td><td>ART</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00025</td><td>Additive Model</td><td>加性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00026</td><td>Adversarial</td><td>对抗</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00027</td><td>Adversarial Example</td><td>对抗样本</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-06-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00028</td><td>Adversarial Networks</td><td>对抗网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-08-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00029</td><td>Adversarial Training</td><td>对抗训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00030</td><td>Affine Layer</td><td>仿射层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00031</td><td>Affine Transformation</td><td>仿射变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00032</td><td>Affinity Matrix</td><td>亲和矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00033</td><td>Agent</td><td>智能体</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-04-06-6">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-15-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-10-2">[3]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-29-5">[4]</a></td><td></td></tr><tr class="odd"><td>AITD-00034</td><td>Agglomerative</td><td>聚合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00035</td><td>Agnostic PAC Learnable</td><td>不可知PAC可学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00036</td><td>Algorithm</td><td>算法</td><td></td><td><ahref="https://jiqizhixin.github.io/AI-Terminology-page/">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-05-23-4">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-04-2">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00037</td><td>Almost Everywhere</td><td>几乎处处</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00038</td><td>Almost Sure</td><td>几乎必然</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00039</td><td>Almost Sure Convergence</td><td>几乎必然收敛</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00040</td><td>Alpha-Beta Pruning</td><td>α-β修剪法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00041</td><td>Alternative Splicing Dataset</td><td>选择性剪接数据集</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00042</td><td>Ambiguity</td><td>分歧</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00043</td><td>Analytic Gradient</td><td>解析梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00044</td><td>Ancestral Sampling</td><td>原始采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00045</td><td>Annealed Importance Sampling</td><td>退火重要采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00046</td><td>Anomaly Detection</td><td>异常检测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00047</td><td>Aperiodic</td><td>非周期的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00048</td><td>Aperiodic Graph</td><td>非周期性图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00049</td><td>Application-Specific Integrated Circuit</td><td>专用集成电路</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00050</td><td>Approximate Bayesian Computation</td><td>近似贝叶斯计算</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00051</td><td>Approximate Dynamic Programming</td><td>近似动态规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00052</td><td>Approximate Inference</td><td>近似推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00053</td><td>Approximation</td><td>近似</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00054</td><td>Approximation Error</td><td>近似误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00055</td><td>Architecture</td><td>架构</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-12">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00056</td><td>Area Under ROC Curve</td><td>AUC（ROC曲线下方面积，度量分类模型好坏的标准）</td><td>AUC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00057</td><td>Arithmetic Coding</td><td>算术编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00058</td><td>Artificial General Intelligence</td><td>通用人工智能</td><td>AGI</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-06-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00059</td><td>Artificial Intelligence</td><td>人工智能</td><td>AI</td><td><a href="https://www.jiqizhixin.com/articles/2017-05-21-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-05-21-7">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-05-17-16">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[5]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00060</td><td>Artificial Neural Network</td><td>人工神经网络</td><td>ANN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00061</td><td>Artificial Neuron</td><td>人工神经元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00062</td><td>Association Analysis</td><td>关联分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00063</td><td>Associative Memory</td><td>联想记忆</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00064</td><td>Associative Memory Model</td><td>联想记忆模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00065</td><td>Asymptotically Unbiased</td><td>渐近无偏</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00066</td><td>Asynchronous Stochastic Gradient Descent</td><td>异步随机梯度下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00067</td><td>Asynchronous</td><td>异步</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00068</td><td>Atrous Convolution</td><td>空洞卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00069</td><td>Attention</td><td>注意力</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00070</td><td>Attention Cue</td><td>注意力提示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00071</td><td>Attention Distribution</td><td>注意力分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00072</td><td>Attention Mechanism</td><td>注意力机制</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-06-19-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-14-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-28-5">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00073</td><td>Attention Model</td><td>注意力模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00074</td><td>Attractor</td><td>吸引点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00075</td><td>Attribute</td><td>属性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00076</td><td>Attribute Conditional Independence Assumption</td><td>属性条件独立性假设</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00077</td><td>Attribute Space</td><td>属性空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00078</td><td>Attribute Value</td><td>属性值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00079</td><td>Augmented Lagrangian</td><td>增广拉格朗日法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00080</td><td>Auto-Regressive Network</td><td>自回归网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00081</td><td>Autoencoder</td><td>自编码器</td><td>AE</td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-26-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00082</td><td>Automatic Differentiation</td><td>自动微分</td><td>AD</td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-07">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00083</td><td>Automatic Speech Recognition</td><td>自动语音识别</td><td>ASR</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00084</td><td>Automatic Summarization</td><td>自动摘要</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00085</td><td>Autoregressive Generative Model</td><td>自回归生成模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00086</td><td>Autoregressive Model</td><td>自回归模型</td><td>AR</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00087</td><td>Autoregressive Process</td><td>自回归过程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00088</td><td>Average Gradient</td><td>平均梯度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00089</td><td>Average Pooling Layer</td><td>平均汇聚层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00090</td><td>Average-Pooling</td><td>平均汇聚</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00091</td><td>Averaged Perceptron</td><td>平均感知器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00092</td><td>Back Propagation</td><td>反向传播</td><td>BP</td><td><a href="https://www.jiqizhixin.com/articles/2016-11-25-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00093</td><td>Back Propagation Algorithm</td><td>反向传播算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00094</td><td>Back Propagation Through Time</td><td>随时间反向传播</td><td>BPTT</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00095</td><td>Back-Off</td><td>回退</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00096</td><td>Backward</td><td>后向</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00097</td><td>Backward Induction</td><td>反向归纳</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00098</td><td>Backward Search</td><td>反向搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00099</td><td>Bag of Words</td><td>词袋</td><td>BOW</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00100</td><td>Bagging</td><td>袋装</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00101</td><td>Bandit</td><td>赌博机/老虎机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00102</td><td>Bandpass Filter</td><td>带通滤波器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00103</td><td>Base</td><td>基</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00104</td><td>Base Classifier</td><td>基分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00105</td><td>Base Learner</td><td>基学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00106</td><td>Base Learning Algorithm</td><td>基学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00107</td><td>Base Vector</td><td>基向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00108</td><td>Baseline</td><td>基准</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00109</td><td>Basin of Attraction</td><td>吸引域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00110</td><td>Batch</td><td>批量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00111</td><td>Batch Gradient Descent</td><td>批量梯度下降法</td><td>BGD</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00112</td><td>Batch Learning</td><td>批量学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00113</td><td>Batch Normalization</td><td>批量规范化</td><td>BN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00114</td><td>Batch Size</td><td>批量大小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00115</td><td>Baum-Welch Algorithm</td><td>Baum-Welch算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00116</td><td>Bayes Classifier</td><td>贝叶斯分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00117</td><td>Bayes Decision Rule</td><td>贝叶斯决策准则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00118</td><td>Bayes Error</td><td>贝叶斯误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00119</td><td>Bayes Model Averaging</td><td>贝叶斯模型平均</td><td>BMA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00120</td><td>Bayes Optimal Classifier</td><td>贝叶斯最优分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00121</td><td>Bayes Risk</td><td>贝叶斯风险</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00122</td><td>Bayes' Rule</td><td>贝叶斯规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00123</td><td>Bayes' Theorem</td><td>贝叶斯定理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00124</td><td>Bayesian Decision Theory</td><td>贝叶斯决策理论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00125</td><td>Bayesian Estimation</td><td>贝叶斯估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00126</td><td>Bayesian Inference</td><td>贝叶斯推断</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-00127</td><td>Bayesian Learning</td><td>贝叶斯学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00128</td><td>Bayesian Linear Regression</td><td>贝叶斯线性回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00129</td><td>Bayesian Network</td><td>贝叶斯网/贝叶斯网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>Network翻译为网或网络皆可，只要统一翻译成网或者统一翻译成网络即可；统计，机器学习</td></tr><tr class="odd"><td>AITD-00130</td><td>Bayesian Optimization</td><td>贝叶斯优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-28">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00131</td><td>Bayesian Probability</td><td>贝叶斯概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00132</td><td>Bayesian Statistics</td><td>贝叶斯统计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00133</td><td>Beam Search</td><td>束搜索</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-31-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00134</td><td>Benchmark</td><td>基准</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00135</td><td>Belief Network</td><td>信念网/信念网络</td><td>BN</td><td>[1]</td><td>Network翻译为网或网络皆可，只要统一翻译成网或者统一翻译成网络即可</td></tr><tr class="odd"><td>AITD-00136</td><td>Belief Propagation</td><td>信念传播</td><td>BP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00137</td><td>Bellman Equation</td><td>贝尔曼方程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00138</td><td>Bellman Optimality Equation</td><td>贝尔曼最优方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00139</td><td>Bernoulli Distribution</td><td>伯努利分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-00140</td><td>Bernoulli Output Distribution</td><td>伯努利输出分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00141</td><td>Best-Arm Problem</td><td>最优臂问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00142</td><td>Beta Distribution</td><td>贝塔分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00143</td><td>Between-Class Scatter Matrix</td><td>类间散度矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00144</td><td>BFGS</td><td>BFGS</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00145</td><td>Bi-Directional Long-Short Term Memory</td><td>双向长短期记忆</td><td>Bi-LSTM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00146</td><td>Bi-Partition</td><td>二分法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00147</td><td>Bias</td><td>偏差/偏置</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[4]</a></td><td>看上下语境；机器学习</td></tr><tr class="odd"><td>AITD-00148</td><td>Bias In Affine Function</td><td>偏置</td><td></td><td>[1]</td><td>看上下语境</td></tr><tr class="even"><td>AITD-00149</td><td>Bias In Statistics</td><td>偏差</td><td></td><td>[1]</td><td>看上下语境</td></tr><tr class="odd"><td>AITD-00150</td><td>Bias Shift</td><td>偏置偏移</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00151</td><td>Bias-Variance Decomposition</td><td>偏差 - 方差分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00152</td><td>Bias-Variance Dilemma</td><td>偏差 - 方差困境</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00153</td><td>Biased</td><td>有偏</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00154</td><td>Biased Importance Sampling</td><td>有偏重要采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00155</td><td>Bidirectional Language Model</td><td>双向语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00156</td><td>Bidirectional Recurrent Neural Network</td><td>双向循环神经网络</td><td>Bi-RNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00157</td><td>Bigram</td><td>二元语法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00158</td><td>Bilingual Evaluation Understudy</td><td>BLEU</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00159</td><td>Binary Classification</td><td>二分类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00160</td><td>Binary Relation</td><td>二元关系</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00161</td><td>Binary Sparse Coding</td><td>二值稀疏编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00162</td><td>Binomial Distribution</td><td>二项分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00163</td><td>Binomial Logistic Regression Model</td><td>二项对数几率回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00164</td><td>Binomial Test</td><td>二项检验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00165</td><td>Biological Plausibility</td><td>生物学合理性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00166</td><td>Bit</td><td>比特</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00167</td><td>Block</td><td>块</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00168</td><td>Block Coordinate Descent</td><td>块坐标下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00169</td><td>Block Gibbs Sampling</td><td>块吉布斯采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00170</td><td>Boilerplate Code</td><td>样板代码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00171</td><td>Boltzmann</td><td>玻尔兹曼</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00172</td><td>Boltzmann Distribution</td><td>玻尔兹曼分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00173</td><td>Boltzmann Factor</td><td>玻尔兹曼因子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00174</td><td>Boltzmann Machine</td><td>玻尔兹曼机</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-08-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00175</td><td>Boosting</td><td>Boosting（一种模型训练加速方式）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00176</td><td>Boosting Tree</td><td>提升树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00177</td><td>Bootstrap Aggregating</td><td>Bagging</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00178</td><td>Bootstrap Sampling</td><td>自助采样法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00179</td><td>Bootstrapping</td><td>自助法/自举法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00180</td><td>Bottleneck Layer</td><td>瓶颈层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00181</td><td>Bottom-Up</td><td>自下而上</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00182</td><td>Bounding Boxes</td><td>边界框</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00183</td><td>Break-Event Point</td><td>平衡点</td><td>BEP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00184</td><td>Bridge Sampling</td><td>桥式采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00185</td><td>Broadcasting</td><td>广播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00186</td><td>Broyden's Algorithm</td><td>Broyden类算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00187</td><td>Bucketing</td><td>分桶</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00188</td><td>Burn-In Period</td><td>预烧期</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00189</td><td>Burning-In</td><td>磨合</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00190</td><td>Calculus</td><td>微积分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00191</td><td>Calculus of Variations</td><td>变分法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00192</td><td>Calibration</td><td>校准</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00193</td><td>Canonical</td><td>正则的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00194</td><td>Canonical Correlation Analysis</td><td>典型相关分析</td><td>CCA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00195</td><td>Capacity</td><td>容量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00196</td><td>Cartesian Coordinate</td><td>笛卡尔坐标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00197</td><td>Cascade</td><td>级联</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00198</td><td>Cascade-Correlation</td><td>级联相关</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00199</td><td>Catastrophic Forgetting</td><td>灾难性遗忘</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00200</td><td>Categorical Attribute</td><td>分类属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00201</td><td>Categorical Distribution</td><td>类别分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00202</td><td>Causal Factor</td><td>因果因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00203</td><td>Causal Modeling</td><td>因果模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00204</td><td>Cell</td><td>单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00205</td><td>Centered Difference</td><td>中心差分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00206</td><td>Central Limit Theorem</td><td>中心极限定理</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00207</td><td>Chain Rule</td><td>链式法则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00208</td><td>Channel</td><td>通道</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00209</td><td>Chaos</td><td>混沌</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00210</td><td>Chebyshev Distance</td><td>切比雪夫距离</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00211</td><td>Chord</td><td>弦</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00212</td><td>Chordal Graph</td><td>弦图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00213</td><td>City Block Distance</td><td>街区距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00214</td><td>Class</td><td>类别</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00215</td><td>Class Label</td><td>类标记</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00216</td><td>Class-Conditional Probability</td><td>类条件概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00217</td><td>Class-Imbalance</td><td>类别不平衡</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00218</td><td>Classification</td><td>分类</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00219</td><td>Classification And Regression Tree</td><td>分类与回归树</td><td>CART</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00220</td><td>Classifier</td><td>分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00221</td><td>Clip Gradient</td><td>梯度截断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00222</td><td>Clipping The Gradient</td><td>截断梯度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00223</td><td>Clique</td><td>团</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00224</td><td>Clique Potential</td><td>团势能</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00225</td><td>Clockwork RNN</td><td>时钟循环神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00226</td><td>Closed Form Solution</td><td>闭式解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00227</td><td>Closed-Form</td><td>闭式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00228</td><td>Cluster</td><td>簇</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00229</td><td>Cluster Analysis</td><td>聚类分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00230</td><td>Cluster Assumption</td><td>聚类假设</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00231</td><td>Clustering</td><td>聚类</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-09">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00232</td><td>Clustering Ensemble</td><td>聚类集成</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00233</td><td>Co-Adapting</td><td>共适应</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00234</td><td>Co-Occurrence</td><td>共现</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00235</td><td>Co-Occurrence Frequency</td><td>共现词频</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00236</td><td>Co-Training</td><td>协同训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00237</td><td>Code</td><td>编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00238</td><td>Codebook Learning</td><td>码书学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00239</td><td>Coding Matrix</td><td>编码矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00240</td><td>Collaborative Filtering</td><td>协同过滤</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-23-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00241</td><td>Collapsed Gibbs Sampling</td><td>收缩的吉布斯抽样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00242</td><td>Collinearity</td><td>共线性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00243</td><td>COLT</td><td>国际学习理论会议</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00244</td><td>Column</td><td>列</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00245</td><td>Column Space</td><td>列空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00246</td><td>Combinatorial Optimization</td><td>组合优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00247</td><td>Committee-Based Learning</td><td>基于委员会的学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00248</td><td>Common Cause</td><td>共因</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00249</td><td>Common Parent</td><td>同父</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00250</td><td>Compact Singular Value Decomposition</td><td>紧奇异值分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00251</td><td>Competitive Learning</td><td>竞争型学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00252</td><td>Complementary Slackness</td><td>互补松弛</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00253</td><td>Complete Graph</td><td>完全图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00254</td><td>Complete Linkage</td><td>完全连接</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00255</td><td>Complete-Data</td><td>完全数据</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00256</td><td>Complex Cell</td><td>复杂细胞</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00257</td><td>Component Learner</td><td>组件学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00258</td><td>Compositionality</td><td>组合性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00259</td><td>Comprehensibility</td><td>可解释性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00260</td><td>Computation Cost</td><td>计算代价</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00261</td><td>Computation Graph</td><td>计算图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00262</td><td>Computational Learning Theory</td><td>计算学习理论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00263</td><td>Computational Linguistics</td><td>计算语言学</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00264</td><td>Computer Vision</td><td>计算机视觉</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00265</td><td>Concatenate</td><td>连结</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00266</td><td>Concept Class</td><td>概念类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00267</td><td>Concept Drift</td><td>概念漂移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00268</td><td>Concept Learning System</td><td>概念学习系统</td><td>CLS</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00269</td><td>Concept Shift</td><td>概念偏移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00270</td><td>Conditional Computation</td><td>条件计算</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00271</td><td>Conditional Entropy</td><td>条件熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00272</td><td>Conditional Independence</td><td>条件独立</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00273</td><td>Conditional Language Model</td><td>条件语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00274</td><td>Conditional Mutual Information</td><td>条件互信息</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00275</td><td>Conditional Probability</td><td>条件概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00276</td><td>Conditional Probability Density Function</td><td>条件概率密度函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00277</td><td>Conditional Probability Distribution</td><td>条件概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00278</td><td>Conditional Probability Table</td><td>条件概率表</td><td>CPT</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00279</td><td>Conditional Random Field</td><td>条件随机场</td><td>CRF</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00280</td><td>Conditional Risk</td><td>条件风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00281</td><td>Conditionally Independent</td><td>条件独立的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00282</td><td>Conference On Neural Information Processing Systems</td><td>国际神经信息处理系统会议</td><td>NeurIPS</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-18-9">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00283</td><td>Confidence</td><td>置信度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00284</td><td>Conflict Resolution</td><td>冲突消解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00285</td><td>Confusion Matrix</td><td>混淆矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00286</td><td>Conjugate</td><td>共轭</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00287</td><td>Conjugate Directions</td><td>共轭方向</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00288</td><td>Conjugate Distribution</td><td>共轭分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00289</td><td>Conjugate Gradient</td><td>共轭梯度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>优化，数学</td></tr><tr class="odd"><td>AITD-00290</td><td>Conjugate Prior</td><td>共轭先验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00291</td><td>Connection Weight</td><td>连接权</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00292</td><td>Connectionism</td><td>连接主义</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00293</td><td>Consistency</td><td>一致性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00294</td><td>Consistency Convergence</td><td>一致性收敛</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00295</td><td>Constrained Optimization</td><td>约束优化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00296</td><td>Content-Addressable Memory</td><td>基于内容寻址的存储</td><td>CAM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00297</td><td>Context Variable</td><td>上下文变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00298</td><td>Context Vector</td><td>上下文向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00299</td><td>Context Window</td><td>上下文窗口</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00300</td><td>Context Word</td><td>上下文词</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00301</td><td>Context-Specific Independences</td><td>特定上下文独立</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00302</td><td>Contextual Bandit</td><td>上下文赌博机/上下文老虎机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00303</td><td>Contextualized Representation</td><td>基于上下文的表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00304</td><td>Contingency Table</td><td>列联表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00305</td><td>Continous Bag-Of-Words Model</td><td>连续词袋模型</td><td>CBOW</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00306</td><td>Continuation Method</td><td>延拓法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00307</td><td>Continuing Task</td><td>持续式任务</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00308</td><td>Continuous Attribute</td><td>连续属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00309</td><td>Continuous Learning</td><td>持续学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00310</td><td>Continuous Optimization</td><td>连续优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00311</td><td>Contractive</td><td>收缩</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00312</td><td>Contractive Autoencoder</td><td>收缩自编码器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00313</td><td>Contractive Neural Network</td><td>收缩神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00314</td><td>Contrastive Divergence</td><td>对比散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00315</td><td>Controller</td><td>控制器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00316</td><td>Convergence</td><td>收敛</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00317</td><td>Conversational Agent</td><td>会话智能体</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00318</td><td>Convex Optimization</td><td>凸优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-29-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00319</td><td>Convex Quadratic Programming</td><td>凸二次规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00320</td><td>Convex Set</td><td>凸集</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00321</td><td>Convexity</td><td>凸性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00322</td><td>Convolution</td><td>卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00323</td><td>Convolutional Boltzmann Machine</td><td>卷积玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00324</td><td>Convolutional Deep Belief Network</td><td>卷积深度信念网络</td><td>CDBN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00325</td><td>Convolutional Kernel</td><td>卷积核</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00326</td><td>Convolutional Network</td><td>卷积网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00327</td><td>Convolutional Neural Network</td><td>卷积神经网络</td><td>CNN</td><td><a href="https://www.jiqizhixin.com/articles/2017-12-19-8">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-08-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-18-2">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-00328</td><td>Coordinate</td><td>坐标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00329</td><td>Coordinate Ascent</td><td>坐标上升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00330</td><td>Coordinate Descent</td><td>坐标下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00331</td><td>Coparent</td><td>共父</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00332</td><td>Corpus</td><td>语料库</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00333</td><td>Correlation</td><td>相关系数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00334</td><td>Correlation Coefficient</td><td>相关系数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00335</td><td>Cosine</td><td>余弦</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00336</td><td>Cosine Decay</td><td>余弦衰减</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00337</td><td>Cosine Similarity</td><td>余弦相似度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00338</td><td>Cost</td><td>代价</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00339</td><td>Cost Curve</td><td>代价曲线</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00340</td><td>Cost Function</td><td>代价函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00341</td><td>Cost Matrix</td><td>代价矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00342</td><td>Cost-Sensitive</td><td>代价敏感</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00343</td><td>Covariance</td><td>协方差</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00344</td><td>Covariance Matrix</td><td>协方差矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00345</td><td>Covariance RBM</td><td>协方差RBM</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00346</td><td>Covariate Shift</td><td>协变量偏移</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00347</td><td>Coverage</td><td>覆盖</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00348</td><td>Credit Assignment Problem</td><td>贡献度分配问题</td><td>CAP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00349</td><td>Criterion</td><td>准则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00350</td><td>Critic</td><td>评论员</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00351</td><td>Critic Network</td><td>评价网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00352</td><td>Critical Point</td><td>临界点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00353</td><td>Critical Temperatures</td><td>临界温度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00354</td><td>Cross Correlation</td><td>互相关</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00355</td><td>Cross Entropy</td><td>交叉熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00356</td><td>Cross Validation</td><td>交叉验证</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-16-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00357</td><td>Cross-Entropy Loss Function</td><td>交叉熵损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00358</td><td>Crowdsourcing</td><td>众包</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-28-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00359</td><td>Cumulative Distribution Function</td><td>累积分布函数</td><td>CDF</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00360</td><td>Cumulative Function</td><td>累积函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00361</td><td>Curriculum Learning</td><td>课程学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00362</td><td>Curse of Dimensionality</td><td>维数灾难</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00363</td><td>Curvature</td><td>曲率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00364</td><td>Curve-Fitting</td><td>曲线拟合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00365</td><td>Cut Point</td><td>截断点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00366</td><td>Cutting Plane Algorithm</td><td>割平面法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00367</td><td>Cybernetics</td><td>控制论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00368</td><td>Cyclic Learning Rate</td><td>循环学习率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00369</td><td>Damping</td><td>衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00370</td><td>Damping Factor</td><td>阻尼因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00371</td><td>Data</td><td>数据</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00372</td><td>Data Augmentation</td><td>数据增强</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00373</td><td>Data Generating Distribution</td><td>数据生成分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00374</td><td>Data Generating Process</td><td>数据生成过程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00375</td><td>Data Instance</td><td>数据样本</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00376</td><td>Data Mining</td><td>数据挖掘</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00377</td><td>Data Parallelism</td><td>数据并行</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00378</td><td>Data Point</td><td>数据点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00379</td><td>Data Preprocessing</td><td>数据预处理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00380</td><td>Data Set</td><td>数据集</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-04-6">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00381</td><td>Data Wrangling</td><td>数据整理</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-25-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00382</td><td>Dataset Augmentation</td><td>数据集增强</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00383</td><td>Davidon-Fletcher-Powell</td><td>DFP</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00384</td><td>Debugging Strategy</td><td>调试策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00385</td><td>Decision Boundary</td><td>决策边界</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00386</td><td>Decision Function</td><td>决策函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00387</td><td>Decision Stump</td><td>决策树桩</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00388</td><td>Decision Surface</td><td>决策平面</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00389</td><td>Decision Tree</td><td>决策树</td><td>DT</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-10">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-29-5">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[4]</a></td><td></td></tr><tr class="odd"><td>AITD-00390</td><td>Decoder</td><td>解码器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00391</td><td>Decoding</td><td>解码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00392</td><td>Decompose</td><td>分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00393</td><td>Deconvolution</td><td>反卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00394</td><td>Deconvolutional Network</td><td>反卷积网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-14">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00395</td><td>Deduction</td><td>演绎</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00396</td><td>Deep Belief Network</td><td>深度信念网络</td><td>DBN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00397</td><td>Deep Boltzmann Machine</td><td>深度玻尔兹曼机</td><td>DBM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00398</td><td>Deep Circuit</td><td>深度回路</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00399</td><td>Deep Convolutional Generative Adversarial Network</td><td>深度卷积生成对抗网络</td><td>DCGAN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00400</td><td>Deep Feedforward Network</td><td>深度前馈网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00401</td><td>Deep Generative Model</td><td>深度生成模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00402</td><td>Deep Learning</td><td>深度学习</td><td>DL</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-17-2">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-15-4">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-15-2">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[4]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[5]</a></td><td></td></tr><tr class="even"><td>AITD-00403</td><td>Deep Model</td><td>深度模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00404</td><td>Deep Network</td><td>深度网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00405</td><td>Deep Neural Network</td><td>深度神经网络</td><td>DNN</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-15-2">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-10">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-07-2">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[5]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[6]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[7]</a></td><td></td></tr><tr class="odd"><td>AITD-00406</td><td>Deep Q-Learning</td><td>深度 Q 学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-10-10-2">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-08-22-8">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00407</td><td>Deep Q-Network</td><td>深度Q网络</td><td>DQN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00408</td><td>Deep Reinforcement Learning</td><td>深度强化学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00409</td><td>Deep Sequence Model</td><td>深度序列模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00410</td><td>Default Rule</td><td>默认规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00411</td><td>Definite Integral</td><td>定积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00412</td><td>Degree Of Belief</td><td>信任度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00413</td><td>Delta-Bar-Delta</td><td>Delta-Bar-Delta</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00414</td><td>Denoising</td><td>去噪</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00415</td><td>Denoising Autoencoder</td><td>去噪自编码器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00416</td><td>Denoising Score Matching</td><td>去躁分数匹配</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00417</td><td>Denominator Layout</td><td>分母布局</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00418</td><td>Dense</td><td>稠密</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00419</td><td>Density Estimation</td><td>密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00420</td><td>Density-Based Clustering</td><td>密度聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00421</td><td>Dependency</td><td>依赖</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00422</td><td>Depth</td><td>深度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00423</td><td>Derivative</td><td>导数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00424</td><td>Description</td><td>描述</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00425</td><td>Design Matrix</td><td>设计矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00426</td><td>Detailed Balance</td><td>细致平衡</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00427</td><td>Detailed Balance Equation</td><td>细致平衡方程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00428</td><td>Detector Stage</td><td>探测级</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00429</td><td>Determinant</td><td>行列式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00430</td><td>Deterministic</td><td>确定性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00431</td><td>Deterministic Model</td><td>确定性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00432</td><td>Deterministic Policy</td><td>确定性策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00433</td><td>Development Set</td><td>开发集</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00434</td><td>Diagonal Matrix</td><td>对角矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00435</td><td>Diameter</td><td>直径</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00436</td><td>Dictionary</td><td>字典</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00437</td><td>Dictionary Learning</td><td>字典学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00438</td><td>Differentiable Function</td><td>可微函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00439</td><td>Differentiable Neural Computer</td><td>可微分神经计算机</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-11-7">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00440</td><td>Differential Entropy</td><td>微分熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00441</td><td>Differential Equation</td><td>微分方程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00442</td><td>Differentiation</td><td>微分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00443</td><td>Dilated Convolution</td><td>膨胀卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00444</td><td>Dimension</td><td>维度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00445</td><td>Dimension Reduction</td><td>降维</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00446</td><td>Dimensionality Reduction Algorithm</td><td>降维算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-08-31-2">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00447</td><td>Dirac Delta Function</td><td>Dirac Delta函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00448</td><td>Dirac Distribution</td><td>Dirac分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00449</td><td>Directed</td><td>有向</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00450</td><td>Directed Acyclic Graph</td><td>有向非循环图</td><td>DAG</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00451</td><td>Directed Edge</td><td>有向边</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00452</td><td>Directed Graph</td><td>有向图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00453</td><td>Directed Graphical Model</td><td>有向图模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00454</td><td>Directed Model</td><td>有向模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00455</td><td>Directed Separation</td><td>有向分离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00456</td><td>Directional Derivative</td><td>方向导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00457</td><td>Dirichlet Distribution</td><td>狄利克雷分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00458</td><td>Disagreement Measure</td><td>不合度量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00459</td><td>Disagreement-Based Methods</td><td>基于分歧的方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00460</td><td>Discount Factor</td><td>衰减系数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00461</td><td>Discounted Return</td><td>折扣回报</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00462</td><td>Discrete Optimization</td><td>离散优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00463</td><td>Discriminant Function</td><td>判别函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00464</td><td>Discriminative Approach</td><td>判别方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00465</td><td>Discriminative Model</td><td>判别式模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00466</td><td>Discriminative RBM</td><td>判别RBM</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00467</td><td>Discriminator</td><td>判别器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00468</td><td>Discriminator Network</td><td>判别网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00469</td><td>Distance</td><td>距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00470</td><td>Distance Measure</td><td>距离度量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00471</td><td>Distance Metric Learning</td><td>距离度量学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00472</td><td>Distributed Representation</td><td>分布式表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00473</td><td>Distribution</td><td>分布</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-09">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00474</td><td>Diverge</td><td>发散</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00475</td><td>Divergence</td><td>散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00476</td><td>Diversity</td><td>多样性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00477</td><td>Diversity Measure</td><td>多样性度量/差异性度量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00478</td><td>Divide-And-Conquer</td><td>分而治之</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00479</td><td>Divisive</td><td>分裂</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00480</td><td>Domain</td><td>领域</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00481</td><td>Domain Adaptation</td><td>领域自适应</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00482</td><td>Dominant Eigenvalue</td><td>主特征值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00483</td><td>Dominant Eigenvector</td><td>主特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00485</td><td>Dominant Strategy</td><td>占优策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00486</td><td>Dot Product</td><td>点积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00487</td><td>Double Backprop</td><td>双反向传播</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00488</td><td>Doubly Block Circulant Matrix</td><td>双重分块循环矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00489</td><td>Down Sampling</td><td>下采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00490</td><td>Downstream Task</td><td>下游任务</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00491</td><td>Dropout</td><td>暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00492</td><td>Dropout Boosting</td><td>暂退Boosting</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00493</td><td>Dropout Mask</td><td>暂退掩码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00494</td><td>Dropout Method</td><td>暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00495</td><td>Dual Algorithm</td><td>对偶算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00496</td><td>Dual Problem</td><td>对偶问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00497</td><td>Dummy Node</td><td>哑结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00498</td><td>Dying ReLU Problem</td><td>死亡ReLU问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00499</td><td>Dynamic Bayesian Network</td><td>动态贝叶斯网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00500</td><td>Dynamic Computational Graph</td><td>动态计算图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00501</td><td>Dynamic Fusion</td><td>动态融合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00502</td><td>Dynamic Programming</td><td>动态规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00503</td><td>Dynamic Structure</td><td>动态结构</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00504</td><td>Dynamical System</td><td>动力系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00505</td><td>Eager Learning</td><td>急切学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00506</td><td>Early Stopping</td><td>早停</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00507</td><td>Earth-Mover's Distance</td><td>推土机距离</td><td>EMD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00508</td><td>Echo State Network</td><td>回声状态网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00509</td><td>Edge</td><td>边</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00510</td><td>Edge Device</td><td>边缘设备</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-24-8">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00511</td><td>Effective Capacity</td><td>有效容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00512</td><td>Eigendecomposition</td><td>特征分解</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-07-05-2">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00513</td><td>Eigenvalue</td><td>特征值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00514</td><td>Eigenvalue Decomposition</td><td>特征值分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00515</td><td>Elastic Net Regularization</td><td>弹性网络正则化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00516</td><td>Elastic Weight Consolidation</td><td>弹性权重巩固</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00517</td><td>Element-Wise Product</td><td>逐元素积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00518</td><td>Elementary Basis Vectors</td><td>基本单位向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00519</td><td>Ellipsoid Method</td><td>椭球法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00520</td><td>Embedding</td><td>嵌入</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-02-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00521</td><td>Embedding Lookup Table</td><td>嵌入表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00522</td><td>Emotional Analysis</td><td>情绪分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00523</td><td>Empirical Conditional Entropy</td><td>经验条件熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00524</td><td>Empirical Distribution</td><td>经验分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00525</td><td>Empirical Entropy</td><td>经验熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00526</td><td>Empirical Error</td><td>经验误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00527</td><td>Empirical Frequency</td><td>经验频率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00528</td><td>Empirical Loss</td><td>经验损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00529</td><td>Empirical Risk</td><td>经验风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00530</td><td>Empirical Risk Minimization</td><td>经验风险最小化</td><td>ERM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00531</td><td>Encoder</td><td>编码器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00532</td><td>Encoder-Decoder</td><td>编码器-解码器（模型）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-00533</td><td>Encoding</td><td>编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00534</td><td>End-To-End</td><td>端到端</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-15">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00535</td><td>End-To-End Learning</td><td>端到端学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00536</td><td>End-To-End Memory Network</td><td>端到端记忆网络</td><td>Memn2N</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00537</td><td>Energy Function</td><td>能量函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00538</td><td>Energy Gap</td><td>能量差异</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00539</td><td>Energy-Based Model</td><td>基于能量的模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00540</td><td>Ensemble</td><td>集成</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00541</td><td>Ensemble Learning</td><td>集成学习</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-14-8">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00542</td><td>Ensemble Pruning</td><td>集成修剪</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00543</td><td>Entropy</td><td>熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00544</td><td>Entropy Encoding</td><td>熵编码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00545</td><td>Environment</td><td>环境</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00546</td><td>Episode</td><td>回合</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00547</td><td>Episodic Task</td><td>回合式任务</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00548</td><td>Epoch</td><td>轮</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00549</td><td>Equal-Width Convolution</td><td>等宽卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00550</td><td>Equality Constraint</td><td>等式约束</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00551</td><td>Equilibrium Distribution</td><td>均衡分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00552</td><td>Equivariance</td><td>等变</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00553</td><td>Equivariant Representations</td><td>等变表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00554</td><td>Error</td><td>误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00555</td><td>Error Backpropagation Algorithm</td><td>误差反向传播算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00556</td><td>Error Backpropagation</td><td>误差反向传播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00557</td><td>Error Bar</td><td>误差条</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00558</td><td>Error Correcting Output Codes</td><td>纠错输出编码</td><td>ECOC</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00559</td><td>Error Function</td><td>误差函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00560</td><td>Error Metric</td><td>误差度量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00561</td><td>Error Rate</td><td>错误率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00562</td><td>Error-Ambiguity Decomposition</td><td>误差－分歧分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00563</td><td>Estimation Error</td><td>估计误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00564</td><td>Estimation Of Mathematical Expectation</td><td>数学期望估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00565</td><td>Estimator</td><td>估计/估计量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00566</td><td>Euclidean Distance</td><td>欧氏距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00567</td><td>Euclidean Norm</td><td>欧几里得范数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00568</td><td>Euclidean Space</td><td>欧氏空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00569</td><td>Euler-Lagrange Equation</td><td>欧拉-拉格朗日方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00570</td><td>Evaluation Criterion</td><td>评价准则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00571</td><td>Evidence</td><td>证据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00572</td><td>Evidence Lower Bound</td><td>证据下界</td><td>ELBO</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00573</td><td>Evolution</td><td>演化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00574</td><td>Evolutionary Computation</td><td>演化计算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00575</td><td>Exact</td><td>确切的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00576</td><td>Exact Inference</td><td>精确推断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00577</td><td>Example</td><td>样例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00578</td><td>Excess Error</td><td>额外误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00579</td><td>Exchangeable</td><td>可交换的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00580</td><td>Expectation</td><td>期望</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00581</td><td>Expectation Maximization Algorithm</td><td>期望极大算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00582</td><td>Expectation Maximization</td><td>期望最大化</td><td>EM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00583</td><td>Expectation Step</td><td>E步</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00584</td><td>Expected Error</td><td>期望错误</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00585</td><td>Expected Loss</td><td>期望损失</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00586</td><td>Expected Return</td><td>期望回报</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00587</td><td>Expected Risk</td><td>期望风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00588</td><td>Expected Value</td><td>期望值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00589</td><td>Experience</td><td>经验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00590</td><td>Experience Replay</td><td>经验回放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00591</td><td>Expert Network</td><td>专家网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00592</td><td>Expert System</td><td>专家系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00593</td><td>Explaining Away</td><td>相消解释</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00594</td><td>Explaining Away Effect</td><td>相消解释作用</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00595</td><td>Explanatory Factort</td><td>解释因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00596</td><td>Explicit Density Model</td><td>显式密度模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00597</td><td>Exploding Gradient</td><td>梯度爆炸</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00598</td><td>Exploitation</td><td>利用</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00599</td><td>Exploration</td><td>探索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00600</td><td>Exploration-Exploitation Dilemma</td><td>探索-利用窘境</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00601</td><td>Exponential Decay</td><td>指数衰减</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00602</td><td>Exponential Distribution</td><td>指数分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00603</td><td>Exponential Linear Unit</td><td>指数线性单元</td><td>ELU</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00604</td><td>Exponential Loss</td><td>指数损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00605</td><td>Exponential Loss Function</td><td>指数损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00606</td><td>Exponentially Weighted Moving Average</td><td>指数加权移动平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00607</td><td>Exposure Bias</td><td>曝光偏差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00608</td><td>External Memory</td><td>外部记忆</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00609</td><td>Extreme Learning Machine</td><td>超限学习机</td><td>ELM</td><td><ahref="https://www.jiqizhixin.com/articles/2016-09-30-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00610</td><td>F Measure</td><td>F值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00611</td><td>F-Score</td><td>F分数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00612</td><td>Factor</td><td>因子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00613</td><td>Factor Analysis</td><td>因子分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00614</td><td>Factor Graph</td><td>因子图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00615</td><td>Factor Loading</td><td>因子负荷量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00616</td><td>Factorization</td><td>因子分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00617</td><td>Factorized</td><td>分解的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00618</td><td>Factors of Variation</td><td>变差因素</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00619</td><td>False Negative</td><td>假负例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00620</td><td>False Positive</td><td>假正例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00621</td><td>False Positive Rate</td><td>假正例率</td><td>FPR</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00622</td><td>Fast Dropout</td><td>快速暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00623</td><td>Fast Persistent Contrastive Divergence</td><td>快速持续性对比散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00624</td><td>Fault-Tolerant Asynchronous Training</td><td>容错异步训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00625</td><td>Feasible</td><td>可行</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00626</td><td>Feature</td><td>特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00627</td><td>Feature Engineering</td><td>特征工程</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00628</td><td>Feature Extraction</td><td>特征抽取</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00629</td><td>Feature Extractor</td><td>特征提取器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00630</td><td>Feature Function</td><td>特征函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00631</td><td>Feature Map</td><td>特征图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00632</td><td>Feature Scaling Transform</td><td>特征尺度变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00633</td><td>Feature Selection</td><td>特征选择</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00634</td><td>Feature Space</td><td>特征空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00635</td><td>Feature Vector</td><td>特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00636</td><td>Featured Learning</td><td>特征学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00637</td><td>Feedback</td><td>反馈</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00638</td><td>Feedforward</td><td>前馈</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00639</td><td>Feedforward Classifier</td><td>前馈分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00640</td><td>Feedforward Network</td><td>前馈网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00641</td><td>Feedforward Neural Network</td><td>前馈神经网络</td><td>FNN</td><td><a href="https://www.jiqizhixin.com/articles/2017-09-07-9">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00642</td><td>Few-Shot Learning</td><td>少试学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00643</td><td>Fidelity</td><td>逼真度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00644</td><td>Field Programmable Gated Array</td><td>现场可编程门阵列</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00645</td><td>Filter</td><td>滤波器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00646</td><td>Filter Method</td><td>过滤式方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00647</td><td>Fine-Tuning</td><td>微调</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00648</td><td>Finite Difference</td><td>有限差分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00649</td><td>First Layer</td><td>第一层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00650</td><td>First-Order Method</td><td>一阶方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00651</td><td>First-Order Rule</td><td>一阶规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00652</td><td>Fisher Information Matrix</td><td>Fisher信息矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00653</td><td>Fixed Point Equation</td><td>不动点方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00654</td><td>Fixed-Point Arithmetic</td><td>不动点运算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00655</td><td>Flat Minima</td><td>平坦最小值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00656</td><td>Flip</td><td>翻转</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00657</td><td>Flipping Output</td><td>翻转法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00658</td><td>Float-Point Arithmetic</td><td>浮点运算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00659</td><td>Fluctuation</td><td>振荡</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00660</td><td>Focus Attention</td><td>聚焦式注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00661</td><td>Folk Theorem</td><td>无名氏定理</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00662</td><td>Forget Gate</td><td>遗忘门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00663</td><td>Forward</td><td>前向</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00664</td><td>Forward KL Divergence</td><td>前向KL散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00665</td><td>Forward Mode Accumulation</td><td>前向模式累加</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00666</td><td>Forward Propagation</td><td>前向传播/正向传播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00667</td><td>Forward Search</td><td>前向搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00668</td><td>Forward Stagewise Algorithm</td><td>前向分步算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00669</td><td>Forward-Backward Algorithm</td><td>前向-后向算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00670</td><td>Fourier Transform</td><td>傅立叶变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00671</td><td>Fovea</td><td>中央凹</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00672</td><td>Fractionally Strided Convolution</td><td>微步卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00673</td><td>Free Energy</td><td>自由能</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00674</td><td>Frequentist</td><td>频率主义学派</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00675</td><td>Frequentist Probability</td><td>频率派概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00676</td><td>Frequentist Statistics</td><td>频率派统计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00677</td><td>Frobenius Norm</td><td>Frobenius 范数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00678</td><td>Full</td><td>全</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00679</td><td>Full Conditional Distribution</td><td>满条件分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00680</td><td>Full Conditional Probability</td><td>全条件概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00681</td><td>Full Padding</td><td>全填充</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00682</td><td>Full Singular Value Decomposition</td><td>完全奇异值分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00683</td><td>Full-Rank Matrix</td><td>满秩矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00684</td><td>Fully Connected Layer</td><td>全连接层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00685</td><td>Fully Connected Neural Network</td><td>全连接神经网络</td><td>FCNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00686</td><td>Fully Convolutional Network</td><td>全卷积网络</td><td>FCN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00687</td><td>Function</td><td>函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00688</td><td>Functional</td><td>泛函</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00689</td><td>Functional Derivative</td><td>泛函导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00690</td><td>Functional Margin</td><td>函数间隔</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00691</td><td>Functional Neuron</td><td>功能神经元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00692</td><td>Gabor Function</td><td>Gabor函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00693</td><td>Gain Ratio</td><td>増益率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00694</td><td>Game Payoff</td><td>博弈效用</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00695</td><td>Game Theory</td><td>博弈论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00696</td><td>Gamma Distribution</td><td>Gamma分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00697</td><td>Gate</td><td>门</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00698</td><td>Gate Controlled RNN</td><td>门控循环神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00699</td><td>Gated</td><td>门控</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00700</td><td>Gated Control</td><td>门控</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00701</td><td>Gated Recurrent Net</td><td>门控循环网络</td><td>GRN</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-24">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00702</td><td>Gated Recurrent Unit</td><td>门控循环单元</td><td>GRU</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00703</td><td>Gated RNN</td><td>门控RNN</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00704</td><td>Gater</td><td>选通器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00705</td><td>Gating Mechanism</td><td>门控机制</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00706</td><td>Gaussian Distribution</td><td>高斯分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00707</td><td>Gaussian Error Linear Unit</td><td>高斯误差线性单元</td><td>GELU</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00708</td><td>Gaussian Kernel</td><td>高斯核</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00709</td><td>Gaussian Kernel Function</td><td>高斯核函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00710</td><td>Gaussian Mixture Model</td><td>高斯混合模型</td><td>GMM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00711</td><td>Gaussian Mixtures</td><td>高斯混合（模型）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00712</td><td>Gaussian Output Distribution</td><td>高斯输出分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00713</td><td>Gaussian Process</td><td>高斯过程</td><td>GP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00714</td><td>Gaussian Process Regression</td><td>高斯过程回归</td><td>GPR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00715</td><td>Gaussian RBM</td><td>高斯RBM</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00716</td><td>Gaussian-Bernoulli RBM</td><td>高斯-伯努利RBM</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00717</td><td>General Problem Solving</td><td>通用问题求解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00718</td><td>General Purpose GPU</td><td>通用GPU</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00719</td><td>Generalization Ability</td><td>泛化能力</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00720</td><td>Generalization Error</td><td>泛化误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00721</td><td>Generalization Error Bound</td><td>泛化误差上界</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00722</td><td>Generalize</td><td>泛化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-25-10">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00723</td><td>Generalized Bregman Divergence</td><td>一般化 Bregman 散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00724</td><td>Generalized Expectation Maximization</td><td>广义期望极大</td><td>GEM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00725</td><td>Generalized Function</td><td>广义函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00726</td><td>Generalized Lagrange Function</td><td>广义拉格朗日函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00727</td><td>Generalized Lagrangian</td><td>广义拉格朗日</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00728</td><td>Generalized Linear Model</td><td>广义线性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00729</td><td>Generalized Pseudolikelihood</td><td>广义伪似然</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00730</td><td>Generalized Pseudolikelihood Estimator</td><td>广义伪似然估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00731</td><td>Generalized Rayleigh Quotient</td><td>广义瑞利商</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00732</td><td>Generalized Score Matching</td><td>广义得分匹配</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00733</td><td>Generative Adversarial Framework</td><td>生成式对抗框架</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00734</td><td>Generative Adversarial Network</td><td>生成对抗网络</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-26-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-08-5">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-13-2">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-00735</td><td>Generative Approach</td><td>生成方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00736</td><td>Generative Model</td><td>生成式模型</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-19-7">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-11-6">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-04-5">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-00737</td><td>Generative Modeling</td><td>生成式建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00738</td><td>Generative Moment Matching Network</td><td>生成矩匹配网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00739</td><td>Generative Pre-Training</td><td>生成式预训练</td><td>GPT</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00740</td><td>Generative Stochastic Network</td><td>生成随机网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00741</td><td>Generative Weight</td><td>生成权重</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00742</td><td>Generator</td><td>生成器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00743</td><td>Generator Network</td><td>生成器网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00744</td><td>Genetic Algorithm</td><td>遗传算法</td><td>GA</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-17-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-22">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-12-2">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[5]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[6]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-00745</td><td>Geometric Margin</td><td>几何间隔</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00746</td><td>Giant Magnetoresistance</td><td>巨磁阻</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00747</td><td>Gibbs Distribution</td><td>吉布斯分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00748</td><td>Gibbs Sampling</td><td>吉布斯采样/吉布斯抽样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00749</td><td>Gibbs Steps</td><td>吉布斯步数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00750</td><td>Gini Index</td><td>基尼指数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00751</td><td>Global Contrast Normalization</td><td>全局对比度规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00752</td><td>Global Markov Property</td><td>全局马尔可夫性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00753</td><td>Global Minima</td><td>全局极小值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00754</td><td>Global Minimizer</td><td>全局极小解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00755</td><td>Global Minimum</td><td>全局最小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00756</td><td>Global Optimization</td><td>全局优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-03-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00757</td><td>Gradient</td><td>梯度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00758</td><td>Gradient Ascent</td><td>梯度上升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00759</td><td>Gradient Ascent Method</td><td>梯度上升法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00760</td><td>Gradient Boosting</td><td>梯度提升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00761</td><td>Gradient Boosting Tree</td><td>梯度提升树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00762</td><td>Gradient Clipping</td><td>梯度截断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00763</td><td>Gradient Descent</td><td>梯度下降</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00764</td><td>Gradient Descent In One-Dimensional Space</td><td>一维梯度下降</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00765</td><td>Gradient Descent Method</td><td>梯度下降法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00766</td><td>Gradient Energy Distribution</td><td>梯度能量分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00767</td><td>Gradient Estimation</td><td>梯度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00768</td><td>Gradient Exploding Problem</td><td>梯度爆炸问题</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-21-14">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00769</td><td>Gradient Field</td><td>梯度场</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00770</td><td>Gradual Warmup</td><td>逐渐预热</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00771</td><td>Gram Matrix</td><td>Gram 矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00772</td><td>Graph</td><td>图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00773</td><td>Graph Analytics</td><td>图分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00774</td><td>Graph Attention Network</td><td>图注意力网络</td><td>GAT</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00775</td><td>Graph Convolutional Network</td><td>图卷积神经网络/图卷积网络</td><td>GCN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00776</td><td>Graph Neural Network</td><td>图神经网络</td><td>GNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00777</td><td>Graph Theory</td><td>图论</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-04-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00778</td><td>Graphical Model</td><td>图模型</td><td>GM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00779</td><td>Graphics Processing Unit</td><td>图形处理器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00780</td><td>Greedy</td><td>贪心</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00781</td><td>Greedy Algorithm</td><td>贪心算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00782</td><td>Greedy Layer-Wise Pretraining</td><td>贪心逐层预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00783</td><td>Greedy Layer-Wise Training</td><td>贪心逐层训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00784</td><td>Greedy Layer-Wise Unsupervised Pretraining</td><td>贪心逐层无监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00785</td><td>Greedy Search</td><td>贪心搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00786</td><td>Greedy Supervised Pretraining</td><td>贪心监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00787</td><td>Greedy Unsupervised Pretraining</td><td>贪心无监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00788</td><td>Grid Search</td><td>网格搜索</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00789</td><td>Grid World</td><td>网格世界</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00790</td><td>Ground Truth</td><td>真实值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00791</td><td>Growth Function</td><td>增长函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00792</td><td>Hadamard Product</td><td>Hadamard积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00793</td><td>Hamming Distance</td><td>汉明距离</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00794</td><td>Hard Attention</td><td>硬性注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00795</td><td>Hard Clustering</td><td>硬聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00796</td><td>Hard Margin</td><td>硬间隔</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00797</td><td>Hard Margin Maximization</td><td>硬间隔最大化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00798</td><td>Hard Mixture Of Experts</td><td>硬专家混合体</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00799</td><td>Hard Tanh</td><td>硬双曲正切函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00800</td><td>Hard Target</td><td>硬目标</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00801</td><td>Hard Voting</td><td>硬投票</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00802</td><td>Harmonic Mean</td><td>调和平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00803</td><td>Harmonium</td><td>簧风琴</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00804</td><td>Harmony</td><td>Harmony</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00805</td><td>Harris Chain</td><td>哈里斯链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00806</td><td>Hausdorff Distance</td><td>豪斯多夫距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00807</td><td>Hebbian Rule</td><td>赫布法则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00808</td><td>Hebbian Theory</td><td>赫布理论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00809</td><td>Helmholtz Machine</td><td>Helmholtz机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00810</td><td>Hesse Matrix</td><td>海赛矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00811</td><td>Hessian</td><td>Hessian</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00812</td><td>Hessian Matrix</td><td>黑塞矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00813</td><td>Heterogeneous Information Network</td><td>异质信息网络</td><td>HIN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00814</td><td>Heteroscedastic</td><td>异方差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00815</td><td>Hidden Dynamic Model</td><td>隐动态模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00816</td><td>Hidden Layer</td><td>隐藏层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00817</td><td>Hidden Markov Model</td><td>隐马尔可夫模型</td><td>HMM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-21-8">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00818</td><td>Hidden State</td><td>隐状态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00819</td><td>Hidden Unit</td><td>隐藏单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00820</td><td>Hidden Variable</td><td>隐变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00821</td><td>Hierarchical Clustering</td><td>层次聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00822</td><td>Hierarchical Reinforcement Learning</td><td>分层强化学习</td><td>HRL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00823</td><td>Hierarchical Softmax</td><td>层序Softmax/层序软最大化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00824</td><td>Hilbert Space</td><td>希尔伯特空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00825</td><td>Hill Climbing</td><td>爬山</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00826</td><td>Hinge Loss Function</td><td>合页损失函数/Hinge损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00827</td><td>Histogram Method</td><td>直方图方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00828</td><td>Hold-Out</td><td>留出法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00829</td><td>Homogeneous</td><td>同质</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00830</td><td>Hopfield Network</td><td>Hopfield网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00831</td><td>Huffman Coding</td><td>霍夫曼编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00832</td><td>Hybrid Computing</td><td>混合计算</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00833</td><td>Hyperbolic Tangent Function</td><td>双曲正切函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00834</td><td>Hyperparameter</td><td>超参数</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-08-18-5">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-28">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-00835</td><td>Hyperparameter Optimization</td><td>超参数优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00836</td><td>Hyperplane</td><td>超平面</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-00837</td><td>Hypothesis</td><td>假设</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00838</td><td>Hypothesis Space</td><td>假设空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00839</td><td>Hypothesis Test</td><td>假设检验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00840</td><td>I.I.D. Assumption</td><td>独立同分布假设</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00841</td><td>Identically Distributed</td><td>同分布的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00842</td><td>Identifiable</td><td>可辨认的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00843</td><td>Identity Function</td><td>恒等函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00844</td><td>Identity Mapping</td><td>恒等映射</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00845</td><td>Identity Matrix</td><td>单位矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00846</td><td>Ill Conditioning</td><td>病态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00847</td><td>Ill-Formed Problem</td><td>病态问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00848</td><td>Image</td><td>图像</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00849</td><td>Image Restoration</td><td>图像还原</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00850</td><td>Imitation Learning</td><td>模仿学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00851</td><td>Immorality</td><td>不道德</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00852</td><td>Imperfect Information</td><td>不完美信息</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-16-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00853</td><td>Implicit Density Model</td><td>隐式密度模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00854</td><td>Import</td><td>导入</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00855</td><td>Importance Sampling</td><td>重要性采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00856</td><td>Improved Iterative Scaling</td><td>改进的迭代尺度法</td><td>IIS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00857</td><td>Incomplete-Data</td><td>不完全数据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00858</td><td>Incremental Learning</td><td>增量学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00859</td><td>Indefinite Integral</td><td>不定积分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00860</td><td>Independence</td><td>独立</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00861</td><td>Independent</td><td>相互独立的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00862</td><td>Independent and Identically Distributed</td><td>独立同分布</td><td>I.I.D.</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00863</td><td>Independent Component Analysis</td><td>独立成分分析</td><td>ICA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00864</td><td>Independent Subspace Analysis</td><td>独立子空间分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00865</td><td>Index of Matrix</td><td>索引</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00866</td><td>Indicator Function</td><td>指示函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00867</td><td>Individual Learner</td><td>个体学习器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00868</td><td>Induction</td><td>归纳</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00869</td><td>Inductive Bias</td><td>归纳偏好</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00870</td><td>Inductive Learning</td><td>归纳学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00871</td><td>Inductive Logic Programming</td><td>归纳逻辑程序设计</td><td>ILP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00872</td><td>Inductive Transfer Learning</td><td>归纳迁移学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00873</td><td>Inequality Constraint</td><td>不等式约束</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00874</td><td>Inference</td><td>推断</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-14-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00875</td><td>Infinite</td><td>无限</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00876</td><td>Infinitely Exchangeable</td><td>无限可交换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00877</td><td>Information Divergence</td><td>信息散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00878</td><td>Information Entropy</td><td>信息熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00879</td><td>Information Gain</td><td>信息增益</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-00880</td><td>Information Gain Ratio</td><td>信息增益比</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-00881</td><td>Information Retrieval</td><td>信息检索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00882</td><td>Information Theory</td><td>信息论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00883</td><td>Inner Product</td><td>内积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00884</td><td>Input</td><td>输入</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00885</td><td>Input Distribution</td><td>输入分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00886</td><td>Input Gate</td><td>输入门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00887</td><td>Input Layer</td><td>输入层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00888</td><td>Input Space</td><td>输入空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00889</td><td>Insensitive Loss</td><td>不敏感损失</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00890</td><td>Instance</td><td>示例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00891</td><td>Instance Segmentation</td><td>实例分割</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00892</td><td>Integer Linear Programming</td><td>整数线性规划</td><td>ILP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00893</td><td>Integer Programming</td><td>整数规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00894</td><td>Integration</td><td>积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00895</td><td>Inter-Cluster Similarity</td><td>簇间相似度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00896</td><td>Internal Covariate Shift</td><td>内部协变量偏移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00897</td><td>Internal Node</td><td>内部结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00898</td><td>International Conference For Machine Learning</td><td>国际机器学习大会</td><td>ICML</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-31">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00899</td><td>Intervention Query</td><td>干预查询</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00900</td><td>Intra-Attention</td><td>内部注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00901</td><td>Intra-Cluster Similarity</td><td>簇内相似度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00902</td><td>Intrinsic Value</td><td>固有值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00903</td><td>Invariance</td><td>不变性</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-16-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00904</td><td>Invariant</td><td>不变</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00905</td><td>Inverse Matrix</td><td>逆矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00906</td><td>Inverse Reinforcement Learning</td><td>逆强化学习</td><td>IRL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00907</td><td>Inverse Resolution</td><td>逆归结</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00908</td><td>Inverse Time Decay</td><td>逆时衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00909</td><td>Invert</td><td>求逆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00910</td><td>Irreducible</td><td>不可约的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00911</td><td>Irrelevant Feature</td><td>无关特征</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00912</td><td>Isometric Mapping</td><td>等度量映射</td><td>Isomap</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00913</td><td>Isotonic Regression</td><td>等分回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00914</td><td>Isotropic</td><td>各向同性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00915</td><td>Isotropic Gaussian Distribution</td><td>各向同性高斯分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00916</td><td>Iteration</td><td>迭代</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td>数学、机器学习</td></tr><tr class="odd"><td>AITD-00917</td><td>Iterative Dichotomiser</td><td>迭代二分器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00918</td><td>Jacobian</td><td>雅克比</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00919</td><td>Jacobian Matrix</td><td>雅可比矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00920</td><td>Jensen Inequality</td><td>Jensen不等式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00921</td><td>Jensen-Shannon Divergence</td><td>JS散度</td><td>JSD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00922</td><td>Joint Probability Density Function</td><td>联合概率密度函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00923</td><td>Joint Probability Distribution</td><td>联合概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00924</td><td>Junction Tree Algorithm</td><td>联合树算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00925</td><td>K-Armed Bandit Problem</td><td>k-摇臂老虎机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00926</td><td>K-Fold Cross Validation</td><td>k 折交叉验证</td><td>K-FOLD CV</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-00927</td><td>K-Means Clustering</td><td>k-均值聚类</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-11-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-00928</td><td>K-Nearest Neighbor Classifier</td><td>k-近邻分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00929</td><td>K-Nearest Neighbor Method</td><td>k-近邻</td><td>K-NN</td><td>[1]</td><td>统计</td></tr><tr class="even"><td>AITD-00930</td><td>Karush-Kuhn-Tucker Condition</td><td>KKT条件</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00931</td><td>Karush–Kuhn–Tucker</td><td>Karush–Kuhn–Tucker</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00932</td><td>Kd Tree</td><td>Kd 树</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00933</td><td>Kernel Density Estimation</td><td>核密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00934</td><td>Kernel Function</td><td>核函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00935</td><td>Kernel Machine</td><td>核机器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00936</td><td>Kernel Matrix</td><td>核矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00937</td><td>Kernel Method</td><td>核方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-00938</td><td>Kernel Regression</td><td>核回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00939</td><td>Kernel Trick</td><td>核技巧</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00940</td><td>Kernelized</td><td>核化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00941</td><td>Kernelized Linear Discriminant Analysis</td><td>核线性判别分析</td><td>KLDA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00942</td><td>Kernelized PCA</td><td>核主成分分析</td><td>KPCA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00943</td><td>Key-Value Store</td><td>键-值数据库</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00944</td><td>KL Divergence</td><td>KL散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00945</td><td>Knowledge</td><td>知识</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00946</td><td>Knowledge Base</td><td>知识库</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-21-10">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00947</td><td>Knowledge Distillation</td><td>知识蒸馏</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00948</td><td>Knowledge Engineering</td><td>知识工程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00949</td><td>Knowledge Graph</td><td>知识图谱</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-11-03-5">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-03-24">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-09-26-8">[3]</a></td><td></td></tr><tr class="even"><td>AITD-00950</td><td>Knowledge Representation</td><td>知识表征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00951</td><td>Kronecker Product</td><td>Kronecker积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00952</td><td>Krylov Method</td><td>Krylov方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00953</td><td>L-BFGS</td><td>L-BFGS</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00954</td><td>Label</td><td>标签/标记</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00955</td><td>Label Propagation</td><td>标记传播</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00956</td><td>Label Smoothing</td><td>标签平滑</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00957</td><td>Label Space</td><td>标记空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00958</td><td>Labeled</td><td>标注</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00959</td><td>Lagrange Dual Problem</td><td>拉格朗日对偶问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00960</td><td>Lagrange Duality</td><td>拉格朗日对偶性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00961</td><td>Lagrange Function</td><td>拉格朗日函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00962</td><td>Lagrange Multiplier</td><td>拉格朗日乘子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00963</td><td>Language Model</td><td>语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00964</td><td>Language Modeling</td><td>语言模型化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00965</td><td>Laplace Distribution</td><td>Laplace分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00966</td><td>Laplace Smoothing</td><td>拉普拉斯平滑</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00967</td><td>Laplacian Correction</td><td>拉普拉斯修正</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00968</td><td>Large Learning Step</td><td>大学习步骤</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00969</td><td>Las Vegas Method</td><td>拉斯维加斯方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00970</td><td>Latent</td><td>潜在</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00971</td><td>Latent Dirichlet Allocation</td><td>潜在狄利克雷分配</td><td>LDA</td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-01-7">[1]</a></td><td></td></tr><tr class="even"><td>AITD-00972</td><td>Latent Layer</td><td>潜层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00973</td><td>Latent Semantic Analysis</td><td>潜在语义分析</td><td>LSA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00974</td><td>Latent Semantic Indexing</td><td>潜在语义索引</td><td>LSI</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00975</td><td>Latent Variable</td><td>潜变量/隐变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00976</td><td>Law of Large Numbers</td><td>大数定律</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00977</td><td>Layer</td><td>层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00978</td><td>Layer Normalization</td><td>层规范化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00979</td><td>Layer-Wise</td><td>逐层的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00980</td><td>Layer-Wise Adaptive Rate Scaling</td><td>逐层适应率缩放</td><td>LARS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00981</td><td>Layer-Wise Normalization</td><td>逐层规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00982</td><td>Layer-Wise Pretraining</td><td>逐层预训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00983</td><td>Layer-Wise Training</td><td>逐层训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00984</td><td>Lazy Learning</td><td>懒惰学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-00985</td><td>Leaf Node</td><td>叶结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00986</td><td>Leaky Lelu Function</td><td>泄漏线性整流函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00987</td><td>Leaky Relu</td><td>泄漏修正线性单元/泄漏整流线性单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00988</td><td>Leaky Unit</td><td>渗漏单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00989</td><td>Learned</td><td>学成</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00990</td><td>Learned Approximate Inference</td><td>学习近似推断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00991</td><td>Learner</td><td>学习器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00992</td><td>Learning</td><td>学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00993</td><td>Learning Algorithm</td><td>学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00994</td><td>Learning By Analogy</td><td>类比学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00995</td><td>Learning Rate</td><td>学习率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00996</td><td>Learning Rate Annealing</td><td>学习率退火</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00997</td><td>Learning Rate Decay</td><td>学习率衰减</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-00998</td><td>Learning Rate Warmup</td><td>学习率预热</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-00999</td><td>Learning To Learn</td><td>学习的学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01000</td><td>Learning Vector Quantization</td><td>学习向量量化</td><td>LVQ</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01001</td><td>Least General Generalization</td><td>最小一般泛化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01002</td><td>Least Mean Squares</td><td>最小均方</td><td>LMS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01003</td><td>Least Square Method</td><td>最小二乘法</td><td>LSM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-24-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01004</td><td>Least Squares Regression Tree</td><td>最小二乘回归树</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01005</td><td>Leave-One-Out Cross Validation</td><td>留一交叉验证</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01006</td><td>Leave-One-Out</td><td>留一法</td><td>LOO</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01007</td><td>Lebesgue-Integrable</td><td>勒贝格可积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01008</td><td>Left Eigenvector</td><td>左特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01009</td><td>Left Singular Vector</td><td>左奇异向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01010</td><td>Leibniz's Rule</td><td>莱布尼兹法则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01011</td><td>Lifelong Learning</td><td>终身学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01012</td><td>Likelihood</td><td>似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01013</td><td>Line Search</td><td>线搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01014</td><td>Linear Auto-Regressive Network</td><td>线性自回归网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01015</td><td>Linear Chain</td><td>线性链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01016</td><td>Linear Chain Conditional Random Field</td><td>线性链条件随机场</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01017</td><td>Linear Classification Model</td><td>线性分类模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01018</td><td>Linear Classifier</td><td>线性分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01019</td><td>Linear Combination</td><td>线性组合</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="even"><td>AITD-01020</td><td>Linear Dependence</td><td>线性相关</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01021</td><td>Linear Discriminant Analysis</td><td>线性判别分析</td><td>LDA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01022</td><td>Linear Factor Model</td><td>线性因子模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01023</td><td>Linear Mapping</td><td>线性映射</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01024</td><td>Linear Model</td><td>线性模型</td><td>LR</td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01025</td><td>Linear Programming</td><td>线性规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01026</td><td>Linear Regression</td><td>线性回归</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-01">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-17-5">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a></td><td>统计、数学</td></tr><tr class="odd"><td>AITD-01027</td><td>Linear Scaling Rule</td><td>线性缩放规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01028</td><td>Linear Scan</td><td>线性扫描</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01029</td><td>Linear Space</td><td>线性空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01030</td><td>Linear Support Vector Machine</td><td>线性支持向量机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01031</td><td>Linear Support Vector Machine In Linearly Separable Case</td><td>线性可分支持向量机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01032</td><td>Linear Threshold Units</td><td>线性阈值单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01033</td><td>Linear Transformation</td><td>线性变换</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01034</td><td>Linearly Independent</td><td>线性无关</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01035</td><td>Linearly Separable</td><td>线性可分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01036</td><td>Linearly Separable Data Set</td><td>线性可分数据集</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01037</td><td>Link Analysis</td><td>链接分析</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01038</td><td>Link Function</td><td>联系函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01039</td><td>Link Prediction</td><td>链接预测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01040</td><td>Link Table</td><td>连接表</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01041</td><td>Linkage</td><td>连接</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01042</td><td>Linked Importance Sampling</td><td>链接重要采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01043</td><td>Lipschitz</td><td>Lipschitz</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01044</td><td>Lipschitz Constant</td><td>Lipschitz常数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01045</td><td>Lipschitz Continuous</td><td>Lipschitz连续</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01046</td><td>Liquid State Machine</td><td>流体状态机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01047</td><td>Local Conditional Probability Distribution</td><td>局部条件概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01048</td><td>Local Constancy Prior</td><td>局部不变性先验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01049</td><td>Local Contrast Normalization</td><td>局部对比度规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01050</td><td>Local Curvature</td><td>局部曲率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01051</td><td>Local Descent</td><td>局部下降</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01052</td><td>Local Invariances</td><td>局部不变性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01053</td><td>Local Kernel</td><td>局部核</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01054</td><td>Local Markov Property</td><td>局部马尔可夫性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01055</td><td>Local Maxima</td><td>局部极大值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01056</td><td>Local Maximum</td><td>局部极大点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01057</td><td>Local Minima</td><td>局部极小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01058</td><td>Local Minimizer</td><td>局部最小解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01059</td><td>Local Minimum</td><td>局部极小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01060</td><td>Local Representation</td><td>局部式表示/局部式表征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01061</td><td>Local Response Normalization</td><td>局部响应规范化</td><td>LRN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01062</td><td>Locally Linear Embedding</td><td>局部线性嵌入</td><td>LLE</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01063</td><td>Log Likelihood</td><td>对数似然函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01064</td><td>Log Linear Model</td><td>对数线性模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01065</td><td>Log-Likelihood</td><td>对数似然</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01066</td><td>Log-Likelihood Loss Function</td><td>对数似然损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01067</td><td>Log-Linear Regression</td><td>对数线性回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01068</td><td>Logarithmic Loss Function</td><td>对数损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01069</td><td>Logarithmic Scale</td><td>对数尺度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01070</td><td>Logistic Distribution</td><td>对数几率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01071</td><td>Logistic Function</td><td>对数几率函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01072</td><td>Logistic Loss</td><td>对率损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01073</td><td>Logistic Regression</td><td>对数几率回归(逻辑回归)</td><td>LR</td><td><a href="https://www.jiqizhixin.com/articles/2017-11-23-6">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[2]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01074</td><td>Logistic Sigmoid</td><td>对数几率Sigmoid</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01075</td><td>Logit</td><td>对数几率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01076</td><td>Long Short Term Memory</td><td>长短期记忆</td><td>LSTM</td><td><a href="https://www.jiqizhixin.com/articles/2017-12-18-6">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-10-04-2">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-09-29-7">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[4]</a></td><td></td></tr><tr class="odd"><td>AITD-01077</td><td>Long Short-Term Memory Network</td><td>长短期记忆网络</td><td>LSTM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01078</td><td>Long-Term Dependencies Problem</td><td>长程依赖问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01079</td><td>Long-Term Dependency</td><td>长期依赖</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01080</td><td>Long-Term Memory</td><td>长期记忆</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01081</td><td>Loop</td><td>环</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01082</td><td>Loopy Belief Propagation</td><td>环状信念传播</td><td>LBP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01083</td><td>Loss</td><td>损失</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01084</td><td>Loss Function</td><td>损失函数</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-03-4">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01085</td><td>Low Rank Matrix Approximation</td><td>低秩矩阵近似</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01086</td><td>Lp Distance</td><td>Lp距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01087</td><td>Machine Learning Model</td><td>机器学习模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01088</td><td>Machine Learning</td><td>机器学习</td><td>ML</td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01089</td><td>Machine Translation</td><td>机器翻译</td><td>MT</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-13-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01090</td><td>Macro Average</td><td>宏平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01091</td><td>Macro-F1</td><td>宏F1</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01092</td><td>Macro-P</td><td>宏查准率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01093</td><td>Macron-R</td><td>宏查全率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01094</td><td>Mahalanobis Distance</td><td>马哈拉诺比斯距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01095</td><td>Main Diagonal</td><td>主对角线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01096</td><td>Majority Voting</td><td>绝对多数投票</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01097</td><td>Majority Voting Rule</td><td>多数表决规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01098</td><td>Manhattan Distance</td><td>曼哈顿距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01099</td><td>Manifold</td><td>流形</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01100</td><td>Manifold Assumption</td><td>流形假设</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01101</td><td>Manifold Learning</td><td>流形学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01102</td><td>Manifold Tangent Classifier</td><td>流形正切分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01103</td><td>Margin</td><td>间隔</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01104</td><td>Margin Theory</td><td>间隔理论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01105</td><td>Marginal Distribution</td><td>边缘分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01106</td><td>Marginal Independence</td><td>边缘独立性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01107</td><td>Marginal Likelihood</td><td>边缘似然函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01108</td><td>Marginal Probability Distribution</td><td>边缘概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01109</td><td>Marginalization</td><td>边缘化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01110</td><td>Markov Blanket</td><td>马尔可夫毯</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01111</td><td>Markov Chain</td><td>马尔可夫链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01112</td><td>Markov Chain Monte Carlo</td><td>马尔可夫链蒙特卡罗</td><td>MCMC</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-24-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01113</td><td>Markov Decision Process</td><td>马尔可夫决策过程</td><td>MDP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01114</td><td>Markov Network</td><td>马尔可夫网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01115</td><td>Markov Process</td><td>马尔可夫过程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01116</td><td>Markov Property</td><td>马尔可夫性质</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01117</td><td>Markov Random Field</td><td>马尔可夫随机场</td><td>MRF</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01118</td><td>Mask</td><td>掩码</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01119</td><td>Mask Language Modeling</td><td>掩码语言模型化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01120</td><td>Masked Self-Attention</td><td>掩蔽自注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01121</td><td>Mathematical Optimization</td><td>数学优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01122</td><td>Matrix</td><td>矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01123</td><td>Matrix Calculus</td><td>矩阵微积分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01124</td><td>Matrix Completion</td><td>矩阵补全</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01125</td><td>Matrix Decomposition</td><td>矩阵分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01126</td><td>Matrix Inversion</td><td>逆矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01127</td><td>Matrix Product</td><td>矩阵乘积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01128</td><td>Max Norm</td><td>最大范数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01129</td><td>Max Pooling</td><td>最大汇聚</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-02-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01130</td><td>Maxima</td><td>极大值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01131</td><td>Maximal Clique</td><td>最大团</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01132</td><td>Maximization</td><td>极大</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01133</td><td>Maximization Step</td><td>M步</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01134</td><td>Maximization-Maximization Algorithm</td><td>极大-极大算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01135</td><td>Maximum A Posteriori</td><td>最大后验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01136</td><td>Maximum A Posteriori Estimation</td><td>最大后验估计</td><td>MAP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01137</td><td>Maximum Entropy Model</td><td>最大熵模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01138</td><td>Maximum Likelihood</td><td>极大似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01139</td><td>Maximum Likelihood Estimation</td><td>极大似然估计</td><td>MLE</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-09-6">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01140</td><td>Maximum Likelihood Method</td><td>极大似然法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01141</td><td>Maximum Margin</td><td>最大间隔</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01142</td><td>Maximum Mean Discrepancy</td><td>最大平均偏差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01143</td><td>Maximum Posterior Probability Estimation</td><td>最大后验概率估计</td><td>MAP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01144</td><td>Maximum Weighted Spanning Tree</td><td>最大带权生成树</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01145</td><td>Maxout</td><td>Maxout</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01146</td><td>Maxout Unit</td><td>Maxout单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01147</td><td>Mean</td><td>均值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01148</td><td>Mean Absolute Error</td><td>平均绝对误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01149</td><td>Mean And Covariance RBM</td><td>均值和协方差RBM</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01150</td><td>Mean Filed</td><td>平均场</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01151</td><td>Mean Filter</td><td>均值滤波</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01152</td><td>Mean Pooling</td><td>平均汇聚</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01153</td><td>Mean Product of Student t-Distribution</td><td>学生 t 分布均值乘积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01154</td><td>Mean Squared Error</td><td>均方误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01155</td><td>Mean-Covariance Restricted Boltzmann Machine</td><td>均值-协方差受限玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01156</td><td>Mean-Field</td><td>平均场</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01157</td><td>Meanfield</td><td>均匀场</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01158</td><td>Measure Theory</td><td>测度论</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01159</td><td>Measure Zero</td><td>零测度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01160</td><td>Median</td><td>中位数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01161</td><td>Memory</td><td>记忆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01162</td><td>Memory Augmented Neural Network</td><td>记忆增强神经网络</td><td>MANN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01163</td><td>Memory Capacity</td><td>记忆容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01164</td><td>Memory Cell</td><td>记忆元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01165</td><td>Memory Network</td><td>记忆网络</td><td>MN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01166</td><td>Memory Segment</td><td>记忆片段</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01167</td><td>Mercer Kernel</td><td>Mercer 核</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01168</td><td>Message</td><td>消息</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01169</td><td>Message Passing</td><td>消息传递</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01170</td><td>Message Passing Neural Network</td><td>消息传递神经网络</td><td>MPNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01171</td><td>Meta-Learner</td><td>元学习器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01172</td><td>Meta-Learning</td><td>元学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01173</td><td>Meta-Optimization</td><td>元优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01174</td><td>Meta-Rule</td><td>元规则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01175</td><td>Metric</td><td>指标</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-01176</td><td>Metric Learning</td><td>度量学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01177</td><td>Micro Average</td><td>微平均</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01178</td><td>Micro-F1</td><td>微F1</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01179</td><td>Micro-P</td><td>微査准率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01180</td><td>Micro-R</td><td>微查全率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01181</td><td>Min-Max Normalization</td><td>最小最大值规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01182</td><td>Mini-Batch Gradient</td><td>小批量梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01183</td><td>Mini-Batch Gradient Descent</td><td>小批量梯度下降法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01184</td><td>Mini-Batch SGD</td><td>小批次随机梯度下降</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01185</td><td>Minibatch</td><td>小批量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01186</td><td>Minibatch Stochastic</td><td>小批量随机</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01187</td><td>Minima</td><td>极小值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01188</td><td>Minimal Description Length</td><td>最小描述长度</td><td>MDL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01189</td><td>Minimax Game</td><td>极小极大博弈</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01190</td><td>Minimum</td><td>极小点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01191</td><td>Minkowski Distance</td><td>闵可夫斯基距离</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01192</td><td>Misclassification Cost</td><td>误分类代价</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01193</td><td>Mixing</td><td>混合</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01194</td><td>Mixing Time</td><td>混合时间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01195</td><td>Mixture Density Network</td><td>混合密度网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01196</td><td>Mixture Distribution</td><td>混合分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01197</td><td>Mixture of Experts</td><td>混合专家模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01198</td><td>Mixture-of-Gaussian</td><td>高斯混合</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01199</td><td>Modality</td><td>模态</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01200</td><td>Mode</td><td>峰值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01201</td><td>Model</td><td>模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01202</td><td>Model Averaging</td><td>模型平均</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01203</td><td>Model Collapse</td><td>模型坍塌</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01204</td><td>Model Complexity</td><td>模型复杂度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01205</td><td>Model Compression</td><td>模型压缩</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01206</td><td>Model Identifiability</td><td>模型可辨识性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01207</td><td>Model Parallelism</td><td>模型并行</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01208</td><td>Model Parameter</td><td>模型参数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01209</td><td>Model Predictive Control</td><td>模型预测控制</td><td>MPC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01210</td><td>Model Selection</td><td>模型选择</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01211</td><td>Model-Agnostic Meta-Learning</td><td>模型无关的元学习</td><td>MAML</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01212</td><td>Model-Based Learning</td><td>有模型学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01213</td><td>Model-Based Reinforcement Learning</td><td>基于模型的强化学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01214</td><td>Model-Free Learning</td><td>免模型学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01215</td><td>Model-Free Reinforcement Learning</td><td>模型无关的强化学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01216</td><td>Moment</td><td>矩</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01217</td><td>Moment Matching</td><td>矩匹配</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01218</td><td>Momentum</td><td>动量</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-07-01-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01219</td><td>Momentum Method</td><td>动量法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01220</td><td>Monte Carlo</td><td>蒙特卡罗</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01221</td><td>Monte Carlo Estimate</td><td>蒙特卡罗估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01222</td><td>Monte Carlo Integration</td><td>蒙特卡罗积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01223</td><td>Monte Carlo Method</td><td>蒙特卡罗方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01224</td><td>Moore's Law</td><td>摩尔定律</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01225</td><td>Moore-Penrose Pseudoinverse</td><td>Moore-Penrose 伪逆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01226</td><td>Moral Graph</td><td>端正图/道德图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01227</td><td>Moralization</td><td>道德化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01228</td><td>Most General Unifier</td><td>最一般合一置换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01229</td><td>Moving Average</td><td>移动平均</td><td>MA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01230</td><td>Multi-Armed Bandit Problem</td><td>多臂赌博机问题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01231</td><td>Multi-Class Classification</td><td>多分类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01232</td><td>Multi-Classifier System</td><td>多分类器系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01233</td><td>Multi-Document Summarization</td><td>多文档摘要</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01234</td><td>Multi-Head Attention</td><td>多头注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01235</td><td>Multi-Head Self-Attention</td><td>多头自注意力</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01236</td><td>Multi-Hop</td><td>多跳</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01237</td><td>Multi-Kernel Learning</td><td>多核学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01238</td><td>Multi-Label Classification</td><td>多标签分类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01239</td><td>Multi-Label Learning</td><td>多标记学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01240</td><td>Multi-Layer Feedforward Neural Networks</td><td>多层前馈神经网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01241</td><td>Multi-Layer Perceptron</td><td>多层感知机</td><td>MLP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-01242</td><td>Multi-Nominal Logistic Regression Model</td><td>多项对数几率回归模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01243</td><td>Multi-Prediction Deep Boltzmann Machine</td><td>多预测深度玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01244</td><td>Multi-Response Linear Regression</td><td>多响应线性回归</td><td>MLR</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01245</td><td>Multi-View Learning</td><td>多视图学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01246</td><td>Multicollinearity</td><td>多重共线性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01247</td><td>Multimodal</td><td>多峰值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01248</td><td>Multimodal Learning</td><td>多模态学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01249</td><td>Multinomial Distribution</td><td>多项分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01250</td><td>Multinoulli Distribution</td><td>Multinoulli分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01251</td><td>Multinoulli Output Distribution</td><td>Multinoulli输出分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01252</td><td>Multiple Dimensional Scaling</td><td>多维缩放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01253</td><td>Multiple Linear Regression</td><td>多元线性回归</td><td>MLR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[3]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01254</td><td>Multitask Learning</td><td>多任务学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01255</td><td>Multivariate Decision Tree</td><td>多变量决策树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01256</td><td>Multivariate Gaussian Distribution</td><td>多元高斯分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01257</td><td>Multivariate Normal Distribution</td><td>多元正态分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01258</td><td>Mutual Information</td><td>互信息</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01259</td><td>N-Gram</td><td>N元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01260</td><td>N-Gram Feature</td><td>N元特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01261</td><td>N-Gram Model</td><td>N元模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01262</td><td>Naive Bayes Algorithm</td><td>朴素贝叶斯算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01263</td><td>Naive Bayes Classifier</td><td>朴素贝叶斯分类器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01264</td><td>Naive Bayes</td><td>朴素贝叶斯</td><td>NB</td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-20-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01265</td><td>Named Entity Recognition</td><td>命名实体识别</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01266</td><td>Narrow Convolution</td><td>窄卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01267</td><td>Nash Equilibrium</td><td>纳什均衡</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01268</td><td>Nash Reversion</td><td>纳什回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01269</td><td>Nats</td><td>奈特</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01270</td><td>Natural Exponential Decay</td><td>自然指数衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01271</td><td>Natural Language Generation</td><td>自然语言生成</td><td>NLG</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01272</td><td>Natural Language Processing</td><td>自然语言处理</td><td>NLP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[4]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-14-5">[5]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-14-4">[6]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-12-3">[7]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01273</td><td>Nearest Neighbor</td><td>最近邻</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01274</td><td>Nearest Neighbor Classifier</td><td>最近邻分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01275</td><td>Nearest Neighbor Graph</td><td>最近邻图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01276</td><td>Nearest Neighbor Regression</td><td>最近邻回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01277</td><td>Nearest-Neighbor Search</td><td>最近邻搜索</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-24-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01278</td><td>Negative Class</td><td>负类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01279</td><td>Negative Correlation</td><td>负相关法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01280</td><td>Negative Definite</td><td>负定</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01281</td><td>Negative Log Likelihood</td><td>负对数似然函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01282</td><td>Negative Part Function</td><td>负部函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01283</td><td>Negative Phase</td><td>负相</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01284</td><td>Negative Sample</td><td>负例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01285</td><td>Negative Sampling</td><td>负采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01286</td><td>Negative Semidefinite</td><td>半负定</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01287</td><td>Neighbourhood Component Analysis</td><td>近邻成分分析</td><td>NCA</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01288</td><td>Nesterov Accelerated Gradient</td><td>Nesterov加速梯度</td><td>NAG</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01289</td><td>Nesterov Momentum</td><td>Nesterov动量法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01290</td><td>Net Activation</td><td>净活性值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01291</td><td>Net Input</td><td>净输入</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01292</td><td>Network</td><td>网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01293</td><td>Network Capacity</td><td>网络容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01294</td><td>Neural Architecture Search</td><td>神经架构搜索</td><td>NAS</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01295</td><td>Neural Auto-Regressive Density Estimator</td><td>神经自回归密度估计器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01296</td><td>Neural Auto-Regressive Network</td><td>神经自回归网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01297</td><td>Neural Language Model</td><td>神经语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01298</td><td>Neural Machine Translation</td><td>神经机器翻译</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-08-22-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01299</td><td>Neural Model</td><td>神经模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01300</td><td>Neural Network</td><td>神经网络</td><td>NN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01301</td><td>Neural Turing Machine</td><td>神经图灵机</td><td>NTM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-11-7">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01302</td><td>Neurodynamics</td><td>神经动力学</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01303</td><td>Neuromorphic Computing</td><td>神经形态计算</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-09-26-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-26-2">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-06-16-6">[3]</a></td><td></td></tr><tr class="even"><td>AITD-01304</td><td>Neuron</td><td>神经元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01305</td><td>Newton Method</td><td>牛顿法</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-03-11-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01306</td><td>No Free Lunch Theorem</td><td>没有免费午餐定理</td><td>NFL</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-03-6">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01307</td><td>Node</td><td>结点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01308</td><td>Noise</td><td>噪声</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01309</td><td>Noise Distribution</td><td>噪声分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01310</td><td>Noise-Contrastive Estimation</td><td>噪声对比估计</td><td>NCE</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01311</td><td>Nominal Attribute</td><td>列名属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01312</td><td>Non-Autoregressive Process</td><td>非自回归过程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01313</td><td>Non-Convex Optimization</td><td>非凸优化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-29-4">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01314</td><td>Non-Informative Prior</td><td>无信息先验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01315</td><td>Non-Linear Model</td><td>非线性模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01316</td><td>Non-Linear Oscillation</td><td>非线性振荡</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01317</td><td>Non-Linear Support Vector Machine</td><td>非线性支持向量机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01318</td><td>Non-Metric Distance</td><td>非度量距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01319</td><td>Non-Negative Matrix Factorization</td><td>非负矩阵分解</td><td>NMF</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01320</td><td>Non-Ordinal Attribute</td><td>无序属性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01321</td><td>Non-Parametric</td><td>非参数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01322</td><td>Non-Parametric Model</td><td>非参数化模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01323</td><td>Non-Probabilistic Model</td><td>非概率模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01324</td><td>Non-Saturating Game</td><td>非饱和博弈</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01325</td><td>Non-Separable</td><td>不可分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01326</td><td>Nonconvex</td><td>非凸</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01327</td><td>Nondistributed</td><td>非分布式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01328</td><td>Nondistributed Representation</td><td>非分布式表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01329</td><td>Nonlinear Autoregressive With Exogenous Inputs Model</td><td>有外部输入的非线性自回归模型</td><td>NARX</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01330</td><td>Nonlinear Conjugate Gradients</td><td>非线性共轭梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01331</td><td>Nonlinear Independent Components Estimation</td><td>非线性独立成分估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01332</td><td>Nonlinear Programming</td><td>非线性规划</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01333</td><td>Nonparametric Density Estimation</td><td>非参数密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01334</td><td>Norm</td><td>范数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01335</td><td>Norm-Preserving</td><td>范数保持性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01336</td><td>Normal Distribution</td><td>正态分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01337</td><td>Normal Equation</td><td>正规方程</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01338</td><td>Normalization</td><td>规范化</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01339</td><td>Normalization Factor</td><td>规范化因子</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01340</td><td>Normalized</td><td>规范化的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01341</td><td>Normalized Initialization</td><td>标准初始化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01342</td><td>Nuclear Norm</td><td>核范数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01343</td><td>Null Space</td><td>零空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01344</td><td>Number of Epochs</td><td>轮数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01345</td><td>Numerator Layout</td><td>分子布局</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01346</td><td>Numeric Value</td><td>数值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01347</td><td>Numerical Attribute</td><td>数值属性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01348</td><td>Numerical Differentiation</td><td>数值微分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01349</td><td>Numerical Method</td><td>数值方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01350</td><td>Numerical Optimization</td><td>数值优化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01351</td><td>Object Detection</td><td>目标检测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01352</td><td>Object Recognition</td><td>对象识别</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01353</td><td>Objective</td><td>目标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01354</td><td>Objective Function</td><td>目标函数</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-11-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01355</td><td>Oblique Decision Tree</td><td>斜决策树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01356</td><td>Observable Variable</td><td>观测变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01357</td><td>Observation Sequence</td><td>观测序列</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01358</td><td>Occam's Razor</td><td>奥卡姆剃刀</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01359</td><td>Odds</td><td>几率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01360</td><td>Off-Policy</td><td>异策略</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01361</td><td>Offline Inference</td><td>离线推断</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-06-5">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01362</td><td>Offset</td><td>偏移量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01363</td><td>Offset Vector</td><td>偏移向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01364</td><td>On-Policy</td><td>同策略</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01365</td><td>One-Shot Learning</td><td>单试学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-03-13-2">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-01366</td><td>One-Dependent Estimator</td><td>独依赖估计</td><td>ODE</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01367</td><td>One-Hot</td><td>独热</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01368</td><td>Online</td><td>在线</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01369</td><td>Online Inference</td><td>在线推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01370</td><td>Online Learning</td><td>在线学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01371</td><td>Operation</td><td>操作</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01372</td><td>Operator</td><td>运算符</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01373</td><td>Optimal Capacity</td><td>最佳容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01374</td><td>Optimization</td><td>最优化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01375</td><td>Optimization Landscape</td><td>优化地形</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01376</td><td>Optimizer</td><td>优化器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01377</td><td>Ordered Rule</td><td>带序规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01378</td><td>Ordinal Attribute</td><td>有序属性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01379</td><td>Origin</td><td>原点</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01380</td><td>Orthogonal</td><td>正交</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-01381</td><td>Orthogonal Initialization</td><td>正交初始化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01382</td><td>Orthogonal Matrix</td><td>正交矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01383</td><td>Orthonormal</td><td>标准正交</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01384</td><td>Out-Of-Bag Estimate</td><td>包外估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01385</td><td>Outer Product</td><td>外积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01386</td><td>Outlier</td><td>异常点</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-01387</td><td>Output</td><td>输出</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01388</td><td>Output Gate</td><td>输出门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01389</td><td>Output Layer</td><td>输出层</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01390</td><td>Output Smearing</td><td>输出调制法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01391</td><td>Output Space</td><td>输出空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01392</td><td>Over-Parameterized</td><td>过度参数化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01393</td><td>Overcomplete</td><td>过完备</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01394</td><td>Overestimation</td><td>过估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01395</td><td>Overfitting</td><td>过拟合</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01396</td><td>Overfitting Regime</td><td>过拟合机制</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01397</td><td>Overflow</td><td>上溢</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01398</td><td>Oversampling</td><td>过采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01399</td><td>PAC Learning</td><td>PAC学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01400</td><td>Pac-Learnable</td><td>PAC可学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01401</td><td>Padding</td><td>填充</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01402</td><td>Paired t -Test</td><td>成对 t 检验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01403</td><td>Pairwise</td><td>成对型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01404</td><td>Pairwise Markov Property</td><td>成对马尔可夫性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01405</td><td>Parallel Distributed Processing</td><td>分布式并行处理</td><td>PDP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01406</td><td>Parallel Tempering</td><td>并行回火</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01407</td><td>Parameter</td><td>参数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01408</td><td>Parameter Estimation</td><td>参数估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01409</td><td>Parameter Server</td><td>参数服务器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01410</td><td>Parameter Sharing</td><td>参数共享</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01411</td><td>Parameter Space</td><td>参数空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01412</td><td>Parameter Tuning</td><td>调参</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-03-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01413</td><td>Parametric Case</td><td>有参情况</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01414</td><td>Parametric Density Estimation</td><td>参数密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01415</td><td>Parametric Model</td><td>参数化模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01416</td><td>Parametric ReLU</td><td>参数化修正线性单元/参数化整流线性单元</td><td>PReLU</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01417</td><td>Parse Tree</td><td>解析树</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01418</td><td>Part-Of-Speech Tagging</td><td>词性标注</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01419</td><td>Partial Derivative</td><td>偏导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01420</td><td>Partially Observable Markov Decision Processes</td><td>部分可观测马尔可夫决策过程</td><td>POMDP</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01421</td><td>Particle Swarm Optimization</td><td>粒子群优化算法</td><td>PSO</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01422</td><td>Partition</td><td>划分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01423</td><td>Partition Function</td><td>配分函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01424</td><td>Path</td><td>路径</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01425</td><td>Pattern</td><td>模式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01426</td><td>Pattern Recognition</td><td>模式识别</td><td>PR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-01427</td><td>Penalty Term</td><td>罚项</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01428</td><td>Perceptron</td><td>感知机</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-15-2">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[2]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01429</td><td>Performance Measure</td><td>性能度量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01430</td><td>Periodic</td><td>周期的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01431</td><td>Permutation Invariant</td><td>置换不变性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01432</td><td>Perplexity</td><td>困惑度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01433</td><td>Persistent Contrastive Divergence</td><td>持续性对比散度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01434</td><td>Phoneme</td><td>音素</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01435</td><td>Phonetic</td><td>语音</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01436</td><td>Pictorial Structure</td><td>图形结构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01437</td><td>Piecewise</td><td>分段</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01438</td><td>Piecewise Constant Decay</td><td>分段常数衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01439</td><td>Pipeline</td><td>流水线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01440</td><td>Plate Notation</td><td>板块表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01441</td><td>Plug And Play Generative Network</td><td>即插即用生成网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01442</td><td>Plurality Voting</td><td>相对多数投票</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01443</td><td>Point Estimator</td><td>点估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01444</td><td>Pointer Network</td><td>指针网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01445</td><td>Polarity Detection</td><td>极性检测</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01446</td><td>Policy</td><td>策略</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01447</td><td>Policy Evaluation</td><td>策略评估</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01448</td><td>Policy Gradient</td><td>策略梯度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01449</td><td>Policy Improvement</td><td>策略改进</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01450</td><td>Policy Iteration</td><td>策略迭代</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01451</td><td>Policy Search</td><td>策略搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01452</td><td>Polynomial Basis Function</td><td>多项式基函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01453</td><td>Polynomial Kernel Function</td><td>多项式核函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01454</td><td>Polysemy</td><td>一词多义性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01455</td><td>Pool</td><td>汇聚</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01456</td><td>Pooling</td><td>汇聚</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-02-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01457</td><td>Pooling Function</td><td>汇聚函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01458</td><td>Pooling Layer</td><td>汇聚层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01459</td><td>Poor Conditioning</td><td>病态条件</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01460</td><td>Position Embedding</td><td>位置嵌入</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01461</td><td>Positional Encoding</td><td>位置编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01462</td><td>Positive Class</td><td>正类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01463</td><td>Positive Definite</td><td>正定</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01464</td><td>Positive Definite Kernel Function</td><td>正定核函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01465</td><td>Positive Definite Matrix</td><td>正定矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01466</td><td>Positive Part Function</td><td>正部函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01467</td><td>Positive Phase</td><td>正相</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01468</td><td>Positive Recurrent</td><td>正常返的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01469</td><td>Positive Sample</td><td>正例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01470</td><td>Positive Semidefinite</td><td>半正定</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01471</td><td>Positive-Semidefinite Matrix</td><td>半正定矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01472</td><td>Post-Hoc Test</td><td>后续检验</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01473</td><td>Post-Pruning</td><td>后剪枝</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01474</td><td>Posterior Distribution</td><td>后验分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01475</td><td>Posterior Inference</td><td>后验推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01476</td><td>Posterior Probability</td><td>后验概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01477</td><td>Potential Function</td><td>势函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01478</td><td>Power Method</td><td>幂法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01479</td><td>PR Curve</td><td>P-R曲线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01480</td><td>Pre-Trained Initialization</td><td>预训练初始化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01481</td><td>Pre-Training</td><td>预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01482</td><td>Precision</td><td>查准率/准确率</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>数学、HPC</td></tr><tr class="odd"><td>AITD-01483</td><td>Precision Matrix</td><td>精度矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01484</td><td>Predictive Sparse Decomposition</td><td>预测稀疏分解</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01485</td><td>Prepruning</td><td>预剪枝</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01486</td><td>Pretrained Language Model</td><td>预训练语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01487</td><td>Primal Problem</td><td>主问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01488</td><td>Primary Visual Cortex</td><td>初级视觉皮层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01489</td><td>Principal Component Analysis</td><td>主成分分析</td><td>PCA</td><td><a href="https://www.jiqizhixin.com/articles/2017-12-03-4">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[4]</a></td><td></td></tr><tr class="even"><td>AITD-01490</td><td>Principle Of Multiple Explanations</td><td>多释原则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01491</td><td>Prior</td><td>先验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01492</td><td>Prior Knowledge</td><td>先验知识</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-01493</td><td>Prior Probability</td><td>先验概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01494</td><td>Prior Probability Distribution</td><td>先验概率分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01495</td><td>Prior Pseudo-Counts</td><td>伪计数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01496</td><td>Prior Shift</td><td>先验偏移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01497</td><td>Priority Rule</td><td>优先级规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01498</td><td>Probabilistic Context-Free Grammar</td><td>概率上下文无关文法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01499</td><td>Probabilistic Density Estimation</td><td>概率密度估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01500</td><td>Probabilistic Generative Model</td><td>概率生成模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01501</td><td>Probabilistic Graphical Model</td><td>概率图模型</td><td>PGM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-29-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01502</td><td>Probabilistic Latent Semantic Analysis</td><td>概率潜在语义分析</td><td>PLSA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01503</td><td>Probabilistic Latent Semantic Indexing</td><td>概率潜在语义索引</td><td>PLSI</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01504</td><td>Probabilistic Model</td><td>概率模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01505</td><td>Probabilistic PCA</td><td>概率PCA</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01506</td><td>Probabilistic Undirected Graphical Model</td><td>概率无向图模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01507</td><td>Probability</td><td>概率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01508</td><td>Probability Density Function</td><td>概率密度函数</td><td>PDF</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01509</td><td>Probability Distribution</td><td>概率分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01510</td><td>Probability Mass Function</td><td>概率质量函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01511</td><td>Probability Model Estimation</td><td>概率模型估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01512</td><td>Probably Approximately Correct</td><td>概率近似正确</td><td>PAC</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01513</td><td>Product of Expert</td><td>专家之积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01514</td><td>Product Rule</td><td>乘法法则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01515</td><td>Properly PAC Learnable</td><td>恰PAC可学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01516</td><td>Proportional</td><td>成比例</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01517</td><td>Proposal Distribution</td><td>提议分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01518</td><td>Propositional Atom</td><td>原子命题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01519</td><td>Propositional Rule</td><td>命题规则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01520</td><td>Prototype-Based Clustering</td><td>原型聚类</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01521</td><td>Proximal Gradient Descent</td><td>近端梯度下降</td><td>PGD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01522</td><td>Pruning</td><td>剪枝</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-26">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01523</td><td>Pseudo-Label</td><td>伪标记</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01524</td><td>Pseudolikelihood</td><td>伪似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01525</td><td>Q Function</td><td>Q函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01526</td><td>Q-Learning</td><td>Q学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01527</td><td>Q-Network</td><td>Q网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01528</td><td>Quadratic Loss Function</td><td>平方损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01529</td><td>Quadratic Programming</td><td>二次规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01530</td><td>Quadrature Pair</td><td>象限对</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01531</td><td>Quantized Neural Network</td><td>量子化神经网络</td><td>QNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01532</td><td>Quantum Computer</td><td>量子计算机</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-13">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-11-30-5">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-29-5">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-01533</td><td>Quantum Computing</td><td>量子计算</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-13">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-17">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-29-5">[3]</a></td><td></td></tr><tr class="even"><td>AITD-01534</td><td>Quantum Machine Learning</td><td>量子机器学习</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-04-5">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01535</td><td>Quantum Mechanics</td><td>量子力学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-01536</td><td>Quasi Newton Method</td><td>拟牛顿法</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-16-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01537</td><td>Quasi-Concave</td><td>拟凹</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01538</td><td>Query</td><td>查询</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01539</td><td>Query Vector</td><td>查询向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01540</td><td>Query-Key-Value</td><td>查询-键-值</td><td>QKV</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01541</td><td>Radial Basis Function</td><td>径向基函数</td><td>RBF</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-01542</td><td>Random Access Memory</td><td>随机访问存储</td><td>RAM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01543</td><td>Random Field</td><td>随机场</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01544</td><td>Random Forest Algorithm</td><td>随机森林算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01545</td><td>Random Forest</td><td>随机森林</td><td>RF、RFS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[3]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[5]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01546</td><td>Random Initialization</td><td>随机初始化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01547</td><td>Random Sampling</td><td>随机采样</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[2]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01548</td><td>Random Search</td><td>随机搜索</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01549</td><td>Random Subspace</td><td>随机子空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01550</td><td>Random Variable</td><td>随机变量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01551</td><td>Random Walk</td><td>随机游走</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01552</td><td>Range</td><td>值域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01553</td><td>Rank</td><td>秩</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01554</td><td>Ratio Matching</td><td>比率匹配</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01555</td><td>Raw Feature</td><td>原始特征</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01556</td><td>Re-Balance</td><td>再平衡</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01557</td><td>Re-Sampling</td><td>重采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01558</td><td>Re-Weighting</td><td>重赋权</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01559</td><td>Readout Function</td><td>读出函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01560</td><td>Real-Time Recurrent Learning</td><td>实时循环学习</td><td>RTRL</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01561</td><td>Recall</td><td>查全率/召回率</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01562</td><td>Recall-Oriented Understudy For Gisting Evaluation</td><td>ROUGE</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01563</td><td>Receiver Operating Characteristic</td><td>受试者工作特征</td><td>ROC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01564</td><td>Receptive Field</td><td>感受野</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01565</td><td>Recirculation</td><td>再循环</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01566</td><td>Recognition Weight</td><td>认知权重</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01567</td><td>Recommender System</td><td>推荐系统</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01568</td><td>Reconstruction</td><td>重构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01569</td><td>Reconstruction Error</td><td>重构误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01570</td><td>Rectangular Diagonal Matrix</td><td>矩形对角矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01571</td><td>Rectified Linear</td><td>整流线性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01572</td><td>Rectified Linear Transformation</td><td>整流线性变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01573</td><td>Rectified Linear Unit</td><td>修正线性单元/整流线性单元</td><td>ReLU</td><td><a href="https://www.jiqizhixin.com/articles/2017-10-21-4">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td>CHAPTER 2</td></tr><tr class="even"><td>AITD-01574</td><td>Rectifier Network</td><td>整流网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01575</td><td>Recurrence</td><td>循环</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01576</td><td>Recurrent Convolutional Network</td><td>循环卷积网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01577</td><td>Recurrent Multi-Layer Perceptron</td><td>循环多层感知器</td><td>RMLP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01578</td><td>Recurrent Network</td><td>循环网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01579</td><td>Recurrent Neural Network</td><td>循环神经网络</td><td>RNN</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-13-4">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-05-5">[2]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-21-15">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[4]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[5]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[6]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01580</td><td>Recursive Neural Network</td><td>递归神经网络</td><td>RecNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01581</td><td>Reducible</td><td>可约的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01582</td><td>Redundant Feature</td><td>冗余特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01583</td><td>Reference Model</td><td>参考模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01584</td><td>Region</td><td>区域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01585</td><td>Regression</td><td>回归</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-21-13">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[3]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01586</td><td>Regularization</td><td>正则化</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-20">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01587</td><td>Regularizer</td><td>正则化项</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01588</td><td>Reinforcement Learning</td><td>强化学习</td><td>RL</td><td><a href="https://www.jiqizhixin.com/articles/2018-01-17-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-28-6">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[5]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01589</td><td>Rejection Sampling</td><td>拒绝采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01590</td><td>Relation</td><td>关系</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01591</td><td>Relational Database</td><td>关系型数据库</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01592</td><td>Relative Entropy</td><td>相对熵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01593</td><td>Relevant Feature</td><td>相关特征</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01594</td><td>Reparameterization</td><td>再参数化/重参数化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01595</td><td>Reparametrization Trick</td><td>重参数化技巧</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01596</td><td>Replay Buffer</td><td>经验池</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01597</td><td>Representation</td><td>表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01598</td><td>Representation Learning</td><td>表示学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01599</td><td>Representational Capacity</td><td>表示容量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01600</td><td>Representer Theorem</td><td>表示定理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01601</td><td>Reproducing Kernel Hilbert Space</td><td>再生核希尔伯特空间</td><td>RKHS</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01602</td><td>Rescaling</td><td>再缩放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01603</td><td>Reservoir Computing</td><td>储层计算</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01604</td><td>Reset Gate</td><td>重置门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01605</td><td>Residual Blocks</td><td>残差块</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01606</td><td>Residual Connection</td><td>残差连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01607</td><td>Residual Mapping</td><td>残差映射</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01608</td><td>Residual Network</td><td>残差网络</td><td>ResNet</td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-18-2">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01609</td><td>Residual Unit</td><td>残差单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01610</td><td>Residue Function</td><td>残差函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01611</td><td>Resolution Quotient</td><td>归结商</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01612</td><td>Restricted Boltzmann Machine</td><td>受限玻尔兹曼机</td><td>RBM</td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-08-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01613</td><td>Restricted Isometry Property</td><td>限定等距性</td><td>RIP</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01614</td><td>Return</td><td>总回报</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01615</td><td>Reverse Correlation</td><td>反向相关</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01616</td><td>Reverse KL Divergence</td><td>逆向KL散度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01617</td><td>Reverse Mode Accumulation</td><td>反向模式累加</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01618</td><td>Reversible Markov Chain</td><td>可逆马尔可夫链</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01619</td><td>Reward</td><td>奖励</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01620</td><td>Reward Function</td><td>奖励函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01621</td><td>Ridge Regression</td><td>岭回归</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01622</td><td>Riemann Integral</td><td>黎曼积分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01623</td><td>Right Eigenvector</td><td>右特征向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01624</td><td>Right Singular Vector</td><td>右奇异向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01625</td><td>Risk</td><td>风险</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01626</td><td>Risk Function</td><td>风险函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01627</td><td>Robustness</td><td>稳健性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>计算机、机器学习</td></tr><tr class="even"><td>AITD-01628</td><td>Root Node</td><td>根结点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01629</td><td>Round-Off Error</td><td>舍入误差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01630</td><td>Row</td><td>行</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01631</td><td>Rule Engine</td><td>规则引擎</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01632</td><td>Rule Learning</td><td>规则学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01633</td><td>S-Fold Cross Validation</td><td>S 折交叉验证</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01634</td><td>Saccade</td><td>扫视</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01635</td><td>Saddle Point</td><td>鞍点</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-09-08">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01636</td><td>Saddle-Free Newton Method</td><td>无鞍牛顿法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01637</td><td>Saliency Map</td><td>显著图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01638</td><td>Saliency-Based Attention</td><td>基于显著性的注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01639</td><td>Same</td><td>相同</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01640</td><td>Sample</td><td>样本</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01641</td><td>Sample Complexity</td><td>样本复杂度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01642</td><td>Sample Mean</td><td>样本均值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01643</td><td>Sample Space</td><td>样本空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01644</td><td>Sample Variance</td><td>样本方差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01645</td><td>Sampling</td><td>采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01646</td><td>Sampling Method</td><td>采样法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01647</td><td>Saturate</td><td>饱和</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01648</td><td>Saturating Function</td><td>饱和函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01649</td><td>Scalar</td><td>标量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01650</td><td>Scale Invariance</td><td>尺度不变性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01651</td><td>Scatter Matrix</td><td>散布矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01652</td><td>Scheduled Sampling</td><td>计划采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01653</td><td>Score</td><td>得分</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01654</td><td>Score Function</td><td>评分函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01655</td><td>Score Matching</td><td>分数匹配</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01656</td><td>Second Derivative</td><td>二阶导数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01657</td><td>Second Derivative Test</td><td>二阶导数测试</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01658</td><td>Second Layer</td><td>第二层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01659</td><td>Second-Order Method</td><td>二阶方法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01660</td><td>Selective Attention</td><td>选择性注意力</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01661</td><td>Selective Ensemble</td><td>选择性集成</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01662</td><td>Self Information</td><td>自信息</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01663</td><td>Self-Attention</td><td>自注意力</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01664</td><td>Self-Attention Model</td><td>自注意力模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01665</td><td>Self-Contrastive Estimation</td><td>自对比估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01666</td><td>Self-Driving</td><td>自动驾驶</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-27-7">[1]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-16">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-08-9">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-01667</td><td>Self-Gated</td><td>自门控</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01668</td><td>Self-Organizing Map</td><td>自组织映射网</td><td>SOM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01669</td><td>Self-Taught Learning</td><td>自学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01670</td><td>Self-Training</td><td>自训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01671</td><td>Semantic Gap</td><td>语义鸿沟</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01672</td><td>Semantic Hashing</td><td>语义哈希</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01673</td><td>Semantic Segmentation</td><td>语义分割</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01674</td><td>Semantic Similarity</td><td>语义相似度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01675</td><td>Semi-Definite Programming</td><td>半正定规划</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01676</td><td>Semi-Naive Bayes Classifiers</td><td>半朴素贝叶斯分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01677</td><td>Semi-Restricted Boltzmann Machine</td><td>半受限玻尔兹曼机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01678</td><td>Semi-Supervised</td><td>半监督</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01679</td><td>Semi-Supervised Clustering</td><td>半监督聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01680</td><td>Semi-Supervised Learning</td><td>半监督学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-12-22-3">[1]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-02">[2]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-07">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-01681</td><td>Semi-Supervised Support Vector Machine</td><td>半监督支持向量机</td><td>S3VM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01682</td><td>Sentiment Analysis</td><td>情感分析</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-07-7">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01683</td><td>Separable</td><td>可分离的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01684</td><td>Separate</td><td>分离的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01685</td><td>Separating Hyperplane</td><td>分离超平面</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01686</td><td>Separation</td><td>分离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01687</td><td>Sequence Labeling</td><td>序列标注</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01688</td><td>Sequence To Sequence Learning</td><td>序列到序列学习</td><td>Seq2Seq</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01689</td><td>Sequence-To-Sequence</td><td>序列到序列</td><td>Seq2Seq</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01690</td><td>Sequential Covering</td><td>序贯覆盖</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01691</td><td>Sequential Minimal Optimization</td><td>序列最小最优化</td><td>SMO</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01692</td><td>Sequential Model-Based Optimization</td><td>时序模型优化</td><td>SMBO</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01693</td><td>Sequential Partitioning</td><td>顺序分区</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01694</td><td>Setting</td><td>情景</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01695</td><td>Shadow Circuit</td><td>浅度回路</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01696</td><td>Shallow Learning</td><td>浅层学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01697</td><td>Shannon Entropy</td><td>香农熵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01698</td><td>Shannons</td><td>香农</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01699</td><td>Shaping</td><td>塑造</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01700</td><td>Sharp Minima</td><td>尖锐最小值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01701</td><td>Shattering</td><td>打散</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01702</td><td>Shift Invariance</td><td>平移不变性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01703</td><td>Short-Term Memory</td><td>短期记忆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01704</td><td>Shortcut Connection</td><td>直连边</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01705</td><td>Shortlist</td><td>短列表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01706</td><td>Siamese Network</td><td>孪生网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-02-4">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01707</td><td>Sigmoid</td><td>Sigmoid（一种激活函数）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01708</td><td>Sigmoid Belief Network</td><td>Sigmoid信念网络</td><td>SBN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01709</td><td>Sigmoid Curve</td><td>S 形曲线</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01710</td><td>Sigmoid Function</td><td>Sigmoid函数</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-11-02-26">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01711</td><td>Sign Function</td><td>符号函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01712</td><td>Signed Distance</td><td>带符号距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01713</td><td>Similarity</td><td>相似度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01714</td><td>Similarity Measure</td><td>相似度度量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01715</td><td>Simple Cell</td><td>简单细胞</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01716</td><td>Simple Recurrent Network</td><td>简单循环网络</td><td>SRN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01717</td><td>Simple Recurrent Neural Network</td><td>简单循环神经网络</td><td>S-RNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01718</td><td>Simplex</td><td>单纯形</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01719</td><td>Simulated Annealing</td><td>模拟退火</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01720</td><td>Simultaneous Localization And Mapping</td><td>即时定位与地图构建</td><td>SLAM</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01721</td><td>Single Component Metropolis-Hastings</td><td>单分量Metropolis-Hastings</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01722</td><td>Single Linkage</td><td>单连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01723</td><td>Singular</td><td>奇异的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01724</td><td>Singular Value</td><td>奇异值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01725</td><td>Singular Value Decomposition</td><td>奇异值分解</td><td>SVD</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01726</td><td>Singular Vector</td><td>奇异向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01727</td><td>Size</td><td>大小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01728</td><td>Skip Connection</td><td>跳跃连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01729</td><td>Skip-Gram Model</td><td>跳元模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01730</td><td>Skip-Gram Model With Negative Sampling</td><td>跳元模型加负采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01731</td><td>Slack Variable</td><td>松弛变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01732</td><td>Slow Feature Analysis</td><td>慢特征分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01733</td><td>Slowness Principle</td><td>慢性原则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01734</td><td>Smoothing</td><td>平滑</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01735</td><td>Smoothness Prior</td><td>平滑先验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01736</td><td>Soft Attention Mechanism</td><td>软性注意力机制</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01737</td><td>Soft Clustering</td><td>软聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01738</td><td>Soft Margin</td><td>软间隔</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01739</td><td>Soft Margin Maximization</td><td>软间隔最大化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01740</td><td>Soft Target</td><td>软目标</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01741</td><td>Soft Voting</td><td>软投票</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01742</td><td>Softmax</td><td>Softmax/软最大化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01743</td><td>Softmax Function</td><td>Softmax函数/软最大化函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01744</td><td>Softmax Regression</td><td>Softmax回归/软最大化回归</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01745</td><td>Softmax Unit</td><td>Softmax单元/软最大化单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01746</td><td>Softplus</td><td>Softplus</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01747</td><td>Softplus Function</td><td>Softplus函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01748</td><td>Source Domain</td><td>源领域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01749</td><td>Span</td><td>张成子空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01750</td><td>Sparse</td><td>稀疏</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01751</td><td>Sparse Activation</td><td>稀疏激活</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01752</td><td>Sparse Auto-Encoder</td><td>稀疏自编码器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01753</td><td>Sparse Coding</td><td>稀疏编码</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01754</td><td>Sparse Connectivity</td><td>稀疏连接</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01755</td><td>Sparse Initialization</td><td>稀疏初始化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01756</td><td>Sparse Interactions</td><td>稀疏交互</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01757</td><td>Sparse Representation</td><td>稀疏表示</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01758</td><td>Sparse Weights</td><td>稀疏权重</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01759</td><td>Sparsity</td><td>稀疏性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01760</td><td>Specialization</td><td>特化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01761</td><td>Spectral Clustering</td><td>谱聚类</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01762</td><td>Spectral Radius</td><td>谱半径</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01763</td><td>Speech Recognition</td><td>语音识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a><ahref="https://www.jiqizhixin.com/articles/2018-01-01-3">[4]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-04">[5]</a><ahref="https://www.jiqizhixin.com/articles/2017-12-15">[6]</a></td><td></td></tr><tr class="even"><td>AITD-01764</td><td>Sphering</td><td>Sphering</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01765</td><td>Spike And Slab</td><td>尖峰和平板</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01766</td><td>Spike And Slab RBM</td><td>尖峰和平板RBM</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01767</td><td>Spiking Neural Nets</td><td>脉冲神经网络</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-13-7">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01768</td><td>Splitting Point</td><td>切分点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01769</td><td>Splitting Variable</td><td>切分变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01770</td><td>Spurious Modes</td><td>虚假模态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01771</td><td>Square</td><td>方阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01772</td><td>Square Loss</td><td>平方损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01773</td><td>Squared Euclidean Distance</td><td>欧氏距离平方</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01774</td><td>Squared Exponential</td><td>平方指数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01775</td><td>Squashing Function</td><td>挤压函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01776</td><td>Stability</td><td>稳定性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01777</td><td>Stability-Plasticity Dilemma</td><td>可塑性-稳定性窘境</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01778</td><td>Stable Base Learner</td><td>稳定基学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01779</td><td>Stacked Auto-Encoder</td><td>堆叠自编码器</td><td>SAE</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01780</td><td>Stacked Deconvolutional Network</td><td>堆叠解卷积网络</td><td>SDN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01781</td><td>Stacked Recurrent Neural Network</td><td>堆叠循环神经网络</td><td>SRNN</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01782</td><td>Standard Basis</td><td>标准基</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01783</td><td>Standard Deviation</td><td>标准差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01784</td><td>Standard Error</td><td>标准差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01785</td><td>Standard Normal Distribution</td><td>标准正态分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01786</td><td>Standardization</td><td>标准化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01787</td><td>State</td><td>状态</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01788</td><td>State Action Reward State Action</td><td>SARSA算法</td><td>SARSA</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01789</td><td>State Sequence</td><td>状态序列</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01790</td><td>State Space</td><td>状态空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01791</td><td>State Value Function</td><td>状态值函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01792</td><td>State-Action Value Function</td><td>状态-动作值函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01793</td><td>Statement</td><td>声明</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01794</td><td>Static Computational Graph</td><td>静态计算图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01795</td><td>Static Game</td><td>静态博弈</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01796</td><td>Stationary</td><td>平稳的</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01797</td><td>Stationary Distribution</td><td>平稳分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01798</td><td>Stationary Point</td><td>驻点</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01799</td><td>Statistic Efficiency</td><td>统计效率</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01800</td><td>Statistical Learning</td><td>统计学习</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-01801</td><td>Statistical Learning Theory</td><td>统计学习理论</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01802</td><td>Statistical Machine Learning</td><td>统计机器学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01803</td><td>Statistical Relational Learning</td><td>统计关系学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01804</td><td>Statistical Simulation Method</td><td>统计模拟方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01805</td><td>Statistics</td><td>统计量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01806</td><td>Status Feature Function</td><td>状态特征函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01807</td><td>Steepest Descent</td><td>最速下降法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01808</td><td>Step Decay</td><td>阶梯衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01809</td><td>Stochastic</td><td>随机</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01810</td><td>Stochastic Curriculum</td><td>随机课程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01811</td><td>Stochastic Dynamical System</td><td>随机动力系统</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01812</td><td>Stochastic Gradient Ascent</td><td>随机梯度上升</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01813</td><td>Stochastic Gradient Descent</td><td>随机梯度下降</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-12-25-10">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01814</td><td>Stochastic Gradient Descent With Warm Restarts</td><td>带热重启的随机梯度下降</td><td>SGDR</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01815</td><td>Stochastic Matrix</td><td>随机矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01816</td><td>Stochastic Maximum Likelihood</td><td>随机最大似然</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01817</td><td>Stochastic Neighbor Embedding</td><td>随机近邻嵌入</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01818</td><td>Stochastic Neural Network</td><td>随机神经网络</td><td>SNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01819</td><td>Stochastic Policy</td><td>随机性策略</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01820</td><td>Stochastic Process</td><td>随机过程</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01821</td><td>Stop Words</td><td>停用词</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01822</td><td>Stratified Sampling</td><td>分层采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01823</td><td>Stream</td><td>流</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01824</td><td>Stride</td><td>步幅</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01825</td><td>String Kernel Function</td><td>字符串核函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01826</td><td>Strong Classifier</td><td>强分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01827</td><td>Strong Duality</td><td>强对偶性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01828</td><td>Strongly Connected Graph</td><td>强连通图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01829</td><td>Strongly Learnable</td><td>强可学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01830</td><td>Structural Risk</td><td>结构风险</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01831</td><td>Structural Risk Minimization</td><td>结构风险最小化</td><td>SRM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01832</td><td>Structure Learning</td><td>结构学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01833</td><td>Structured Learning</td><td>结构化学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01834</td><td>Structured Probabilistic Model</td><td>结构化概率模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01835</td><td>Structured Variational Inference</td><td>结构化变分推断</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01836</td><td>Student Network</td><td>学生网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01837</td><td>Sub-Optimal</td><td>次最优</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01838</td><td>Subatomic</td><td>亚原子</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01839</td><td>Subsample</td><td>子采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01840</td><td>Subsampling</td><td>下采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01841</td><td>Subsampling Layer</td><td>子采样层</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01842</td><td>Subset Evaluation</td><td>子集评价</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01843</td><td>Subset Search</td><td>子集搜索</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01844</td><td>Subspace</td><td>子空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01845</td><td>Substitution</td><td>置换</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01846</td><td>Successive Halving</td><td>逐次减半</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01847</td><td>Sum Rule</td><td>求和法则</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01848</td><td>Sum-Product</td><td>和积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01849</td><td>Sum-Product Network</td><td>和-积网络</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01850</td><td>Super-Parent</td><td>超父</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01851</td><td>Supervised</td><td>监督</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01852</td><td>Supervised Learning</td><td>监督学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01853</td><td>Supervised Learning Algorithm</td><td>监督学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01854</td><td>Supervised Model</td><td>监督模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01855</td><td>Supervised Pretraining</td><td>监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01856</td><td>Support Vector</td><td>支持向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01857</td><td>Support Vector Expansion</td><td>支持向量展式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01858</td><td>Support Vector Machine</td><td>支持向量机</td><td>SVM</td><td><a href="https://www.jiqizhixin.com/articles/2017-10-08">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[4]</a></td><td>统计、机器学习</td></tr><tr class="odd"><td>AITD-01859</td><td>Support Vector Regression</td><td>支持向量回归</td><td>SVR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[3]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-01860</td><td>Surrogat Loss</td><td>替代损失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01861</td><td>Surrogate Function</td><td>替代函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01862</td><td>Surrogate Loss Function</td><td>代理损失函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01863</td><td>Symbol</td><td>符号</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01864</td><td>Symbolic Differentiation</td><td>符号微分</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01865</td><td>Symbolic Learning</td><td>符号学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01866</td><td>Symbolic Representation</td><td>符号表示</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01867</td><td>Symbolism</td><td>符号主义</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01868</td><td>Symmetric</td><td>对称</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01869</td><td>Symmetric Matrix</td><td>对称矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01870</td><td>Synonymy</td><td>多词一义性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01871</td><td>Synset</td><td>同义词集</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01872</td><td>Synthetic Feature</td><td>合成特征</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01873</td><td>T-Distribution Stochastic Neighbour Embedding</td><td>T分布随机近邻嵌入</td><td>T-SNE</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01874</td><td>Tabular Value Function</td><td>表格值函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01875</td><td>Tagging</td><td>标注</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01876</td><td>Tangent Distance</td><td>切面距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01877</td><td>Tangent Plane</td><td>切平面</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01878</td><td>Tangent Propagation</td><td>正切传播</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01879</td><td>Target</td><td>目标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01880</td><td>Target Domain</td><td>目标领域</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01881</td><td>Taylor</td><td>泰勒</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01882</td><td>Taylor's Formula</td><td>泰勒公式</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01883</td><td>Teacher Forcing</td><td>强制教学</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01884</td><td>Teacher Network</td><td>教师网络</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01885</td><td>Temperature</td><td>温度</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01886</td><td>Tempered Transition</td><td>回火转移</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01887</td><td>Tempering</td><td>回火</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01888</td><td>Temporal-Difference Learning</td><td>时序差分学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01889</td><td>Tensor</td><td>张量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01890</td><td>Tensor Processing Units</td><td>张量处理单元</td><td>TPU</td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-05-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01891</td><td>Term Frequency-Inverse Document Frequency</td><td>单词频率-逆文本频率</td><td>TF-IDF</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01892</td><td>Terminal State</td><td>终止状态</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01893</td><td>Test Data</td><td>测试数据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01894</td><td>Test Error</td><td>测试误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01895</td><td>Test Sample</td><td>测试样本</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01896</td><td>Test Set</td><td>测试集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01897</td><td>The Collider Case</td><td>碰撞情况</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01898</td><td>Threshold</td><td>阈值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-01899</td><td>Threshold Logic Unit</td><td>阈值逻辑单元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01900</td><td>Threshold-Moving</td><td>阈值移动</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01901</td><td>Tied Weight</td><td>捆绑权重</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01902</td><td>Tikhonov Regularization</td><td>Tikhonov正则化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01903</td><td>Tiled Convolution</td><td>平铺卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01904</td><td>Time Delay Neural Network</td><td>时延神经网络</td><td>TDNN</td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01905</td><td>Time Homogenous Markov Chain</td><td>时间齐次马尔可夫链</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01906</td><td>Time Step</td><td>时间步</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01907</td><td>Toeplitz Matrix</td><td>Toeplitz矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01908</td><td>Token</td><td>词元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01909</td><td>Tokenize</td><td>词元化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01910</td><td>Tokenization</td><td>词元化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01911</td><td>Tokenizer</td><td>词元分析器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01912</td><td>Tolerance</td><td>容差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01913</td><td>Top-Down</td><td>自顶向下</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-01914</td><td>Topic</td><td>话题</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01915</td><td>Topic Model</td><td>话题模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01916</td><td>Topic Modeling</td><td>话题分析</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01917</td><td>Topic Vector Space</td><td>话题向量空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01918</td><td>Topic Vector Space Model</td><td>话题向量空间模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01919</td><td>Topic-Document Matrix</td><td>话题-文本矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01920</td><td>Topographic ICA</td><td>地质ICA</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01921</td><td>Total Cost</td><td>总体代价</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01922</td><td>Trace</td><td>迹</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01923</td><td>Tractable</td><td>易处理的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01924</td><td>Training</td><td>训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01925</td><td>Training Data</td><td>训练数据</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01926</td><td>Training Error</td><td>训练误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01927</td><td>Training Instance</td><td>训练实例</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01928</td><td>Training Sample</td><td>训练样本</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-01929</td><td>Training Set</td><td>训练集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01930</td><td>Trajectory</td><td>轨迹</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01931</td><td>Transcribe</td><td>转录</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01932</td><td>Transcription System</td><td>转录系统</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01933</td><td>Transductive Learning</td><td>直推学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01934</td><td>Transductive Transfer Learning</td><td>直推迁移学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01935</td><td>Transfer Learning</td><td>迁移学习</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-04-7">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[4]</a></td><td></td></tr><tr class="even"><td>AITD-01936</td><td>Transform</td><td>变换</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01937</td><td>Transformer</td><td>Transformer</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01938</td><td>Transformer Model</td><td>Transformer模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01939</td><td>Transition</td><td>转移</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01940</td><td>Transition Kernel</td><td>转移核</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01941</td><td>Transition Matrix</td><td>状态转移矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01942</td><td>Transition Probability</td><td>转移概率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01943</td><td>Transpose</td><td>转置</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01944</td><td>Transposed Convolution</td><td>转置卷积</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01945</td><td>Tree-Structured LSTM</td><td>树结构的长短期记忆模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01946</td><td>Treebank</td><td>树库</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01947</td><td>Trial</td><td>试验</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01948</td><td>Trial And Error</td><td>试错</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01949</td><td>Triangle Inequality</td><td>三角不等式</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01950</td><td>Triangular Cyclic Learning Rate</td><td>三角循环学习率</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01951</td><td>Triangulate</td><td>三角形化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01952</td><td>Triangulated Graph</td><td>三角形化图</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01953</td><td>Trigram</td><td>三元语法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01954</td><td>True Negative</td><td>真负例</td><td>TN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-01955</td><td>True Positive</td><td>真正例</td><td>TP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-01956</td><td>True Positive Rate</td><td>真正例率</td><td>TPR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-01957</td><td>Truncated Singular Value Decomposition</td><td>截断奇异值分解</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01958</td><td>Truncation Error</td><td>截断误差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01959</td><td>Turing Completeness</td><td>图灵完备</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01960</td><td>Turing Machine</td><td>图灵机</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-04-11-7">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-01961</td><td>Twice-Learning</td><td>二次学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01962</td><td>Two-Dimensional Array</td><td>二维数组</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01963</td><td>Ugly Duckling Theorem</td><td>丑小鸭定理</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01964</td><td>Unbiased</td><td>无偏</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01965</td><td>Unbiased Estimate</td><td>无偏估计</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01966</td><td>Unbiased Sample Variance</td><td>无偏样本方差</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01967</td><td>Unconstrained Optimization</td><td>无约束优化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01968</td><td>Undercomplete</td><td>欠完备</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01969</td><td>Underdetermined</td><td>欠定的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01970</td><td>Underestimation</td><td>欠估计</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01971</td><td>Underfitting</td><td>欠拟合</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-01972</td><td>Underfitting Regime</td><td>欠拟合机制</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01973</td><td>Underflow</td><td>下溢</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01974</td><td>Underlying</td><td>潜在</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01975</td><td>Underlying Cause</td><td>潜在成因</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01976</td><td>Undersampling</td><td>欠采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01977</td><td>Understandability</td><td>可理解性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01978</td><td>Undirected</td><td>无向</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01979</td><td>Undirected Graph</td><td>无向图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01980</td><td>Undirected Graphical Model</td><td>无向图模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01981</td><td>Undirected Model</td><td>无向模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01982</td><td>Unequal Cost</td><td>非均等代价</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01983</td><td>Unfolded Graph</td><td>展开图</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01984</td><td>Unfolding</td><td>展开</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01985</td><td>Unidirectional Language Model</td><td>单向语言模型</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01986</td><td>Unification</td><td>合一</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01987</td><td>Uniform Distribution</td><td>均匀分布</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01988</td><td>Uniform Sampling</td><td>均匀采样</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01989</td><td>Uniform Stability</td><td>均匀稳定性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01990</td><td>Unigram</td><td>一元语法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01991</td><td>Unimodal</td><td>单峰值</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01992</td><td>Unit</td><td>单元</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01993</td><td>Unit Norm</td><td>单位范数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01994</td><td>Unit Test</td><td>单元测试</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01995</td><td>Unit Variance</td><td>单位方差</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01996</td><td>Unit Vector</td><td>单位向量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01997</td><td>Unit-Step Function</td><td>单位阶跃函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-01998</td><td>Unitary Matrix</td><td>酉矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-01999</td><td>Univariate Decision Tree</td><td>单变量决策树</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02000</td><td>Universal Approximation Theorem</td><td>通用近似定理</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02001</td><td>Universal Approximator</td><td>通用近似器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02002</td><td>Universal Function Approximator</td><td>通用函数近似器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02003</td><td>Unknown Token</td><td>未知词元</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02004</td><td>Unlabeled</td><td>未标记</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02005</td><td>Unnormalized Probability Function</td><td>未规范化概率函数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02006</td><td>Unprojection</td><td>反投影</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02007</td><td>Unshared Convolution</td><td>非共享卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02008</td><td>Unsupervised</td><td>无监督</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02009</td><td>Unsupervised Feature Learning</td><td>无监督特征学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02010</td><td>Unsupervised Layer-Wise Training</td><td>无监督逐层训练</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02011</td><td>Unsupervised Learning Algorithm</td><td>无监督学习算法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02012</td><td>Unsupervised Learning</td><td>无监督学习</td><td>UL</td><td><a href="https://www.jiqizhixin.com/articles/2017-11-17-5">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="odd"><td>AITD-02013</td><td>Unsupervised Pretraining</td><td>无监督预训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02014</td><td>Update Gate</td><td>更新门</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02015</td><td>Update Model Parameter</td><td>迭代模型参数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02016</td><td>Upper Confidence Bounds</td><td>上置信界限</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02017</td><td>Upsampling</td><td>上采样</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02018</td><td>V-Structure</td><td>V型结构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02019</td><td>Valid</td><td>有效</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02020</td><td>Validation Set</td><td>验证集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02021</td><td>Validity Index</td><td>有效性指标</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02022</td><td>Value Function</td><td>价值函数</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02023</td><td>Value Function Approximation</td><td>值函数近似</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02024</td><td>Value Iteration</td><td>值迭代</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02025</td><td>Vanishing And Exploding Gradient Problem</td><td>梯度消失与爆炸问题</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02026</td><td>Vanishing Gradient</td><td>梯度消失</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02027</td><td>Vanishing Gradient Problem</td><td>梯度消失问题</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2018-01-07-2">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02028</td><td>Vapnik-Chervonenkis Dimension</td><td>VC维</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02029</td><td>Variable Elimination</td><td>变量消去</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02030</td><td>Variance</td><td>方差</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02031</td><td>Variance Reduction</td><td>方差减小</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02032</td><td>Variance Scaling</td><td>方差缩放</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02033</td><td>Variational Autoencoder</td><td>变分自编码器</td><td>VAE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02034</td><td>Variational Bayesian</td><td>变分贝叶斯</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02035</td><td>Variational Derivative</td><td>变分导数</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02036</td><td>Variational Distribution</td><td>变分分布</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02037</td><td>Variational Dropout</td><td>变分暂退法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02038</td><td>Variational EM Algorithm</td><td>变分EM算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02039</td><td>Variational Free Energy</td><td>变分自由能</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02040</td><td>Variational Inference</td><td>变分推断</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02041</td><td>Vector</td><td>向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02042</td><td>Vector Space</td><td>向量空间</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02043</td><td>Vector Space Model</td><td>向量空间模型</td><td>VSM</td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02044</td><td>Vectorization</td><td>向量化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02045</td><td>Version Space</td><td>版本空间</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02046</td><td>Virtual Adversarial Example</td><td>虚拟对抗样本</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02047</td><td>Virtual Adversarial Training</td><td>虚拟对抗训练</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02048</td><td>Visible Layer</td><td>可见层</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02049</td><td>Visible Variable</td><td>可见变量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02050</td><td>Viterbi Algorithm</td><td>维特比算法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02051</td><td>Vocabulary</td><td>词表</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02052</td><td>Von Neumann Architecture</td><td>冯 · 诺伊曼架构</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02053</td><td>Voted Perceptron</td><td>投票感知器</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02054</td><td>Wake Sleep</td><td>醒眠</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02055</td><td>Warp</td><td>线程束</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02056</td><td>Wasserstein Distance</td><td>Wasserstein距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02057</td><td>Wasserstein GAN</td><td>Wasserstein生成对抗网络</td><td>WGAN</td><td><ahref="https://www.jiqizhixin.com/articles/2017-10-05">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02058</td><td>Weak Classifier</td><td>弱分类器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02059</td><td>Weak Duality</td><td>弱对偶性</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02060</td><td>Weak Learner</td><td>弱学习器</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02061</td><td>Weakly Learnable</td><td>弱可学习</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02062</td><td>Weakly Supervised Learning</td><td>弱监督学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02063</td><td>Weight</td><td>权重</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2018-01-08-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02064</td><td>Weight Decay</td><td>权重衰减</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02065</td><td>Weight Normalization</td><td>权重规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02066</td><td>Weight Scaling Inference Rule</td><td>权重比例推断规则</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02067</td><td>Weight Sharing</td><td>权共享</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02068</td><td>Weight Space Symmetry</td><td>权重空间对称性</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02069</td><td>Weight Vector</td><td>权值向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02070</td><td>Weighted Distance</td><td>加权距离</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02071</td><td>Weighted Voting</td><td>加权投票</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02072</td><td>Whitening</td><td>白化</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02073</td><td>Wide Convolution</td><td>宽卷积</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02074</td><td>Width</td><td>宽度</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02075</td><td>Winner-Take-All</td><td>胜者通吃</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02076</td><td>Within-Class Scatter Matrix</td><td>类内散度矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02077</td><td>Word Embedding</td><td>词嵌入</td><td></td><td><a href="https://www.jiqizhixin.com/articles/2017-11-20-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02078</td><td>Word Sense Disambiguation</td><td>词义消歧</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02079</td><td>Word Vector</td><td>词向量</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02080</td><td>Word Vector Space Model</td><td>单词向量空间模型</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02081</td><td>Word-Document Matrix</td><td>单词-文本矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02082</td><td>Word-Topic Matrix</td><td>单词-话题矩阵</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02083</td><td>Working Memory</td><td>工作记忆</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02084</td><td>Wrapper Method</td><td>包裹式方法</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02085</td><td>Z-Score Normalization</td><td>Z值规范化</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02086</td><td>Zero Mean</td><td>零均值</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02087</td><td>Zero Padding</td><td>零填充</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02088</td><td>Zero Tensor</td><td>零张量</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02089</td><td>Zero-Centered</td><td>零中心化的</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02090</td><td>Zero-Data Learning</td><td>零数据学习</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02091</td><td>Zero-Shot Learning</td><td>零试学习</td><td></td><td><ahref="https://www.jiqizhixin.com/articles/2017-03-31-6">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02092</td><td>Zipf's Law</td><td>齐普夫定律</td><td></td><td>[1]</td><td></td></tr><tr class="odd"><td>AITD-02093</td><td>ε-Greedy Method</td><td>ε-贪心法</td><td></td><td>[1]</td><td></td></tr><tr class="even"><td>AITD-02094</td><td>2D Qsar Models</td><td>二维定量构效关系模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="odd"><td>AITD-02095</td><td>3D Cartesian</td><td>三维笛卡尔（坐标）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="even"><td>AITD-02096</td><td>3D Conformation</td><td>三维构象</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>化学、生化</td></tr><tr class="odd"><td>AITD-02097</td><td>3D Grids</td><td>三维（坐标）网格</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02098</td><td>3D Qsar Models</td><td>三维定量构效关系模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="odd"><td>AITD-02099</td><td>Aberration-Corrected</td><td>像差矫正</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-02100</td><td>Active Machine Learning</td><td>主动机器学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02101</td><td>Adaptive Fuzzy Neural Network</td><td>自适应模糊神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02102</td><td>Adaptive Sampling</td><td>自适应采样</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02103</td><td>Admet Evaluation</td><td>毒性评估</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="even"><td>AITD-02104</td><td>Alexnet</td><td>AlexNet</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02105</td><td>Alphago</td><td>阿尔法狗</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02106</td><td>Adaptive Neuro Fuzzy Inference System</td><td>自适应神经模糊推理系统</td><td>ANFIS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02107</td><td>Approximate Probabilistic Models</td><td>近似概率模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02108</td><td>Artificial Neurons</td><td>人工神经元</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02109</td><td>Artificial Synapses</td><td>人工突触</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02110</td><td>Attention-Based</td><td>基于注意力（机制）的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02111</td><td>Automating Synthetic Planning</td><td>自动化综合规划</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02112</td><td>Automation</td><td>自动化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02113</td><td>Autonomous Decision-Making</td><td>自主决策</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02114</td><td>B-Clustering Algorithms</td><td>B树聚类算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02115</td><td>Balanced Accuracy</td><td>平衡精度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02116</td><td>Bandgap Energy</td><td>带隙能量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="odd"><td>AITD-02117</td><td>Baseline Test</td><td>基准测试</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02118</td><td>Basin Hopping</td><td>盆地跳跃（算法）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02119</td><td>Bayesian Approach</td><td>贝叶斯方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-02120</td><td>Bayesian Induction</td><td>贝叶斯归纳</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02121</td><td>Bayesian Mcmc Methods</td><td>贝叶斯马尔可夫链蒙特卡洛方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-02122</td><td>Bayesian Methods</td><td>贝叶斯方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02123</td><td>Bayesian Molecular</td><td>贝叶斯分子（设计方法）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td>统计，机器学习，化学</td></tr><tr class="even"><td>AITD-02124</td><td>Bayesian Prior</td><td>贝叶斯先验</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02125</td><td>Bayesian Program Learning</td><td>贝叶斯程序学习</td><td>BPL</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="even"><td>AITD-02126</td><td>Bayesian Regularized Neural Network</td><td>贝叶斯正则化神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td>统计，机器学习</td></tr><tr class="odd"><td>AITD-02127</td><td>Beam-Scanning</td><td>波束扫描</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-02128</td><td>Best Separates</td><td>最优分离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02129</td><td>Biased Dataset</td><td>有偏数据集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02130</td><td>Bit Collisions</td><td>字节碰撞/冲突</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td>数据库</td></tr><tr class="odd"><td>AITD-02131</td><td>Black Box</td><td>黑盒子</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02132</td><td>Black-Box Attack</td><td>黑盒攻击</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02133</td><td>Bonding Environments</td><td>成键环境</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02134</td><td>Bonferroni Correction</td><td>邦弗朗尼校正</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02135</td><td>Bootstrap Aggregation</td><td>引导聚合</td><td>bagging</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02136</td><td>Broyden–Fletcher–Goldfarb–Shanno</td><td>BFGS（算法）</td><td>BFGS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>一种拟牛顿法，数学计算</td></tr><tr class="odd"><td>AITD-02137</td><td>Buchwald−Hartwig Cross-Coupling</td><td>Buchwald–Hartwig 偶联（反应）</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>化学</td></tr><tr class="even"><td>AITD-02138</td><td>C4.5 Algorithm</td><td>C4.5 算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>一种决策树算法，数据挖掘</td></tr><tr class="odd"><td>AITD-02139</td><td>Calculation Uncertainties</td><td>计算不确定性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02140</td><td>Canonical Ml Methods</td><td>经典机器学习方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02141</td><td>Cartesian Distance Vector</td><td>笛卡尔距离向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02142</td><td>CASP</td><td>国际蛋白质结构预测竞赛</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>生物</td></tr><tr class="odd"><td>AITD-02143</td><td>Categorical Data</td><td>分类数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02144</td><td>Categorization Algorithms</td><td>分类算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02145</td><td>ChemDataExtractor</td><td>化学数据提取器</td><td>CDE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02146</td><td>Chi-Squared</td><td>卡方（分布）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02147</td><td>Classification Model</td><td>分类模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02148</td><td>Cluster Resolution Feature Selection</td><td>聚类分辨率特征选择</td><td>CR-FS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02149</td><td>Cluster-Based Splitting</td><td>基于聚类的分离方法</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02150</td><td>Clustering Methods</td><td>聚类方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02151</td><td>Code Pipeline</td><td>代码流水线</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02152</td><td>Coefficient of Determination</td><td>决定系数</td><td>r^2 or R^2</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02153</td><td>Combined Gradient</td><td>组合梯度（算法）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02154</td><td>Complex Data</td><td>复合数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02155</td><td>Computational Cost</td><td>计算成本</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02156</td><td>Computational Optimisation</td><td>计算优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02157</td><td>Computational Science</td><td>计算科学</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02158</td><td>Computational Toxicology</td><td>计算毒理学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02159</td><td>Computer Science</td><td>计算机科学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02160</td><td>Computer Simulations</td><td>计算机模拟</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02161</td><td>Computer-Aided</td><td>计算机辅助</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02162</td><td>Constraint</td><td>约束</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02163</td><td>Core-Loss Spectrum</td><td>（电子能量损失谱中的）高能区域</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02164</td><td>Coulomb Matrix</td><td>库仑矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02165</td><td>Coupled-Cluster Predictions</td><td>耦合簇预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02166</td><td>Cross-Validated Coefficient of Determination</td><td>交叉验证的决定系数</td><td>q^2 or Q^2</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02167</td><td>Cross-Validation</td><td>交叉验证</td><td>CV</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02168</td><td>Crowd-Sourcing</td><td>众包</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>商业模式</td></tr><tr class="odd"><td>AITD-02169</td><td>Cut-Points</td><td>切点</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02170</td><td>Cutoff Radial Function</td><td>截断径向函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02171</td><td>Data Availability</td><td>数据可用性</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02172</td><td>Data Cleaning</td><td>数据清洗</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02173</td><td>Data Collection</td><td>数据采集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02174</td><td>Data Considerations</td><td>数据注意事项</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02175</td><td>Data Curation</td><td>数据监管</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02176</td><td>Data Disparity</td><td>数据差异</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02177</td><td>Data Dredging</td><td>数据挖掘</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02178</td><td>Data Imputation</td><td>数据填补</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02179</td><td>Data Labels</td><td>数据标签</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02180</td><td>Data Leakage</td><td>数据泄露</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02181</td><td>Data Pre-Processing</td><td>数据预处理</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02182</td><td>Data Processing</td><td>数据处理</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02183</td><td>Data Quality</td><td>数据质量</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02184</td><td>Data Reduction</td><td>数据缩减</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02185</td><td>Data Representation</td><td>数据表示</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02186</td><td>Data Selection</td><td>数据选择</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02187</td><td>Data Sources</td><td>数据源</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02188</td><td>Data Splitting</td><td>数据拆分</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02189</td><td>Data Transformation</td><td>数据转换</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02190</td><td>Data-Driven</td><td>数据驱动</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02191</td><td>Data-Driven Decision-Making</td><td>数据驱动的决策</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02192</td><td>Data-Driven Methods</td><td>数据驱动的方法</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02193</td><td>Data-Driven Spectral Analysis</td><td>数据驱动的光谱分析</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02194</td><td>Data-Mining</td><td>数据挖掘</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02195</td><td>Database</td><td>数据库</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02196</td><td>DE Algorithm</td><td>差分进化算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02197</td><td>Deeplift</td><td>DeepLift模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02198</td><td>Dendrogram</td><td>树状图</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02199</td><td>Density Functional Theory</td><td>密度泛函理论</td><td>DFT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[3]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[4]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[5]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[6]</a></td><td></td></tr><tr class="even"><td>AITD-02200</td><td>Density-Based Spatial Clustering Of Applications With Noise</td><td>DBSCAN密度聚类</td><td>DBSCAN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02201</td><td>Descriptor</td><td>描述符</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02202</td><td>DFT Calculations</td><td>DFT计算</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02203</td><td>Dice Similarity</td><td>戴斯相似度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02204</td><td>Differential Evolution</td><td>差分进化</td><td>DE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02205</td><td>Dimensionality Reduction</td><td>降维</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02206</td><td>Direct Neural Network Modeling</td><td>正向神经网络建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02207</td><td>Discrete Manner</td><td>离散方式</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02208</td><td>Discrete Quanta</td><td>离散量子</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02209</td><td>Discretization</td><td>离散化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02210</td><td>Distillation</td><td>蒸馏</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02211</td><td>Dynamic Datasets</td><td>动态数据集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02212</td><td>Dynamic Filter Networks</td><td>动态过滤网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02213</td><td>Dynamic Sampling</td><td>动态采样</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02214</td><td>Dynamics Simulations</td><td>动力学模拟</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02215</td><td>Eigenfunction</td><td>特征函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02216</td><td>Electronegativity</td><td>电负性</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02217</td><td>Elman</td><td>埃尔曼</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02218</td><td>Empirical Models</td><td>经验模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02219</td><td>Energy Derivatives</td><td>能源衍生品</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>在DP模型中：能量的导数</td></tr><tr class="even"><td>AITD-02220</td><td>Energy Potentials</td><td>能量潜力</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02221</td><td>Ensemble Methods</td><td>集成方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://www.nature.com/articles/s41557-021-00716-z">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02222</td><td>Entity Normalisation</td><td>实体规范化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02223</td><td>Ethical Considerations</td><td>道德考虑</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02224</td><td>Euclidean Distances</td><td>欧几里得距离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02225</td><td>Evolutionary Algorithms</td><td>进化算法</td><td>EA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02226</td><td>Evolutionary Method</td><td>进化方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02227</td><td>Exchange–Correlation</td><td>交换关联（的能量/泛函）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02228</td><td>Excited-State Potentials</td><td>激发态能量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02229</td><td>Expected Reduction In Distortion</td><td>符合预期的失真减少</td><td>ERD</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02230</td><td>Experimental Validation Data</td><td>实验验证数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02231</td><td>Expert Systems</td><td>专家系统</td><td>ESS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02232</td><td>Extended-Connectivity Circular Fingerprint</td><td>扩展连接环形指纹</td><td>ECFP</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02233</td><td>Extraction Techniques</td><td>提取技术</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02234</td><td>Faber-Christensen-Huang-Lilienfeld</td><td>Faber-Christensen-Huang-Lilienfeld</td><td>FCHL</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>四个人提出的化学结构量子机器学习方法</td></tr><tr class="odd"><td>AITD-02235</td><td>Facial Recognition</td><td>面部识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02236</td><td>FAIR Data Principles</td><td>FAIR数据原则</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>Findability可找寻 Accessibility可访问 Interoperability可交互Reuse可再用</td></tr><tr class="odd"><td>AITD-02237</td><td>False Negatives</td><td>假阴性</td><td>FNs</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02238</td><td>False Positives</td><td>假阳性</td><td>FPs</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02239</td><td>Fchl Representation</td><td>Fchl 表示</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02240</td><td>Feature Binarization</td><td>特征二值化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02241</td><td>Feature Transform</td><td>特征变换</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02242</td><td>Feature Vectors</td><td>特征向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02243</td><td>Features</td><td>特征</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02244</td><td>Feed Back</td><td>反馈</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02245</td><td>Feed-Forward Neural Networks</td><td>前馈神经网络</td><td>FFNN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02246</td><td>Feedback Structure</td><td>反馈结构</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02247</td><td>Final Evaluation</td><td>最终评估</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02248</td><td>Findable, Accessible, Interoperable, Reusable</td><td>可查找、可访问、可互操作、可重用</td><td>FAIR</td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02249</td><td>First-Principles</td><td>第一性原理</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02250</td><td>Flow Rate</td><td>流速</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02251</td><td>Forward Cross-Validation</td><td>前向交叉验证</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02252</td><td>Forward Prediction</td><td>前向预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02253</td><td>Forward Reaction Prediction</td><td>前向反应预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02254</td><td>Fuzzy Logic</td><td>模糊逻辑</td><td>FL</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02255</td><td>Fuzzy Neural Networks</td><td>模糊神经网络</td><td>FNN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02256</td><td>Ga-Based Approaches</td><td>基于遗传算法的方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02257</td><td>Garbage In, Garbage Out</td><td>无用数据入、无用数据出</td><td>GIGO</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02258</td><td>Gas-Phase Networks</td><td>气相网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02259</td><td>Gaussian Kernels</td><td>高斯核</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02260</td><td>Gaussian-Type Structure Descriptors</td><td>高斯型结构描述符</td><td>GTSD</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02261</td><td>General Intelligence</td><td>通用智能</td><td>GI</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02262</td><td>Generalized Gradient Approximation</td><td>广义梯度近似</td><td>GGA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02263</td><td>Generative Adversarial Networks</td><td>生成对抗网络</td><td>GAN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02264</td><td>Gradient Boosting Decision Tree</td><td>梯度提升决策树</td><td>GBDT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02265</td><td>Gradient-Based</td><td>基于梯度的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02266</td><td>Grain-Surface Networks</td><td>粒面网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02267</td><td>Graph Convolutional</td><td>图卷积</td><td>GC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02268</td><td>Graph Models</td><td>图模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02269</td><td>Graph Neural Networks</td><td>图神经网络</td><td>GNNS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02270</td><td>Graph-Based</td><td>基于图形</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02271</td><td>Graph-Based Models</td><td>基于图的模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02272</td><td>Graph-Based Neural Networks</td><td>基于图的神经网络</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02273</td><td>Graph-Based Representation</td><td>基于图的表示</td><td>GB-GA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02274</td><td>Graph-Convolutional Neural Network</td><td>图卷积神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02275</td><td>Graphics Processing Units</td><td>图形处理器</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02276</td><td>Gravimetric Polymerization Degree</td><td>比重聚合度</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02277</td><td>Hamiltonian Matrix</td><td>哈密顿矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="even"><td>AITD-02278</td><td>Hamiltonian Operator</td><td>哈密顿算符</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="odd"><td>AITD-02279</td><td>Heterogeneous Data</td><td>异构数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[2]</a></td><td></td></tr><tr class="even"><td>AITD-02280</td><td>Hidden Layers</td><td>隐藏层</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02281</td><td>High Data Throughput</td><td>高数据吞吐量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02282</td><td>High Throughput</td><td>高通量</td><td>HT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02283</td><td>High Throughput Screening</td><td>高通量筛选</td><td>HTS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02284</td><td>High Variance Models</td><td>高方差模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02285</td><td>High-Dimensional Data</td><td>高维数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02286</td><td>High-Dimensional NN</td><td>高维神经网络</td><td>HDNN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02287</td><td>High-Dimensional Objects</td><td>高维对象</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02288</td><td>High-Throughput</td><td>高通量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02289</td><td>Higher-Dimensional Space</td><td>高维空间</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="even"><td>AITD-02290</td><td>Higher-Dimensional Spectral Space</td><td>高维光谱空间</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02291</td><td>Homogenization</td><td>同质化</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02292</td><td>Homomorphic Encryption</td><td>同态加密</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02293</td><td>Human Face Recognition</td><td>人脸识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02294</td><td>Human-Encoded</td><td>人工编码的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02295</td><td>Hybrid Model</td><td>混合模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02296</td><td>Hybrid Technique</td><td>混合技术</td><td>HM</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02297</td><td>Hybrid-Neural Model</td><td>混合神经模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02298</td><td>Hyperparameter Opimization</td><td>超参数优化</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02299</td><td>Hyperparameters</td><td>超参数</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[3]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02300</td><td>Hyperplanes Separate</td><td>超平面分离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02301</td><td>Id3 Algorithm</td><td>Id3 算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02302</td><td>Image And Speech Recognition</td><td>图像和语音识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02303</td><td>Image Classification</td><td>图像分类</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02304</td><td>Image Classifier</td><td>图像分类器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02305</td><td>Image Recognition</td><td>图像识别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02306</td><td>Informative Priors</td><td>信息先验</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02307</td><td>Input-Output Pairs</td><td>输入输出对</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02308</td><td>Instance-Based</td><td>基于实例的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02309</td><td>Intelligent Machine</td><td>智能机器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02310</td><td>Intermediate Neurons</td><td>中间神经元</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02311</td><td>Internet Of Things</td><td>物联网</td><td>IoT</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02312</td><td>Interpolation Coordinate</td><td>插值坐标</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02313</td><td>Interpretability</td><td>可解释性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02314</td><td>Inverse Neural Modeling</td><td>逆神经建模</td><td>INN</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02315</td><td>Inverse Neural Network Modeling</td><td>逆神经网络建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02316</td><td>Iterative Learning</td><td>迭代学习</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02317</td><td>Joint Distribution</td><td>联合分布</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02318</td><td>Jordan-Elman Neural Networks</td><td>Jordan-Elman 神经网络</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02319</td><td>K Clusters</td><td>K聚类</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02320</td><td>K Nearest Points</td><td>K 最近点</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02321</td><td>K-1 Folds</td><td>K-1 折</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02322</td><td>K-Edge (O-K Edge)</td><td>K-边缘（O-K 边缘）</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02323</td><td>K-Means</td><td>K-均值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02324</td><td>Kendall’S Tau</td><td>肯德尔等级相关系数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02325</td><td>Kernel Ridge Regression</td><td>核岭回归</td><td>KRR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[3]</a></td><td></td></tr><tr class="even"><td>AITD-02326</td><td>Kernels</td><td>内核</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02327</td><td>Kinetic Curve</td><td>动力学曲线</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02328</td><td>KNN Model</td><td>K 近邻模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02329</td><td>Knowledge Extraction</td><td>知识提取</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02330</td><td>Knowledge Gradient</td><td>知识梯度</td><td>KG</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02331</td><td>L1 And L2 Regularization</td><td>L1与L2正则化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02332</td><td>Laboratory Level</td><td>实验室级别</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02333</td><td>Language Processing</td><td>语言处理</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02334</td><td>Laplacian Prior</td><td>拉普拉斯先验</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02335</td><td>Large-Scale Data Storage</td><td>大规模数据存储</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02336</td><td>Lasers</td><td>激光器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02337</td><td>Lasso Regression</td><td>拉索回归</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02338</td><td>LBP</td><td>局部二值模式</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02339</td><td>Least Absolute Shrinkage And Selection Operator</td><td>Lasso回归</td><td>LASSO</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02340</td><td>Least Square Support Vector Machine</td><td>最小二乘支持向量机</td><td>LSSVM</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02341</td><td>Ligand-Field</td><td>配位场</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02342</td><td>Linear</td><td>线性的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[2]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-02343</td><td>Linear Dimension Reduction Methods</td><td>线性降维方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02344</td><td>Linear Vibronic Coupling Model</td><td>线性振子耦合模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02345</td><td>Local Recurrent</td><td>本地卷积</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02346</td><td>Logic And Heuristics Applied To Synthetic Analysis</td><td>LHASA 程序</td><td>LHASA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02347</td><td>Long-Range Prediction</td><td>长期预测</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02348</td><td>Long-Range Prediction Models</td><td>长期预测模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02349</td><td>Long-Term Planning</td><td>长期规划</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02350</td><td>Long-Term Reward</td><td>长期回报</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02351</td><td>Machine-Readable Data</td><td>机器可读的数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02352</td><td>Mae</td><td>平均绝对误差</td><td>MAE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02353</td><td>Mahalanobis Distances</td><td>马氏距离</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02354</td><td>Matrices</td><td>矩阵</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>数学</td></tr><tr class="odd"><td>AITD-02355</td><td>Matthews Correlation Coefficient</td><td>马修斯相关系数</td><td>MCC</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02356</td><td>Maximum Likelihood Methods</td><td>最大似然法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02357</td><td>Maximum Likelihood Procedures</td><td>最大似然估计法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02358</td><td>MCTS Method</td><td>蒙特卡洛树搜索方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02359</td><td>Mean-Squared Error</td><td>均方误差</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>统计、机器学习</td></tr><tr class="even"><td>AITD-02360</td><td>Mechanical Sympathy</td><td>机械同感，软硬件协同编程</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02361</td><td>Merging</td><td>合并</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02362</td><td>Message Passing Neural Networks</td><td>消息传递神经网络</td><td>MPNNS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02363</td><td>Microarray Data</td><td>微阵列数据</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02364</td><td>Mini Batch</td><td>小批次</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02365</td><td>Mining</td><td>挖掘</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02366</td><td>Mining Out</td><td>挖掘</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02367</td><td>Missing Values</td><td>缺失值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02368</td><td>ML Algorithm</td><td>机器学习算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02369</td><td>ML Modelling</td><td>机器学习建模</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02370</td><td>ML Potentials</td><td>机器学习势能</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02371</td><td>ML-Driven</td><td>机器学习驱动的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02372</td><td>ML-Driven Optimization</td><td>机器学习驱动的最优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02373</td><td>MLP Neural Model</td><td>多层感知机神经模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02374</td><td>Model Construction</td><td>模型构建</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02375</td><td>Model Evaluation</td><td>模型评估</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02376</td><td>Model Performance</td><td>模型性能</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02377</td><td>Model Statistics</td><td>模型统计</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02378</td><td>Model Training</td><td>模型训练</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02379</td><td>Model Validation</td><td>模型验证</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02380</td><td>Model-Based Iterative Reconstruction</td><td>基于模型的迭代重建</td><td>MBIR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02381</td><td>Model-Construction</td><td>模型构建</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02382</td><td>Modelling Scenario</td><td>建模场景</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02383</td><td>Molecular Graph Theory</td><td>分子图论</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02384</td><td>Molecular Modelling</td><td>分子建模</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02385</td><td>Monte Carlo Tree Search</td><td>蒙特卡洛树搜索</td><td>MCTS</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[2]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[3]</a></td><td>数学</td></tr><tr class="even"><td>AITD-02386</td><td>Moore’S Law</td><td>摩尔定律</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00512/978-1-78801-789-3">[1]</a></td><td>计算机</td></tr><tr class="odd"><td>AITD-02387</td><td>ms-QSBER-EL Model</td><td>基于人工神经网络组合的结构生物学效应定量关系多尺度模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02388</td><td>Multi-Agent Control System</td><td>多智能体控制系统</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02389</td><td>Multi-Core Desktop Computer</td><td>多核台式计算机</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>计算机</td></tr><tr class="even"><td>AITD-02390</td><td>Multi-Dimensional Big Data Analysis</td><td>多维度大数据分析</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00424/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02391</td><td>Multi-Layer Feed-Forward</td><td>多层前馈</td><td>MLFF</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02392</td><td>Multi-Objective Genetic Algorithm</td><td>多目标遗传算法</td><td>MOGA</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02393</td><td>Multi-Objective Optimization</td><td>多目标优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02394</td><td>Multi-Reaction Synthesis</td><td>多反应合成</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02395</td><td>Multilayer Perceptron</td><td>多层感知机</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02396</td><td>Multivariate Regression</td><td>多变量回归</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02397</td><td>N-Dimensional Space</td><td>N维空间</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02398</td><td>Naive Bayesian</td><td>朴素贝叶斯</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="odd"><td>AITD-02399</td><td>Naive Bayesian Methods</td><td>朴素贝叶斯方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02400</td><td>Named Entity Recognition，NER</td><td>命名实体识别</td><td>NER</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00280/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02401</td><td>Nearest Neighbors</td><td>近邻</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02402</td><td>Nearest Neighbour Model</td><td>近邻模型</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02403</td><td>Negative Predictive Value</td><td>阴性预测值</td><td>NPV</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02404</td><td>Network Architecture</td><td>网络结构</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02405</td><td>Network Geometry</td><td>网络几何</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02406</td><td>Neural Turing Machines</td><td>神经图灵机</td><td>NTM</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[2]</a></td><td></td></tr><tr class="odd"><td>AITD-02407</td><td>Neural-Network-Based Function</td><td>基于神经网络的函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02408</td><td>Neurons</td><td>神经元</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02409</td><td>Nuclear Magnetic Resonance</td><td>核磁共振</td><td>NMR</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02410</td><td>Noise Filters</td><td>噪声过滤器</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02411</td><td>Noise-Free</td><td>无噪的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02412</td><td>Non-Linear</td><td>非线性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>数学、统计</td></tr><tr class="odd"><td>AITD-02413</td><td>Non-Linear Correlation</td><td>非线性相关</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00195/978-1-83916-023-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02414</td><td>Non-Linearity</td><td>非线性</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02415</td><td>Non-Parametric Algorithm</td><td>非参数化学习算法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02416</td><td>Non-Safety-Critical Applications</td><td>非安全关键型应用</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02417</td><td>Non-Steady-State</td><td>非稳态</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02418</td><td>Non-Stochastic</td><td>非随机的</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02419</td><td>Non-Template</td><td>非模板</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02420</td><td>Non-Template Methods</td><td>非模板方法</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02421</td><td>Non-Zero Weight</td><td>非零权重</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02422</td><td>On-The-Fly Optimization</td><td>运行中优化</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>计算机</td></tr><tr class="odd"><td>AITD-02423</td><td>One-Hot Vector</td><td>独热向量</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3">[1]</a></td><td>整个矢量中之后一个数为1 其余为0</td></tr><tr class="even"><td>AITD-02424</td><td>Open-Source</td><td>开源</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>软件工程</td></tr><tr class="odd"><td>AITD-02425</td><td>Open-Source Dataset</td><td>开源数据集</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02426</td><td>Predicted Label</td><td>预测值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02427</td><td>Prediction</td><td>预测</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02428</td><td>Prediction Accuracy</td><td>预测准确率</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02429</td><td>Predictor</td><td>预测器/决策函数</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00251/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="even"><td>AITD-02430</td><td>Protein Folding</td><td>蛋白折叠</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[2]</a></td><td>生物</td></tr><tr class="odd"><td>AITD-02431</td><td>Quantum Chemistry</td><td>量子化学</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3">[1]</a></td><td>化学</td></tr><tr class="even"><td>AITD-02432</td><td>Quantum Theory</td><td>量子理论</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>物理</td></tr><tr class="odd"><td>AITD-02433</td><td>Random Selection</td><td>随机选择</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02434</td><td>Raw Datasets</td><td>原始数据集</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02435</td><td>Root Mean Square Errors</td><td>均方根</td><td>RMSE</td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00488/978-1-78801-789-3">[1]</a></td><td>统计</td></tr><tr class="even"><td>AITD-02436</td><td>Scaling</td><td>缩放</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00109/978-1-78801-789-3">[1]</a></td><td>图像处理</td></tr><tr class="odd"><td>AITD-02437</td><td>Simulation</td><td>仿真</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02438</td><td>The Global Minimum</td><td>全局最小值</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3">[1]</a></td><td>机器学习</td></tr><tr class="odd"><td>AITD-02439</td><td>Turing Test</td><td>图灵测试</td><td></td><td><ahref="https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3">[1]</a></td><td>AI，CS</td></tr><tr class="even"><td>AITD-02440</td><td>Version Control</td><td>版本控制</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="odd"><td>AITD-02441</td><td>Workflow</td><td>工作流</td><td></td><td><ahref="https://www.nature.com/articles/s41557-021-00716-z">[1]</a></td><td></td></tr><tr class="even"><td>AITD-02442</td><td>Sequence-Function</td><td>序列-功能</td><td></td><td>[1]</td><td></td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>AI</tag>
      
      <tag>专业名词</tag>
      
      <tag>基础概念</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【原创小诗】完整</title>
    <link href="/%E3%80%90%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F%E3%80%91-%20%E5%AE%8C%E6%95%B4.html"/>
    <url>/%E3%80%90%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F%E3%80%91-%20%E5%AE%8C%E6%95%B4.html</url>
    
    <content type="html"><![CDATA[<center><p>我们本是完整的一个，但是分开了<br></p><p>于是开始有了期待，有了思念 <br></p><p>笑声很轻，眼泪也无声 <br></p><p>对天空诉说着爱是永恒 <br></p><p>跌跌撞撞寻找属于我们的完整 <br></p><p>未来的那一天在心里很重 <br></p><p>两只手里藏着全世界的星星 <br></p><p>在彼此的眼睛里看到了 <br></p><p>海洋陆地，银河苍穹 <br></p><p>北极光在脚下划过 <br></p><p>猎户座的虹手中缤纷 <br></p><p>你说那斑驳的星尘好像我们 <br></p><p>后来所有的时间空间都消失了 <br></p><p>拥抱里我们变成宇宙的最初 <br></p><p>完整 <br></p></center>]]></content>
    
    
    <categories>
      
      <category>生活感悟</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活感悟</tag>
      
      <tag>原创诗歌</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>进栈与出栈-递归案例演示</title>
    <link href="/%E8%BF%9B%E6%A0%88%E4%B8%8E%E5%87%BA%E6%A0%88-%E9%80%92%E5%BD%92%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.html"/>
    <url>/%E8%BF%9B%E6%A0%88%E4%B8%8E%E5%87%BA%E6%A0%88-%E9%80%92%E5%BD%92%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.html</url>
    
    <content type="html"><![CDATA[<p>栈作为一种基本的数据结构，其简单性和高效性使得它在各种计算和编程任务中具有广泛的应用。理解和掌握栈的原理和操作，对于编写高效、可靠的代码至关重要。</p><span id="more"></span><h3 id="什么是栈">什么是栈</h3><p>栈是一种特殊的线性表，仅允许在表的一端进行插入和删除运算。这一端被称为栈顶（top），相对地，把另一端称为栈底（bottom）。向一个栈插入新元素又称作进栈、入栈或压栈（push），它是把新元素放到栈顶元素的上面，使之成为新的栈顶元素；从一个栈删除元素又称作出栈或退栈（pop），它是把栈顶元素删除掉，使其相邻的元素成为新的栈顶元素。所以栈具有“后入先出”的特点（LIFO）。</p><h3 id="栈的作用">栈的作用</h3><p>在程序执行过程中，函数调用是通过栈来管理的。每当一个函数被调用时，会将当前的执行环境（例如局部变量、参数和返回地址）压入栈中。当函数执行完毕后，这些信息会从栈中弹出，恢复之前的执行环境。</p><p>同时在一些回溯算法中，如深度优先搜索、迷宫求解等，使用栈来保存回溯路径，便于在需要时返回上一步。</p><h3 id="递归函数中栈的内存演示">递归函数中栈的内存演示</h3><blockquote><p>这里演示了一个爬台阶的案例，题目要求是有jieshu阶台阶，要求每次可以爬1阶或2阶，求解一共有多少爬法。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">stairs</span>(<span class="hljs-params">jieshu</span>):<br>    <span class="hljs-keyword">if</span> jieshu ==<span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">elif</span> jieshu==<span class="hljs-number">2</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">2</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> (jieshu-<span class="hljs-number">1</span>)+stairs(jieshu-<span class="hljs-number">2</span>)<br><br>stairs(<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><p>上述代码中</p><h4 id="进栈的过程图示为">进栈的过程图示为：</h4><figure><img src="/images/递归进出栈内存图/递归可视化_进栈.png"alt="递归可视化_进栈" /><figcaption aria-hidden="true">递归可视化_进栈</figcaption></figure><h4 id="出栈的过程为">出栈的过程为：</h4><figure><img src="/images/递归进出栈内存图/递归可视化_出栈.png"alt="递归可视化_出栈" /><figcaption aria-hidden="true">递归可视化_出栈</figcaption></figure><h4 id="总的流程示意">总的流程示意：</h4><figure><img src="/images/递归进出栈内存图/递归可视化.png" alt="递归可视化" /><figcaption aria-hidden="true">递归可视化</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>数据结构</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>栈</tag>
      
      <tag>递归</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL数据库基础知识</title>
    <link href="/SQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html"/>
    <url>/SQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html</url>
    
    <content type="html"><![CDATA[<p>数据库操作在日常工作非常常见，以下知识点你都掌握了吗？<span id="more"></span></p><h2 id="内容大纲">内容大纲</h2><ul><li>SQL的相关概述</li><li>环境搭建</li><li>SQL语句分类<ul><li>DDL</li><li>DML</li><li>DCL</li><li>DQL</li></ul></li><li>DDL语句之操作数据库</li><li>DDL语句之操作数据表</li><li>DML语句之操作表数据(增删改)</li><li>DQL语句之操作表数据(查)</li></ul><hr /><h2 id="sql概念">1.SQL概念</h2><p>结构化查询语言(Structured QueryLanguage)简称SQL，是<strong>关系型数据库</strong>管理系统都需要遵循的规范，是数据库认识的语句。不同的数据库生产厂商都支持SQL语句，但都有自己特有内容.</p><h3 id="数据库概念">1.1数据库概念</h3><p>数据库就是存储数据的仓库，其本质是一个文件系统，按照特定的格式将数据存储起来，用户可以对数据库中的数据进行增加，修改，删除及查询(<strong>CURD)</strong>操作。</p><ul><li><p>C: Create, 增</p></li><li><p>U: Update, 改</p></li><li><p>R: Read, 查</p></li><li><p>D: Delete, 删</p></li></ul><h3id="关系型数据库与非关系型数据库">1.2关系型数据库与非关系型数据库</h3><h4 id="关系型数据">关系型数据</h4><p>指采用了<strong>关系模型</strong>来组织数据的数据库。关系模型指的就是<strong>二维表格</strong>模型，而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织。</p><h4 id="非关系型数据">非关系型数据</h4><p>又被称为NoSQL（Not Only SQL)，<strong>意为不仅仅是SQL</strong>，对NoSQL最普遍的定义是“非关联型的”，强调 <strong>Key-Value</strong>的方式存储数据。</p><h3 id="sql常用数据类型">1.3 SQL常用数据类型</h3><p>-- SQL 根据每列值的不同, 数据类型也不同, 常用的如下.</p><ul><li><p>整数: int</p></li><li><p>小数; decimal, float, double</p></li><li><p>字符串: varchar(长度), char(长度)</p></li><li><p>日期: date, datetime</p></li></ul><h2 id="mysql基础语法">2.MySql基础语法</h2><ul><li><p>建议先通过小皮安装MySql数据库,并将mysql.exe的路径添加到path</p></li><li><p>建议通过Pycharm专业版或DataGrip运行MySql相关命令及可视化</p></li></ul><h3 id="sql通用语法">2.1 SQL通用语法</h3><ul><li><ol type="1"><li>SQL语句可以写单行, 也可以写多行, 最后以 分号; 结尾.</li></ol></li><li><ol start="2" type="1"><li>为了阅读方便, 我们可以用 者 空格来隔开SQL语句.</li></ol></li><li><ol start="3" type="1"><li>SQL语句不区分大小写, 为了阅读方便, 建议: 关键字大写, 其它小写.</li></ol></li><li><ol start="4" type="1"><li>SQL的注释写法如下 -- 单行注释 '# 单行注释' /<em> 多行 注释</em>/</li></ol></li></ul><p>-- 5. 我们目前在PyCharm或者DataGrip中写SQL语句, 是选中执行的, 即:不要漏选, 防止出错.</p><h3 id="sql语句分类">2.2 SQL语句分类</h3><ul><li><p><strong>DDL</strong>语句, DataBase Definition Language,数据定义语言</p><blockquote><p>作用对象: <strong>数据库, 数据表, 列的</strong>, 进行: CURD.</p><p><strong>关键字: create, drop, alter, show</strong></p></blockquote></li><li><p><strong>DML</strong>语句, DataBase Manipulation Language,数据操作语言.</p><blockquote><p>作用对象: <strong>表数据的, 进行: 增删改操作</strong>, 统称为:<strong>更新语句</strong></p><p><strong>关键字: insert, delete, update</strong></p></blockquote></li><li><p><strong>DQL</strong>语句, DataBase Query Language,数据查询语言.</p><blockquote><p>作用对象: <strong>表数据的, 进行: 查询操作</strong>.</p><p><strong>关键字: select, from, where...</strong></p></blockquote></li><li><p><strong>DCL</strong>语句, DataBase Control Language,数据控制语言.</p><blockquote><p>作用对象: 设置权限, 访问级别(隔离级别), 创建用户等的...</p></blockquote></li></ul><h2 id="ddl语句">3.DDL语句</h2><h3 id="ddl操作数据库">3.1DDL操作数据库</h3><ul><li><ol type="1"><li><strong>查看</strong>所有的<strong>数据库</strong>.<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">show databases;         # ctrl + 回车, 执行该行代.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li><strong>创建</strong>数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">create database day01 character set &#x27;utf8&#x27;;                 # 创建day01数据库, 采用: utf8 码表.  库不存在就创建, 存在就: 报错.<br>create database if not exists day01 character set &#x27;utf8&#x27;;   # 创建day01数据库, 采用: utf8 码表.  库不存在就创建, 存在就: 啥也不做.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">上述格式, 语法糖1: character <span class="hljs-built_in">set</span> =&gt; 可以写成 charset</span><br>create database day02 charset &#x27;utf8&#x27;;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li><strong>查看</strong>对象数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">show create database day01;     # utf8<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li><strong>修改</strong>数据库码表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">alter database day03 charset =&#x27;gbk&#x27;;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="5" type="1"><li><strong>删除</strong>数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">drop database day01;            # 删除数据库, 如果数据库存在就删除, 不存在就: 报错.<br>drop database if exists day01;  # 删除数据库, 如果数据库存在就删除, 不存在就: 啥也不做.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="6" type="1"><li><strong>应用</strong>数据库. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">use day01; #之后: 建表, 查表, 查数据等操作, 都是基于数据库完成的.<br></code></pre></td></tr></table></figure> ### 3.2DDL操作数据表</li></ol></li><li><ol type="1"><li>查看当前库中, 所有的数据表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">show tables;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>查看表结构. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">show create table student;      # 查看建表的详细过程.<br>describe student;               # 语法糖,  desc student;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li>创建数据表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">create table if not exists student(<br>    sid int primary key,        # 学生id, primary key: 主键约束, 特点为: 唯一, 非空.<br>    name varchar(20) not null,  # 学生姓名, 非空约束(即: 不能为空)<br>    gender varchar(2),          # 学生性别<br>    age int                     # 学生年龄, 整数.<br>);<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li>删除数据表. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">drop table if exists student;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="5" type="1"><li>修改表(名字) <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式: rename table 旧表名 to 新表名;</span><br>rename table student to stu;<br></code></pre></td></tr></table></figure> ### 3.3 DDL操作列</li></ol></li><li><ol type="1"><li>查看表结构. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">desc stu;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>给表新增一列, desc varchar(200), 非空约束. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">alter table stu add `desc` varchar(200) not null;       # 如果列名和关键字重名, 记得用 反引号包裹.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li>修改表的字段(列), 只修改: 数据类型, 约束. 将desc列改为: int类型..<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">alter table stu modify `desc` int;      # 因为没有加非空约束, 所以本次会认为, 不要非空约束了, 即: 会删除它.<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li>修改表的字段(列), 修改: 列名, 数据类型, 约束. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">-- 格式: alter table 表名 change 旧列名 新列名 数据类型 约束;<br>alter table stu change `desc` address varchar(10) not null;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="5" type="1"><li>删除表的字段 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式: alter table 表名 drop 旧列名;</span><br>alter table stu drop address;<br></code></pre></td></tr></table></figure> ## 4 DML语句 ### 4.1添加数据</li></ol></li><li><ol type="1"><li>查看表数据, 这个数据DQL语句, 先用一下, 稍后详解. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">select * from stu;<br><span class="hljs-meta prompt_"># </span><span class="language-bash">查看表结构.</span><br>desc stu;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>添加表数据. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">场景1: 添加单条数据, 格式为: insert into 表名(列名1, 列名2, 列名3...) values(值1, 值2, 值3...);</span><br>insert into stu(sid, name, gender, age) values (1, &#x27;乔峰&#x27;, null, 38);<br><br>insert into stu(sid, name, gender, age) values (2, null, null, 38);     # 报错, name列有非空约束, 不能为null<br>场景2:添加多条数据, 格式为: insert into 表名(列名1, 列名2, 列名3...) values(值1, 值2, 值3...), (...), (...);<br>insert into stu(sid, name, gender, age)<br>values<br>    (2, &#x27;虚竹&#x27;, null, 26),<br>    (3, &#x27;段誉&#x27;, &#x27;男&#x27;, 21),<br>    (4, &#x27;阿朱&#x27;, &#x27;女&#x27;, 35),<br>    (5, &#x27;梦姑&#x27;, &#x27;女&#x27;, 23),<br>    (6, &#x27;钟灵儿&#x27;, &#x27;女&#x27;, 19);<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="3" type="1"><li>上述格式的变形版. 不一定非得是全列名, 只要值的个数, 类型 和列名的个数, 类型保持一致即可. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">insert into stu(sid, name) values (7, &#x27;木婉清&#x27;);<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="4" type="1"><li>上述格式的语法糖, 掌握, 实际开发一般是用这个.. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">insert into stu values (8, &#x27;鸠摩智&#x27;, &#x27;男&#x27;, 49);      # 如果不写列名, 则默认是: 全列名, 需要给每一个列都要传入值.<br></code></pre></td></tr></table></figure> ###4.2DML修改数据</li></ol></li><li><ol type="1"><li>修改 sid为3的数据, 姓名为: 段氏小王子, 渣男 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">update stu set name=&#x27;段氏小王子&#x27;, gender=&#x27;渣男&#x27; where sid = 3;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>危险操作, 修改数据时, 没有写 where条件,则会一次性修改表中所有的数据. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">update stu set name=&#x27;段氏小王子&#x27;, gender=&#x27;渣男&#x27;;<br></code></pre></td></tr></table></figure> ### 4.3DML删除数据</li></ol></li><li><ol type="1"><li>正常删除数据, 删除id &gt; 3的数据.(主键ID不变) <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">delete from stu where sid &gt; 3;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>删除数据, 删除id &gt; 3的数据.(主键ID改变) <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">truncate table stu where sid &gt; 3;<br></code></pre></td></tr></table></figure> ##5.备份表数据</li></ol></li><li><ol type="1"><li>场景1: 备份表不存在. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">create table stu_tmp select * from stu;<br></code></pre></td></tr></table></figure></li></ol></li><li><ol start="2" type="1"><li>场景1: 备份表存在. <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">insert into hero_tmp select * from hero;<br></code></pre></td></tr></table></figure></li></ol></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL之DQL详解</title>
    <link href="/SQL%E4%B9%8BDQL%E8%AF%A6%E8%A7%A3.html"/>
    <url>/SQL%E4%B9%8BDQL%E8%AF%A6%E8%A7%A3.html</url>
    
    <content type="html"><![CDATA[<p>SQL数据库中的查询语句整理 <span id="more"></span></p><img src="/images/00194-3462269573.jpeg" /><table><thead><tr class="header"><th><h3 id="基础查询">基础查询</h3></th><th></th></tr></thead><tbody><tr class="odd"><td>Select * from 数据表;</td><td>查看所有数据</td></tr><tr class="even"><td>Select 字段1,字段2 from 数据表;</td><td>查看字段</td></tr><tr class="odd"><td>select 地段名 as 字段别名 from 数据表;</td><td>别名查询</td></tr><tr class="even"><td>select distinct 地段名 from 数据表;</td><td>去重查询</td></tr><tr class="odd"><td><h3 id="运算符筛选">运算符筛选</h3></td><td></td></tr><tr class="even"><td>select * from 表名 where 字段名 = '字段值';</td><td>筛选某字段值的行信息</td></tr><tr class="odd"><td>select * from 表名 where 字段名 != '字段值';</td><td>筛选不含某字段值的行信息</td></tr><tr class="even"><td>select * from 表名 where 字段名 &gt; 字段值;</td><td>筛选显示大于字段值的行</td></tr><tr class="odd"><td>select * from 表名 where 字段名 in (a,b);</td><td>筛选显示包含a,b值的行</td></tr><tr class="even"><td>select * from 表名 where 字段名 <strong>between</strong> 20<strong>and</strong> 80;</td><td>筛选显示介于20 - 80之间的行</td></tr><tr class="odd"><td>select * from 表名 where 字段名=a <strong>and</strong> 字段名&gt;b;</td><td>交集条件</td></tr><tr class="even"><td>select * from 表名 where 字段名=a <strong>or</strong> 字段名&gt;b;</td><td>并集条件</td></tr><tr class="odd"><td>select * from 表名 where 字段名 like '_值%';</td><td>近似查询,_占位符,%任意字符</td></tr><tr class="even"><td>select * from 表名 where 字段名 is null;</td><td>空字段查询</td></tr><tr class="odd"><td>select * from 表名 where 字段名 is not null;</td><td>非空字段查询</td></tr><tr class="even"><td><h3 id="排序">排序</h3></td><td></td></tr><tr class="odd"><td>select * from 表名 order by 字段名 asc | desc;</td><td>排序</td></tr><tr class="even"><td>select * from 表名 order by 字段名1 asc,字段名2 desc</td><td>多重排序</td></tr><tr class="odd"><td><h3 id="聚合与分组">聚合与分组</h3></td><td></td></tr><tr class="even"><td><p>select 聚合函数 from 表名 where 字段名xxx;</p><p>聚合函数为:count(),max(),min(),sum(),avg()</p><p>xxx为筛选条件</p></td><td>相关条件值下的统计值</td></tr><tr class="odd"><td><p></p><p>select</p><p><strong>分组字段</strong>, 聚合函数(<strong>count(*)</strong>)...</p><p>from</p><p>数据表名</p><p>where</p><p>组前筛选</p><p><strong>group by</strong></p><p><strong>分组字段</strong></p><p>having</p><p>组后筛选;</p></td><td><p>分组查询</p><p>一般结合聚合函数一起用, 否则: 无意义</p><p></p><p>where: 组前筛选, 后边不能跟: 聚合函数.</p><p>having: 组后筛选, 后边可以跟: 聚合函数.</p></td></tr><tr class="even"><td><h3 id="分页查询">分页查询</h3></td><td></td></tr><tr class="odd"><td><p>select * from 表名 limit 起始索引, 数据条数;</p><p>注:索引从0开始,从0开始则0可以省略不写</p><p>经验:总页数:=(总条数 + 每页的数据条数 - 1) // 每页的数据条数</p></td><td>分页查询较为常用,有效减少服务器/用户压力</td></tr><tr class="even"><td><h3 id="重分类查询">重分类查询</h3></td><td></td></tr><tr class="odd"><td><p>select case</p><p>when 条件1 then 重命名值</p><p>when 条件2 then 重命名值2</p><p>.....</p><p>else 重命名值3</p><p>end as 字段名,</p><p>from 数据表</p></td><td></td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>查询语句</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo-fluid数学公式显示问题处理</title>
    <link href="/hexo_fluid%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86.html"/>
    <url>/hexo_fluid%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86.html</url>
    
    <content type="html"><![CDATA[<p>在此补充一下之前公式不显示的问题。</p><span id="more"></span><p>虽然<ahref="https://hexo.fluid-dev.com/docs/">Fluid</a>主题支持<strong>LaTeX数学公式</strong>，但是需要手动操作，而且我按照<ahref="https://hexo.fluid-dev.com/docs/guide/#latex-数学公式">教程</a>开启本功能<code>mathjax</code>没有成功，即公式在网页里并没有被渲染和转换。通过网上查找，发现解决这类问题的思路主要是换渲染引擎，例如<code>pandoc</code>、<code>mathjax</code>、<code>katex</code>。我目前使用<code>mathjax</code>，操作如下：</p><ul><li><p><strong>卸载</strong>默认引擎，并<strong>安装</strong>这个新的渲染引擎</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ada">$ npm uninstall hexo-renderer-marked <span class="hljs-comment">--save </span><br>$ npm install hexo-renderer-kramed <span class="hljs-comment">--saveCopy</span><br></code></pre></td></tr></table></figure></li><li><p>修改<code>/node_modules/hexo-renderer-kramed/lib/renderer.js</code></p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs arcade"><span class="hljs-comment">// Change inline math rule</span><br><span class="hljs-keyword">function</span> <span class="hljs-title function_">formatText</span>(<span class="hljs-params">text</span>) &#123;<br>  <span class="hljs-comment">// Fit kramed&#x27;s rule: $$ + \1 + $$</span><br>  <span class="hljs-comment">// 直接返回text</span><br>  <span class="hljs-comment">// return text.replace(/`\$(.*?)\$`/g, &#x27;$$$$$1$$$$&#x27;);</span><br>  <span class="hljs-keyword">return</span> <span class="hljs-built_in">text</span>;<br>&#125;Copy<br></code></pre></td></tr></table></figure></li><li><p>修改hexo的渲染源码<code>/node_modules/kramed/lib/rules/inline.js</code></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs r"><span class="hljs-operator">/</span><span class="hljs-operator">/</span> 去掉`\\`的额外转义，第<span class="hljs-number">11</span>行，将其修改为<br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> escape<span class="hljs-operator">:</span> <span class="hljs-operator">/</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">\</span>`*&#123;&#125;\[\]()# +\-.!_&gt;])/, <br>escape: /^\\([`<span class="hljs-operator">*</span><span class="hljs-punctuation">&#123;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">)</span><span class="hljs-comment"># +\-.!_&gt;])/,</span><br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> 将em标签对应的符号中，去掉`_`，第<span class="hljs-number">20</span>行，将其修改为<br><span class="hljs-operator">/</span><span class="hljs-operator">/</span> em<span class="hljs-operator">:</span> <span class="hljs-operator">/</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span>b_<span class="hljs-punctuation">(</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">:</span>__<span class="hljs-operator">|</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span>s<span class="hljs-punctuation">\</span>S<span class="hljs-punctuation">]</span><span class="hljs-punctuation">)</span><span class="hljs-operator">+</span><span class="hljs-operator">?</span><span class="hljs-punctuation">)</span>_<span class="hljs-punctuation">\</span>b<span class="hljs-operator">|</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">:</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-operator">|</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span>s<span class="hljs-punctuation">\</span>S<span class="hljs-punctuation">]</span><span class="hljs-punctuation">)</span><span class="hljs-operator">+</span><span class="hljs-operator">?</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">!</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">)</span><span class="hljs-operator">/</span><span class="hljs-punctuation">,</span>    <br>em<span class="hljs-operator">:</span> <span class="hljs-operator">/</span><span class="hljs-operator">^</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">:</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-operator">|</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">\</span>s<span class="hljs-punctuation">\</span>S<span class="hljs-punctuation">]</span><span class="hljs-punctuation">)</span><span class="hljs-operator">+</span><span class="hljs-operator">?</span><span class="hljs-punctuation">)</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">(</span><span class="hljs-operator">?</span><span class="hljs-operator">!</span><span class="hljs-punctuation">\</span><span class="hljs-operator">*</span><span class="hljs-punctuation">)</span><span class="hljs-operator">/</span><span class="hljs-punctuation">,</span>Copy<br></code></pre></td></tr></table></figure></li><li><p>停止使用 <code>hexo-math</code>，安装<code>hexo-renderer-mathjax</code></p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-symbol">$</span> npm uninstall hexo-math --save<br><span class="hljs-comment">// 不知道是不是必要的</span><br><span class="hljs-symbol">$</span> npm install hexo-renderer-mathjax --saveCopy<br></code></pre></td></tr></table></figure></li><li><p>更新 <code>Mathjax</code> 的 <code>CDN</code>链接，打开<code>/node_modules/hexo-renderer-mathjax/mathjax.html</code>，在最后一行添加js：</p><ul><li>网上推荐的上面这个，但我使用失败了</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml">// <span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span>Copy<br></code></pre></td></tr></table></figure><ul><li>推荐下面这个，亲测可行，不过偶尔出问题，需要多部署几次就ok</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span>Copy<br></code></pre></td></tr></table></figure><ul><li><strong>更新于2020年6月6日</strong>：如果有人看到这，可以注意下<code>MathJax.js</code>版本已经到3.0.5了，参照mathjax<ahref="https://www.npmjs.com/package/mathjax#installation-and-use">文档</a>，那么现在的上面的一步可以自行修改，如果控制台报错可以到<ahref="https://cdn.jsdelivr.net/npm/mathjax@3/es5/">mathjax CDNfiles</a>下找到合适的js代替</li></ul><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs applescript">&lt;<span class="hljs-keyword">script</span> <span class="hljs-built_in">id</span>=<span class="hljs-string">&quot;MathJax-script&quot;</span> async<br>  src=<span class="hljs-string">&quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js&quot;</span>&gt;<br>&lt;/<span class="hljs-keyword">script</span>&gt;Copy<br></code></pre></td></tr></table></figure><ul><li>当然，如果博客的<strong>内部静态</strong>文件<strong>第三方库</strong>包含了mathjax，上面的<code>MathJax.js</code>不用导入都行，导的不对甚至有冲突，虽然不影响公式的显示，但会在控制台报错。</li></ul></li></ul><p>经过<strong><ahref="https://github.com/Ningsir">Ningsir</a></strong>提醒，删除掉hexo-renderer-mathjax就行了，简单省事。</p><ul><li><p>按照<a href="https://hexo.fluid-dev.com/docs/">Fluid</a>的<ahref="https://hexo.fluid-dev.com/docs/guide/#快速开始">快速开始</a>，需要修改<strong>主题配置</strong>，打开<code>/source/_data/fluid_config.yml</code>文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">post:</span><br>  <span class="hljs-attr">math:</span>  <br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span>  <br>    <span class="hljs-attr">specific:</span> <span class="hljs-literal">false</span>   <br>    <span class="hljs-attr">engine:</span> <span class="hljs-string">mathjaxCopy</span><br></code></pre></td></tr></table></figure></li><li><p>在根目录下修改<code>_config.yml</code>，添加</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angelscript">mathjax: <span class="hljs-literal">true</span>Copy<br></code></pre></td></tr></table></figure></li><li><p>在<code>Front-matter</code>中打开<code>MathJax</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br>  <span class="hljs-attr">layout:</span> <span class="hljs-string">post</span><br>  <span class="hljs-attr">title:</span> <span class="hljs-string">title</span><br>  <span class="hljs-attr">date:</span> <span class="hljs-string">date</span><br>  <span class="hljs-attr">categories:</span> <br>  <span class="hljs-bullet">-</span> <span class="hljs-string">categories</span><br>  <span class="hljs-attr">tags:</span> <br>  <span class="hljs-bullet">-</span> <span class="hljs-string">tags1</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">tags2</span><br>  <span class="hljs-attr">mathjax:</span> <span class="hljs-literal">true</span><br><span class="hljs-string">---Copy</span><br></code></pre></td></tr></table></figure></li><li><p>显示数学公式 <span class="math display">\[\Sigma({n} ;{p})=\left\{\left(\zeta_{1}, \ldots, \zeta_{r}\right) \in\mathbb{C}^{n_{1}} \times \cdots \times \mathbb{C}^{n_{r}}:\sum_{k=1}^{r}\left\|{\zeta}_{k}\right\|^{2 p_{k}} &lt;1\right\}\]</span></p></li></ul><p>最后如果公式还是乱码可以尝试重启电脑，然后先尝试部署一下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hexo clean&amp;&amp;hexo g&amp;&amp;hexo d&amp;&amp;hexo s<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客维护</tag>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux常用命令整理</title>
    <link href="/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86.html"/>
    <url>/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86.html</url>
    
    <content type="html"><![CDATA[<p>整理了一下linux常用的一些命令 <span id="more"></span> ## 基本格式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>命令名 [-选项] [参数]# 有些命令要选项和参数, 有些不需要. 这里的[]表示可选项. <br></code></pre></td></tr></table></figure><h2 id="文件目录操作">文件目录操作</h2><h4 id="ls命令">2.ls命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">ls</span>命令, 来源于: list(列表)  即: 查看指定目录下所有的子级(不包括子级的子级)</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>ls [-a -l -h] [Linux的路径]<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">参数解释</span><br>-a显示所有(包括隐藏的) all<br>-l以行的形式展示详细信息 line<br>-h以人性化的方式展示.   human<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">例如:</span> <br>ls# 查看当前目录的子级, 不包括隐藏.<br>ls /# 查看根目录(/)下的内容.<br>ls -a # 查看当前目录的子级, 包括隐藏.<br>ls -l# 以行的方式, 查看当前目录的子级. 简写形式: ll<br>ls -h# 以人性化的方式展示当前目录的内容, 但是: 无效果.<br>ls -lh# 行的方式, 人性化展示当前目录下的内容. 简写形式:  ll -h<br>ls -al# 以行的形式, 展示当前目录下所有子级(包括 隐藏)<br>ls -alh # 以行, 人性化的方式展示当前目录下所有子级(包括 隐藏)<br></code></pre></td></tr></table></figure></p><h4 id="cd命令">3.cd命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">cd</span>命令, 来源于: change directory, 改变目录</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>cd 要切换到的目录的路径<br></code></pre></td></tr></table></figure></p><h4 id="pwd命令">4.pwd命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">来源于 Print Work Directory</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>pwd # 查看当前所在的工作目录,  即: 当前在Linux的哪个路径下. <br></code></pre></td></tr></table></figure></p><h4 id="linux中的路径写法">5.Linux中的路径写法</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">路径介绍</span><br>就是用来描述文件 或者 文件夹(目录)的路径的, 有: 绝对路径 和 相对路径两种写法.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">绝对路径</span><br>以 / 根目录开头.   <br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">相对路径</span><br>默认是相对于当前路径来写的. <br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">例如: 当前目录是在 /aa/bb  如果相切换到 /aa/bb/cc目录, 有如下两种写法.</span><br>绝对路径:   cd /aa/bb/cc<br>相对路径:   cd cc<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">几个特殊的路径写法</span><br>./# 代表当前路径, 例如: 上述的 cd cc 还可以写成 cd ./cc<br>..# 代表上级路径<br>../..# 代表上上级路径<br>~# 代表: 回到家目录, root账号的家 /root,  其它账号的家 /home/账号名<br><span class="hljs-meta prompt_"># </span><span class="language-bash">语法糖, 可以直接写 <span class="hljs-built_in">cd</span> 也是回家命令.</span><br>-# 代表: 在最近的两个目录之间做切换.<br></code></pre></td></tr></table></figure></p><h4 id="mkdir命令">6.mkdir命令</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">来源于 make directory, 创建目录(文件夹)的.</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>mkdir [-p] 文件夹路径# -p表示parent, 即: 父目录不存在, 也会自动创建.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">简单理解, 假设: 目前只有 /root/aa 文件夹</span><br>mkdir /root/aa/bb/cc# 报错, 因为不写-p, 只能创建单级文件夹.<br>mkdir -p /root/aa/bb/cc# 不报错, 加上-p可以创建多级目录.<br></code></pre></td></tr></table></figure></p><h4 id="文件相关">7.文件相关</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">touch</span>创建文件的.</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>touch 文件路径1 文件路径2...# 可以同时创建多个文件.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">cat</span>查看文件内容的</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>cat文件路径# 一次性查看文件所有内容, 如果内容较多, 会翻页, 只留最后一页.<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">more查看文件内容的, 可以分页查看.</span><br>more 文件路径# 以分页的形式查看文件内容.<br><span class="hljs-meta prompt_"># </span><span class="language-bash">空格向下翻一页</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">d  down的意思, 向下翻半页</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">enter 向下翻一行</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">b  back, 向上翻一页.</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">q     quit, 表示退出.  也可以按下 ctrl + 字母C</span><br></code></pre></td></tr></table></figure></p><h2 id="文件和文件夹相关命令">文件和文件夹相关命令</h2><h4 id="cp命令-来源于-copy单词-可以拷贝-文件-文件夹">8.cp命令, 来源于copy单词, 可以拷贝 文件, 文件夹</h4><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"> # </span><span class="language-bash">格式</span><br>cp [-r] 数据源 目的地# -r表示recursive(递归), 即: 拷贝文件夹时, 要写. <br>cp -r /root/aa /root/test<br><br>[root@linxkon ~]# cd /root/<br>[root@linxkon ~]# ls<br>2.avi  3.jpg  4.mp3  aa  anaconda-ks.cfg  a.txt<br>[root@linxkon ~]# mkdir lk<br>[root@linxkon ~]# <br>[root@linxkon ~]# cp a.txt lk# 拷贝<br>[root@linxkon ~]# ls<br>2.avi  3.jpg  4.mp3  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# ls lk/<br>a.txt<br>[root@linxkon ~]# <br>[root@linxkon ~]# cp 2.avi lk/abc.avi# 拷贝, 并改名<br>[root@linxkon ~]# ls lk/<br>abc.avi  a.txt<br>[root@linxkon ~]# cp aa lk# 报错, 拷贝文件夹必须夹-r, 递归拷贝.<br>cp: 略过目录&quot;aa&quot;<br>[root@linxkon ~]# cp -r aa lk# 拷贝文件夹<br>[root@linxkon ~]# ls lk/<br>aa  abc.avi  a.txt<br></code></pre></td></tr></table></figure></p><h4 id="mvmove剪切移动重命名">9.mv（move）剪切移动/重命名</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">格式</span><br>mv 数据源 目的地# 注意: 如果是同级路径, 就是改名.<br><br>[root@linxkon ~]# ls<br>2.avi  3.jpg  4.mp3  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# ls lk/<br>aa  abc.avi  a.txt<br>[root@linxkon ~]# <br>[root@linxkon ~]# mv 3.jpg lk/# 剪切文件<br>[root@linxkon ~]# ls lk/<br>3.jpg  aa  abc.avi  a.txt<br>[root@linxkon ~]# ls<br>2.avi  4.mp3  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# <br>[root@linxkon ~]# <br>[root@linxkon ~]# mv 4.mp3 lk/好日子.xyz# 剪切(文件)并改名<br>[root@linxkon ~]# ls<br>2.avi  aa  anaconda-ks.cfg  a.txt  lk<br>[root@linxkon ~]# ls lk/<br>3.jpg  aa  abc.avi  a.txt  好日子.xyz<br><br>[root@linxkon ~]# mkdir xyz<br>[root@linxkon ~]# ls<br>2.avi  aa  anaconda-ks.cfg  a.txt  lk  xyz<br>[root@linxkon ~]# mv aa xyz# 剪切文件夹, 无需加: -r<br>[root@linxkon ~]# ls<br>2.avi  anaconda-ks.cfg  a.txt  lk  xyz<br>[root@linxkon ~]# ls xyz/<br>aa<br><br><br>[root@linxkon ~]# ls<br>[root@linxkon ~]# touch 1.txt<br>[root@linxkon ~]# <br>[root@linxkon ~]# mv 1.txt abc.txt# 改名操作<br>[root@linxkon ~]# ls<br>abc.txt<br></code></pre></td></tr></table></figure><h4 id="rm命令-来源于-remove单词-可以删除-文件-文件夹">10.rm命令, 来源于remove单词, 可以删除 文件, 文件夹</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">rm [-r -f] 要删除的文件或者文件夹路径# -r:递归,  -f: force(强制)<br><br>[root@linxkon ~]# rm -rf lk# 强制删除 lk文件夹, 且不询问<br>[root@linxkon ~]# ls<br>anaconda-ks.cfg  xyz<br>[root@linxkon ~]# touch 1.txt 2.txt 3.avi 4.avi 5.jpg<br>[root@linxkon ~]# ls<br>1.txt  2.txt  3.avi  4.avi  5.jpg  anaconda-ks.cfg  xyz<br>[root@linxkon ~]# rm -rf *.txt<br>[root@linxkon ~]# ls<br>3.avi  4.avi  5.jpg  anaconda-ks.cfg  xyz<br>[root@linxkon ~]# rm -rf *# 清空当前文件夹<br>[root@linxkon ~]# ls<br>[root@linxkon ~]# rm -rf /*  ^C# 慎用<br></code></pre></td></tr></table></figure><h4 id="一个坐牢命令">11.一个坐牢命令</h4><figure class="highlight shell"><figcaption><span>rm -rf</span><a href="/*">link</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">rm -rf /* #删除系统<br></code></pre></td></tr></table></figure><h2 id="查找命令">查找命令</h2><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">### 12.which命令,  查找Linux可执行命令 的路径的.</span></span> <br>  which ls# /usr/bin/ls<br>  which pwd# /usr/bin/pwd<br>  <br>  which ifconfig# /usr/sbin/ifconfig<br>  <br><span class="hljs-meta prompt_">  </span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">### 13.find命令, 根据文件名, 或者 文件大小查找指定文件.</span></span><br><span class="hljs-meta prompt_">  # </span><span class="language-bash">格式</span><br>  find 要被检索的目录路径 -name &#x27;要检索的文件名&#x27;<br>  <br>  find / -name &#x27;abc*&#x27;# 查找Linux中, 以abc开头的内容.<br>  <br><span class="hljs-meta prompt_">  # </span><span class="language-bash">格式</span><br>  find 要被检索的目录路径 -size +100M# 超过100MB,  -10K, 小于10KB<br>  <br>  find / -size +100M# 查找Linux中, 文件大小超过100M的文件.<br></code></pre></td></tr></table></figure></p><p>—————————————————华丽的分割线—————————————————<img src="/images/2024年5月10日genk.jpg" title="毕加索" alt="dolor"></p>]]></content>
    
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>基础命令</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo博客搭建教程</title>
    <link href="/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B.html"/>
    <url>/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B.html</url>
    
    <content type="html"><![CDATA[<p>hexo + github 搭建你的静态博客 <span id="more"></span> #一，搭建前的软件准备（git，node）</p><blockquote><p>搭建之前需要准备的软件： Git：官网下载：https://git-scm.com/ Node.js官网下载：http://nodejs.cn/</p></blockquote><h1 id="二-安装hexo完成简单本地页面展示">二，安装hexo，完成简单本地页面展示</h1><p>1.进入cmd窗口输入指令：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm">npm install -g hexo-<span class="hljs-keyword">cli</span><br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/d6d9c791b8f449fea0b64b1a72bd32b2.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>2.你可以先创建一个文件夹myblog，然后cd到这个文件夹下（或者在这个文件夹下直接右键gitbash打开）。 <imgsrc="/images/hexo博客搭建教程/5b5554d54f20471098768040624585ba.png"alt="在这里插入图片描述" /></p><p>接下来初始化一下hexo</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs csharp">hexo <span class="hljs-keyword">init</span><br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/6010660448004f25871ce6b5a479f832.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>3.查看是否能启动成功</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clike">hexo s<br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/4df56b5dc3ce40a2a2c9dc99585fd8ed.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><blockquote><p>新建完成后，指定文件夹目录下有： node_modules: 依赖包public：存放生成的页面 scaffolds：生成文章的一些模板source：用来存放你的文章 themes：主题 **_config.yml:博客的配置文件**</p></blockquote><p>4.复制网址打开</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clike">http://localhost:4000/<br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/2bcb9a6a09024ce68d095901dc1ca203.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>这是初始界面，我们需要部署到github上。</p><p>ctrl+C可以停止；</p><h1 id="三将hexo部署到github">三，将Hexo部署到Github</h1><h2 id="github创建个人仓库">1.Github创建个人仓库</h2><blockquote><p>首先，需要有一个github账号。登上账号后建一个仓库：仓库名为你的用户名.github.io，举例如下： 创建一个和你用户名相同的仓库，后面加.github.io，只有这样，将来要部署到GitHub的时候，才会被识别，也就是xxxx.github.io，其中xxx就是你注册GitHub的用户名.</p></blockquote><figure><img src="/images/hexo博客搭建教程/a697d02a363e48e08d07854051642860.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h2 id="生成ssh添加到github">2.生成ssh添加到Github</h2><blockquote><p>在Github上创建仓库完成之后，需要设置ssh免密登录</p></blockquote><p>1.打开cmd窗口：执行如下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clike">git config --global user.name &quot;yourname&quot;<br>git config --global user.email &quot;youremail&quot;<br></code></pre></td></tr></table></figure><p>这里的yourname输入你的GitHub用户名，youremail输入你GitHub的邮箱。这样GitHub才能知道你是不是对应它的账户。用户名为仓库的名称，邮箱为注册github的邮箱，举例如下：</p><figure><img src="/images/hexo博客搭建教程/ef3ce50ba8ce4e39bb4dd1387a9da316.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>防止输错可以检查：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clike">git config user.name<br>git config user.email<br></code></pre></td></tr></table></figure><p>2.接着进入到家目录：C:，右击打开git bash 。 输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clike">ssh-keygen -t rsa -C 2412757158@qq.com<br></code></pre></td></tr></table></figure><p>后面是自己注册github的邮箱，然后敲三次回车，</p><figure><img src="/images/hexo博客搭建教程/b07cadba4a484a7eac9c19884ea6f3b5.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>3.接着就会发现C:.ssh目录，打开后有一个公钥，一个私钥。id_rsa.pub是公钥，我们需要打开它，复制里面的内容。</p><p>然后进入github：</p><p>点击setings <imgsrc="/images/hexo博客搭建教程/2f3217c541b94d59bc17c3d8119e8801.png"alt="在这里插入图片描述" /></p><p>进行以下操作</p><p><imgsrc="/images/hexo博客搭建教程/a1def242038c4c77b125d0c0b597f987.png"alt="在这里插入图片描述" />发现我们需要一个密钥，把我们刚刚复制的密钥粘进去，title随便起</p><p><imgsrc="/images/hexo博客搭建教程/821106b4621d4a1a91cfc2f1510abd99.png"alt="在这里插入图片描述" /> 点击 Add SSH Key</p><h2 id="进行部署">3.进行部署</h2><blockquote><p>这一步，我们就可以将hexo和GitHub关联起来，也就是将hexo生成的文章部署到GitHub上，打开站点配置文件_config.yml，翻到最后，修改为 YourgithubName就是你的GitHub账户</p></blockquote><p>1.修改配置文件 <imgsrc="/images/hexo博客搭建教程/c87432a9b49c4552b931c51e0e94e61d.png"alt="在这里插入图片描述" /></p><p>修改内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs clike">deploy:<br>  type: git<br>  repo: git@github.com:goubin18/goubin18.github.io.git<br>  branch: main<br></code></pre></td></tr></table></figure><figure><img src="/images/hexo博客搭建教程/7955e295748647388285871fcf65b511.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p><strong>注意：后面有空格</strong></p><p><strong>repo：获取步骤如下</strong></p><p><strong>点进自己刚刚创建的仓库，复制</strong></p><p><strong><imgsrc="/images/hexo博客搭建教程/a8b5f30ed44448b88f759faf8f104ecb.png"alt="在这里插入图片描述" /></strong></p><p><strong>2.找到自己的博客路径打开</strong></p><p><strong><imgsrc="/images/hexo博客搭建教程/fa09bd6a0d7448deb1ce16a424e0c987.png"alt="在这里插入图片描述" /></strong></p><p><strong>这个时候需要先安装deploy-git，也就是部署的命令,这样你才能用命令部署到GitHub。</strong></p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">npm install hexo-deployer-git <span class="hljs-comment">--save</span><br></code></pre></td></tr></table></figure><p><strong><imgsrc="/images/hexo博客搭建教程/a4ff5a3aed8443d6b98b366fa63e724d.png"alt="在这里插入图片描述" /></strong></p><p><strong>2.然后依次执行以下命令：</strong></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs verilog">hexo c   #清除缓存文件 db<span class="hljs-variable">.json</span> 和已生成的静态文件 public<br>hexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo <span class="hljs-keyword">generate</span> 的缩写)<br>hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)<br></code></pre></td></tr></table></figure><p><strong>注意deploy时会让输个yes</strong></p><p><strong><em>*最后回到github上查看自己的仓库：*</em></strong></p><p><imgsrc="/images/hexo博客搭建教程/5a62c4630f164385831ad449065b5b03.png"alt="在这里插入图片描述" /> 这就表示上传成功。</p><p>现在就可以使用xxx.github.io来访问你的博客啦例如：我的用户名是linxkon，那么我的博客地址就是<code>linxkon.github.io</code></p><p>举例如下：</p><figure><img src="/images/hexo博客搭建教程/9eefc08e36464040bc8fbe8d9716073b.png"alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><h1 id="写在最后">写在最后：</h1><blockquote><p>现在简单的博客已经搭建完成了 现在你的个人网站的地址是xxx.github.io，如果觉得这个网址配不上帅气多金的你，你就可以设置个人域名了。但是需要花钱。小提示： 操作要细心，如果出现了问题可以私信留言，大家一起想办法！</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客搭建</tag>
      
      <tag>知识管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tensor基础2——张量的计算</title>
    <link href="/Tensor%E5%9F%BA%E7%A1%802--%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97.html"/>
    <url>/Tensor%E5%9F%BA%E7%A1%802--%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97.html</url>
    
    <content type="html"><![CDATA[<h4 id="张量的数值计算">1. 张量的数值计算</h4><h5 id="张量基本运算">1）张量基本运算</h5><ul><li>加减乘除取负号：</li><li>add、sub、mul、div、neg</li><li>add_（其中带下划线的版本会修改原数据）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python">data = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(data)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <span class="hljs-number">4</span>],<br>            [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>]])<br><br><span class="hljs-comment"># 1. 不修改原数据</span><br>new_data = data.add(<span class="hljs-number">10</span>)  <span class="hljs-comment"># 等价 new_data = data + 10</span><br><span class="hljs-built_in">print</span>(new_data)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">13</span>, <span class="hljs-number">17</span>, <span class="hljs-number">14</span>],<br>            [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">16</span>]])<br><br><span class="hljs-comment"># 2. 直接修改原数据 注意: 带下划线的函数为修改原数据本身</span><br>data.add_(<span class="hljs-number">10</span>)  <span class="hljs-comment"># 等价 data += 10</span><br><span class="hljs-built_in">print</span>(data)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">13</span>, <span class="hljs-number">17</span>, <span class="hljs-number">14</span>],<br>            [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">16</span>]])<br><br><span class="hljs-comment"># 3. 其他函数</span><br><span class="hljs-built_in">print</span>(data.sub(<span class="hljs-number">100</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[-<span class="hljs-number">87</span>, -<span class="hljs-number">83</span>, -<span class="hljs-number">86</span>],<br>            [-<span class="hljs-number">90</span>, -<span class="hljs-number">90</span>, -<span class="hljs-number">84</span>]])<br><br><span class="hljs-built_in">print</span>(data.mul(<span class="hljs-number">100</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">1300</span>, <span class="hljs-number">1700</span>, <span class="hljs-number">1400</span>],<br>            [<span class="hljs-number">1000</span>, <span class="hljs-number">1000</span>, <span class="hljs-number">1600</span>]])<br><br><span class="hljs-built_in">print</span>(data.div(<span class="hljs-number">100</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">0.1300</span>, <span class="hljs-number">0.1700</span>, <span class="hljs-number">0.1400</span>],<br>            [<span class="hljs-number">0.1000</span>, <span class="hljs-number">0.1000</span>, <span class="hljs-number">0.1600</span>]])<br><br><span class="hljs-built_in">print</span>(data.neg())<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[-<span class="hljs-number">13</span>, -<span class="hljs-number">17</span>, -<span class="hljs-number">14</span>],<br>            [-<span class="hljs-number">10</span>, -<span class="hljs-number">10</span>, -<span class="hljs-number">16</span>]])<br></code></pre></td></tr></table></figure><h5 id="张量点乘运算">2）张量点乘运算</h5><ul><li>点乘指（Hadamard）的是两个同维矩阵对应位置的元素相乘，使用mul和运算符 * 实现。</li></ul><figure><img src="assets/image-20240528102236446.png"alt="image-20240528102236446" /><figcaption aria-hidden="true">image-20240528102236446</figcaption></figure><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs lua">data1 = torch.tensor(<span class="hljs-string">[[1, 2], [3, 4]]</span>)<br>data2 = torch.tensor(<span class="hljs-string">[[5, 6], [7, 8]]</span>)<br># 第一种方式<br>data = torch.mul(data1, data2)<br><span class="hljs-built_in">print</span>(data)<br>&gt;&gt;&gt; tensor(<span class="hljs-string">[[ 5, 12],</span><br><span class="hljs-string">            [21, 32]]</span>)<br><br># 第二种方式<br>data = data1 * data2<br><span class="hljs-built_in">print</span>(data)<br>&gt;&gt;&gt; tensor(<span class="hljs-string">[[ 5, 12],</span><br><span class="hljs-string">            [21, 32]]</span>)<br></code></pre></td></tr></table></figure><h5 id="张量矩阵乘法运算">3）张量矩阵乘法运算</h5><ul><li>矩阵乘法运算要求第一个矩阵 shape: (n, m)，第二个矩阵 shape: (m, p),两个矩阵点积运算 shape 为: (n, p)。</li><li>运算符 @ 用于进行两个矩阵的乘积运算</li><li>torch.matmul 对进行乘积运算的两矩阵形状没有限定.对数输入的 shape不同的张量, 对应的最后几个维度必须符合矩阵运算规则</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 点积运算</span><br>data1 = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br>data2 = torch.tensor([[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]])<br><span class="hljs-comment"># 方式一:</span><br>data3 = data1 @ data2<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;data3--&gt;&quot;</span>, data3)<br><span class="hljs-meta">&gt;&gt;&gt; </span>data3--&gt; tensor([[<span class="hljs-number">19</span>, <span class="hljs-number">22</span>],<br>                     [<span class="hljs-number">43</span>, <span class="hljs-number">50</span>],<br>                     [<span class="hljs-number">67</span>, <span class="hljs-number">78</span>]])<br><br><span class="hljs-comment"># 方式二:</span><br>data4 = torch.matmul(data1, data2)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;data4--&gt;&quot;</span>, data4)<br><span class="hljs-meta">&gt;&gt;&gt; </span>data4--&gt; tensor([[<span class="hljs-number">19</span>, <span class="hljs-number">22</span>],<br>                     [<span class="hljs-number">43</span>, <span class="hljs-number">50</span>],<br>                     [<span class="hljs-number">67</span>, <span class="hljs-number">78</span>]])<br></code></pre></td></tr></table></figure><h5 id="总结">4）总结</h5><p><strong>&lt;1&gt; 张量基本运算函数</strong></p><ul><li>add、sub、mul、div、neg等函数</li><li>add_、sub_、mul_、div_、neg_等函数</li></ul><p><strong>&lt;2&gt; 张量的点乘运算</strong></p><ul><li>mul 和运算符 *</li></ul><p><strong>&lt;3&gt; 点积运算</strong></p><ul><li>运算符@用于进行两个矩阵的点乘运算</li><li>torch.matmul 对进行点乘运算的两矩阵形状没有限定，对数输入的 shape不同的张量, 对应的最后几个维度必须符合矩阵运算规则</li></ul><h4 id="张量的运算函数">2. 张量的运算函数</h4><p>PyTorch 为每个张量封装很多实用的计算函数：</p><ul><li>均值</li><li>平方根</li><li>求和</li><li>指数计算</li><li>对数计算等等</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>data = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=torch.float64)<br><span class="hljs-built_in">print</span>(data)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">4.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">7.</span>],<br>            [<span class="hljs-number">6.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">5.</span>]], dtype=torch.float64)<br><br><span class="hljs-comment"># 1. 计算均值</span><br><span class="hljs-comment"># 注意: tensor 必须为 Float 或者 Double 类型</span><br><span class="hljs-built_in">print</span>(data.mean())<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor(<span class="hljs-number">4.1667</span>, dtype=torch.float64)<br><br><span class="hljs-built_in">print</span>(data.mean(dim=<span class="hljs-number">0</span>))  <span class="hljs-comment"># 按列计算均值</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([<span class="hljs-number">5.0000</span>, <span class="hljs-number">1.5000</span>, <span class="hljs-number">6.0000</span>], dtype=torch.float64)<br><br><span class="hljs-built_in">print</span>(data.mean(dim=<span class="hljs-number">1</span>))  <span class="hljs-comment"># 按行计算均值</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([<span class="hljs-number">3.6667</span>, <span class="hljs-number">4.6667</span>], dtype=torch.float64)<br><br><span class="hljs-comment"># 2. 计算总和</span><br><span class="hljs-built_in">print</span>(data.<span class="hljs-built_in">sum</span>())<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor(<span class="hljs-number">25.</span>, dtype=torch.float64)<br><br><span class="hljs-built_in">print</span>(data.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([<span class="hljs-number">10.</span>,  <span class="hljs-number">3.</span>, <span class="hljs-number">12.</span>], dtype=torch.float64)<br><br><span class="hljs-built_in">print</span>(data.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([<span class="hljs-number">11.</span>, <span class="hljs-number">14.</span>], dtype=torch.float64)<br><br><span class="hljs-comment"># 3. 计算平方</span><br><span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">pow</span>(data，<span class="hljs-number">2</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">16.</span>,  <span class="hljs-number">0.</span>, <span class="hljs-number">49.</span>],<br>            [<span class="hljs-number">36.</span>,  <span class="hljs-number">9.</span>, <span class="hljs-number">25.</span>]], dtype=torch.float64)<br><br><span class="hljs-comment"># 4. 计算平方根</span><br><span class="hljs-built_in">print</span>(data.sqrt())<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">2.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">2.6458</span>],<br>            [<span class="hljs-number">2.4495</span>, <span class="hljs-number">1.7321</span>, <span class="hljs-number">2.2361</span>]], dtype=torch.float64)<br><br><span class="hljs-comment"># 5. 指数计算, e^n 次方</span><br><span class="hljs-built_in">print</span>(data.exp())<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">5.4598e+01</span>, <span class="hljs-number">1.0000e+00</span>, <span class="hljs-number">1.0966e+03</span>],<br>            [<span class="hljs-number">4.0343e+02</span>, <span class="hljs-number">2.0086e+01</span>, <span class="hljs-number">1.4841e+02</span>]], dtype=torch.float64)<br><br><span class="hljs-comment"># 6. 对数计算</span><br><span class="hljs-built_in">print</span>(data.log())  <span class="hljs-comment"># 以 e 为底</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">1.3863</span>,   -inf, <span class="hljs-number">1.9459</span>],<br>            [<span class="hljs-number">1.7918</span>, <span class="hljs-number">1.0986</span>, <span class="hljs-number">1.6094</span>]], dtype=torch.float64)<br><br><span class="hljs-built_in">print</span>(data.log2())<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">2.0000</span>,   -inf, <span class="hljs-number">2.8074</span>],<br>            [<span class="hljs-number">2.5850</span>, <span class="hljs-number">1.5850</span>, <span class="hljs-number">2.3219</span>]], dtype=torch.float64)<br><br><span class="hljs-built_in">print</span>(data.log10())<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">0.6021</span>,   -inf, <span class="hljs-number">0.8451</span>],<br>            [<span class="hljs-number">0.7782</span>, <span class="hljs-number">0.4771</span>, <span class="hljs-number">0.6990</span>]], dtype=torch.float64)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>张量</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tensor基础3——张量的索引与变形</title>
    <link href="/Tensor%E5%9F%BA%E7%A1%803--%E5%BC%A0%E9%87%8F%E7%9A%84%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%8F%98%E5%BD%A2.html"/>
    <url>/Tensor%E5%9F%BA%E7%A1%803--%E5%BC%A0%E9%87%8F%E7%9A%84%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%8F%98%E5%BD%A2.html</url>
    
    <content type="html"><![CDATA[<h4 id="张量的索引操作">1. 张量的索引操作</h4><ul><li>在操作张量时，经常要去获取某些元素进行处理或者修改操作，在这里需要了解torch中的索引操作。</li></ul><p><strong>准备数据：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-comment"># 随机生成数据</span><br>data = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br><span class="hljs-built_in">print</span>(data)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">9</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">3</span>],<br>            [<span class="hljs-number">4</span>, <span class="hljs-number">9</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>]])<br></code></pre></td></tr></table></figure><h5 id="简单行列索引的使用">1）简单行列索引的使用</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(data[<span class="hljs-number">0</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([<span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">9</span>])<br><br><span class="hljs-built_in">print</span>(data[:, <span class="hljs-number">0</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([<span class="hljs-number">0</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><h5 id="列表索引的使用">2）列表索引的使用</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 返回 (0, 1)、(1, 2) 两个位置的元素</span><br><span class="hljs-built_in">print</span>(data[[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([<span class="hljs-number">7</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-comment"># 返回 0、1 行的 1、2 列共4个元素</span><br><span class="hljs-built_in">print</span>(data[[[<span class="hljs-number">0</span>], [<span class="hljs-number">1</span>]], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">7</span>, <span class="hljs-number">6</span>],<br>            [<span class="hljs-number">8</span>, <span class="hljs-number">3</span>]])<br></code></pre></td></tr></table></figure><h5 id="范围索引的使用">3）范围索引的使用</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 前3行的前2列数据</span><br><span class="hljs-built_in">print</span>(data[:<span class="hljs-number">3</span>, :<span class="hljs-number">2</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">7</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">8</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>]])<br><br><span class="hljs-comment"># 第2行到最后的前2列数据</span><br><span class="hljs-built_in">print</span>(data[<span class="hljs-number">2</span>:, :<span class="hljs-number">2</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">6</span>, <span class="hljs-number">3</span>],<br>            [<span class="hljs-number">4</span>, <span class="hljs-number">9</span>]])<br></code></pre></td></tr></table></figure><h5 id="布尔索引的使用">4）布尔索引的使用</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 第三列大于5的行数据</span><br><span class="hljs-built_in">print</span>(data[data[:, <span class="hljs-number">2</span>] &gt; <span class="hljs-number">5</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">9</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">3</span>]])<br><br><span class="hljs-comment"># 第二行大于5的列数据</span><br><span class="hljs-built_in">print</span>(data[:, data[<span class="hljs-number">1</span>] &gt; <span class="hljs-number">5</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">7</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">8</span>],<br>            [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>],<br>            [<span class="hljs-number">4</span>, <span class="hljs-number">9</span>]])<br></code></pre></td></tr></table></figure><h5 id="多维索引的使用">5）多维索引的使用</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python">data = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br><span class="hljs-built_in">print</span>(data)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>             [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>],<br>             [<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>],<br>             [<span class="hljs-number">7</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>]],<br><br>            [[<span class="hljs-number">9</span>, <span class="hljs-number">7</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],<br>             [<span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>             [<span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],<br>             [<span class="hljs-number">9</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>]],<br><br>            [[<span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],<br>             [<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>],<br>             [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>],<br>             [<span class="hljs-number">9</span>, <span class="hljs-number">6</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]])<br><br><span class="hljs-comment"># 获取0轴上的第一个数据</span><br><span class="hljs-built_in">print</span>(data[<span class="hljs-number">0</span>, :, :])<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>            [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>],<br>            [<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>],<br>            [<span class="hljs-number">7</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>]])<br><br><span class="hljs-comment"># 获取1轴上的第一个数据</span><br><span class="hljs-built_in">print</span>(data[:, <span class="hljs-number">0</span>, :])<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>            [<span class="hljs-number">9</span>, <span class="hljs-number">7</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],<br>            [<span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br><br><span class="hljs-comment"># 获取2轴上的第一个数据</span><br><span class="hljs-built_in">print</span>(data[:, :, <span class="hljs-number">0</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">7</span>],<br>            [<span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>],<br>            [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">9</span>]])<br></code></pre></td></tr></table></figure><h4 id="张量的形状操作">2. 张量的形状操作</h4><p>有重塑、堆叠、挤压和解压：</p><table><thead><tr class="header"><th style="text-align: left;">方法</th><th style="text-align: left;">单行描述</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">torch.reshape(input, shape)</td><td style="text-align: left;">重塑 input 到 shape（如果兼容），也可以使用 torch.Tensor.reshape()。</td></tr><tr class="even"><td style="text-align: left;">tensor.view(shape)</td><td style="text-align: left;">返回不同 shape中的原始张量视图，但与原始张量共享相同的数据。</td></tr><tr class="odd"><td style="text-align: left;">tensor.contiguous()</td><td style="text-align: left;">将张量转换到整块内存上</td></tr><tr class="even"><td style="text-align: left;">torch.stack(tensors, dim=0)</td><td style="text-align: left;">沿着新的维度（dim）连接 tensors的序列，所有 tensors 必须具有相同的大小。</td></tr><tr class="odd"><td style="text-align: left;">torch.squeeze(input)</td><td style="text-align: left;">挤压 input 以移除值为 1 的所有尺寸。</td></tr><tr class="even"><td style="text-align: left;">torch.unsqueeze(input, dim)</td><td style="text-align: left;">返回在 dim 处添加了维度值 1 的input。</td></tr><tr class="odd"><td style="text-align: left;">torch.transpose(input,dim1,dim2)</td><td style="text-align: left;">实现交换张量形状的指定维度</td></tr><tr class="even"><td style="text-align: left;">torch.permute(input, dims)</td><td style="text-align: left;">返回原始 input的视图，其尺寸被置换（重新排列）为 dims。</td></tr></tbody></table><p>深度学习模型（神经网络）都是以某种方式操纵张量。由于矩阵乘法的规则，如果形状不匹配，就会遇到错误。这些方法可帮助您确保张量的正确元素与其他张量的正确元素混合。</p><p>举例说明：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">import torch<br>x = torch<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">1</span>., <span class="hljs-number">8</span>.)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x)</span></span><br>&gt;&gt;&gt; <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[1., 2., 3., 4., 5., 6., 7.]</span>)<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(x.shape)</span></span><br>&gt;&gt;&gt; torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[7]</span>)<br></code></pre></td></tr></table></figure><h5 id="reshape">1）RESHAPE</h5><p>reshape函数可以在保证张量数据不变的前提下改变数据的维度，将其转换成指定的形状。</p><p>使用 <code>torch.reshape()</code> 增加一个维度。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs lua"># 增加一个维度<br>x_reshaped = x.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">7</span>)<br><span class="hljs-built_in">print</span>(x_reshaped)<br>&gt;&gt;&gt; tensor(<span class="hljs-string">[[1., 2., 3., 4., 5., 6., 7.]]</span>)<br><br><span class="hljs-built_in">print</span>(x_reshaped.shape)<br>&gt;&gt;&gt; torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">7</span>])<br></code></pre></td></tr></table></figure><p>使用 <code>torch.reshape()</code> 改变张量的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>data = torch.tensor([[<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>], [<span class="hljs-number">40</span>, <span class="hljs-number">50</span>, <span class="hljs-number">60</span>]])<br><span class="hljs-comment"># 1. 使用 shape 属性或者 size 方法都可以获得张量的形状</span><br><span class="hljs-built_in">print</span>(data.shape, data.shape[<span class="hljs-number">0</span>], data.shape[<span class="hljs-number">1</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]) <span class="hljs-number">2</span> <span class="hljs-number">3</span><br><br><span class="hljs-built_in">print</span>(data.size(), data.size(<span class="hljs-number">0</span>), data.size(<span class="hljs-number">1</span>))<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]) <span class="hljs-number">2</span> <span class="hljs-number">3</span><br><br><span class="hljs-comment"># 2. 使用 reshape 函数修改张量形状</span><br>new_data = data.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>)<br><span class="hljs-built_in">print</span>(new_data.shape)<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6</span>])<br></code></pre></td></tr></table></figure><h5 id="view-contiguous">2）VIEW / CONTIGUOUS</h5><ul><li>view函数也可以用于修改张量的形状，只能用于<strong>存储在整块内存中的张量</strong>。</li><li>在 PyTorch中，有些张量是由不同的数据块组成的，它们并没有存储在整块的内存中，view函数无法对这样的张量进行变形处理。例如: 一个张量经过了 transpose 或者permute 函数的处理之后，就无法使用 view 函数进行形状操作。</li><li>此时需要先<strong>使用 contiguous函数转换为整块内存的张量</strong>，再使用 view 函数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1 一个张量经过了 transpose 或者 permute 函数的处理之后，就无法使用 view 函数进行形状操作</span><br><span class="hljs-comment">#   若要使用view函数, 需要使用contiguous() 变成连续以后再使用view函数</span><br><span class="hljs-comment"># 2 判断张量是否使用整块内存</span><br>data = torch.tensor( [[<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>],[<span class="hljs-number">40</span>, <span class="hljs-number">50</span>, <span class="hljs-number">60</span>]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;data---&gt;&#x27;</span>, data, data.shape)<br><span class="hljs-meta">&gt;&gt;&gt; </span>data---&gt; tensor([[<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>],<br>                     [<span class="hljs-number">40</span>, <span class="hljs-number">50</span>, <span class="hljs-number">60</span>]]) torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-comment"># 1 判断是否使用整块内存</span><br><span class="hljs-built_in">print</span>(data.is_contiguous())<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-literal">True</span><br><br><span class="hljs-comment"># 2 view</span><br>mydata2 = data.view(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;mydata2---&gt;&#x27;</span>, mydata2, mydata2.shape)<br><span class="hljs-meta">&gt;&gt;&gt; </span>mydata2---&gt; tensor([[<span class="hljs-number">10</span>, <span class="hljs-number">20</span>],<br>                        [<span class="hljs-number">30</span>, <span class="hljs-number">40</span>],<br>                        [<span class="hljs-number">50</span>, <span class="hljs-number">60</span>]]) torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>])<br><br><span class="hljs-comment"># 3 判断是否使用整块</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;mydata2.is_contiguous()---&gt;&#x27;</span>, mydata2.is_contiguous())<br><span class="hljs-meta">&gt;&gt;&gt; </span>mydata2.is_contiguous()---&gt; <span class="hljs-literal">True</span><br><br><br><span class="hljs-comment"># 4 使用 transpose 函数修改形状</span><br>mydata3 = torch.transpose(data, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)  <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;mydata3---&gt;&#x27;</span>, mydata3, mydata3.shape)<br><span class="hljs-meta">&gt;&gt;&gt; </span>mydata3---&gt; tensor([[<span class="hljs-number">10</span>, <span class="hljs-number">40</span>],<br>                        [<span class="hljs-number">20</span>, <span class="hljs-number">50</span>],<br>                        [<span class="hljs-number">30</span>, <span class="hljs-number">60</span>]]) torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;mydata3.is_contiguous()---&gt;&#x27;</span>, mydata3.is_contiguous())<br><span class="hljs-meta">&gt;&gt;&gt; </span>mydata3.is_contiguous()---&gt; <span class="hljs-literal">False</span><br><br><span class="hljs-comment"># 5 需要先使用 contiguous 函数转换为整块内存的张量，再使用 view 函数</span><br><span class="hljs-built_in">print</span> (mydata3.contiguous().is_contiguous())<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-literal">True</span><br><br>mydata4 = mydata3.contiguous().view(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;mydata4---&gt;&#x27;</span>, mydata4.shape, mydata4)<br><span class="hljs-meta">&gt;&gt;&gt; </span>mydata4---&gt; torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]) tensor([[<span class="hljs-number">10</span>, <span class="hljs-number">40</span>, <span class="hljs-number">20</span>],<br>                                           [<span class="hljs-number">50</span>, <span class="hljs-number">30</span>, <span class="hljs-number">60</span>]])<br></code></pre></td></tr></table></figure><h5 id="stack">3）STACK</h5><p>如果想将新张量堆叠五次，使用 <code>torch.stack()</code> 来实现。</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs inform7"># Stack tensors on top <span class="hljs-keyword">of</span> each other<br>x_stacked = torch.stack(<span class="hljs-comment">[x, x, x, x]</span>, dim=0) # 同pandas的axis，按行堆叠<br>&gt;&gt;&gt;tensor(<span class="hljs-comment">[<span class="hljs-comment">[5., 2., 3., 4., 5., 6., 7.]</span>,</span><br><span class="hljs-comment">           <span class="hljs-comment">[5., 2., 3., 4., 5., 6., 7.]</span>,</span><br><span class="hljs-comment">           <span class="hljs-comment">[5., 2., 3., 4., 5., 6., 7.]</span>,</span><br><span class="hljs-comment">           <span class="hljs-comment">[5., 2., 3., 4., 5., 6., 7.]</span>]</span>)<br></code></pre></td></tr></table></figure><h5 id="squeeze-unsqueeze">4）SQUEEZE / UNSQUEEZE</h5><p>squeeze 函数删除形状为 1 的维度（降维），unsqueeze函数添加形状为1的维度（升维）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python">mydata1 = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])             <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;mydata1---&gt;&#x27;</span>, mydata1.shape, mydata1) <span class="hljs-comment"># 一个普通的数组 1维数据</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>mydata1---&gt; torch.Size([<span class="hljs-number">5</span>]) tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br><br>mydata2 = mydata1.unsqueeze(dim=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;在0维度上 拓展维度：&#x27;</span>, mydata2, mydata2.shape)  <span class="hljs-comment">#1*5</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>在<span class="hljs-number">0</span>维度上 拓展维度： tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]]) torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>])<br><br>mydata3 = mydata1.unsqueeze(dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;在1维度上 拓展维度：&#x27;</span>, mydata3, mydata3.shape)  <span class="hljs-comment">#5*1</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>在<span class="hljs-number">1</span>维度上 拓展维度： tensor([[<span class="hljs-number">1</span>],<br>                              [<span class="hljs-number">2</span>],<br>                              [<span class="hljs-number">3</span>],<br>                              [<span class="hljs-number">4</span>],<br>                              [<span class="hljs-number">5</span>]]) torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">1</span>])<br><br><br>mydata4 = mydata1.unsqueeze(dim=-<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;在-1维度上 拓展维度：&#x27;</span>, mydata4, mydata4.shape) <span class="hljs-comment">#5*1</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>在-<span class="hljs-number">1</span>维度上 拓展维度： tensor([[<span class="hljs-number">1</span>],<br>                               [<span class="hljs-number">2</span>],<br>                               [<span class="hljs-number">3</span>],<br>                               [<span class="hljs-number">4</span>],<br>                               [<span class="hljs-number">5</span>]]) torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">1</span>])<br><br>mydata5 = mydata4.squeeze()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;压缩维度：&#x27;</span>, mydata5, mydata5.shape)  <span class="hljs-comment">#1*5</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>压缩维度： tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]) torch.Size([<span class="hljs-number">5</span>])<br></code></pre></td></tr></table></figure><h5 id="transpose-permute">5）TRANSPOSE/ PERMUTE</h5><p>transpose 函数可以实现交换张量形状的指定维度, 例如: 一个张量的形状为(2, 3, 4) 可以通过 transpose 函数把 3 和 4 进行交换, 将张量的形状变为(2, 4, 3) 。 permute 函数可以一次交换更多的维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python">data = torch.tensor(np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;data shape:&#x27;</span>, data.size())<br><span class="hljs-meta">&gt;&gt;&gt; </span>data shape: torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br><br><span class="hljs-comment"># 1 交换1和2维度</span><br>mydata2 = torch.transpose(data, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;mydata2.shape---&gt;&#x27;</span>, mydata2.shape)<br><span class="hljs-meta">&gt;&gt;&gt; </span>mydata2.shape---&gt; torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>])<br><br><span class="hljs-comment"># 2 将data 的形状修改为 (4, 5, 3), 需要变换多次</span><br>mydata3 =  torch.transpose(data, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>mydata4 = torch.transpose(mydata3, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;mydata4.shape---&gt;&#x27;</span>, mydata4.shape)<br><span class="hljs-meta">&gt;&gt;&gt; </span>mydata4.shape---&gt; torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-comment"># 3 使用 permute 函数将形状修改为 (4, 5, 3)</span><br><span class="hljs-comment"># 3-1 方法1</span><br>mydata5 = torch.permute(data, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;mydata5.shape---&gt;&#x27;</span>, mydata5.shape)<br><span class="hljs-meta">&gt;&gt;&gt; </span>mydata5.shape---&gt; torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-comment"># 3-2 方法2</span><br>mydata6 = data.permute([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;mydata6.shape---&gt;&#x27;</span>, mydata6.shape)<br><span class="hljs-meta">&gt;&gt;&gt; </span>mydata6.shape---&gt; torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>])<br></code></pre></td></tr></table></figure><h5 id="总结">6）总结</h5><p>&lt;1&gt; reshape函数可以在保证张量数据不变的前提下改变数据的维度</p><p>&lt;2&gt; squeeze 和 unsqueeze 函数可以用来增加或者减少维度</p><p>&lt;3&gt; transpose 函数可以实现交换张量形状的指定维度, permute可以一次交换更多的维度</p><p>&lt;4&gt; view 函数也可以用于修改张量的形状,但是它要求被转换的张量内存必须连续，所以一般配合 contiguous 函数使用</p><h4 id="张量的拼接操作">3. 张量的拼接操作</h4><ul><li>torch.cat()</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>data1 = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>data2 = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(data1)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>],<br>         [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>]]])<br><br><br><br><span class="hljs-built_in">print</span>(data2)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[[<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>],<br>         [<span class="hljs-number">7</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>]]])<br><br><br><span class="hljs-comment"># 1. 按0维度拼接</span><br>new_data = torch.cat([data1, data2], dim=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(new_data)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>],<br>             [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>]],<br><br>            [[<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>],<br>             [<span class="hljs-number">7</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>]]])<br><br><span class="hljs-built_in">print</span>(new_data.shape)<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-comment"># 2. 按1维度拼接</span><br>new_data = torch.cat([data1, data2], dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(new_data)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>],<br>             [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>],<br>             [<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>],<br>             [<span class="hljs-number">7</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>]]])<br><br><span class="hljs-built_in">print</span>(new_data.shape)<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>])<br><br><span class="hljs-comment"># 3. 按2维度拼接</span><br>new_data = torch.cat([data1, data2], dim=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(new_data)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([[[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>],<br>             [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>]]])<br><br><span class="hljs-built_in">print</span>(new_data.shape)<br><span class="hljs-meta">&gt;&gt;&gt; </span>torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>])<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>张量</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tensor基础1——张量的创建与转换</title>
    <link href="/Tensor%E5%9F%BA%E7%A1%801-%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E8%BD%AC%E6%8D%A2.html"/>
    <url>/Tensor%E5%9F%BA%E7%A1%801-%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E8%BD%AC%E6%8D%A2.html</url>
    
    <content type="html"><![CDATA[<h4 id="张量的创建">1. 张量的创建</h4><h5 id="张量的基本创建方式">1）张量的基本创建方式</h5><ul><li>torch.tensor 根据指定数据创建张量</li><li>torch.Tensor 根据形状创建张量, 其也可用来创建指定数据的张量</li><li>torch.IntTensor、torch.FloatTensor、torch.DoubleTensor创建指定类型的张量</li></ul><p><strong>1、torch.tensor() 根据指定数据创建张量</strong></p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-meta"># 1. 创建张量标量</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.tensor(10)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor(<span class="hljs-number">10</span>)<br><br><span class="hljs-meta"># 2. numpy 数组, 由于 data 为 float64, 下面代码也使用该类型</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = np.random.randn(2, 3)</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.tensor(<span class="hljs-title">data</span>)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([[ <span class="hljs-number">0.1345</span>,  <span class="hljs-number">0.1149</span>,  <span class="hljs-number">0.2435</span>],<br>            [ <span class="hljs-number">0.8026</span>, -<span class="hljs-number">0.6744</span>, -<span class="hljs-number">1.0918</span>]], dtype=torch.float64)<br><br><span class="hljs-meta"># 3. 列表, 下面代码使用默认元素类型 float32</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = [[10., 20., 30.], [40., 50., 60.]]</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.tensor(<span class="hljs-title">data</span>)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([[<span class="hljs-number">10</span>., <span class="hljs-number">20</span>., <span class="hljs-number">30</span>.],<br>            [<span class="hljs-number">40</span>., <span class="hljs-number">50</span>., <span class="hljs-number">60</span>.]])<br></code></pre></td></tr></table></figure><p><strong>2、torch.Tensor()根据指定形状创建张量，也可以用来创建指定数据的张量</strong></p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 1. 创建2行3列的张量, 默认 dtype 为 float32</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.<span class="hljs-type">Tensor</span>(2, 3)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([[<span class="hljs-number">0.0000e+00</span>, <span class="hljs-number">3.6893e+19</span>, <span class="hljs-number">2.2018e+05</span>],<br>            [<span class="hljs-number">4.6577e-10</span>, <span class="hljs-number">2.4158e-12</span>, <span class="hljs-number">1.1625e+33</span>]])<br><br><span class="hljs-meta"># 2. 注意: 如果传递列表, 则创建包含指定元素的张量</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.<span class="hljs-type">Tensor</span>([10])</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([<span class="hljs-number">10</span>.])<br><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.<span class="hljs-type">Tensor</span>([10, 20])</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([<span class="hljs-number">10</span>., <span class="hljs-number">20</span>.])<br></code></pre></td></tr></table></figure><p><strong>3、torch.IntTensor()、torch.FloatTensor()、torch.DoubleTensor()创建指定类型的张量</strong></p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 1. 创建2行3列, dtype 为 int32 的张量</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.<span class="hljs-type">IntTensor</span>(2, 3)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([[ <span class="hljs-number">0</span>, <span class="hljs-number">1610612736</span>, <span class="hljs-number">1213662609</span>],<br>            [ <span class="hljs-number">805308409</span>,  <span class="hljs-number">156041223</span>,  <span class="hljs-number">1</span>]], dtype=torch.int32)<br><br><span class="hljs-meta"># 2. 注意: 如果传递的元素类型不正确, 则会进行类型转换</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.<span class="hljs-type">IntTensor</span>([2.5, 3.3])</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=torch.int32)<br><br><span class="hljs-meta"># 3. 其他的类型</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.<span class="hljs-type">ShortTensor</span>()  # int16</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.<span class="hljs-type">LongTensor</span>()   # int64</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.<span class="hljs-type">FloatTensor</span>()  # float32</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.<span class="hljs-type">DoubleTensor</span>() # float64</span><br></code></pre></td></tr></table></figure><h5 id="创建线性张量和随机张量">2）创建线性张量和随机张量</h5><ul><li>torch.arange 和 torch.linspace 创建线性张量</li><li>torch.random.init_seed 和 torch.random.manual_seed 随机种子设置</li><li>torch.randn 创建随机张量</li></ul><p><strong>1、torch.arange()、torch.linspace() 创建线性张量</strong></p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 1. 在指定区间按照步长生成元素 [start, end, step)</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.arange(0, 10, 2)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>])<br><br><span class="hljs-meta"># 2. 在指定区间按照元素个数生成 [start, end, steps]</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.linspace(0, 11, 10)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([<span class="hljs-number">0.0000</span>, <span class="hljs-number">1.2222</span>, <span class="hljs-number">2.4444</span>, <span class="hljs-number">3.6667</span>, <span class="hljs-number">4.8889</span>, <span class="hljs-number">6.1111</span>, <span class="hljs-number">7.3333</span>, <span class="hljs-number">8.5556</span>, <span class="hljs-number">9.7778</span>,                     <span class="hljs-number">11.0000</span>])<br></code></pre></td></tr></table></figure><p><strong>2、随机种子操作</strong></p><ul><li>torch.random.initial_seed()查看随机种子</li><li>torch.random.manual_seed() 设置随机数种子</li><li>torch.randn() 创建随机张量</li></ul><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs lua"># <span class="hljs-number">1.</span> 创建随机张量<br>data = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)  # 创建<span class="hljs-number">2</span>行<span class="hljs-number">3</span>列张量<br><span class="hljs-built_in">print</span>(data)<br>&gt;&gt;&gt; tensor(<span class="hljs-string">[[-0.5209, -0.2439, -1.1780],</span><br><span class="hljs-string">            [ 0.8133,  1.1442,  0.6790]]</span>)<br><br># <span class="hljs-number">2.</span>查看随机数种子<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;随机数种子:&#x27;</span>, torch.<span class="hljs-built_in">random</span>.initial_seed())<br>&gt;&gt;&gt; 随机数种子: <span class="hljs-number">4508475192273306739</span><br><br># <span class="hljs-number">3.</span>设置随机数种子 <br>torch.<span class="hljs-built_in">random</span>.manual_seed(<span class="hljs-number">100</span>)<br>data = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(data)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;随机数种子:&#x27;</span>, torch.<span class="hljs-built_in">random</span>.initial_seed())<br>&gt;&gt;&gt; tensor(<span class="hljs-string">[[ 0.3607, -0.2859, -0.3938],</span><br><span class="hljs-string">            [ 0.2429, -1.3833, -2.3134]]</span>)<br>    随机数种子: <span class="hljs-number">100</span><br></code></pre></td></tr></table></figure><h5 id="创建0-1张量">3）创建0-1张量</h5><ul><li>torch.ones 和 torch.ones_like 创建全1张量</li><li>torch.zeros 和 torch.zeros_like 创建全0张量</li><li>torch.full 和 torch.full_like 创建全为指定值张量</li></ul><p><strong>1、torch.ones()、torch.ones_like() 创建全1张量</strong></p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 1. 创建指定形状全0张量</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.zeros(2, 3)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([[<span class="hljs-number">0</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>.],<br>            [<span class="hljs-number">0</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>.]])<br><br><span class="hljs-meta"># 2. 根据张量形状创建全0张量</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.zeros_like(<span class="hljs-title">data</span>)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([[<span class="hljs-number">0</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>.],<br>            [<span class="hljs-number">0</span>., <span class="hljs-number">0</span>., <span class="hljs-number">0</span>.]])<br></code></pre></td></tr></table></figure><p><strong>2、torch.zeros()、torch.zeros_like() 创建全0张量</strong></p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 1. 创建指定形状全1张量</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.ones(2, 3)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([[<span class="hljs-number">1</span>., <span class="hljs-number">1</span>., <span class="hljs-number">1</span>.],<br>            [<span class="hljs-number">1</span>., <span class="hljs-number">1</span>., <span class="hljs-number">1</span>.]])<br><br><span class="hljs-meta"># 2. 根据张量形状创建全1张量</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.ones_like(<span class="hljs-title">data</span>)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([[<span class="hljs-number">1</span>., <span class="hljs-number">1</span>., <span class="hljs-number">1</span>.],<br>            [<span class="hljs-number">1</span>., <span class="hljs-number">1</span>., <span class="hljs-number">1</span>.]])<br></code></pre></td></tr></table></figure><p><strong>3、torch.full()、torch.full_like()创建全为指定值张量</strong></p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 1. 创建指定形状指定值的张量</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.full([2, 3], 10)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([[<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>],<br>            [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>]])<br><br><span class="hljs-meta"># 2. 根据张量形状创建指定值的张量</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.full_like(<span class="hljs-title">data</span>, 20)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>)</span><br>&gt;&gt;&gt; tensor([[<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>],<br>            [<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>]])<br></code></pre></td></tr></table></figure><h5 id="张量元素类型转换">4）张量元素类型转换</h5><ul><li>data.type(torch.DoubleTensor)</li><li>data.double()</li></ul><p><strong>1、data.type(torch.DoubleTensor)</strong></p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-title">data</span> = torch.full([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], <span class="hljs-number">10</span>)<br><span class="hljs-title">print</span>(data.d<span class="hljs-keyword">type</span>)<br>&gt;&gt;&gt; torch.int64<br><br># 将 data 元素类型转换为 float64 类型<br><span class="hljs-title">data</span> = data.<span class="hljs-keyword">type</span>(torch.<span class="hljs-type">DoubleTensor</span>)<br><span class="hljs-title">print</span>(data.d<span class="hljs-keyword">type</span>)<br>&gt;&gt;&gt; torch.float64<br><br># 转换为其他类型<br># data = data.<span class="hljs-keyword">type</span>(torch.<span class="hljs-type">ShortTensor</span>)   # int16<br># data = data.<span class="hljs-keyword">type</span>(torch.<span class="hljs-type">IntTensor</span>)    # int32<br># data = data.<span class="hljs-keyword">type</span>(torch.<span class="hljs-type">LongTensor</span>)   # int64<br># data = data.<span class="hljs-keyword">type</span>(torch.<span class="hljs-type">FloatTensor</span>)  # float32<br></code></pre></td></tr></table></figure><p><strong>2、data.double()</strong></p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.full([2, 3], 10)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>.dtype)</span><br>&gt;&gt;&gt; torch.int64<br><br><span class="hljs-meta"># 将 data 元素类型转换为 float64 类型</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = <span class="hljs-keyword">data</span>.double()</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>.dtype)</span><br>&gt;&gt;&gt; torch.float64<br><br><span class="hljs-meta"># 转换为其他类型</span><br><span class="hljs-meta"># data = data.short()</span><br><span class="hljs-meta"># data = data.int()</span><br><span class="hljs-meta"># data = data.long()</span><br><span class="hljs-meta"># data = data.float()</span><br></code></pre></td></tr></table></figure><h5 id="总结">5）总结</h5><p><strong>&lt;1&gt; 创建张量的方式</strong></p><ul><li>torch.tensor() 根据指定数据创建张量</li><li>torch.Tensor() 根据形状创建张量, 其也可用来创建指定数据的张量</li><li>torch.IntTensor()、torch.FloatTensor()、torch.DoubleTensor()创建指定类型的张量</li></ul><p><strong>&lt;2&gt; 创建线性和随机张量</strong></p><ul><li>torch.arrange() 和 torch.linspace() 创建线性张量</li><li>torch.random.initial_seed() 和 torch.random.manual_seed()随机种子设置</li><li>torch.randn() 创建随机张量</li></ul><p><strong>&lt;3&gt; 创建01张量</strong></p><ul><li>torch.ones() 和 torch.ones_like() 创建全1张量</li><li>torch.zeros() 和 torch.zeros_like() 创建全0张量</li><li>torch.full() 和 torch.full_like() 创建全为指定值张量</li></ul><p><strong>&lt;4&gt; 张量元素类型转换</strong></p><ul><li>data.type(torch.DoubleTensor)</li><li>data.double()</li></ul><h4 id="张量的类型转换">2. 张量的类型转换</h4><h5 id="张量转换为numpy数组">1）张量转换为NUMPY数组</h5><ul><li>使用 Tensor.numpy 函数可以将张量转换为 ndarray数组，但是共享内存，可以使用 copy 函数避免共享。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. 将张量转换为 numpy 数组</span><br>data_tensor = torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br><span class="hljs-comment"># 使用张量对象中的 numpy 函数进行转换</span><br>data_numpy = data_tensor.numpy()<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(data_tensor))<br><span class="hljs-meta">&gt;&gt;&gt; </span>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;torch.Tensor&#x27;</span>&gt;<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(data_numpy))<br><span class="hljs-meta">&gt;&gt;&gt; </span>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;numpy.ndarray&#x27;</span>&gt;<br><br><span class="hljs-comment"># 注意: data_tensor 和 data_numpy 共享内存</span><br><span class="hljs-comment"># 修改其中的一个，另外一个也会发生改变</span><br><span class="hljs-comment"># data_tensor[0] = 100</span><br>data_numpy[<span class="hljs-number">0</span>] = <span class="hljs-number">100</span><br><span class="hljs-built_in">print</span>(data_tensor)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([<span class="hljs-number">100</span>,   <span class="hljs-number">3</span>,   <span class="hljs-number">4</span>])<br><br><span class="hljs-built_in">print</span>(data_numpy)<br><span class="hljs-meta">&gt;&gt;&gt; </span>[<span class="hljs-number">100</span>   <span class="hljs-number">3</span>   <span class="hljs-number">4</span>]<br><br><span class="hljs-comment"># 2. 对象拷贝避免共享内存</span><br>data_tensor = torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br><span class="hljs-comment"># 使用张量对象中的 numpy 函数进行转换，通过copy方法拷贝对象</span><br>data_numpy = data_tensor.numpy().copy()<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(data_tensor))<br><span class="hljs-meta">&gt;&gt;&gt; </span>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;torch.Tensor&#x27;</span>&gt;<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(data_numpy))<br><span class="hljs-meta">&gt;&gt;&gt; </span>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;numpy.ndarray&#x27;</span>&gt;<br><br><span class="hljs-comment"># 注意: data_tensor 和 data_numpy 此时不共享内存</span><br><span class="hljs-comment"># 修改其中的一个，另外一个不会发生改变</span><br><span class="hljs-comment"># data_tensor[0] = 100</span><br>data_numpy[<span class="hljs-number">0</span>] = <span class="hljs-number">100</span><br><span class="hljs-built_in">print</span>(data_tensor)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tensor([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br><br><span class="hljs-built_in">print</span>(data_numpy)<br><span class="hljs-meta">&gt;&gt;&gt; </span>[<span class="hljs-number">100</span>   <span class="hljs-number">3</span>   <span class="hljs-number">4</span>]<br></code></pre></td></tr></table></figure><h5 id="numpy数组转换为张量">2）NUMPY数组转换为张量</h5><ul><li>使用 <strong>from_numpy</strong> 可以将 ndarray 数组转换为Tensor，默认<strong>共享内存</strong>，使用 copy 函数避免共享。</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs routeros">data_numpy = np.array([2, 3, 4])<br><span class="hljs-comment"># 将 numpy 数组转换为张量类型</span><br><span class="hljs-comment"># 1. from_numpy</span><br><span class="hljs-comment"># 2. torch.tensor(ndarray)</span><br>data_tensor = torch.from_numpy(data_numpy)<br><span class="hljs-comment"># nunpy 和 tensor 共享内存</span><br><span class="hljs-comment"># data_numpy[0] = 100</span><br>data_tensor[0] = 100<br><span class="hljs-built_in">print</span>(data_tensor)<br>&gt;&gt;&gt; tensor([100,   3,   4], <span class="hljs-attribute">dtype</span>=torch.int32)<br><br><span class="hljs-built_in">print</span>(data_numpy)<br>&gt;&gt;&gt; [100   3   4]<br></code></pre></td></tr></table></figure><ul><li>使用 <strong>torch.tensor</strong> 可以将 ndarray 数组转换为Tensor，默认<strong>不共享内存</strong>。</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs routeros">data_numpy = np.array([2, 3, 4])<br>data_tensor = torch.tensor(data_numpy)<br><span class="hljs-comment"># nunpy 和 tensor 不共享内存</span><br><span class="hljs-comment"># data_numpy[0] = 100</span><br>data_tensor[0] = 100<br><span class="hljs-built_in">print</span>(data_tensor)<br>&gt;&gt;&gt; tensor([100,   3,   4], <span class="hljs-attribute">dtype</span>=torch.int32)<br><br><span class="hljs-built_in">print</span>(data_numpy)<br>&gt;&gt;&gt; [2 3 4]<br></code></pre></td></tr></table></figure><h5 id="标量张量和数字转换">3）标量张量和数字转换</h5><ul><li>对于只有一个元素的张量，使用item()函数将该值从张量中提取出来</li></ul><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 当张量只包含一个元素时, 可以通过 item() 函数提取出该值</span><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.tensor([30,])</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>.item())</span><br>&gt;&gt;&gt; <span class="hljs-number">30</span><br><br><span class="hljs-class"><span class="hljs-keyword">data</span> = torch.tensor(30)</span><br><span class="hljs-title">print</span>(<span class="hljs-class"><span class="hljs-keyword">data</span>.item())</span><br>&gt;&gt;&gt; <span class="hljs-number">30</span><br></code></pre></td></tr></table></figure><h5 id="总结-1">4）总结</h5><p><strong>1. 张量转换为 numpy 数组</strong></p><ul><li>data_tensor.numpy()</li><li>data_tensor.numpy().copy()</li></ul><p><strong>2. numpy 转换为张量</strong></p><ul><li>torch.from_numpy(data_numpy)</li><li>torch.tensor(data_numpy)</li></ul><p><strong>3. 标量张量和数字转换</strong></p><ul><li>data.item()</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记整理</tag>
      
      <tag>张量</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>集成学习常见模型比对</title>
    <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A6%81%E7%82%B9%E5%AF%B9%E6%AF%94.html"/>
    <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A6%81%E7%82%B9%E5%AF%B9%E6%AF%94.html</url>
    
    <content type="html"><![CDATA[<p>集成学习的基础思想是通过组合多个基学习器形成整体强学习器,使基学习器在预测准确性、降低过拟合风险、增强模型的鲁棒性等方面获得明显提升,集成学习主要包含Bagging和Boosting两大分类。本文对比总结了四种集成学习常见模型。</p><span id="more"></span><p>Bagging是一种并行式的集成学习方法，其特点包括通过有放回的抽样产生不同的训练集，从而训练多个不同的学习器，并通过平均投票或多数表决的方式决定预测结果。此外，Bagging允许弱学习器并行训练，代表算法包括随机森林算法。</p><p>Boosting是一种串行式的集成学习方法，其特点是随着学习的积累从弱到强，每加入一个弱学习器，整体能力会得到提升。Boosting对学习器进行加权投票，采用串行方式进行学习，具有明确的先后顺序。代表算法包括Adaboost、GBDT、XGBoost以及LightGBM。</p><p><img src="/images/集成学习常见模型对比/集成学习对比示意.png" /></p><table><thead><tr class="header"><th>模型</th><th>核心要点</th><th>模型优缺点</th><th>模型应用</th></tr></thead><tbody><tr class="odd"><td>Bagging随机森林</td><td>1. 随机有放回的抽样产生不同的训练集(boostrap)<br>2.基于不同抽样训练多个基学习器（如决策树）<br>3.通过投票或平均组合预测结果</td><td>优点:<br>- 泛化错误率低<br>-易于并行训练<br>缺点:<br>-性能上限低</td><td>1. 分类问题<br>2. 回归问题</td></tr><tr class="even"><td>Adaptive Boosting</td><td>1. 迭代构建弱学习器<br>2.聚焦错误样本,每轮根据分类结果调整样本及模型权重<br>3.组合加权弱学习器成强学习器</td><td>优点:<br>- 泛化能力强<br>- 易于处理多种数据<br>缺点:<br>-对离群点敏感<br>- 需要预处理高维或不平衡数据</td><td>1. 分类问题<br>2. 图像识别</td></tr><tr class="odd"><td>GBDT (梯度提升树)</td><td>1. 迭代构建决策树<br>2. 拟合损失函数的负梯度训练新树<br>3.累加方式构建最终模型</td><td>优点:<br>- 准确性高<br>- 可以适应多种损失函数<br>缺点:<br>-容易过拟合<br>- 计算量大</td><td>1. 回归问题<br>2. 排名问题<br>3. 分类问题</td></tr><tr class="even"><td>XGBoost</td><td>1. 基于GBDT的高效实现<br>2. 加入正则化项解决GBDT过拟合问题<br>3.损失函数泰勒二阶近似优化拟合函数<br/>4.支持并行化和缺失值处理</td><td>优点:<br>- 速度快<br>- 准确性高,防止过拟合<br>-支持多种目标函数和评估指标<br>缺点:<br>- 参数调整复杂<br>-可能需要更多的内存</td><td>1. 赢取竞赛的首选算法<br>2. 排名问题<br>3. 分类和回归问题</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>集成学习</tag>
      
      <tag>笔记整理</tag>
      
      <tag>总结归纳</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模型调优指南--过拟合</title>
    <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97--%E8%BF%87%E6%8B%9F%E5%90%88.html"/>
    <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%B0%83%E4%BC%98%E6%8C%87%E5%8D%97--%E8%BF%87%E6%8B%9F%E5%90%88.html</url>
    
    <content type="html"><![CDATA[<p>过拟合是机器学习模型在训练数据上表现很好，但在新数据上表现不佳的一种现象。它的发生通常是由于模型过于复杂，以至于能够记住训练数据的噪声和细节，而不是学习到数据的普遍模式和特征。以下是导致过拟合的常见原因以及相应的解决方法：<span id="more"></span></p><h3 id="过拟合的原因">过拟合的原因</h3><ol type="1"><li><strong>模型复杂度过高</strong>：模型参数过多（如深层神经网络的层数和节点数过多），导致模型具有很强的表达能力，能够拟合训练数据中的噪声。</li><li><strong>训练数据不足</strong>：训练数据量太少，使得模型只能依赖于有限的数据，容易记住而不是泛化。</li><li><strong>数据噪声</strong>：训练数据中包含大量噪声或异常值，模型在训练时会把这些噪声也当作有效模式来学习。</li><li><strong>训练次数过多</strong>：模型在训练数据上迭代次数过多，导致模型对训练数据的拟合过于精细。</li></ol><h3 id="解决过拟合的方法">解决过拟合的方法</h3><ol type="1"><li><strong>增加训练数据量</strong>：通过收集更多的数据或使用数据增强技术来扩展训练集，可以帮助模型学习到更加普遍的特征。<ul><li><strong>数据增强</strong>：对于图像数据，可以使用翻转、旋转、缩放等技术来生成更多的训练样本。</li></ul></li><li><strong>简化模型</strong>：减少模型的参数数量，选择一个较为简单的模型结构。<ul><li><strong>正则化</strong>：在损失函数中加入正则化项，如L1正则化（Lasso）和L2正则化（Ridge），可以防止模型参数过大，减小模型的复杂度。</li></ul></li><li><strong>使用交叉验证</strong>：将数据集划分为多个子集，进行交叉验证，以确保模型在不同数据子集上的表现一致，帮助发现和防止过拟合。<ul><li><strong>K折交叉验证</strong>：将数据集分成K个子集，每次用K-1个子集训练模型，剩下的一个子集测试，循环K次，综合评估模型表现。</li></ul></li><li><strong>提前停止（EarlyStopping）</strong>：在训练过程中监控验证集的误差，当验证误差不再降低时，停止训练，避免模型在训练集上过度拟合。</li><li><strong>集成方法</strong>：使用多种模型的组合来降低单一模型过拟合的风险。<ul><li><strong>袋装（Bagging）</strong>：如随机森林，通过对数据进行多次采样并训练多个模型，最后进行投票或平均来得到最终结果。</li><li><strong>提升（Boosting）</strong>：如梯度提升决策树（GradientBoosting Decision Trees,GBDT），通过逐步训练多个弱模型，每次针对前一轮模型的错误进行改进。</li></ul></li><li><strong>正则化技术</strong>：<ul><li><strong>Dropout</strong>：在神经网络训练中随机将一部分神经元输出设置为0，以防止模型过于依赖某些特定的路径。</li><li><strong>数据标准化</strong>：对输入数据进行归一化处理，使其均值为0，标准差为1，帮助模型更快收敛，并减少过拟合的可能。</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>AI基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记整理</tag>
      
      <tag>模型训练</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git的原理及常用指令</title>
    <link href="/Git%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8.html"/>
    <url>/Git%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8.html</url>
    
    <content type="html"><![CDATA[<p>Git是一种分布式版本控制系统，用于跟踪项目中的更改，并允许多个开发者协作，本文介绍了Git的版本管理特点及常用指令。</p><span id="more"></span><h2 id="git的版本控制要点">Git的版本控制要点</h2><p>git与传统的集中式版本控制系统（如 Subversion 和CVS）有显著不同。分布式版本控制系统（DVCS）提供了更高的灵活性和可靠性。下面是关于Git 分布式管理的一些关键点：</p><p><img src="/images/git/git版本管理模式.png" /></p><h3 id="分布式架构">1. 分布式架构</h3><p>在 Git中，每个开发者的工作目录都包含了整个项目的完整版本库。这意味着每个开发者都有一个项目的完整副本，包括所有的历史记录和分支。这样，即使中央服务器出现问题，开发者仍然可以继续工作并且不会丢失任何数据。</p><h3 id="本地操作">2. 本地操作</h3><p>Git的大部分操作都是在本地完成的，例如提交（commit）、创建分支（branching）、合并（merging）等。这使得Git 操作非常快速，因为不需要与远程仓库通信。</p><h3 id="分支和合并">3. 分支和合并</h3><p>Git的分支和合并功能非常强大且灵活。创建和合并分支的操作都是本地的，效率高并且不会影响其他开发者的工作。分支在Git 中是轻量级的，这鼓励开发者频繁使用分支来进行独立开发和实验。</p><h3 id="协作工作流">4. 协作工作流</h3><p>Git 支持多种协作工作流，例如：</p><ul><li><strong>集中式工作流</strong>：所有的开发者都从中央仓库中拉取（pull）和推送（push）代码。</li><li><strong>功能分支工作流</strong>：每个新功能都有一个单独的分支，开发完成后合并回主分支。</li><li><strong>Forking 工作流</strong>：开发者从主仓库 fork出自己的仓库，在自己的仓库中工作，完成后向主仓库提交 pull request。</li></ul><h3 id="远程仓库">5. 远程仓库</h3><p>虽然 Git是分布式的，但它仍然支持通过远程仓库来进行团队协作。远程仓库通常托管在GitHub、GitLab 或 Bitbucket等平台上。开发者可以将本地的更改推送到远程仓库，也可以从远程仓库拉取其他开发者的更改。</p><h3 id="数据完整性">6. 数据完整性</h3><p>Git 使用 SHA-1哈希函数来确保数据的完整性。每一个文件、提交和标记都由一个唯一的哈希值标识，这些哈希值在版本库中是唯一的，可以确保数据不会被意外篡改。</p><h3 id="离线工作">7. 离线工作</h3><p>由于 Git的分布式特性，开发者可以在没有网络连接的情况下进行大部分操作。所有的操作都是在本地完成的，等到有网络连接时，再将更改推送到远程仓库。</p><h2 id="git的常用命令">Git的常用命令</h2><h3 id="配置">配置</h3><ol type="1"><li><p><strong>配置用户信息</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">git config --global user.name <span class="hljs-string">&quot;Your Name&quot;</span><br>git config --global user.email <span class="hljs-string">&quot;your.email@example.com&quot;</span><br></code></pre></td></tr></table></figure></p></li><li><p><strong>查看配置</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git config --list<br></code></pre></td></tr></table></figure></p></li></ol><h3 id="基本操作">基本操作</h3><ol type="1"><li><p><strong>初始化仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git init<br></code></pre></td></tr></table></figure>在当前目录中创建一个新的 Git 仓库。</p></li><li><p><strong>克隆仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> &lt;repository_url&gt;<br></code></pre></td></tr></table></figure>从远程仓库克隆一个副本到本地。</p></li><li><p><strong>查看状态</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git status<br></code></pre></td></tr></table></figure>显示工作目录和暂存区的状态。</p></li><li><p><strong>添加文件到暂存区</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git add &lt;file&gt;<br></code></pre></td></tr></table></figure>将文件添加到暂存区。使用 <code>git add .</code>可以添加所有更改的文件。</p></li><li><p><strong>提交更改</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git commit -m <span class="hljs-string">&quot;Commit message&quot;</span><br></code></pre></td></tr></table></figure>提交暂存区的更改并附带提交信息。</p></li><li><p><strong>查看日志</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git <span class="hljs-built_in">log</span><br></code></pre></td></tr></table></figure>查看提交历史记录。</p></li></ol><h3 id="分支操作">分支操作</h3><ol type="1"><li><p><strong>创建新分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git branch &lt;branch_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>切换分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git checkout &lt;branch_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>创建并切换到新分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git checkout -b &lt;branch_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>合并分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git merge &lt;branch_name&gt;<br></code></pre></td></tr></table></figure>将指定分支合并到当前分支。</p></li><li><p><strong>删除分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git branch -d &lt;branch_name&gt;<br></code></pre></td></tr></table></figure>删除指定的分支。</p></li></ol><h3 id="远程操作">远程操作</h3><ol type="1"><li><p><strong>添加远程仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git remote add &lt;remote_name&gt; &lt;url&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>查看远程仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git remote -v<br></code></pre></td></tr></table></figure></p></li><li><p><strong>从远程仓库拉取更改</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git pull &lt;remote_name&gt; &lt;branch_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>推送更改到远程仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git push &lt;remote_name&gt; &lt;branch_name&gt;<br></code></pre></td></tr></table></figure></p></li></ol><h3 id="撤销操作">撤销操作</h3><ol type="1"><li><p><strong>撤销工作目录中的更改</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git checkout -- &lt;file&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>重置暂存区的更改</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git reset &lt;file&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>撤销提交</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git revert &lt;commit&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>强制重置分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git reset --hard &lt;commit&gt;<br></code></pre></td></tr></table></figure></p></li></ol><h3 id="标签">标签</h3><ol type="1"><li><p><strong>创建标签</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git tag &lt;tag_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>查看标签</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git tag<br></code></pre></td></tr></table></figure></p></li><li><p><strong>推送标签到远程仓库</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git push &lt;remote_name&gt; &lt;tag_name&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>删除标签</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git tag -d &lt;tag_name&gt;<br></code></pre></td></tr></table></figure></p></li></ol><h3 id="比较">比较</h3><ol type="1"><li><p><strong>比较文件</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git diff &lt;file&gt;<br></code></pre></td></tr></table></figure></p></li><li><p><strong>比较分支</strong> <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">git diff &lt;branch1&gt; &lt;branch2&gt;<br></code></pre></td></tr></table></figure></p></li></ol><p>这些 Git的基本操作，熟练使用可以极大地提升管理代码和协作开发的工作效率。</p>]]></content>
    
    
    <categories>
      
      <category>编程基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>git</tag>
      
      <tag>代码管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性回归知识点梳理</title>
    <link href="/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86.html"/>
    <url>/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86.html</url>
    
    <content type="html"><![CDATA[<p>🏠线性回归可以称得上最经典的回归模型，从房子值多少钱，再到股票价格的涨跌🌈，疾病与各种因素的关联，广告投放的收益等，都使它擅长的领域💼接下来让我们走进线性回归💖</p><span id="more"></span><h3 id="线性回归的概念">1.线性回归的概念</h3><h5id="利用-回归方程函数-对-一个或多个自变量特征值和因变量目标值之间-关系进行建模的一种分析方式.">利用回归方程(函数) 对 一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式.</h5><h5 id="yw1x1w2x2....wnb-wtb">y=w1x1+w2x2+....+w(n)+b= wT+b</h5><ul><li>w=weight</li><li>b=bias wT x为将权重系数转置并与x相乘,矩阵的乘法</li></ul><h5 id="常用于分类与回归问题">常用于分类与回归问题</h5><h3 id="损失函数">2.损失函数</h3><h5 id="loss-functioncost-function目标函数成本函数">loss function/costfunction/目标函数/成本函数</h5><h5 id="最小二乘损失计算">最小二乘损失计算:</h5><p><span class="math display">\[J(w,b)=\sum_{i=0}^m\bigl(h\bigl(x^i\bigr)-y^i\bigr)^2\]</span></p><h5 id="均方误差mse">均方误差MSE</h5><p><span class="math display">\[J(w,b)=\frac1m\sum_{i=0}^m\left(h\left(x^i\right)-y^i\right)^2\]</span></p><h5 id="平均绝对误差mae">平均绝对误差MAE</h5><h5id="注-mse与mae既能在模型训练阶段作为损失函数求解拟合函数最优解又可作为模型评估阶段衡量已有模型误差大小">注:MSE与MAE既能在模型训练阶段作为损失函数求解拟合函数最优解,又可作为模型评估阶段,衡量已有模型误差大小</h5><h3 id="损失函数推导_正规方程法">3.损失函数推导_正规方程法</h3><p><span class="math display">\[J(w) =∥Xw−y∥_2^2\]</span></p><p><span class="math display">\[w=(X^TX)^{-1}*X^Ty\]</span></p><h5 id="优点可以精确求解">优点:可以精确求解</h5><h5 id="缺点计算量极大xt-x的逆不存在时无解">缺点:计算量极大,X^TX的逆不存在时无解</h5><h3 id="梯度与导数">4.梯度与导数</h3><h5id="导数表征了函数在某点处的变化速率.">导数表征了函数在某点处的变化速率.</h5><h5 id="梯度gradient">梯度（gradient）</h5><ul><li>多元函数中，导数不再是一个单一的数值，而是一个向量，因为它涉及到函数对多个自变量的变化率。这个向量被称为梯度（gradient），它表示了函数在某一点上沿着各个自变量方向的变化率。</li></ul><h5 id="梯度的性质">梯度的性质</h5><ul><li>梯度的方向是函数在给定点增长最快的方向。</li><li>梯度的模（长度）是函数在给定点处沿最大增长方向的增长率</li><li>梯度是垂直于等值面的</li><li>对函数求导</li></ul><p><span class="math display">\[f(\theta)=\theta_0x_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\cdots+\theta_nx_n\]</span></p><p><span class="math display">\[\nabla f=\begin{bmatrix}\frac{\partialf}{\partial\theta_0}\\\frac{\partialf}{\partial\theta_1}\\\vdots\\\frac{\partialf}{\partial\theta_n}\end{bmatrix}=\begin{bmatrix}x_0\\x_1\\\vdots\\x_n\end{bmatrix}\]</span></p><h3 id="梯度下降">5.梯度下降</h3><h5 id="梯度下降公式"><strong>梯度下降公式</strong></h5><ul><li>循环迭代求当前点的梯度，更新当前的权重参数</li><li></li></ul><p><span class="math display">\[\theta_{i+1}=\theta_i-\alpha\frac\partial{\partial\theta_i}J(\theta)\]</span> - θ_i:初始位置 α:学习率(步长),一般取值范围0.001 ~ 0.01 ∂/(∂θ_i) J(θ) :损失函数在i处的导数</p><h5 id="梯度下降优化过程"><strong>梯度下降优化过程 </strong></h5><ul><li><p>给定学习率,步长,初始位置</p></li><li><p>计算该点梯度方向并取反</p></li><li><p>向梯度反方向移动</p></li><li><p>重复以上步骤</p></li><li><p>达到收敛条件</p><ul><li>两次差距小于指定的阈值 •</li><li>达到指定的迭代次数</li></ul></li></ul><h5 id="学习率"><strong>学习率</strong></h5><ul><li>步长决定了在梯度下降迭代过程中</li><li>学习率太小，下降的速度会慢</li><li>学习率太大：容易造成错过最低点、产生下降过程中的震荡、甚至梯度爆炸</li></ul><h5 id="推导过程">推导过程</h5><ul><li>已知</li></ul><p><span class="math display">\[h_{(\theta)}=\theta_{1}x_{1}+\theta_{2}x_{2}+\cdots+\theta_{\mathrm{m}}x_{\mathrm{m}}+b\\=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\cdots+\theta_{\mathrm{m}}x_{\mathrm{m}}\]</span> - 损失函数</p><p><span class="math display">\[J_{(\theta)}=\frac1{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2\]</span> - 梯度下降公式</p><p><span class="math display">\[\theta_{i+1}=\theta_i-\alpha\frac\partial{\partial\theta_i}J(\theta)\]</span></p><ul><li>对损失函数求导</li></ul><p><span class="math display">\[J&#39;_{(\theta)}=\frac{\partial\mathrm{J}(\theta)}{\partial\theta}=\frac{2*1}{2m}\sum_{i=1}^{m}(h_{\theta}\big(x^{i}\big)-y^{i})^{2-1}*h_{\theta}\big(x^{i}\big)^{\prime}\]</span> - <strong>带入梯度下降公式</strong></p><p><span class="math display">\[\theta_j=\theta_j-\alpha\frac1m\sum_{i=1}^m(h_\theta(x^i)-y^i)*x_j^i\]</span> - 参数说明</p><p>θ_j:当前损失函数的梯度位置/原函数的特征权重<br />m,n:行数,列数<br />i,j:列索引,行索引<br />x,y:特征向量与目标向量</p><h3 id="常见梯度下降算法">6.常见梯度下降算法</h3><h5 id="全梯度下降算法-fgd">全梯度下降算法 FGD</h5><ul><li>每次迭代使用全样本梯度<br /></li><li>(硬件要求极高,数据量大时无法实现)</li></ul><h5 id="随机梯度下降-sgd">随机梯度下降 SGD</h5><ul><li>每次迭代随机选择并使用一个样本梯度<br /></li><li>(容易受异常值影响)</li></ul><h5id="小批量梯度下降算法-mini-bantch-最常用"><strong>小批量梯度下降算法mini-bantch 最常用√</strong></h5><ul><li>每次迭代随机选择并使用小批量的样本梯度<br /></li><li>(在硬件性能满足的情况下,每批的量应该尽可能大)</li></ul><h5 id="随机平均梯度下降-sag">随机平均梯度下降 SAG</h5><ul><li>每次迭代随机选择并使用一个样本梯度并和以往样本梯度值做平均</li><li>(解决异常值影响问题,但训练初期受异常值影响较大)</li></ul><h3 id="回归问题的评估方法">7.回归问题的评估方法</h3><h5 id="平方误差mse">平方误差MSE</h5><p><span class="math display">\[=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}\]</span></p><h5 id="平均绝对误差mae-1">平均绝对误差MAE</h5><p><span class="math display">\[=\frac{1}{n}\sum_{i=1}^{n}|y_{i}-\hat{y}_{i}|\]</span></p><h5 id="均方根误差">均方根误差</h5><p><span class="math display">\[RMSE=\sqrt{\frac1n\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2}\]</span></p><h3 id="模型拟合">8.模型拟合</h3><h5 id="过拟合"><strong>过拟合</strong></h5><ul><li><p>原因</p><ul><li>模型过于复杂,学习到了过多异常特征</li><li>数据噪声大</li></ul></li><li><p>解决方案</p><ul><li>数据清洗</li><li>正则化</li><li>精简特征维度</li><li>增加数据量</li></ul></li></ul><h5 id="欠拟合"><strong>欠拟合</strong></h5><ul><li><p>原因</p><ul><li>模型复杂度低</li><li>特征选择不当</li><li>数据量不足</li><li>正则化过度</li></ul></li><li><p>解决方案</p><ul><li>添加多项式特征项</li><li>添加其它特征</li><li>增加训练量</li></ul></li></ul><h3 id="正则化">9.正则化</h3><h5id="概念在模型训练时数据中有些特征影响模型复杂度或者某个特征的异常值较多-所以要尽量减少这个特征的影响甚至删除某个特征的影响这就是正则化正则化是添加在损失函数中的特殊项.">概念:在模型训练时，数据中有些特征影响模型复杂度、或者某个特征的异常值较多，所以要尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化。正则化是添加在损失函数中的特殊项.</h5><h5 id="l1正则化"><strong>L1正则化:</strong></h5><p><span class="math display">\[J(w)=\mathrm{MSE}(w)+\alpha\sum_{i=1}^{n}\mid w_{i}\mid\]</span> - • α叫做惩罚系数，该值越大则权重调整的幅度就越大，即：表示对特征权重惩罚力度就越大- L1 正则化会使得权重趋向于 0，甚至等于0，使得某些特征失效，达到特征筛选的目的 - from sklearn.linear_modelimport Lasso</p><h5 id="l2正则化"><strong>L2正则化</strong></h5><ul><li></li></ul><p><span class="math display">\[J(w)=\mathrm{MSE}(w)+\alpha\sum_{i=1}^nw_i^2\]</span> - L2 正则化会使得权重趋向于 0，一般不等于 0 - fromsklearn.linear_model import Ridge - L2的线性回归又称为岭回归</p>]]></content>
    
    
    <categories>
      
      <category>categories</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>笔记整理</tag>
      
      <tag>回归模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
