

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/linxkon_blog.png">
  <link rel="icon" href="/img/linxkon_blog.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="linxkon">
  <meta name="keywords" content="技术分享，项目实战，生活记录">
  
    <meta name="description" content="高效的微调对于将大语言模型 (LLM) 适应下游任务至关重要。然而，在不同模型上实施这些方法需要付出不小的努力。 LLAMA-FACTORY是一个集成一套高效训练方法的统一框架。它允许用户通过内置的 Web UI LLAMA-BOARD 灵活地自定义 100 多个 LLM 的微调，无需编码。 （本文旨在说明LLAMA-FACTORY引入了NLP领域的哪些训练微调技术及其优势应用领域，项目部">
<meta property="og:type" content="article">
<meta property="og:title" content="高效微调统一框架——LLAMA-FACTORY技术点详解">
<meta property="og:url" content="https://linxkon.github.io/LLAMA-FACTORY%E6%8A%80%E6%9C%AF%E7%82%B9%E6%A2%B3%E7%90%86.html">
<meta property="og:site_name" content="AI·你所爱">
<meta property="og:description" content="高效的微调对于将大语言模型 (LLM) 适应下游任务至关重要。然而，在不同模型上实施这些方法需要付出不小的努力。 LLAMA-FACTORY是一个集成一套高效训练方法的统一框架。它允许用户通过内置的 Web UI LLAMA-BOARD 灵活地自定义 100 多个 LLM 的微调，无需编码。 （本文旨在说明LLAMA-FACTORY引入了NLP领域的哪些训练微调技术及其优势应用领域，项目部">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://linxkon.github.io/images/index_pic/LLAMA-FACTORY.png">
<meta property="article:published_time" content="2024-05-21T14:21:13.000Z">
<meta property="article:modified_time" content="2024-08-15T01:33:39.189Z">
<meta property="article:author" content="linxkon">
<meta property="article:tag" content="工具框架">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="效率工具">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://linxkon.github.io/images/index_pic/LLAMA-FACTORY.png">
  
  
  
  <title>高效微调统一框架——LLAMA-FACTORY技术点详解 - AI·你所爱</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"linxkon.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"enable":true,"app_id":"XLEbEr6BfzRRh34xJtmOEom0-MdYXbMMI","app_key":"3bwflR7evMRYC6JTohHAE31C","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>AI·你所爱 | Linxkon@gmail.com</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/pursenight.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="高效微调统一框架——LLAMA-FACTORY技术点详解"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-05-21 22:21" pubdate>
          2024年5月21日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.7k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          48 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">高效微调统一框架——LLAMA-FACTORY技术点详解</h1>
            
            
              <div class="markdown-body">
                
                <p>高效的微调对于将大语言模型 (LLM)
适应下游任务至关重要。然而，在不同模型上实施这些方法需要付出不小的努力。
LLAMA-FACTORY是一个集成一套高效训练方法的统一框架。它允许用户通过内置的
Web UI LLAMA-BOARD 灵活地自定义 100 多个 LLM 的微调，无需编码。
（本文旨在说明LLAMA-FACTORY引入了NLP领域的哪些训练微调技术及其优势应用领域，项目部署与应用参考官方网址：https://github.com/hiyouga/LLaMA-Factory）
<span id="more"></span></p>
<h3 id="一.-llama-factory简介">一. LLAMA-FACTORY简介</h3>
<p>LLAMA-FACTORY是一个使 LLM
微调任务实现低代码规范化的框架。它通过可扩展的模块统一了各种高效的微调方法，从而能够以最少的资源和高吞吐量对数百种
LLM
进行微调。此外，它还简化了常用的训练方法，包括<strong>生成式预训练</strong>（Radford，2018）<strong>、监督微调
(SFT)</strong>（Wei，2022）、<strong>人类反馈中强化学习
(RLHF)</strong>（Ouyang，2022）和<strong>直接偏好优化
(DPO)</strong>（Rafailov，2023）。用户可以利用<strong>命令行或 Web
界面以最少或无需编码工作量来定制和微调LLM。</strong></p>
<p>下表是其和现存LLM调优工具的特征比较：</p>
<figure>
<img src="/images/llamafactory/image-20240814114808848.png" srcset="/img/loading.gif" lazyload
alt="image-20240814114808848" />
<figcaption aria-hidden="true">image-20240814114808848</figcaption>
</figure>
<p>LLAMA-FACTORY
由三个主要模块组成：模型加载器、数据工作器和训练器。尽量减少这些模块对特定模型和数据集的依赖，使框架能够灵活地扩展到数百个模型和数据集。具体来说，首先建立一个模型注册表，模型加载器可以通过识别精确的层将适配器精确地连接到预训练模型。然后，开发一个数据描述规范，允许数据工作器通过对齐相应的列来收集数据集。此外，提供高效微调方法的即插即用实现，使训练器能够通过替换默认方法激活。允许这些模块在不同的训练方法中重复使用，从而显著降低新方法的集成成本。</p>
<p>下表是支持的LLM清单：</p>
<table style="width:100%;">

<thead>
<tr class="header">
<th>模型名</th>
<th>模型大小</th>
<th>Template</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/baichuan-inc">Baichuan 2</a></td>
<td>7B/13B</td>
<td>baichuan2</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/bigscience">BLOOM/BLOOMZ</a></td>
<td>560M/1.1B/1.7B/3B/7.1B/176B</td>
<td>-</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/THUDM">ChatGLM3</a></td>
<td>6B</td>
<td>chatglm3</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/CohereForAI">Command R</a></td>
<td>35B/104B</td>
<td>cohere</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/deepseek-ai">DeepSeek
(Code/MoE)</a></td>
<td>7B/16B/67B/236B</td>
<td>deepseek</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/tiiuae">Falcon</a></td>
<td>7B/11B/40B/180B</td>
<td>falcon</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/google">Gemma/Gemma
2/CodeGemma</a></td>
<td>2B/7B/9B/27B</td>
<td>gemma</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/THUDM">GLM-4</a></td>
<td>9B</td>
<td>glm4</td>
</tr>
<tr class="odd">
<td><a
target="_blank" rel="noopener" href="https://huggingface.co/internlm">InternLM2/InternLM2.5</a></td>
<td>7B/20B</td>
<td>intern2</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama">Llama</a></td>
<td>7B/13B/33B/65B</td>
<td>-</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/meta-llama">Llama 2</a></td>
<td>7B/13B/70B</td>
<td>llama2</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/meta-llama">Llama 3/Llama
3.1</a></td>
<td>8B/70B</td>
<td>llama3</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/llava-hf">LLaVA-1.5</a></td>
<td>7B/13B</td>
<td>vicuna</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/openbmb">MiniCPM</a></td>
<td>1B/2B</td>
<td>cpm</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/mistralai">Mistral/Mixtral</a></td>
<td>7B/8x7B/8x22B</td>
<td>mistral</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/allenai">OLMo</a></td>
<td>1B/7B</td>
<td>-</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/google">PaliGemma</a></td>
<td>3B</td>
<td>gemma</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/microsoft">Phi-1.5/Phi-2</a></td>
<td>1.3B/2.7B</td>
<td>-</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/microsoft">Phi-3</a></td>
<td>4B/7B/14B</td>
<td>phi</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/Qwen">Qwen/Qwen1.5/Qwen2
(Code/Math/MoE)</a></td>
<td>0.5B/1.5B/4B/7B/14B/32B/72B/110B</td>
<td>qwen</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/bigcode">StarCoder 2</a></td>
<td>3B/7B/15B</td>
<td>-</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/xverse">XVERSE</a></td>
<td>7B/13B/65B</td>
<td>xverse</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/01-ai">Yi/Yi-1.5</a></td>
<td>6B/9B/34B</td>
<td>yi</td>
</tr>
<tr class="even">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/01-ai">Yi-VL</a></td>
<td>6B/34B</td>
<td>yi_vl</td>
</tr>
<tr class="odd">
<td><a target="_blank" rel="noopener" href="https://huggingface.co/IEITYuan">Yuan 2</a></td>
<td>2B/51B/102B</td>
<td>yuan</td>
</tr>
</tbody>
</table>
<p>LLAMA-FACTORY 是用
PyTorch（Paszke，2019）实现的，并且从开源库中获益良多，例如
Transformers（Wolf，2020）、PEFT（Mangrulkar，2022）和 TRL（von
Werra，2020）。在此基础上，提供一个具有更高抽象级的开箱即用框架。此外，用
Gradio（Abid，2019）构建 LLAM-ABOARD，无需编码即可微调 LLM。</p>
<p>高效的 LLM
微调技术可分为两大类：专注于优化和面向计算。高效优化技术的主要目标是调整
LLM 参数，同时将成本保持在最低水平。另一方面，高效的计算方法则力求减少
LLM 中所需计算的时间或空间。LLAMA-FACTORY 中特色功能如下。</p>
<blockquote>
<ul>
<li><strong>多种模型</strong>：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi
等等。</li>
<li><strong>集成方法</strong>：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO
训练、DPO 训练、KTO 训练、ORPO 训练等等。</li>
<li><strong>多种精度</strong>：16 比特全参数微调、冻结微调、LoRA
微调和基于 AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ 的 2/3/4/5/6/8 比特 QLoRA
微调。</li>
<li><strong>先进算法</strong>：GaLore、BAdam、Adam-mini、DoRA、LongLoRA、LLaMA
Pro、Mixture-of-Depths、LoRA+、LoftQ、PiSSA 和 Agent 微调。</li>
<li><strong>实用技巧</strong>：FlashAttention-2、Unsloth、RoPE
scaling、NEFTune 和 rsLoRA。</li>
<li><strong>实验监控</strong>：LlamaBoard、TensorBoard、Wandb、MLflow
等等。</li>
<li><strong>极速推理</strong>：基于 vLLM 的 OpenAI 风格
API、浏览器界面和命令行接口。</li>
</ul>
</blockquote>
<h3 id="二.-特色功能介绍">二. 特色功能介绍</h3>
<p>LLAMA-FACTORY
可以覆盖LLM的预训练和RLHF(人类反馈强化)的完整阶段,以下是详细说明:</p>
<h4 id="训练方法集成">2.1 训练方法集成</h4>
<p><strong>1. （增量）预训练 (Pre-training)</strong></p>
<ul>
<li><strong>预训练:</strong>
预训练是指在大规模文本数据上训练一个语言模型，使其学习通用的语言表示。预训练模型能够捕捉语言的语法、语义和上下文信息，为下游任务提供良好的初始化参数。</li>
<li><strong>增量预训练:</strong>
指在已有的预训练模型基础上，使用新的数据或任务继续训练，以增强模型的性能或使其适应新的领域。增量预训练可以避免从头训练，节省时间和资源。</li>
</ul>
<p><strong>2. （多模态）指令监督微调 (Instruction-tuned
Fine-tuning)</strong></p>
<ul>
<li><strong>指令监督微调:</strong>
指使用指令-答案对数据对预训练模型进行微调，使其能够遵循指令完成各种任务。指令通常描述了任务目标和输入格式，答案则是期望的输出。</li>
<li><strong>多模态指令监督微调:</strong>
将指令监督微调扩展到多模态领域，例如图像-文本对或视频-文本对，使模型能够理解和处理多模态信息。</li>
</ul>
<p><strong>3. 奖励模型训练 (Reward Model Training)</strong></p>
<p>奖励模型用于评估模型生成的文本质量。在强化学习中，奖励模型为模型提供反馈信号，指导模型学习生成更优质的文本。奖励模型通常通过人工标注数据或根据特定指标进行训练。</p>
<p><strong>4. PPO 训练 (Proximal Policy Optimization)</strong></p>
<p>PPO 是一种强化学习算法，用于训练 agent (例如语言模型)
在与环境交互的过程中学习最佳策略。PPO
算法通过迭代更新策略网络的参数，以最大化累积奖励。</p>
<p><strong>5. DPO 训练 (Direct Preference Optimization)</strong></p>
<p>DPO
是一种基于偏好的强化学习算法，它直接从人类偏好数据中学习奖励函数，并使用该奖励函数来优化策略。DPO
避免了手动设计奖励函数的困难，能够更好地捕捉人类的偏好。</p>
<p><strong>6. KTO 训练 (Knowledge-aware Training)</strong></p>
<p>KTO
训练是指将知识图谱等外部知识融入到模型训练中，以增强模型的知识理解和推理能力。KTO
训练可以帮助模型更好地理解文本中的实体、关系和概念，从而提高模型的性能。</p>
<p><strong>7. ORPO 训练 (Off-Policy Reward Policy
Optimization)</strong></p>
<p>ORPO
是一种离线强化学习算法，它利用预先收集的数据来训练策略，而不需要与环境进行实时交互。ORPO
算法可以有效地利用历史数据，并能够在离线环境中进行策略优化</p>
<h4 id="llm微调方法集成">2.2 LLM微调方法集成</h4>
<p>LLAMA-FACTORY
支持六种先进<strong>微调方案</strong>和六种<strong>模型训练加速</strong>方案</p>
<figure>
<img src="/images/llamafactory/image-20240814115905187.png" srcset="/img/loading.gif" lazyload
alt="image-20240814115905187" />
<figcaption aria-hidden="true">image-20240814115905187</figcaption>
</figure>
<p><strong>1.冻结调整方法</strong> (Frozen Fine-tuning, Houlsby,
2019)**</p>
<ul>
<li><strong>核心思想:</strong>
只微调模型的一小部分参数，通常是解码器最后几层，而其余参数保持冻结。</li>
<li><strong>优点:</strong> 简单易实现，计算成本和存储成本低。</li>
<li><strong>缺点:</strong>
限制了模型的表达能力，可能导致性能不如全参数微调。</li>
</ul>
<p><strong>2. 梯度低秩投影 (Gradient Low-Rank Projection, GaLoRA, Zhao,
2024)</strong></p>
<ul>
<li><strong>核心思想:</strong>
将梯度投影到低维空间，从而降低梯度更新的维度，节省内存。</li>
<li><strong>优点:</strong> 允许全参数学习，同时降低内存占用。</li>
<li><strong>缺点:</strong> 相比LoRA等方法，实现较为复杂。</li>
</ul>
<p><strong>3. 低秩自适应 (Low-Rank Adaptation, LoRA, Hu,
2022)</strong></p>
<ul>
<li><strong>核心思想:</strong>
冻结所有预训练权重，并在特定层引入一对可训练的低秩矩阵。这些矩阵捕获微调过程中的关键更新信息。</li>
<li><strong>优点:</strong>
在保持性能的同时，显著降低内存占用和计算成本。</li>
<li><strong>缺点:</strong>
对于某些任务，可能需要仔细调整低秩矩阵的秩。</li>
</ul>
<p><strong>4. 量化低秩自适应 (Quantized Low-Rank Adaptation, QLoRA,
Dettmers, 2023)</strong></p>
<ul>
<li><strong>核心思想:</strong> 将 LoRA
与量化技术结合，进一步压缩模型大小，降低内存需求。</li>
<li><strong>优点:</strong> 在 LoRA
的基础上进一步降低内存占用，使得在更小的设备上进行微调成为可能。</li>
<li><strong>缺点:</strong> 量化可能会导致一定的性能损失。</li>
</ul>
<p><strong>5. 权重分解低秩自适应 (Weight Decomposed Low-Rank Adaptation,
DoRA, Liu et al., 2024)</strong></p>
<ul>
<li><strong>核心思想:</strong> 将预训练权重分解为绝对值和方向分量，只将
LoRA 应用于方向分量。</li>
<li><strong>优点:</strong> 相比
LoRA，可以更有效地捕捉权重更新的方向，提升微调效果。</li>
<li><strong>缺点:</strong> 实现比 LoRA 稍复杂。</li>
</ul>
<p><strong>6. LoRA+ (Hayou et al., 2024)</strong></p>
<ul>
<li><strong>核心思想:</strong> 针对 LoRA
的一些不足进行改进，例如对不同层使用不同的低秩矩阵秩。</li>
<li><strong>优点:</strong> 在 LoRA
的基础上进一步提升性能，并提供更灵活的配置选项。</li>
<li><strong>缺点:</strong> 相对 LoRA 更复杂，需要更多的调参经验。</li>
</ul>
<h4 id="高效训练技术"><strong>2.3 高效训练技术</strong></h4>
<p>在 LLAMA-FACTORY
中，集成了一系列高效的计算技术。常用的技术包括混合精度训练（Micikevicius，2018）和激活检查点（Chen，2016）。从检查注意层的输入输出
(IO) 开销中汲取见解，flash attention（Dao，2022
年）引入一种硬件友好的方法来增强注意计算。S^2
attention（Chen，2024b）解决了在块稀疏注意中扩展上下文的挑战，从而减少了微调长上下文
LLM 中的内存使用量。</p>
<p><strong>混合精度训练 (Mixed Precision Training) (Micikevicius,
2018)</strong></p>
<ul>
<li><strong>核心思想：</strong> 在训练过程中混合使用 FP32 (单精度浮点数)
和 FP16 (半精度浮点数)。</li>
<li><strong>优势：</strong>
<ul>
<li><strong>加速训练：</strong> FP16 计算速度比 FP32
快，减少训练时间。</li>
<li><strong>降低内存占用：</strong> FP16
占用内存更少，允许训练更大模型或使用更大批次。</li>
</ul></li>
<li><strong>关键技术：</strong> 损失缩放 (loss scaling) 防止梯度下溢
(underflow)。</li>
<li><strong>应用：</strong> 广泛应用于各种深度学习模型训练，尤其在 GPU
上训练大型 NLP 模型时效果显著。</li>
</ul>
<p><strong>2. 激活检查点 (Activation Checkpointing) (Chen,
2016)</strong></p>
<ul>
<li><strong>核心思想：</strong>
只保存部分激活值，并在反向传播时重新计算未保存的激活值。</li>
<li><strong>优势：</strong>
<ul>
<li><strong>大幅降低内存占用：</strong>
避免存储所有中间激活值，特别有利于训练深度网络。</li>
</ul></li>
<li><strong>劣势：</strong>
<ul>
<li><strong>增加计算开销：</strong>
需要重新计算部分激活值，延长训练时间。</li>
</ul></li>
<li><strong>应用：</strong>
适用于内存受限的情况下训练大型模型，例如训练长序列的 NLP 模型。</li>
</ul>
<p><strong>3. Flash Attention 2</strong></p>
<ul>
<li><strong>核心思想：</strong>
对注意力机制的计算进行优化，使其更加硬件友好，特别针对 GPU。</li>
<li><strong>优势：</strong>
<ul>
<li><strong>加速训练和推理：</strong>
通过优化内存访问模式和减少冗余计算，提高计算效率。</li>
<li><strong>降低内存占用：</strong>
更高效地利用内存，允许处理更长的序列。</li>
</ul></li>
<li><strong>应用：</strong> 广泛应用于 Transformer
模型，显著提升其性能，特别是在长序列任务上。</li>
</ul>
<p><strong>4. S^2 Attention (Chen, 2024b)</strong></p>
<ul>
<li><strong>核心思想：</strong>
一种针对块稀疏注意力的方法，旨在解决扩展上下文长度时内存使用过大的问题。</li>
<li><strong>优势：</strong>
<ul>
<li><strong>降低内存占用：</strong> 允许在微调长上下文 LLM
时使用更长的序列，而不会导致内存溢出。</li>
</ul></li>
<li><strong>应用：</strong> 主要用于微调大型语言模型
(LLM)，使其能够处理更长的上下文信息，例如长文档或对话历史。</li>
</ul>
<p><strong>5. Unsloth</strong></p>
<ul>
<li><strong>核心思想：</strong> 一种用于优化 Transformer
模型训练的库，主要针对长序列任务。</li>
<li><strong>优势：</strong>
<ul>
<li><strong>加速训练：</strong> 通过一系列优化技术，例如 Flash Attention
和 S^2 Attention，提高训练速度。</li>
<li><strong>降低内存占用：</strong>
允许训练更长序列的模型，或使用更大的批次。</li>
</ul></li>
<li><strong>应用：</strong> 主要用于训练长序列 Transformer
模型，例如用于代码生成或长文本摘要的模型。</li>
</ul>
<h3 id="三.-项目框架">三. 项目框架</h3>
<p><strong>LLAMA-FACTORY 有效地将这些技术组合成一个凝聚的结构，大大提高
LLM 微调的效率</strong>。这使得内存占用从混合精度训练期间的18
字节/参数（Micikevicius，2018）或 Bfloat16 训练期间的 8 字节/参数（Le
Scao，2022）减少到 0.6 字节/参数。</p>
<p><strong>在LLAMA-FACTORY 中， 模型加载器准备了各种用于微调的架构，支持
100 多个
LLM。</strong>数据工作器通过精心设计的流水线处理来自不同任务的数据，<strong>支持
50
多个数据集</strong>。训练器统一了有效的微调方法，使这些模型适应不同的任务和数据集，提供四种训练方法。LLAMA-BOARD
为上述模块提供了友好的可视化界面，使用户能够以无代码的方式配置和启动单个
LLM 微调过程，并实时监控训练状态。如图说明 LLAMA-FACTORY
的整体架构。</p>
<figure>
<img src="/images/llamafactory/image-20240814122034895.png" srcset="/img/loading.gif" lazyload
alt="image-20240814122034895" />
<figcaption aria-hidden="true">image-20240814122034895</figcaption>
</figure>
<h4 id="模型加载器有四个组件模型初始化模型补丁模型量化和适配器连接">3.1
<strong>模型加载器</strong>有四个组件：<strong>模型初始化、模型补丁、模型量化和适配器连接</strong>。</h4>
<p><strong>模型初始化</strong>。用 HF Transformers 的 AutoModel
API（Wolf，2020）来加载模型和初始化参数。为了使框架与不同的模型架构兼容，建立了一个模型注册表来存储每个层的类型，从而更直接地促进高效微调技术的使用。如果token化器的词汇表大小超出了嵌入层的容量，会调整层的大小并使用噪声均值初始化来初始化新参数。为了确定
RoPE
的缩放因子（Chen，2023），将其取做最大输入序列长度与模型的上下文长度之比。</p>
<p><strong>模型补丁</strong>。为了启用 flash attention 和 S^2
attention，用 monkey patch 来代替模型的前向计算。不过，由于自 HF
Transformers 4.34.0 以来就支持 flash attention，用 API 来启用 flash
attention。为了防止动态模块过度分区，在 DeepSpeed ZeRO-3 (Rasley et al.,
2020) 优化时将混合专家 (MoE) 块设置为叶（Leaf）模块。</p>
<p><strong>模型量化</strong>。可以通过 bits-and-bytes 库 (Dettmers,
2021) 用 LLM.int8 (Dettmers et al., 2022a) 将模型动态量化为 8 位或 4
位。对于 4 位量化，用双量化和 4 位普通浮点作为 QLoRA (Dettmers et al.,
2023)。还支持训练后量化 (PTQ) 方法量化的模型微调，包括 GPTQ
(Frantar，2023)、AWQ (Lin，2023) 和 AQLM
(Egiazarian，2024)。请注意，无法直接微调量化权重；因此，量化模型仅与基于适配器的方法兼容。</p>
<p><strong>适配器附加</strong>。用模型注册表自动识别适当的层来附加适配器。适配器默认附加到一个层的子集以节省内存，但将它们附加到所有线性层可能会产生更好的性能
(Dettmers，2023)。 <strong>PEFT</strong> (Mangrulkar et al., 2022)
库提供了一种非常方便的方式来连接适配器，例如 LoRA (Hu et al.,
2022)、rsLoRA (Kalajdzievski, 2023) 和 DoRA (Liu et al.,
2024)。替换后向计算为Unsloth (Han &amp; Han, 2023) 的版本加速
LoRA。为了执行人类反馈中强化学习 (RLHF)
，在模型上添加了一个V头，这是一个将每个 token
的表示映射到标量的线性层。</p>
<p>精度适应。根据设备的功能处理预训练模型的浮点精度。对于 NVIDIA
GPU，如果计算能力为 8.0 或更高，使用 bfloat16 精度。否则，采用
float16。对 Ascend NPU 和 AMD GPU 使用 float16，对非 CUDA 设备使用
float32。请注意，用 float16 精度加载 bfloat16
模型可能会导致溢出问题。在混合精度训练中，将所有可训练参数设置为
float32。尽管如此，在 bfloat16 训练中将可训练参数保留为 bfloat16。</p>
<blockquote>
<p><strong><code>fp16</code></strong>：</p>
<ul>
<li>常用于 GPU（例如 NVIDIA 的 Tensor
Cores）中，以加速深度学习模型的训练。</li>
<li>在部分硬件中，<code>fp16</code>运算速度比 <code>fp32</code>
更快，且内存消耗较少。</li>
<li>由于尾数部分较长，<code>fp16</code>
能在一些需要较高精度的运算中发挥优势。</li>
</ul>
<p><strong><code>bf16</code></strong>：</p>
<ul>
<li><code>bf16</code> 主要由 Google 推广，特别适用于 TPU（Tensor
Processing Unit）。</li>
<li>因为指数位数较多，<code>bf16</code>
在处理较大数值范围时表现优异，同时避免了过度的数值溢出。</li>
<li>尽管 <code>bf16</code> 的精度比 <code>fp16</code>
低，但在深度学习训练中，<code>bf16</code>
被认为在保持足够数值范围的同时具有足够的精度，因此非常适合神经网络的训练。</li>
</ul>
</blockquote>
<h4 id="一个数据处理流水线"><strong>3.2</strong>
<strong>一个数据处理流水线</strong></h4>
<p><strong>包括数据集加载、数据集对齐、数据集合并和数据集预处理</strong>。它将不同任务的数据集标准化为统一的格式，能够在各种格式的数据集上微调模型。</p>
<p><strong>数据集加载</strong>。用数据集（Lhoest，2021）库来加载数据，这使用户可以从
Hugging Face Hub
加载远程数据集或通过脚本或文件读取本地数据集。数据集库显着减少数据处理过程中的内存开销，并加速了用
<strong>Arrow</strong>（Apache，2016）的样本查询。默认情况下，整个数据集会下载到本地磁盘。但是，如果数据集太大而无法存储，框架提供<strong>数据集流动对其进行迭代，而无需下载。</strong></p>
<p><strong>数据集对齐</strong>。为了统一数据集格式，设计了一个数据描述规范来表征数据集的结构。例如，羊驼数据集有三列：指令、输入和输出（Taori，2023）。根据数据描述规范将数据集转换为与各种任务兼容的标准结构。下表显示了一些数据集结构示例。</p>
<figure>
<img
src="/images/llamafactory/v2-2d56e468d3c0215c3a3876e994b79b4f_720w.webp" srcset="/img/loading.gif" lazyload
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>数据集合并</strong>。统一的数据集结构为合并多个数据集提供了一种有效的方法。对于非流动模式下的数据集，只需在训练期间数据集混洗之前先连接起来。然而，在流动模式下，简单地连接数据集会阻碍数据混洗。因此，提供交替读取不同数据集数据的方法。</p>
<p><strong>数据集预处理</strong>。LLAMA-FACTORY
专为微调文本生成模型而设计，主要用于聊天完成。聊天模板是这些模型中的关键组成部分，因为它与这些模型的指令遵循能力高度相关。因此，提供数十种聊天模板，可以根据模型类型自动选择。用token化器在应用聊天模板后对句子进行编码。默认情况下，仅计算完成的损失，而忽略提示（Taori，2023）。或者，利用序列打包（Krell，2021）来减少训练时间，这在执行生成式预训练时会自动启用。</p>
<p>设计了一个 <strong>Formatter
类</strong>，以便将文本输入稳健地转换为其嵌入 ID。具体来说，提供
EmptyFormatter、StringFormatter、FunctionFormatter 和
ToolFormatter。此外，LLAMA-FACTORY 支持微调模型以获得函数调用能力。虽然
ReAct
提示（Yao，2023）是工具使用的流行选择，但它对于嵌套工具参数来说是不够的。优化的工具调用提示如表所示。</p>
<figure>
<img
src="/images/llamafactory/v2-fab6b975715fa57b67090650f542a809_720w.webp" srcset="/img/loading.gif" lazyload
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>高效训练微调</strong>。将最先进的高效微调方法，包括
LoRA+（Hayou，2024）和
GaLore（Zhao，2024），集成到训练器中，替换默认组件。这些训练方法独立于训练器，因此可以轻松应用于各种任务。利用
Transformers（Wolf，2020）的训练器进行预训练和 SFT，同时采用 TRL（von
Werra，2020）的训练器进行 RLHF 和
DPO。利用定制的数据整理器来区分各种训练方法的训练器。为了匹配训练器偏好数据的输入格式，在一个批次中构建
2n 个样本，其中前 n 个样本是选定的示例，后 n 个样本是拒绝的示例。</p>
<p><strong>模型共享RLHF</strong>。允许在消费设备上进行 RLHF 训练是 LLM
微调的一个有用属性。然而，这个实现起来很困难，因为 RLHF
训练需要四种不同的模型。为了解决这个问题，提出了模型共享 RLHF，使整个
RLHF
训练只用一个预训练模型即可完成。具体来说，首先用奖励建模的目标函数训练一个适配器和一个V头，让模型计算奖励分数。然后初始化另一个适配器和V头，并用
PPO 算法 (Ouyang et al., 2022) 训练它们。在训练过程中，适配器和V头通过
PEFT (Mangrulkar et al., 2022) 的 set_adapter 和 disable_adapter API
动态切换，允许预训练模型同时用作策略、价值、参考和奖励模型。</p>
<p><strong>分布式训练</strong>。将上述训练器与
<strong>DeepSpeed</strong> (Rasley et al., 2020)
结合进行分布式训练。利用 DeepSpeed ZeRO
优化器，可以通过分区或卸载进一步减少内存消耗。</p>
<p><strong>加速推理</strong>。在推理期间，重用数据工作器的聊天模板来构建模型输入。支持使用
HF Transformers（Wolf，2020 年）和
vLLM（Kwon，2023）对模型输出进行采样，这两者都支持流解码。此外，还实现一个
OpenAI 风格的 API，该 API 利用异步 LLM 引擎和 vLLM 的paged
attention来提供高吞吐量的并发推理服务，从而促进微调的 LLM
部署到各种应用程序中。</p>
<p><strong>综合评估</strong>。纳入多项 LLM
评估指标，包括多项选择任务，如 <strong>MMLU</strong>（Hendrycks et al.,
2021）、CMMLU（Li et al., 2023a）和 C-Eval（Huang et al.,
2023），以及计算文本相似度分数，如 <strong>BLEU-4</strong>（Papineni et
al., 2002）和 <strong>ROUGE</strong>（Lin, 2004）。</p>
<h4 id="可视化界面">3.3 可视化界面</h4>
<p>LLAMA-BOARD 是一个基于
Gradio（Abid，2019）的统一用户界面，允许用户自定义 LLM
的微调，而无需编写任何代码。它提供了简化的模型微调和推理服务，使用户能够在实践中轻松利用
100 多个 LLM 和 50 多个数据集。LLAMA-BOARD 具有以下显着特点：</p>
<p><strong>易于配置</strong>。LLAMA-BOARD 允许用户通过Web
界面交互自定义微调参数。其为许多参数提供了推荐给大多数用户的默认值，从而简化了配置过程。此外，用户可以在
Web UI 上预览数据集以检查其自定义格式。</p>
<p><strong>可监控的训练</strong>。在训练过程中，训练日志和损失曲线可视化和实时更新，使用户可以监控训练进度。此功能为分析微调过程提供了有价值的见解。</p>
<p><strong>灵活的评估</strong>。LLAMA-BOARD
支持计算数据集上的文本相似度得分，自动评估模型或通过与模型聊天进行人工评估。</p>
<p><strong>多语言支持</strong>。LLAMA-BOARD
提供本地化文件，方便集成新语言呈现界面。目前支持三种语言：英语、俄语和中文，这使得更广泛的用户能够使用
LLAMA-BOARD 来微调 LLM。</p>
<p><strong>训练细节</strong>。采用 10−5 的学习率和 512
的tokens批处理大小，使用 8 位 AdamW 优化器（Dettmers，2022b）以 bfloat16
精度对这些模型进行微调，并使用激活检查点来减少内存占用。在冻结调整中，仅微调模型的最后
3 个解码器层。对于 GaLore，分别将秩和尺度设置为 128 和 2.0。对于 LoRA 和
QLoRA，将适配器连接到所有线性层，并将秩和 alpha 分别设置为 128 和
256。所有实验均在单个 NVIDIA A100 40GB GPU 上进行。在所有实验中启用flash
attention，并在 LoRA 和 QLoRA 实验中启用 Unsloth。</p>
<h3 id="四.总结">四.总结</h3>
<p>LLAMA-FACTORY
无疑可以规范工作流程,大大增加NLPer的工作效率,下表是其不同微调方法的训练效率对比参考：</p>
<figure>
<img src="/images/llamafactory/image-20240814124516886.png" srcset="/img/loading.gif" lazyload
alt="image-20240814124516886" />
<figcaption aria-hidden="true">image-20240814124516886</figcaption>
</figure>
<figure>
<img src="/images/llamafactory/image-20240814124604969.png" srcset="/img/loading.gif" lazyload
alt="image-20240814124604969" />
<figcaption aria-hidden="true">image-20240814124604969</figcaption>
</figure>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/NLP/" class="category-chain-item">NLP</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%B7%A5%E5%85%B7%E6%A1%86%E6%9E%B6/" class="print-no-link">#工具框架</a>
      
        <a href="/tags/NLP/" class="print-no-link">#NLP</a>
      
        <a href="/tags/%E6%95%88%E7%8E%87%E5%B7%A5%E5%85%B7/" class="print-no-link">#效率工具</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>高效微调统一框架——LLAMA-FACTORY技术点详解</div>
      <div>https://linxkon.github.io/LLAMA-FACTORY技术点梳理.html</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>linxkon</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年5月21日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%E6%A6%82%E5%BF%B5%E4%B8%AD%E8%8B%B1%E6%96%87%E6%B1%87%E7%BC%96.html" title="人工智能领域常见概念中英文汇编">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">人工智能领域常见概念中英文汇编</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama3.html" title="从头开始实现llama3">
                        <span class="hidden-mobile">从头开始实现llama3</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://lib.baomitu.com/gitalk/1.8.0/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"Ov23licg1p15oAGiQtDC","clientSecret":"d6ca3873752e3a6eb2d21a98b92a3021fd462cbf","repo":"Waline","owner":"linxkon","admin":["linxkon"],"language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: 'c43a0869fd350e082b138552c2842295'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量
        <span id="leancloud-site-pv"></span>
        次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        访客量
        <span id="leancloud-site-uv"></span>
        次
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
